//===- VectorAttributes.td - Vector Dialect ----------------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This file declares the attributes used in the Vector dialect.
//
//===----------------------------------------------------------------------===//

#ifndef MLIR_DIALECT_VECTOR_IR_VECTOR_ATTRIBUTES
#define MLIR_DIALECT_VECTOR_IR_VECTOR_ATTRIBUTES

include "mlir/Dialect/Vector/IR/Vector.td"
include "mlir/IR/EnumAttr.td"

// The "kind" of combining function for contractions and reductions.
def COMBINING_KIND_ADD : I32EnumAttrCase<"ADD", 0, "add">;
def COMBINING_KIND_MUL : I32EnumAttrCase<"MUL", 1, "mul">;
def COMBINING_KIND_MINUI : I32EnumAttrCase<"MINUI", 2, "minui">;
def COMBINING_KIND_MINSI : I32EnumAttrCase<"MINSI", 3, "minsi">;
def COMBINING_KIND_MINNUMF : I32EnumAttrCase<"MINNUMF", 4, "minnumf">;
def COMBINING_KIND_MAXUI : I32EnumAttrCase<"MAXUI", 5, "maxui">;
def COMBINING_KIND_MAXSI : I32EnumAttrCase<"MAXSI", 6, "maxsi">;
def COMBINING_KIND_MAXNUMF : I32EnumAttrCase<"MAXNUMF", 7, "maxnumf">;
def COMBINING_KIND_AND : I32EnumAttrCase<"AND", 8, "and">;
def COMBINING_KIND_OR  : I32EnumAttrCase<"OR", 9, "or">;
def COMBINING_KIND_XOR : I32EnumAttrCase<"XOR", 10, "xor">;
def COMBINING_KIND_MINIMUMF : I32EnumAttrCase<"MINIMUMF", 11, "minimumf">;
def COMBINING_KIND_MAXIMUMF : I32EnumAttrCase<"MAXIMUMF", 12, "maximumf">;

def CombiningKind : I32EnumAttr<
    "CombiningKind",
    "Kind of combining function for contractions and reductions",
    [COMBINING_KIND_ADD, COMBINING_KIND_MUL, COMBINING_KIND_MINUI,
     COMBINING_KIND_MINSI, COMBINING_KIND_MINNUMF, COMBINING_KIND_MAXUI,
     COMBINING_KIND_MAXSI, COMBINING_KIND_MAXNUMF, COMBINING_KIND_AND,
     COMBINING_KIND_OR, COMBINING_KIND_XOR,
     COMBINING_KIND_MAXIMUMF, COMBINING_KIND_MINIMUMF]> {
  let cppNamespace = "::mlir::vector";
  let genSpecializedAttr = 0;
}

/// An attribute that specifies the combining function for `vector.contract`,
/// and `vector.reduction`.
def Vector_CombiningKindAttr : EnumAttr<Vector_Dialect, CombiningKind, "kind"> {
  let assemblyFormat = "`<` $value `>`";
}

def Vector_IteratorType : I32EnumAttr<"IteratorType", "Iterator type", [
  I32EnumAttrCase<"parallel", 0>,
  I32EnumAttrCase<"reduction", 1>
]> {
    let genSpecializedAttr = 0;
    let cppNamespace = "::mlir::vector";
}

def Vector_IteratorTypeEnum
    : EnumAttr<Vector_Dialect, Vector_IteratorType, "iterator_type"> {
    let assemblyFormat = "`<` $value `>`";
}

def Vector_IteratorTypeArrayAttr
    : TypedArrayAttrBase<Vector_IteratorTypeEnum,
                         "Iterator type should be an enum.">;

def PrintPunctuation : I32EnumAttr<"PrintPunctuation",
                                  "Punctuation for separating vectors or vector elements", [
  I32EnumAttrCase<"NoPunctuation", 0, "no_punctuation">,
  I32EnumAttrCase<"NewLine", 1, "newline">,
  I32EnumAttrCase<"Comma", 2, "comma">,
  I32EnumAttrCase<"Open", 3, "open">,
  I32EnumAttrCase<"Close", 4, "close">
]> {
  let cppNamespace = "::mlir::vector";
  let genSpecializedAttr = 0;
}

def Vector_PrintPunctuation : EnumAttr<Vector_Dialect, PrintPunctuation, "punctuation"> {
  let assemblyFormat = "`<` $value `>`";
}

#endif // MLIR_DIALECT_VECTOR_IR_VECTOR_ATTRIBUTES


//===- LinalgTransformOps.td - Linalg transform ops --------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef LINALG_TRANSFORM_OPS
#define LINALG_TRANSFORM_OPS

include "mlir/Dialect/Linalg/TransformOps/LinalgTransformEnums.td"
include "mlir/Dialect/Transform/IR/TransformAttrs.td"
include "mlir/Dialect/Transform/IR/TransformDialect.td"
include "mlir/Dialect/Transform/Interfaces/TransformInterfaces.td"
include "mlir/Dialect/Transform/IR/TransformTypes.td"
include "mlir/Dialect/SCF/IR/DeviceMappingInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/IR/OpBase.td"
include "mlir/IR/RegionKindInterface.td"

// This is roughly similar to OpFoldResult assuming the handle produces a single
// value in the payload IR.
def TransformAnyParamTypeOrAnyHandle : Type<
    Or<[TransformHandleTypeInterface.predicate,
        TransformParamTypeInterface.predicate]>,
    "transform any param type or any handle type">;

//===----------------------------------------------------------------------===//
// Apply...PatternsOp
//===----------------------------------------------------------------------===//

def ApplyEraseUnnecessaryInputsPatternsOp : Op<Transform_Dialect,
    "apply_patterns.linalg.erase_unnecessary_inputs",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Collects patterns that promote inputs to outputs and remove unused inputs of
    `linalg.generic` ops.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyDecomposeTensorPackUnpackPatternsOp
    : Op<Transform_Dialect, "apply_patterns.linalg.decompose_pack_unpack",
         [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Collect patterns to decompose tensor.pack and tensor.unpack into e.g.
    tensor::PadOp, linalg::transposeOp Ops. Requires all outer dims to be unit.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyDecomposeTensorPadPatternsOp
    : Op<Transform_Dialect, "apply_patterns.linalg.decompose_pad",
         [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Collect patterns to decompose tensor.pad into e.g. tensor::EmptyOp,
    linalg::FillOp and tensor::InsertSliceOp.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyFoldUnitExtentDimsViaReshapesPatternsOp : Op<Transform_Dialect,
    "apply_patterns.linalg.fold_unit_extent_dims_via_reshapes",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Collects patterns to fold unit-extent dimensions in operands/results of
    linalg ops on tensors via reassociative reshape ops.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyFoldUnitExtentDimsViaSlicesPatternsOp : Op<Transform_Dialect,
    "apply_patterns.linalg.fold_unit_extent_dims_via_slices",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Collects patterns to fold unit-extent dimensions in operands/results of
    linalg ops on tensors via rank-reducing slices.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyTilingCanonicalizationPatternsOp : Op<Transform_Dialect,
    "apply_patterns.linalg.tiling_canonicalization",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Collects canonicalization patterns relevant to apply after tiling patterns.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyFoldAddIntoDestPatternsOp : Op<Transform_Dialect,
    "apply_patterns.linalg.fold_add_into_dest",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Collects patterns to replace linalg.add when destination passing suffices
    for achieving the sum.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyPadVectorizationPatternsOp : Op<Transform_Dialect,
    "apply_patterns.linalg.pad_vectorization",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Apply patterns that vectorize tensor.pad.

    These patterns rewrite tensor.pad Ops using vector.transfer_read and
    vector.transfer_write operations. This is done either by:
      1. Folding tensor.pad with an existing vector.transfer_read /
      vector.transfer_write Op (generated prior to running these patterns). 
      2. Rewriting it (when matched together with q tensor.insert_slice
      consumer Op) as a vector.transfer_read + vector.transfer_write pair.

    In both cases, these patterns look at producers and consumers for the
    matched tensor.pad Op to find opportunities for vectorization.
  }];

  let assemblyFormat = "attr-dict";
}

//===----------------------------------------------------------------------===//
// BufferizeToAllocationOp
//===----------------------------------------------------------------------===//

def BufferizeToAllocationOp : Op<Transform_Dialect,
    "structured.bufferize_to_allocation",
    [DeclareOpInterfaceMethods<TransformOpInterface>,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    This transform bufferizes the targeted operation and materializes the
    result in a new allocation. It replaces all original uses of the target
    result with the newly allocated buffer, wrapped in a
    `bufferization.to_tensor` op. It returns a handle to the newly allocated
    buffer. Furthermore, it returns a handle that is mapped to all newly created
    ops.

    Only bufferizable ops are that bufferize to a memory write or have an
    aliasing OpOperand (and do not themselves bufferize to an allocation) are
    supported. They are bufferized using their BufferizableOpInterface
    implementation. E.g.:

    ```
    %0 = tensor.insert %f into %dest[%pos] : tensor<10xf32>
    ```

    Is bufferized to:

    ```
    %alloc = memref.alloc() : memref<10xf32>
    bufferization.materialize_in_destination %dest in %alloc
    memref.store %f, %alloc[%pos] : memref<10xf32>
    %0 = bufferization.to_tensor %alloc restrict writable : memref<10xf32>
    ```

    Selected ops that bufferize to an allocation (or need special handling) are
    also supported:
    - `tensor.pad` is lowered to an allocation, followed by a `linalg.fill` and
      and a buffer copy (all on memrefs).
    - `vector.mask` is bufferized together with its region. The allocation is
      placed in front of the `vector.mask` op.

    An optional memory space attribute can be specified for the materialized
    buffer allocation.

    If a memory copy is needed, a "bufferization.materialize_in_destination" is
    used when possible. This is an op with tensor semantics that will bufferize
    to a memory copy later. Which concrete op will be used for the memory copy
    is up to the bufferization framework. Alternatively, a custom memcpy op can
    be specified via `memcpy_op`. Currently supported are "memref.copy" and
    "linalg.copy". In that case, the source of each memcpy must not have a
    custom memory space. Furthermore, because the future buffer layout unknown
    for a given tensor, a fully dynamic layout is assumed for best
    compatibility. Users should use "bufferization.materialize_in_destination"
    when possible.

    "memref.alloc" is used for new buffer allocations. The buffer is deallocated
    at the end of the block if the "emit_dealloc" attribute is present. If this
    attribute is not present, the allocated memory will be leaked. However,
    running the `-buffer-deallocation-pipeline` after all bufferization is done
    will properly insert the corresponding deallocation(s). Custom allocation
    ops can be specified via `alloc_op`. Currently supported are "memref.alloc"
    and "memref.alloca". In case of a "memref.alloca", the buffer is not
    deallocated.

    If `bufferize_destination_only` is set, only the destination operands of the
    op are bufferized to a new memory allocation, but not the op itself.

    #### Return modes

    This operation consumes the `target` handle and produces the
    `allocated_buffer` and `new_ops` handles. It always succeeds.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       OptionalAttr<AnyAttr>:$memory_space,
                       DefaultValuedAttr<StrAttr,
                              "\"bufferization.materialize_in_destination\"">:
                           $memcpy_op,
                       DefaultValuedAttr<StrAttr, "\"memref.alloc\"">:
                           $alloc_op,
                       UnitAttr:$bufferize_destination_only,
                       UnitAttr:$emit_dealloc);
  let results = (outs Transform_AnyValue:$allocated_buffer,
                      Transform_AnyOpType:$new_ops);
  let assemblyFormat = "$target attr-dict `:` type($target)";
  let hasVerifier = 1;

  let builders = [
    OpBuilder<(ins "Value":$target, "Attribute":$memorySpace)>,
    OpBuilder<(ins "Value":$target, "int64_t":$memorySpace)>
  ];
}

//===----------------------------------------------------------------------===//
// DecomposeOp
//===----------------------------------------------------------------------===//

def DecomposeOp : Op<Transform_Dialect, "structured.decompose",
    [FunctionalStyleTransformOpTrait,
     MemoryEffectsOpInterface,
     TransformOpInterface,
     TransformEachOpTrait,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Decomposes named complex operations, such as higher-dimensional
    (depthwise) convolutions, into combinations of lower-dimensional equivalents
    when possible.

    #### Return modes

    This operation ignores non-Linalg ops and drops them in the return.
    If all the operations referred to by the `target` handle decompose
    properly, the transform succeeds. Otherwise the transform produces a
    silenceable failure. The return handle points to only the subset of
    successfully produced computational operations, which can be empty.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs TransformHandleTypeInterface:$transformed);
  let assemblyFormat =
      "$target attr-dict `:` functional-type(operands, results)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::linalg::LinalgOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// EliminateLinalgOpAnchoredEmptyTensorsOp
//===----------------------------------------------------------------------===//

def EliminateLinalgOpAnchoredEmptyTensorsOp
    : Op<Transform_Dialect, "structured.eliminate_empty_tensors",
        [DeclareOpInterfaceMethods<TransformOpInterface>,
         DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
  let description = [{
    Try to eliminate all `tensor.empty` op uses that are anchored on a LinalgOp
    within the targeted op.

    This op is similar to `bufferization.eliminate_empty_tensors`, but specific
    to LinalgOps.

    `tensor.empty` ops cannot be bufferized. They can either be converted to
    `bufferization.alloc_tensor` or replaced with another tensor (via this
    transform). `tensor.empty` does not specify the contents of the returned
    tensor so their results can be replaced with arbitrary tensor values as long
    as the dimensions match.

    This transform looks for `tensor.empty` ops where the SSA use-def chain of
    the result ends in a supported LinalgOp (always following the aliasing
    OpOperand/OpResult chain). The following LinalgOps are supported:
    - Only parallel iterator types.
    - The use-def chain ends in an input operand of the LinalgOp.
    - The LinalgOp has an unused output operand with the same shape and
      indexing map.

    Example:

    ```
    %0 = tensor.empty()
    %1 = linalg.matmul ins(...) outs(%0)
    %2 = linalg.generic ins(%1) outs(%dest) {
      ^bb0(%in: f32, %out: f32):
      // out not used
    }
    ```

    Is rewritten with:
    ```
    %0 = tensor.empty()
    %1 = linalg.matmul ins(...) outs(%dest)
    %2 = linalg.generic ins(%0) outs(%1) {
      ^bb0(%in: f32, %out: f32):
      // Use %out instead of %in
    }
    ```

    After this transformation, the "ins" operand has no uses inside the body of
    the LinalgOp and can be folded away with existing cleanup patterns.
    Afterwards, the tensor::EmptyOp can also fold away, so that the example can
    bufferize without an allocation (in the absence of other conflicts).

    #### Return modes

    This transform reads the target handle and modifies the payload. It does
    not produce any handle.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);

  let results = (outs);

  let assemblyFormat = "$target attr-dict `:` type($target)";
}

//===----------------------------------------------------------------------===//
// FuseOp
//===----------------------------------------------------------------------===//

def FuseOp : Op<Transform_Dialect, "structured.fuse",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     DeclareOpInterfaceMethods<TransformOpInterface>,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Tiles the operations pointed to by the target handle and fuses their
    producers greedily using the options provided as attributes.

    If `apply_cleanup` is true then slice canonicalization is applied between
    fusion steps.
  }];

  let arguments =
    (ins TransformHandleTypeInterface:$target,
         DefaultValuedAttr<I64ArrayAttr, "{}">:$tile_sizes,
         DefaultValuedAttr<I64ArrayAttr, "{}">:$tile_interchange,
         DefaultValuedAttr<BoolAttr, "false">:$apply_cleanup);
  let results = (outs TransformHandleTypeInterface:$transformed,
                      Variadic<TransformHandleTypeInterface>:$loops);

  let assemblyFormat = [{
    $target ($tile_sizes^)? (`interchange` $tile_interchange^)?
    (`apply_cleanup` `=` $apply_cleanup^)? attr-dict
    `:` functional-type(operands, results)
  }];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// FuseIntoContainingOp
//===----------------------------------------------------------------------===//

def FuseIntoContainingOp :
    Op<Transform_Dialect, "structured.fuse_into_containing_op",
      [DeclareOpInterfaceMethods<TransformOpInterface,
          ["allowsRepeatedHandleOperands"]>,
       DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
       ReportTrackingListenerFailuresOpTrait]> {
  let summary = "Fuse a producer into a containing operation.";

  let description = [{
    Fuses the `producer_op` into the `containing_op`.
    Returns a handle to the fused ops and the `new_containing_op`.

    The producer is typically a slice of a tileable op (i.e., implements
    TilingInterface). In that case, this transform computes the accessed
    producer slice inside of the containing op ("tile and fuse") and if required,
    creates a new containing op with outputs from the fused producer. Otherwise,
    the entire producer is cloned inside the containing op ("clone and fuse").

    The containing op handle must be associated with exactly one payload op. The
    producer op handle may be associated with multiple payload ops. This
    transform fuses producers one-by-one, always picking an unspecified producer
    that has at least one use inside the containing op among the
    producers. A producer can be listed multiple times in the handle.

    Note: If a producer has multiple uses inside the containing op, it is
    currently tiled and/or cloned multiple times into the containing op.
    TODO: Reuse already fused OpResults instead of tiling/cloning a second time
    when possible. Fuse producers according to a topological sorting to achieve
    the largest amount of reuse.

    #### Return modes

    If at least one producer could not be fused, this operation produces a
    silenceable failure.  This is the case when tiling fails or when no
    producer op could be found among the remaining producers that has at least
    one use within the containing op. I.e., "producers" that are not consumed
    within the containing op are rejected by this operation.

    This operation consumes the producer handle.
    This operation only reads the containing op handle.
  }];

  let arguments = (ins TransformHandleTypeInterface:$producer_op,
                       TransformHandleTypeInterface:$containing_op);
  let results = (outs TransformHandleTypeInterface:$fused_op,
                      TransformHandleTypeInterface:$new_containing_op);
  let assemblyFormat = "$producer_op `into` $containing_op attr-dict "
                       " `:` functional-type(operands, results)";

  let builders = [
    OpBuilder<(ins "Value":$producerOp, "Value":$containingOp)>
  ];
}

//===----------------------------------------------------------------------===//
// GeneralizeOp
//===----------------------------------------------------------------------===//

def GeneralizeOp : Op<Transform_Dialect, "structured.generalize",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     TransformOpInterface, TransformEachOpTrait,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Transforms a named structured operation into the generic form with the
    explicit attached region.

    #### Return modes

    This operation ignores non-Linalg ops and drops them in the return.
    If all the operations referred to by the `target` handle generalize
    properly, the transform succeeds. Otherwise the transform produces a
    silenceable failure.  The return handle points to only the subset of
    successfully produced equivalent generic operations, which can be empty or
    contain the original ops if they were already in generic form.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs TransformHandleTypeInterface:$transformed);
  let assemblyFormat = [{
      $target attr-dict `:` 
      custom<SemiFunctionType>(type($target), type($transformed), "false")
  }];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::linalg::LinalgOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// SpecializeOp
//===----------------------------------------------------------------------===//

def SpecializeOp : Op<Transform_Dialect, "structured.specialize",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     TransformOpInterface, TransformEachOpTrait,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Transforms a generic operation into the equivalent named form.

    #### Return modes

    This operation ignores non-Linalg ops and drops them in the return. If all
    the operations referred to by the `target` handle specialize, the transform
    succeeds; otherwise, the operation produces a silenceable failure.  The return
    handle points to only the subset of successfully produced equivalent named
    operations, which can be empty or contain the original ops if they were already
    in named form. The supported specialization to named Linalg operations are:
    - linalg.copy of any rank.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs TransformHandleTypeInterface:$transformed);
  let assemblyFormat = [{
      $target attr-dict `:` 
      custom<SemiFunctionType>(type($target), type($transformed), "false")
  }];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::linalg::LinalgOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// InterchangeOp
//===----------------------------------------------------------------------===//

def InterchangeOp : Op<Transform_Dialect, "structured.interchange",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
    TransformOpInterface, TransformEachOpTrait,
    ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Interchanges the iterators of the operations pointed to by the target handle
    using the iterator interchange attribute.

    #### Return modes

    This operation ignores non-linalg::Generic ops and drops them in the return.
    This operation fails if the interchange attribute is invalid.
    If all the operations referred to by the `target` handle interchange
    properly, the transform succeeds.
    If any interchange fails, the transform produces a definite failure.
    The return handle points to only the subset of successfully produced
    interchanged operations, which can be empty.
  }];

  let arguments =
    (ins TransformHandleTypeInterface:$target,
         ConfinedAttr<DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">,
                      [DenseArrayNonNegative<DenseI64ArrayAttr>]>:$iterator_interchange);
  let results = (outs TransformHandleTypeInterface:$transformed);

  let assemblyFormat = [{
    $target
    (`iterator_interchange` `=` $iterator_interchange^)? attr-dict
    `:` custom<SemiFunctionType>(type($target), type($transformed), "false")
  }];
  let hasVerifier = 1;

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::linalg::GenericOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// LowerPackOp
//===----------------------------------------------------------------------===//
def LowerPackOp : Op<Transform_Dialect, "structured.lower_pack", [
                         FunctionalStyleTransformOpTrait,
                         MemoryEffectsOpInterface,
                         TransformEachOpTrait,
                         TransformOpInterface,
                         ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Rewrite a tensor.pack into tensor.pad + tensor.expand_shape + linalg.transpose.

    #### Return modes

    This operation ignores non-pack ops and drops them in the return.
    This operation produces a silenceable failure if the rewrite fails for any
    reason.
    If all the operations referred to by the `target` are rewritten, the
    transform succeeds.
    Return handles to the newly produced pad, expand_shape and transpose ops.
  }];

  let arguments = (ins Transform_ConcreteOpType<"tensor.pack">:$target,
                       DefaultValuedAttr<BoolAttr, "true">:$lowerPadLikeWithInsertSlice);
  let results = (outs Transform_ConcreteOpType<"tensor.pad">:$pad_op,
                      Transform_ConcreteOpType<"tensor.expand_shape">:$expand_shape_op,
                      Transform_ConcreteOpType<"linalg.transpose">:$transpose_op);
  let assemblyFormat = [{
    $target attr-dict `:` functional-type(operands, results)
  }];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::tensor::PackOp target,
        ::mlir::transform::ApplyToEachResultList &transformResults,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// LowerUnPackOp
//===----------------------------------------------------------------------===//
def LowerUnPackOp : Op<Transform_Dialect, "structured.lower_unpack", [
                         FunctionalStyleTransformOpTrait,
                         MemoryEffectsOpInterface,
                         TransformEachOpTrait,
                         TransformOpInterface,
                         ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Lower a tensor.unpack into empty + linalg.transpose + tensor.collapse_shape +
    tensor.extract_slice.

    #### Return modes

    This operation ignores non-unpack ops and drops them in the return.
    This operation produces a silenceable failure if the rewrite fails for any
    reason.
    If all the operations referred to by the `target` are rewritten, the
    transform succeeds.
    Return handles to the newly produced empty, transpose, collapse_shape and extract_slice ops.
  }];

  let arguments = (ins Transform_ConcreteOpType<"tensor.unpack">:$target,
                       DefaultValuedAttr<BoolAttr, "true">:$lowerUnpadLikeWithExtractSlice);
  let results = (outs Transform_ConcreteOpType<"tensor.empty">:$empty_op,
                      Transform_ConcreteOpType<"linalg.transpose">:$transpose_op,
                      Transform_ConcreteOpType<"tensor.collapse_shape">:$collapse_shape_op,
                      Transform_ConcreteOpType<"tensor.extract_slice">:$extract_slice_op);
  let assemblyFormat = [{
    $target attr-dict `:` functional-type(operands, results)
  }];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::tensor::UnPackOp target,
        ::mlir::transform::ApplyToEachResultList &transformResults,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// MatchOp
//===----------------------------------------------------------------------===//

def MatchOp : Op<Transform_Dialect, "structured.match",
    [MemoryEffectsOpInterface,
     NavigationTransformOpTrait,
     DeclareOpInterfaceMethods<TransformOpInterface>]> {
  let description = [{
    Match op with the specified constraints, within the target op.

    The following constraints are supported:
      - interface: an optional MatchInterfaceEnum specifying an enum
        representation for an interface to target.
      - ops: an optional StrArrayAttr specifying the concrete name of an op.
        Multiple names can be specified. Matched ops must have one of specified
        names.
      - attribute: the matched op must have all specified attributes (with their
        specified values).
      - filter_result_type: the matched op must return exactly this one type.
      - filter_operand_types: all the operands of the matched op must must be of
        this type. If more than a type is specified, then the length of the list
        must be equal to the number of operands in the matched op, and the match
        will succeed only if the operand types match all the types in the list
        in the order in which they are specified.

    Note: Only ops that satisfy all specified constraints are matched.

    TODO: Extend with regions to allow a limited form of constraints.

    #### Return modes

    This op traverses the ops nested under `target` and returns the handles to
    all the operations that match the requirements.

    This op fails if the target is not a handle to exactly one operation.
    Otherwise it succeeds.

    This operation does not consume the target handle and produces new handles:
    it is a navigation op.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       OptionalAttr<StrArrayAttr>:$ops,
                       OptionalAttr<MatchInterfaceEnum>:$interface,
                       OptionalAttr<DictionaryAttr>:$op_attrs,
                       OptionalAttr<TypeAttr>:$filter_result_type,
                       OptionalAttr<TypeArrayAttr>:$filter_operand_types);
  // TODO: variadic results when needed.
  let results = (outs TransformHandleTypeInterface:$results);

  let builders = [
    OpBuilder<(ins "Value":$target, "ArrayRef<StringRef>":$opNames)>,
    OpBuilder<(ins "TypeRange":$resultTypes, "Value":$target, "ArrayRef<StringRef>":$opNames)>
  ];

  let assemblyFormat = [{
    (`ops` `{` $ops^ `}`)?
    (`interface` `{` $interface^ `}`)?
    (`attributes` $op_attrs^)?
    (`filter_result_type` `=` $filter_result_type^)?
    (`filter_operand_types` `=` $filter_operand_types^)?
    `in` $target attr-dict
    `:` functional-type($target, results)
  }];
}

//===----------------------------------------------------------------------===//
// MultiTileSizesOp
//===----------------------------------------------------------------------===//

def MultiTileSizesOp : Op<Transform_Dialect, "structured.multitile_sizes",
    [DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     TransformOpInterface, TransformEachOpTrait,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Emits the IR computing the tile sizes `s1` and `s2` such that:

      - there exists a combination of `n` tiles of size `s1` and `m` tiles of
        size `s2` that covers the entirety of the iteration space `dimension` of
        the target structured op;
      - `s1`, `s2` is less than or equal to `target_size`;
      - `s1` and `s2` are divisible by `divisor.

    For example, for a dimension of size 54 with target size 12 and divisor 2,
    this can emit the IR computing the tile size 10, used for 3 tiles, and 12,
    used for 2 tiles, totally 10*3 + 12*2 = 54. Note that when the divisor does
    not divide the original dimension size, it is impossible to compute such
    tile sizes. An assertion is emitted to guard against this in the dynamic
    case.

    Expects the target size and the divisor to be strictly positive. Folds the
    IR as much as possible, normally obtaining constant sizes and numbers of
    tiles for a statically known dimension.

    This does *not* consume the target handle and produces three handles each
    pointing to single-result index-typed operations (which may be arithmetic
    constant operations) defining the two respective tile sizes and the product
    of the first tile size with the number of tiles of that size (useful for
    splitting the iteration space).

    This operation composes with the regular tiling when applied per-dimension:

    ```mlir
    %sz1, %sz2, %split = structured.multitile_sizes %target
                         { target_size = 10, dimension = 1 }
                       : !transform.any_op, !transform.param<i64>,
                         !transform.param<i64>, !transform.param<i64>
    %handles = structured.split %target after %split { dimension = 1 }
                : !transform.any_op, !transform.param<i64>
    %low, %high = transform.split_handle %handles : (!transform.any_op)
                      -> (!transform.any_op, !transform.any_op)
    %tiled_low, %loop1 = structured.tile_using_for %low [0, %sz1]
                       : (!transform.any_op, !transform.param<i64>)
                      -> (!transform.any_op, !transform.any_op)
    %tiled_high, %loop2 = structured.tile_using_for %high [0, %sz2]
                        : (!transform.any_op, !transform.param<i64>)
                       -> (!transform.any_op, !transform.any_op)
    %common = merge_handles %tiled_low, %tiled_high : !transform.any_op

    %sz3, %sz4, %split = structured.multitile_size %target
                         { target_size = 42, dimension = 0 }
                       : !transform.any_op, !transform.any_op,
                         !transform.any_op, !transform.any_op
    %sz3r, %sz4r, %splitr = replicate num(%common) %sz3, %sz4, %splitr
             : !transform.any_op, !transform.any_op, !transform.any_op
    structured.split %common after %splitr { dimension = 0 }
             : !transform.any_op, !transform.any_op
    // ...
    ```
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       I64Attr:$dimension,
                       I64Attr:$target_size,
                       DefaultValuedAttr<I64Attr, "1">:$divisor);
  let results = (outs TransformAnyParamTypeOrAnyHandle:$low_size,
                      TransformAnyParamTypeOrAnyHandle:$high_size,
                      TransformAnyParamTypeOrAnyHandle:$split_point);
  let hasVerifier = 1;
  let assemblyFormat =
    "$target attr-dict `:` custom<MultitileSizesTypes>("
    "type($target), type($low_size), type($high_size), type($split_point))";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::linalg::LinalgOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// PackOp
//===----------------------------------------------------------------------===//

def PackOp : Op<Transform_Dialect, "structured.pack", [
                DeclareOpInterfaceMethods<TransformOpInterface>,
                DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
                ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Pack a LinalgOp by applying a data tiling transformation on the op and
    packing the operands according to the `packed_sizes` specification.

    Iterator dimensions are tiled in their canonical order in the op spec.
    Operands are packed according to the same canonical order of the op iterator
    dimensions.

    Specifying a packed size of 0 for an iterator removes it from consideration
    for packing.

    `tensor.pack` (resp. `tensor.unpack`) operations are inserted for the operands
    (resp. results) that need to be packed (resp. unpacked) according to the
    `packed_sizes` specification.

    #### Example

    Consider a `linalg.matmul` with indexing maps:
    ```
      //              M   N   K       M   K
      // affine_map<(d0, d1, d2) -> (d0, d2)>
      //                              K   N
      // affine_map<(d0, d1, d2) -> (d2, d1)>
      //                              M   N
      // affine_map<(d0, d1, d2) -> (d0, d1)>
      %0 = linalg.matmul  ins(%A, %B: tensor<?x?xf32>, tensor<?x?xf32>)
                         outs(    %C: tensor<?x?xf32>)
    ```

    Specifying packed_sizes [2, 3, 4] results in tiling the iterator dimensions
    M, N and K, in this order, in both the op and its operands.
    ```
      //              M   N   K   m   n   k       M   K   m   k
      // affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d2, d3, d5)>
      //                                          K   N   n   k
      // affine_map<(d0, d1, d2, d3, d4, d5) -> (d2, d1, d4, d5)>
      //                                          M   N   m   n
      // affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d4)>
      %0 = linalg.generic_representing_some_higher_d_matmul
            ins(%A, %B: tensor<?x?x2x4xf32>, tensor<?x?x4x3xf32>)
           outs(    %C: tensor<?x?x2x3xf32>)
    ```
    In particular, note that the second operand `B` has shape `KxNxnxk` (and not
    `KxNxkxn` as one could expect by looking **only** at the operand).

    Other layouts can be obtained unsurprisingly from this canonical
    transformation by composing the resulting operation with a
    `transform.structured.pack_transpose` op.
    This composition allows separating concerns and composes better compared
    to adding additional permutation attributes to this transform op.

    #### Return modes

    This operation applies to a single Linalg op, otherwise it fails.
    This operation may produce a definite failure if the packing fails for any
    reason.

    The returned handle point to the packed LinalgOp.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                   Variadic<TransformHandleTypeInterface>:$packed_sizes,
                   DefaultValuedAttr<DenseI64ArrayAttr, "{}">:$static_packed_sizes);
  let results = (outs TransformHandleTypeInterface:$packed_op);
  let assemblyFormat = [{
    $target
    `packed_sizes` `=` custom<DynamicIndexList>($packed_sizes,
                                                $static_packed_sizes)
    attr-dict
    `:` functional-type(operands, results)
  }];

  let builders = [
    OpBuilder<(ins "Value":$target,
                   "ArrayRef<OpFoldResult>":$mixedPackedSizes)>
  ];

  let extraClassDeclaration = [{
    ::llvm::SmallVector<::mlir::OpFoldResult> getMixedPackedSizes();
  }];
}

//===----------------------------------------------------------------------===//
// PackGreedilyOp
//===----------------------------------------------------------------------===//

def PackGreedilyOp : Op<Transform_Dialect, "structured.pack_greedily", [
                        DeclareOpInterfaceMethods<TransformOpInterface>,
                        DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
                        ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Target a Linalg op and rewrite it into packed LinalgOp form by trying to
    infer whether a known suboperation is embedded

    Different packing strategies are applied in order, when one applies
    successfully, the transform returns:
      1. Matmul packing: Try to infer a matmul operation embedded in the target op.
         Specifically, this looks for 2 parallel dimensions that participate in
         an outer-product and 1 reduction dimension.
         These dimensions are referred as (m, n, k) to match canonical matmul
         terminology.

         The packed sizes for (m, n, k) are specified by `matmul_packed_sizes`
         and the optional `matmul_padded_sizes_next_multiple_of`.
         When an entry `matmul_packed_sizes[i]` is non-0, the corresponding
         dimension is packed by `matmul_packed_sizes[i]`.
         Otherwise, the dimension is merely padded to the next multiple of
         `matmul_padded_sizes_next_multiple_of[i]`.

         `matmul_padded_sizes_next_multiple_of` is optional and is expected to
         either be empty or of size `3`, matching the size of `matmul_packed_sizes`.
         For each individual element of `matmul_packed_sizes` and
         `matmul_padded_sizes_next_multiple_of`, only one of them is allowed to
         be non-zero.

         The ordering of the packed dimensions (mm, nn, kk) is specified by the
         `matmul_inner_dims_order` attribute.

    Packing occurs as follows:
      1. Find the dimensions to pack according to the strategy.
      2. The target is converted to linalg.generic form.
      3. An interchange transform is applied to isolate the dimensions to pack as
         the most minor indexing dimensions of the linalg.generic. The most minor
         dimensions are themselves ordered according to `inner_dims_order`.
      4. An elementwise traversal of `matmul_packed_sizes` and
         `matmul_padded_sizes_next_multiple_of` is performed and for each
         dimension `d`, either pack to `matmul_packed_sizes[d]` or pad to the
         `matmul_padded_sizes_next_multiple_of[d]`.
      5. Packing/padding is performed by the amounts determined in step 4. and
         following `inner_dims_order`.

    By normalizing the most minor dimensions to `inner_dims_order`, the transform
    guarantees that packing immediately generates inner dimensions in a desirable
    layout.

    Outer dimension layout permutations are not controlled by this transform op
    at the moment and can be obtained by composing with the pack_transpose
    transformation.

    #### Return modes

    This operation ignores non-Linalg ops and drops them in the return.
    It returns the list of packed Linalg ops or the original op when all available
    packing strategies failed to apply.
  }];

  // TODO: Transform_ConcreteOpType<linalg::LinalgOp> needs interface.
  let arguments = (ins TransformHandleTypeInterface:$target,
                   Variadic<TransformHandleTypeInterface>:$matmul_packed_sizes,
                   ConfinedAttr<DefaultValuedAttr<DenseI64ArrayAttr, "{}">,
                                 [DenseArrayCount<3>]>:$static_matmul_packed_sizes,
                   ConfinedAttr<DefaultValuedAttr<DenseI64ArrayAttr, "{}">,
                                 [Attr<
                                    Or<[DenseArrayCount<0>.predicate,
                                        DenseArrayCount<3>.predicate]>,
                                        "with 0 or 3 elements"
                                      >]>
                                 :$matmul_padded_sizes_next_multiple_of,
                   ConfinedAttr<DefaultValuedAttr<DenseI64ArrayAttr, "{}">,
                                 [DenseArrayCount<3>]>:$matmul_inner_dims_order);
  let results = (outs TransformHandleTypeInterface:$packed_op);

  let builders = [
    OpBuilder<(ins "Value":$target,
                   "ArrayRef<OpFoldResult>":$mixedMatmulPackedSizes,
                   "ArrayRef<int64_t>":$matmulPaddededSizesNextMultipleOf,
                   CArg<"ArrayRef<int64_t>", "{}">:$matmulDimsInnerDimsOrder)>
  ];

  let assemblyFormat = [{
    $target
    oilist(
      `matmul_packed_sizes` `=` custom<DynamicIndexList>($matmul_packed_sizes,
                                                         $static_matmul_packed_sizes)
      (`matmul_padded_sizes_next_multiple_of` `=`
        $matmul_padded_sizes_next_multiple_of^)?
      `matmul_inner_dims_order` `=` $matmul_inner_dims_order
    )
    attr-dict
    `:` functional-type(operands, results)
  }];
  let hasVerifier = 1;

  let extraClassDeclaration = [{
    /// Returns the list of tile sizes, which may be static (Attribute) or
    /// dynamic (Value).
    SmallVector<OpFoldResult> getMixedMatmulPackedSizes();
  }];
}

//===----------------------------------------------------------------------===//
// PackTransposeOp
//===----------------------------------------------------------------------===//

def PackTransposeOp : Op<Transform_Dialect, "structured.pack_transpose", [
                         FunctionalStyleTransformOpTrait,
                         MemoryEffectsOpInterface,
                         DeclareOpInterfaceMethods<TransformOpInterface>,
                         ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Apply a transposition to a single `tensor.pack` (resp. `tensor.unpack`) and
    update the `linalg.generic` op that consumes (resp. produces) the operation.

    This transform allows composing a simple `structured.pack` with additional
    transpositions to e.g. match the data format required by a specific library
    call or ISA instruction.

    The transpose spec must specify at least one of `outer_perm` or `inner_perm`
    attributes, which will act upon the `outer_dims_perm` or `inner_dims_pos` of
    the specified `tensor.pack` or `tensor.unpack` op.

    If the `target` of this op is a `tensor.pack` then a new `tensor.empty` will
    be created along with transposed versions of the `tensor.pack` and the
    consuming `linalg.generic`, which is expected to be the sole consumer.

    If the `target` of this op is a `tensor.unpack` then the whole pack / compute
    / unpack chain will be transposed and transposed clones of `tensor.pack`,
    the consuming `linalg.generic` and the tail `tensor.pack` will be created.

    #### Return modes

    This operation targets a single `tensor.pack` / `tensor.unpack` op and a
    single matching `linalg.generic` that consumes / produces the op. Otherwise,
    it produces a silenceableFailure.

    This operation may produce a silenceableFailure if the transpose spec is
    ill-formed (i.e. `outer_perm` or `inner_perm` are not permutations of the
    proper rank) or if the tranposition of all involved operations fails for any
    reason.

    This operation returns 3 handles, one to the transformed LinalgOp, one to
    the transformed `tensor.pack` and one to the transformed `tensor.unpack`.
    The last handle for `tensor.unpack` is empty if `target_pack_or_unpack_op`
    was not itself a `tensor.unpack`.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target_pack_or_un_pack_op,
                       TransformHandleTypeInterface:$target_linalg_op,
                       DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:$outer_perm,
                       DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:$inner_perm);
  let results = (outs TransformHandleTypeInterface:$packed_op,
                      TransformHandleTypeInterface:$pack_op,
                      TransformHandleTypeInterface:$un_pack_op);
  let assemblyFormat = [{
    $target_pack_or_un_pack_op
    `with_compute_op` `(` $target_linalg_op `)`
    (`outer_perm` `=` $outer_perm^ )?
    (`inner_perm` `=` $inner_perm^ )?
    attr-dict
    `:` functional-type(operands, results)
  }];

  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// PadOp
//===----------------------------------------------------------------------===//

def PadOp : Op<Transform_Dialect, "structured.pad",
    [FunctionalStyleTransformOpTrait, DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     TransformOpInterface,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Pads the operations pointed to by the target handle using the options
    provides as operation attributes. The operation returns a handle to the
    padded operation and to the padding operation ("tensor.pad").

    To preserve tensor SSA use-def chains, the unpadded result is copied back to
    the original destination tensor of the targeted op. The op that copies back
    the result can be customized with `copy_back_op`:

    * "bufferization.materialize_in_destination" (default)
    * "linalg.copy"
    * "none" (no copy back)

    #### Return modes

    This operation ignores non-Linalg ops and drops them in the return.
    This operation may produce a definite failure if the padding fails for any
    reason.

    If all the operations referred to by the `target` handle pad
    properly, the transform succeeds. Otherwise the transform produces a
    silenceable failure.
    The return handle points to only the subset of successfully produced
    padded operations, which can be empty.
  }];

  let arguments =
    (ins TransformHandleTypeInterface:$target,
         DefaultValuedAttr<ArrayAttr, "{}">:$padding_values,
         DefaultValuedAttr<I64ArrayAttr, "{}">:$padding_dimensions,
         Variadic<TransformAnyParamTypeOrAnyHandle>:$pad_to_multiple_of,
         DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:
                          $static_pad_to_multiple_of,
         DefaultValuedAttr<I64ArrayAttr, "{}">:$nofold_flags,
         DefaultValuedAttr<
          TypedArrayAttrBase<I64ArrayAttr, "array of arrays of i64">,
          "{}">:$transpose_paddings,
         DefaultValuedAttr<StrAttr, "::mlir::bufferization::MaterializeInDestinationOp::getOperationName()">:$copy_back_op);
  let results = (outs TransformHandleTypeInterface:$padded,
                      TransformHandleTypeInterface:$pad,
                      TransformHandleTypeInterface:$copy);

  let assemblyFormat = [{
    $target 
    (`pad_to_multiple_of` custom<DynamicIndexList>($pad_to_multiple_of, $static_pad_to_multiple_of)^)?
    attr-dict
    `:` functional-type(operands, results)
  }];

  let hasVerifier = 1;

  let builders = [
    // Builder for a transform::PadOp with automatic inference of padding
    // value. Warning: this will set the value 0 for the inferred elemental
    // type without taking the op into account and thus only work for the
    // add/mul ring at the moment.
    // TODO: support other operations (e.g. min, max etc).
    OpBuilder<(ins "Value":$target,
                   "ArrayRef<int64_t>":$paddingDimensions,
                   CArg<"ArrayRef<int64_t>", "{}">:$staticPadToMultipleOf,
                   CArg<"ArrayRef<int64_t>", "{}">:$nofoldFlags,
                   CArg<"ArrayRef<Attribute>", "{}">:$transposePaddings,
                   CArg<"StringRef", "::mlir::bufferization::MaterializeInDestinationOp::getOperationName()">:$copyBackOp)>,
    OpBuilder<(ins "Value":$target,
                   "ArrayRef<int64_t>":$paddingDimensions,
                   "ArrayRef<OpFoldResult>":$mixedPadToMultipleOf,
                   CArg<"ArrayRef<int64_t>", "{}">:$nofoldFlags,
                   CArg<"ArrayRef<Attribute>", "{}">:$transposePaddings,
                   CArg<"StringRef", "::mlir::bufferization::MaterializeInDestinationOp::getOperationName()">:$copyBackOp)>
  ];

  let extraClassDeclaration = [{
    /// copy_back_op attribute value indicating that no copy back is desired.
    static constexpr StringRef kCopyOpNone = "none";

    /// Returns a mix of dynamic `pad_to_multiple_of` and static `static_pad_to_multiple_of`.
    SmallVector<OpFoldResult> getMixedPadToMultipleOf();

    ::mlir::DiagnosedSilenceableFailure apply(
      ::mlir::transform::TransformRewriter &rewriter,
      ::mlir::transform::TransformResults &results,
      ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// HoistPadOp
//===----------------------------------------------------------------------===//

def HoistPadBuildPackingLoopNestOp :
    Op<Transform_Dialect,
       "structured.hoist_pad.build_packing_loop_nest",
    [DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     DeclareOpInterfaceMethods<TransformOpInterface>,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Helper transform used to hoist a tensor.pad target operation. This operation
    creates the packing loop nest required by the hoist_pad operation and makes
    that functionality available independently.

    TODO: In the future, we should consider rewriting as a tensor.pack after
    hoisting since this abstraction is now available.

    #### Return modes

    This operation ignores non-tensor.pad ops and drops them in the result.
    If any non-tensor.pad is passed, the transform emits a silenceable failure.

    The return handle points to only the subset of successfully created packing
    loop nests, which can be empty.
  }];

  // Also allow any payload operation for simpler composition. Non-tensor.pad ops
  // will be dropped from the results.
  let arguments =
    (ins TransformHandleTypeInterface:$target,
         TransformHandleTypeInterface:$loop,
         DefaultValuedAttr<DenseI64ArrayAttr, "{}">:$transpose);
  let results = (outs TransformHandleTypeInterface:$packing_loop);

  let assemblyFormat = [{
    $target
    `above` $loop
    (`,` `transpose` `by` $transpose^)?
    attr-dict
    `:` functional-type(operands, results)
  }];
  let hasVerifier = 1;
}

def HoistPadOp : Op<Transform_Dialect, "structured.hoist_pad",
    [FunctionalStyleTransformOpTrait,
     MemoryEffectsOpInterface,
     TransformOpInterface,
     TransformEachOpTrait]> {
  let description = [{
    Hoist the tensor.pad target operation by at most the given number of loops.
    Optionally apply the transpose attribute to the inner dimensions.

    TODO: In the future, we should consider rewriting as a tensor.pack after
    hoisting since this abstraction is now available.
    TODO: Maybe also return the linalg.generic transpose created at some point.

    #### Return modes

    This operation ignores non-tensor.pad ops and drops them in the result.
    If any non-tensor.pad is passed, the transform emits a silenceable failure.

    If all the operations referred to by the `target` handle padproperly, the
    transform succeeds. Otherwise the transform produces a silenceable failure.

    The return handle points to only the subset of successfully hoisted
    tensor.pad operations, which can be empty.
  }];

  // Also allow any operation for simpler composition. Non-tensor.pad ops
  // will be dropped from the results.
  let arguments =
    (ins TransformHandleTypeInterface:$target,
         I64Attr:$num_loops,
         DefaultValuedAttr<DenseI64ArrayAttr, "{}">:$transpose);
  let results = (outs TransformHandleTypeInterface:$transformed);

  let assemblyFormat = [{
    $target
    `by` $num_loops `loops`
    (`,` `transpose` `by` $transpose^)?
    attr-dict
    `:` functional-type(operands, results)
  }];
  let hasVerifier = 1;

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::tensor::PadOp,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// PromoteOp
//===----------------------------------------------------------------------===//


def PromoteOp : Op<Transform_Dialect, "structured.promote",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
    TransformOpInterface, TransformEachOpTrait,
    ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Promotes the specified operands of the target into a separate memory buffer.

    At this point, this transform does not allow customizing alloc/dealloc
    functions nor the behavior on copy in/out operations.

    #### Return modes

    This operation applies to a single Linalg op that satisfies the
    `promoteSubviewsPrecondition`, otherwise it fails.

    If the operations referred to by the `target` handle promote
    properly, the transform succeeds.

    When successful, the return handle points to the $target operation that
    was modified inplace.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       DefaultValuedAttr<I64ArrayAttr, "{}">:$operands_to_promote,
                       DefaultValuedAttr<BoolArrayAttr, "{}">:$use_full_tile_buffers,
                       UnitAttr:$use_full_tiles_by_default,
                       UnitAttr:$use_alloca,
                       OptionalAttr<AnyAttr>:$memory_space,
                       OptionalAttr<DeviceMappingArrayAttr>:$mapping,
                       OptionalAttr<I64Attr>:$alignment);
  let results = (outs TransformHandleTypeInterface:$transformed);

  let assemblyFormat = [{
    $target attr-dict `:`
    custom<SemiFunctionType>(type($target), type($transformed), "false")
  }];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::linalg::LinalgOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// ReplaceOp
//===----------------------------------------------------------------------===//

def ReplaceOp : Op<Transform_Dialect, "structured.replace",
    [IsolatedFromAbove, DeclareOpInterfaceMethods<TransformOpInterface>,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     ReportTrackingListenerFailuresOpTrait] # GraphRegionNoTerminator.traits> {
  let description = [{
    Replace all `target` payload ops with the single op that is contained in
    this op's region. All targets must have zero arguments and must be isolated
    from above.

    This op is for debugging/experiments only.

    #### Return modes

    This operation consumes the `target` handle.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs TransformHandleTypeInterface:$replacement);
  let regions = (region SizedRegion<1>:$bodyRegion);
  let assemblyFormat = [{
      $target attr-dict-with-keyword regions `:`
      custom<SemiFunctionType>(type($target), type($replacement), "false")
  }];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// ScalarizeOp
//===----------------------------------------------------------------------===//

def ScalarizeOp : Op<Transform_Dialect, "structured.scalarize",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     TransformOpInterface, TransformEachOpTrait,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Indicates that ops of a specific kind in the given function should be
    scalarized (i.e. their dynamic dimensions tiled by 1).

    #### Return modes:

    This operation ignores non-Linalg ops and drops them in the return.
    This operation produces definite failure if the scalarization fails for any
    reason.
    If all the operations referred to by the `target` handle scalarize
    properly, the transform succeeds. Otherwise the transform produces a
    silenceable failure.

    The return handle points to only the subset of successfully produced
    tiled-by-1 operations, which can be empty.

    This operation does not return handles to the tiled loop.
    We make this design choice because it is hard to know ahead of time the
    number of loops that will be produced (it depends on the number of dynamic
    dimensions after multiple transformations have been applied).
    Loops can always be recovered by navigating from the tiled operations if
    needed.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs TransformHandleTypeInterface:$result);

  let assemblyFormat = [{
    $target attr-dict `:`
    custom<SemiFunctionType>(type($target), type($result), "false")
  }];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::linalg::LinalgOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// ConvertToLoopsOp
//===----------------------------------------------------------------------===//

def ConvertToLoopsOp : Op<Transform_Dialect, "structured.convert_to_loops",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     DeclareOpInterfaceMethods<TransformOpInterface>,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    For operations that implement the `TilingInterface`, and implement
    the `generateScalarImplementation` method, lowers the operation to
    loops. The return handle points to all generated loops.
    Fails if the payload ops cannot be lowered to loops.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs TransformHandleTypeInterface:$result);

  let assemblyFormat = [{
    $target attr-dict `:` functional-type(operands, results)
  }];
}

//===----------------------------------------------------------------------===//
// DecomposeInterfaceOp
//===----------------------------------------------------------------------===//

def DecomposeInterfaceOp : Op<Transform_Dialect, "structured.decompose_interface",
    [FunctionalStyleTransformOpTrait,
     MemoryEffectsOpInterface,
     TransformOpInterface,
     TransformEachOpTrait,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    TODO
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs TransformHandleTypeInterface:$transformed);
  let assemblyFormat =
      "$target attr-dict `:` functional-type(operands, results)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::Operation *target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}
//===----------------------------------------------------------------------===//
// RewriteInDestinationPassingStyleOp.
//===----------------------------------------------------------------------===//

def RewriteInDestinationPassingStyleOp : Op<
    Transform_Dialect, "structured.rewrite_in_destination_passing_style",
    [FunctionalStyleTransformOpTrait,
     MemoryEffectsOpInterface,
     TransformOpInterface,
     TransformEachOpTrait,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Rewrite a supported tensor operation that is not in destination-passing style
    into a form that is in destination-passing style.
    Currently supported operations are:
      - tensor.pad
      - tensor.generate
      - tensor.from_elements
    This dichotomy hints at a future interface, for now the implementation just
    switches between different implementation.

    #### Return modes

    This operation ignores non-unsupported ops and drops them from the return.
    If all the operations referred to by the `target` handle generalize
    properly, the transform succeeds. Otherwise the transform produces a
    silenceable failure.
    The return handle points to a subset of successfully produced operations:
      - `tensor.pad` case, the returned handle points to the tensor.insert_slice.
      - `tensor.generate` case, the returned handle points to the linalg.generic.
      - `tensor.from_elements` case, the returned handle points to the last
        `tensor.insert`.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs TransformHandleTypeInterface:$transformed);
  let assemblyFormat = [{
    $target attr-dict
    `:` functional-type($target, results)
  }];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::Operation *target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// SplitOp
//===----------------------------------------------------------------------===//

def SplitOp : Op<Transform_Dialect, "structured.split",
    [DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     DeclareOpInterfaceMethods<TransformOpInterface>,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Splits the given `target` op into two or more complementary
    parts, which combined cover the entire iteration domain of the original op.
    The split is performed along the iteration space dimension provided as
    chunk size attribute specifying the size of the lower part; the remaining
    range in the iteration space is assigned as the upper part. In case of
    dimension overflow, the transformation fails. The split is performed at the
    dimension iterator value specified as either the static chunk size
    attribute when it is known at transform IR construction time or
    as the handle to an operation producing a single index-typed value
    when it is computed by payload IR. In the latter case, the chunk size
    point must be set to `ShapedType::kDynamic` and the dynamic size handle
    must point to as many value-producing operations as there are structured
    operations pointed to by the target handle.

    The operation consumes the target handle, but preserves the chunk size
    handle if provided. Without the `multiway` attribute, it produces a
    new handle that is a list of the two parts of the structured op after
    splitting, whose lower index part corresponding to the part with lower
    iteration space indices.

    Multiway split mode is enabled by specifying the `multiway` attribute.
    In this mode a single `target` op is split into multiple parts covering
    the iteration space of the specified dimension. `static_chunk_sizes` and
    `dynamic_chunk_sizes` in this case is a list of chunk sizes that the given
    dimension should be split into. With `multiway` it also produces a handle;
    The result handle is a list of the multiple parts of the structured op
    after splitting, where the target dimensions for each linalg op in the
    list corresponds to the chunk sizes specfied in the input split list.
    If the chunk sizes do not cover the entire iteration space, the leftover
    chunk is the last payload in the result handle.

    As the result handle is most of time a list, an `transform.split_handle`
    is needed to access individual handle.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       I64Attr:$dimension,
                       Optional<TransformAnyParamTypeOrAnyHandle>:$dynamic_chunk_sizes,
                       I64Attr:$static_chunk_sizes,
                       UnitAttr:$multiway);
  let results = (outs TransformHandleTypeInterface:$split_list);
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// SplitReductionOp
//===----------------------------------------------------------------------===//

def SplitReductionOp : Op<Transform_Dialect, "structured.split_reduction",
       [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
        TransformEachOpTrait, TransformOpInterface,
        ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Indicates that the given `target` op should be transformed with the
    `splitReduction` transformation and split factor provided as attribute.

    The `splitReduction` transformation splits the first single linalg op
    reduction into a parallel and reduction dimension.
    A new `linalg.generic` op is created to perform the rest of the reduction.

    The transformation supports different configurations attributes:
      - split_factor: the factor by which to split (i.e. the size of the
        remaining reduction after splitting).
      - insert_split_dimension: the dimension in the temporary tensor into
        which the new parallel dimension is inserted.
      - inner_parallel: specifies whether the parallel dimension is before or
        after the reduction dimension in the splitting op.
      - use_scaling_algorithm: whether to use a scaling based formulation that
        does not create an ExpandShapeOp (default: do not use scaling)
      - use_alloc: whether to use an alloc op to allocate the temporary
        tensor (default: do not use alloc op)

    #### Return modes

    This operation ignores non-Linalg ops and drops them in the return.
    This operation produces a definite failure if the splitting fails for any
    reason.

    If all the operations referred to by the `target` handle split
    properly, the transform succeeds. Otherwise the transform produces a
    silenceable failure.  The 4 returned handles points to only the subset of
    successfully produced computational operations, which can all be empty.
    This 4 returned handles point to:
      - the init op (or tensor_alloc op if use_alloc = true),
      - the fill op used to initialize the neutral element,
      - the split op and
      - the result-combining op.

    #### Example (default: `use_scaling_algorithm = false, use_alloc = false`):

    ```
      %r = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>,
                                            affine_map<(d0) -> ()>],
            iterator_types = ["reduction"]}
      ins(%in : tensor<32xf32>)
      outs(%out : tensor<f32>) {
      ^bb0(%arg1: f32, %arg2: f32):
        %y = arith.addf %arg1, %arg2 : f32
        linalg.yield %y : f32
      } -> tensor<f32>
    ```

    is split into:

    ```
      %cst = arith.constant 0.000000e+00 : f32
      %0 = tensor.expand_shape %in [[0, 1]] : tensor<32xf32> into tensor<4x8xf32>
      %1 = tensor.empty() : tensor<4xf32>
      %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<4xf32>) -> tensor<4xf32>
      %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>,
                                            affine_map<(d0, d1) -> (d0)>],
        iterator_types = ["parallel", "reduction"]}
        ins(%0 : tensor<4x8xf32>) outs(%2 : tensor<4xf32>) {
        ^bb0(%arg3: f32, %arg5: f32):
        %5 = arith.addf %arg3, %arg4 : f32
        linalg.yield %5 : f32
      } -> tensor<4xf32>
      %r = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>,
                                            affine_map<(d0) -> ()>],
        iterator_types = ["reduction"]}
        ins(%3 : tensor<4xf32>) outs(%out : tensor<f32>) {
        ^bb0(%arg3: f32, %arg4: f32):
        %5 = arith.addf %arg3, %arg4 : f32
        linalg.yield %5 : f32
      } -> tensor<f32>
    ```

    #### Example (`use_scaling_algorithm = true, use_alloc = true`):

    Instead of introducing an ExpandShapeOp, this scaling-based implementation
    rewrites a reduction dimension `k` into `k * split_factor + kk`.
    The dimension `kk` is added as an extra parallel dimension to the
    intermediate output tensor at position `insert_split_dimension`.

    Consider a minimal example where `k` is reduced:
        O(i, j) += I(i, j, k)
    Assume i=3, j=5, k=128, split_factor=16 and insert_split_dimension=0.
    The compute is rewritten as:
      a. O_i(kk, i, j) += I(i, j, 16 * k + kk)
      b. O(i, j) += O_i(kk, i, j)
    The intermediate tensor O_i is of shape (128/16)x3x5 == 8x3x5.

    #### Example:

    ```
     %0 = linalg.matmul ins(%A, %B: tensor<16x256xf32>, tensor<256x32xf32>)
       outs(%C: tensor<16x32xf32>) -> tensor<16x32xf32>
    ```

    Is transformed to:

    ```
     #map0 = affine_map<(d0, d1, d2, d3) -> (d0, d2 * 4 + d3)>
     #map1 = affine_map<(d0, d1, d2, d3) -> (d2 * 4 + d3, d1)>
     #map2 = affine_map<(d0, d1, d2, d3) -> (d2, d3)>
     #map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
     #map4 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
     #map5 = affine_map<(d0, d1, d2) -> (d0, d1)>
     %0 = tensor.empty() : tensor<16x32x64xf32>
     %cst = arith.constant 0.000000e+00 : f32
     %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<16x32x64xf32>) ->
        tensor<16x32x64xf32>
     %2 = tensor.empty() : tensor<64x4xi1>

     %3 = linalg.generic {indexing_maps = [#map0, #map1, #map2, #map3],
       iterator_types = ["parallel", "parallel", "parallel", "reduction"]}
       ins(%A, %B, %2 : tensor<16x256xf32>, tensor<256x32xf32>, tensor<64x4xi1>)
       outs(%1 : tensor<16x32x64xf32>) {
         ^bb0(%arg3: f32, %arg4: f32, %arg5: i1, %arg6: f32):
           %5 = arith.mulf %arg3, %arg4 : f32
           %6 = arith.addf %arg6, %5 : f32
           linalg.yield %6 : f32
     } -> tensor<16x32x64xf32>

     %4 = linalg.generic {indexing_maps = [#map4, #map5],
       iterator_types = ["parallel", "parallel", "reduction"]}
       ins(%3 : tensor<16x32x64xf32>)
       outs(%C : tensor<16x32xf32>) {
         ^bb0(%arg3: f32, %arg4: f32):
           %5 = arith.addf %arg3, %arg4 : f32
           linalg.yield %5 : f32
     } -> tensor<16x32xf32>

     return %4 : tensor<16x32xf32>
    ```
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                   DefaultValuedAttr<I64Attr, "{}">:$split_factor,
                   DefaultValuedAttr<I64Attr, "{}">:$insert_split_dimension,
                   UnitAttr:$inner_parallel,
                   UnitAttr:$use_scaling_algorithm,
                   UnitAttr:$use_alloc);
  let results = (outs TransformHandleTypeInterface:$init_or_alloc_op,
                      TransformHandleTypeInterface:$fill_op,
                      TransformHandleTypeInterface:$split_linalg_op,
                      TransformHandleTypeInterface:$combining_linalg_op);

  let assemblyFormat =
      "$target attr-dict `:`"
      "functional-type(operands, results)";

  let builders = [
    OpBuilder<(ins "Value":$target,
                   "int64_t":$splitFactor,
                   "int64_t":$insertSplitDimension,
                   CArg<"bool", "false">:$innerParallel,
                   CArg<"bool", "false">:$useScalingAlgorithm,
                   CArg<"bool", "false">:$useAlloc)>
  ];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::linalg::LinalgOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// TileReductionUsingForOp
//===----------------------------------------------------------------------===//

def TileReductionUsingForOp : Op<Transform_Dialect, "structured.tile_reduction_using_for",
       [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
        TransformEachOpTrait, TransformOpInterface,
        ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Indicates that the given `target` op should be transformed with the
    `tileReduction` transformation with the tile size provided as attribute.

    This transformation tiles the `target` along the reduction dimensions. It
    creates a tensor initialized with the identity value. Then it creates nested
    loops with a parallel version of `target` op inside. The parallel op
    dimensions are less or equal to the tile size passed by user.
    After the loop a merge operation is created to do a final reduction with the
    partial reductions.
    The initial tensor always uses the tile size dimension. This may overallocate
    if the tile size is greater than the reduction dimension.

    #### Return modes

    Returns 4 handles associated with (in order):
      - the fill op used to initialize the neutral element,
      - the parallel tiled op and
      - the result-combining op,
      - the parent `for` op.

    #### Example:

    ```
      %red = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>,
                                              affine_map<(d0, d1) -> (d0)>],
      iterator_types = ["parallel", "reduction"]}
      ins(%arg0 : tensor<?x?xf32>)
      outs(%out : tensor<?xf32>) {
        ^bb0(%arg7: f32, %arg9: f32):
        %1 = arith.addf %arg7, %arg9 : f32
        linalg.yield %1 : f32
      } -> tensor<?xf32>
      return %red : tensor<?xf32>
    ```

    is transformed into:

    ```
      %0 = tensor.empty(%dim_1) : tensor<?x5xf32>
      %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<?x5xf32>) -> tensor<?x5xf32>
      %2 = scf.for %arg2 = %c0 to %dim_0 step %c5 iter_args(%arg3 = %1) -> (tensor<?x5xf32>) {
        %extracted_slice = tensor.extract_slice %1[0, 0] [%dim, 5] [1, 1] : tensor<?x5xf32> to tensor<?x5xf32>
        %extracted_slice_2 = tensor.extract_slice %arg0[0, %arg2] [%dim, 5] [1, 1] : tensor<?x?xf32> to tensor<?x5xf32>
        %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>,
                                              affine_map<(d0, d1) -> (d0, d1)>],
        iterator_types = ["parallel", "parallel"]}
        ins(%extracted_slice_2 : tensor<?x5xf32>)
        outs(%extracted_slice : tensor<?x5xf32>) {
        ^bb0(%in: f32, %out: f32):
          %5 = arith.addf %in, %out : f32
          linalg.yield %5 : f32
        } -> tensor<?x5xf32>
        %dim_3 = tensor.dim %1, %c0 : tensor<?x5xf32>
        %inserted_slice = tensor.insert_slice %4 into %arg3[0, 0] [%dim_3, 5] [1, 1] : tensor<?x5xf32> into tensor<?x5xf32>
        scf.yield %inserted_slice : tensor<?x5xf32>
      }
      %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>,
                                            affine_map<(d0, d1) -> (d0)>],
      iterator_types = ["parallel", "reduction"]}
      ins(%2 : tensor<?x5xf32>)
      outs(%arg1 : tensor<?xf32>) {
      ^bb0(%in: f32, %out: f32):
        %4 = arith.addf %in, %out : f32
        linalg.yield %4 : f32
      } -> tensor<?xf32>
    ```
  }];

  // TODO: support mixed static-dynamic (see TileUsingForallOp).
  let arguments = (ins TransformHandleTypeInterface:$target,
                   DefaultValuedAttr<DenseI64ArrayAttr, "{}">:$tile_sizes);
  let results = (outs Variadic<TransformHandleTypeInterface>:$fill_op,
                      TransformHandleTypeInterface:$split_linalg_op,
                      TransformHandleTypeInterface:$combining_linalg_op,
                      TransformHandleTypeInterface:$for_op);

  let builders = [
    OpBuilder<(ins "Value":$target,
                   "ArrayRef<int64_t>":$staticTileSizes)>
  ];

  let assemblyFormat = [{
    $target
    `by` `tile_sizes` `=` $tile_sizes
    attr-dict
    `:` functional-type(operands, results)
  }];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::linalg::LinalgOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// TileReductionUsingForallOp
//===----------------------------------------------------------------------===//

def TileReductionUsingForallOp :
  Op<Transform_Dialect, "structured.tile_reduction_using_forall",
       [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
        TransformEachOpTrait, TransformOpInterface,
        ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Tile a PartialReductionOpInterface op to a tiled `scf.forall` doing
    partial reduction.

    This transformation tiles the `target` along the reduction dimensions. It
    creates a tensor initialized with the identity value. Then it creates a
    `scf.forall` loops with the number threads given by `num_threads`.
    The op is tiled op with a size equal to `floordiv(size, num_threads)`.
    All the partial reduction value is are parallel inserted to create a new
    tensor. After the loop a merge operation is created to do a final reduction
    with the partial reductions tensor.
    If an extra `tile_sizes` parameter is passed the tiles are cyclically
    distributed on the threads of the `scf.foralls` loop.

    #### Return modes

    Returns 4 handles associated with (in order):
      - the fill op used to initialize the neutral element,
      - the parallel tiled op and
      - the result-combining op,
      - the parent `forall` op.

    #### Example:

    ```
      %red = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>,
                                              affine_map<(d0, d1) -> (d0)>],
      iterator_types = ["parallel", "reduction"]}
      ins(%arg0 : tensor<?x?xf32>)
      outs(%out : tensor<?xf32>) {
        ^bb0(%arg7: f32, %arg9: f32):
        %1 = arith.addf %arg7, %arg9 : f32
        linalg.yield %1 : f32
      } -> tensor<?xf32>
      return %red : tensor<?xf32>
    ```

    is transformed into:

    ```
      %0 = tensor.empty(%dim_1) : tensor<?x5xf32>
      %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<?x5xf32>) -> tensor<?x5xf32>
      %2 = scf.forall (%arg2) in (%c5) shared_outs(%arg3 = %1) -> (tensor<?x5xf32>) {
        %4 = affine.min #map(%arg2)[%dim_0]
        %5 = affine.max #map1(%4)
        %extracted_slice = tensor.extract_slice %arg3[0, %arg2] [%dim, 1] [1, 1] : tensor<?x5xf32> to tensor<?xf32>
        %6 = affine.apply #map2(%arg2)[%dim_0]
        %extracted_slice_2 = tensor.extract_slice %arg0[0, %6] [%dim, %5] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
        %extracted_slice_3 = tensor.extract_slice %extracted_slice[0] [%dim] [1] : tensor<?xf32> to tensor<?xf32>
        %7 = linalg.generic {indexing_maps = [#map3, #map4], iterator_types = ["parallel", "reduction"]} ins(%extracted_slice_2 : tensor<?x?xf32>) outs(%extracted_slice_3 : tensor<?xf32>) {
        ^bb0(%in: f32, %out: f32):
          %9 = arith.addf %in, %out : f32
          linalg.yield %9 : f32
        } -> tensor<?xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %7 into %arg3[0, %arg2] [%dim, 1] [1, 1] : tensor<?xf32> into tensor<?x5xf32>
        }
      } {mapping = []}
      %3 = linalg.generic {indexing_maps = [#map3, #map4], iterator_types = ["parallel", "reduction"]} ins(%2 : tensor<?x5xf32>) outs(%arg1 : tensor<?xf32>) {
      ^bb0(%in: f32, %out: f32):
        %4 = arith.addf %in, %out : f32
        linalg.yield %4 : f32
      } -> tensor<?xf32>
    ```
  }];

  // TODO: support mixed static-dynamic (see TileUsingForallOp).
  let arguments = (ins TransformHandleTypeInterface:$target,
                   DefaultValuedAttr<DenseI64ArrayAttr, "{}">:$num_threads,
                   DefaultValuedAttr<DenseI64ArrayAttr, "{}">:$tile_sizes,
                   OptionalAttr<DeviceMappingArrayAttr>:$mapping);
  let results = (outs Variadic<TransformHandleTypeInterface>:$fill_op,
                      TransformHandleTypeInterface:$split_linalg_op,
                      TransformHandleTypeInterface:$combining_linalg_op,
                      TransformHandleTypeInterface:$forall_op);

  let builders = [
    OpBuilder<(ins "Value":$target,
                   "ArrayRef<int64_t>":$staticNumThreads,
                   "ArrayRef<int64_t>":$staticTileSizes,
                   CArg<"ArrayAttr", "{}">:$mapping)>
  ];

  let assemblyFormat = [{
    $target
    `by`
    (`num_threads` `=` $num_threads^)?
    (`,` `tile_sizes` `=` $tile_sizes^)?
    (`,` `mapping` `=` $mapping^)?
    attr-dict
    `:` functional-type(operands, results)
  }];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::linalg::LinalgOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];

}

//===----------------------------------------------------------------------===//
// ContinuousTileSizesOp
//===----------------------------------------------------------------------===//

def ContinuousTileSizesOp : Op<Transform_Dialect, "structured.continuous_tile_sizes",
       [DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
        DeclareOpInterfaceMethods<TransformOpInterface>,
        ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    This transform emits the IR computing the list of (1) exponentially
    diminishing tile sizes that are powers of 2; and (2) the corresponding
    chunk-sizes the target op should be split into along the given dimension.

    For example, for `target_size` 9, and `dimension` 0 for the following
    linalg op as target

    ```
      %0 = linalg.matmul  ins(%arg0, %arg1: tensor<25x34xf32>, tensor<34x25xf32>)
                      outs(%arg2: tensor<25x25xf32>)
    ```

    the first result `tile_sizes` will be a list of diminishing tile sizes
    9, 4, 2, 1; and the second result will be a list of chunk sizes
    18, 4, 2, 1 that the corresponding dimension should be split into.

    After the target op has been split along the given dimension (for example
    using multiway split), each chunk can be tiled with the corresponding tile
    size in the `tile_sizes` list generated as a result of this op.

    Specifying the output type as !transform.param<i64> will cause `tile_sizes`
    and `chunk_sizes` to be computed statically and not dynamically.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       ConfinedAttr<I64Attr, [IntNonNegative]>:$dimension,
                       ConfinedAttr<I64Attr, [IntNonNegative]>:$target_size);
  let results = (outs TransformAnyParamTypeOrAnyHandle:$tile_sizes,
                      TransformAnyParamTypeOrAnyHandle:$chunk_sizes);
  let hasVerifier = 1;
  let assemblyFormat =
    "$target attr-dict `:` custom<ContinuousTileSizeTypes>("
    "type($target), type($tile_sizes), type($chunk_sizes))";

}

//===----------------------------------------------------------------------===//
// TileUsingForOp
//===----------------------------------------------------------------------===//

def TileUsingForOp : Op<Transform_Dialect, "structured.tile_using_for",
       [DeclareOpInterfaceMethods<TransformOpInterface>,
        DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
        ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Indicates that the given `target` op should be tiled with the given sizes.
    This transform generates a loop nest with a smaller ("tiled") target
    operation in its body. Currently limited to LinalgOps.

    Tile sizes may be known at transformation time, in which case they are
    expected to be provided in the `static_size` attribute, or not, in which
    case the tile value must be computed by the payload IR and the handle to the
    operation computing it must be provided through `dynamic_sizes`. When the
    sizes are not known statically, the corresponding entry in the
    `static_sizes` attribute must be set to `ShapedType::kDynamic`. Only
    the dynamic sizes must be provided in `dynamic_sizes`, i.e., there should
    be as many handles as `ShapedType::kDynamic` values in the
    `static_sizes` attribute. A static size of `0` indicates that the dimension
    should not be tiled. No loop will be generated for such dimensions. If all
    tile sizes are `0`, this transform is effectively a no-op.

    This op returns handles to the tiled op (in the generated loop nest) and the
    generated loops. The number of loops is the number of tile sizes that are
    statically known to be non-zero.

    #### Return modes

    On success, the resulting handles are associated with co-indexed lists of
    tiled operations and loops around them.

    This operation only supports Linalg ops and produces a silenceable failure
    if the input contains any non-Linalg ops. The ops preceding it in the list
    associated with the `target` handle will have been tiled.

    This operation produces a silenceable failure if the `dynamic_sizes` handles
    are associated with lists of payload operations of a size different than
    that of the list associated with the `target` handle.

    If the internal implementation of tiling for any of the operations fails,
    produces a definite failure.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                   Variadic<TransformAnyParamTypeOrAnyHandle>:$dynamic_sizes,
                   DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:$static_sizes,
                   DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:$interchange,
                   DefaultValuedOptionalAttr<DenseBoolArrayAttr, "{}">:$scalable_sizes);
  let results = (outs TransformHandleTypeInterface:$tiled_linalg_op,
                      Variadic<TransformHandleTypeInterface>:$loops);
  let builders = [
    OpBuilder<(ins "TypeRange":$loopTypes,
                   "Value":$target,
                   "ArrayRef<int64_t>":$staticTileSizes,
                   CArg<"ArrayRef<int64_t>", "{}">:$interchange,
                   CArg<"std::optional<ArrayRef<bool>>", "std::nullopt">:
                      $scalableSizes)>,
    OpBuilder<(ins "TypeRange":$loopTypes,
                   "Value":$target,
                   "ArrayRef<OpFoldResult>":$mixedTileSizes,
                   CArg<"ArrayRef<int64_t>", "{}">:$interchange,
                   CArg<"std::optional<ArrayRef<bool>>", "std::nullopt">:
                      $scalableSizes)>,
    OpBuilder<(ins "Value":$target,
                   "ArrayRef<int64_t>":$staticTileSizes,
                   CArg<"ArrayRef<int64_t>", "{}">:$interchange,
                   CArg<"std::optional<ArrayRef<bool>>", "std::nullopt">:
                      $scalableSizes)>,
    OpBuilder<(ins "Value":$target,
                   "ArrayRef<OpFoldResult>":$mixedTileSizes,
                   CArg<"ArrayRef<int64_t>", "{}">:$interchange,
                   CArg<"std::optional<ArrayRef<bool>>", "std::nullopt">:
                      $scalableSizes)>,
  ];

  let assemblyFormat = [{
    $target
      `tile_sizes` custom<DynamicIndexList>(
        $dynamic_sizes,
        $static_sizes,
        $scalable_sizes)
      (`interchange` `=` $interchange^)?
    attr-dict
    `:` functional-type(operands, results)
  }];

  let hasVerifier = 1;

  let extraClassDeclaration = [{
    /// Returns the list of tile sizes, which may be static (Attribute) or
    /// dynamic (Value).
    SmallVector<OpFoldResult> getMixedSizes();
  }];
}

//===----------------------------------------------------------------------===//
// TileUsingForallOp
//===----------------------------------------------------------------------===//

def TileUsingForallOp :
    Op<Transform_Dialect, "structured.tile_using_forall",
      [AttrSizedOperandSegments,
       DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
       TransformOpInterface, ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Tile a TilingInterface op to a tiled `scf.forall`.

    Tiling is applied by either specifying `num_threads` or `tile_size`. If
    `num_threads` is specified, then the tile size for each dimension `i` is
    calculated dynamically via `ceilDiv(dimSize[i], num_threads[i])`.
    `num_threads` and `tile_size` can be either static index attributes or
    operation handles (or a mix thereof). Operation handles must be mapped to
    exactly one op that has exactly one result of index type.

    Static zero tile sizes indicate that the dimension is not tiled and can be
    thought of as tiling by the full size of data.

    It is the user's responsibility to ensure that `num_threads/tile_sizes` is
    a valid tiling specification (i.e. that only tiles parallel dimensions,
    e.g. in the Linalg case). If the dimension is not parallelizable, a warning
    is issued to notify the user that the generated code is not safe to
    parallelize.

    If non-empty, the `mapping` is added as an attribute to the
    resulting `scf.forall`.

    Note: `tile_sizes` and `num_threads` are variadic. Each tile size/number of
    threads can be an index attribute or a transform handle that is mapped to
    exactly one payload op with exactly one index result.

    #### Return modes

    This operation ignores ops that do not implement the TilingInterface and
    drops them in the return.

    If all the operations referred to by the `target` handle tile
    successfully, the transform succeeds.
    Otherwise the transform produces a silenceable failure.

    The two returned handles point to only the subset of successfully produced
    tiled operations, which can all be empty.

    These two returned handles point to:
      - the tiled op that implements TilingInterface,
      - the new scf.forall op.

    #### Example using `num_threads`

    ```
    %0 = transform.structured.match ops{["linalg.matmul"]} in %arg1
       : (!transform.any_op) -> !transform.any_op
    %3:2 = transform.structured.tile_using_forall %0 num_threads [10, 20]
       : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    ```

    #### Example using `tile_sizes`

    ```
    %0 = transform.structured.match ops{["linalg.matmul"]} in %arg1
       : (!transform.any_op) -> !transform.any_op
    %sz = transform.structured.match ...
    %3:2 = transform.structured.tile_using_forall %0 tile_sizes [0, %sz, 20]
       : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    ```
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                   Variadic<TransformAnyParamTypeOrAnyHandle>:$num_threads,
                   Variadic<TransformAnyParamTypeOrAnyHandle>:$tile_sizes,
                   Optional<TransformAnyParamTypeOrAnyHandle>:$packed_num_threads,
                   Optional<TransformAnyParamTypeOrAnyHandle>:$packed_tile_sizes,
                   DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:$static_num_threads,
                   DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:$static_tile_sizes,
                   OptionalAttr<DeviceMappingArrayAttr>:$mapping);
  let results = (outs TransformHandleTypeInterface:$tiled_op,
                      TransformHandleTypeInterface:$forall_op);

  let builders = [
    OpBuilder<(ins "Value":$target,
                   "ArrayRef<int64_t>":$staticTileSizes,
                   CArg<"::mlir::transform::TileSizesSpec",
                        "::mlir::transform::TileSizesSpec()">,
                   CArg<"ArrayAttr", "{}">:$mapping)>,
    OpBuilder<(ins "Value":$target,
                   "ArrayRef<OpFoldResult>":$mixedTileSizes,
                   CArg<"::mlir::transform::TileSizesSpec",
                        "::mlir::transform::TileSizesSpec()">,
                   CArg<"ArrayAttr", "{}">:$mapping)>,
    OpBuilder<(ins "Value":$target,
                   "ArrayRef<int64_t>":$staticNumThreads,
                   CArg<"::mlir::transform::NumThreadsSpec",
                        "::mlir::transform::NumThreadsSpec()">,
                   CArg<"ArrayAttr", "{}">:$mapping)>,
    OpBuilder<(ins "Value":$target,
                   "ArrayRef<OpFoldResult>":$mixedNumThreads,
                   CArg<"::mlir::transform::NumThreadsSpec",
                        "::mlir::transform::NumThreadsSpec()">,
                   CArg<"ArrayAttr", "{}">:$mapping)>
  ];

  let assemblyFormat = [{
    $target oilist(
        `num_threads` custom<PackedOrDynamicIndexList>($packed_num_threads,
                                                       $num_threads,
                                                       $static_num_threads) |
         `tile_sizes` custom<PackedOrDynamicIndexList>($packed_tile_sizes,
                                                       $tile_sizes,
                                                       $static_tile_sizes))
    (`(` `mapping` `=` $mapping^ `)`)? attr-dict
    `:` functional-type(operands, results)
  }];
  let hasVerifier = 1;

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure apply(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::transform::TransformResults &transformResults,
        ::mlir::transform::TransformState &state);

    ::llvm::SmallVector<::mlir::OpFoldResult> getMixedNumThreads();
    ::llvm::SmallVector<::mlir::OpFoldResult> getMixedTileSizes();
  }];
}

//===----------------------------------------------------------------------===//
// VectorizeChildrenAndApplyPatternsOp
//===----------------------------------------------------------------------===//

def VectorizeChildrenAndApplyPatternsOp :
  Op<Transform_Dialect, "structured.vectorize_children_and_apply_patterns",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     TransformEachOpTrait, TransformOpInterface,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Vectorizes all children contained in the given `target` using the
    configuration specified by the attributes of this op. This only vectorizes
    structured ops that operate on shaped types and does not vectorize loops or
    straight-line. Internally, it applies a set of rewrite patterns, some of
    which enable vectorization and some of which clean up the results.
    Therefore, it can only be applied to an op with the "isolated from above"
    property. This transformation only fails if the entire pattern rewriting
    failed, i.e., it does **not** fail when no ops were vectorized.

    Finer granularity can be achieved either with the `VectorizeOp` for
    individual ops or by outlining the target part of the payload IR into, e.g.,
    a function, performing this transformation, and inlining it back.

    Note that this transformation invalidates the handles to any payload IR
    operation that is contained inside the vectorization target.

    This transformation supports the following attributes:
    - `vectorize_padding`: a `UnitAttr` to activate the vectorization of
      `tensor.pad` ops. Different pipelines may prefer to lower such ops to
      loops.
    - `disable_multi_reduction_to_contract_patterns`: a `UnitAttr` to deactivate
      the rewrite of `vector.multi_reduction` to `vector.contract`. This is
      intended to be used in tests only.
    - `disable_transfer_permutation_map_lowering_patterns`: a `UnitAttr` to
      deactivate the rewrite of `vector.transfer` with permutation maps into
      explicit `vector.transpose` operations. This is intended to be used in
      tests only but may be promoted to a first class attribute in the future.

    #### Return modes:

    This operation produces a definite failure if vectorization fails for any
    reason.
    The operation always returns the handle to the target op that is expected
    to be isolated from above.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                   UnitAttr:$vectorize_padding,
                   UnitAttr:$vectorize_nd_extract,
                   UnitAttr:$flatten_1d_depthwise_conv,
                   UnitAttr:$disable_multi_reduction_to_contract_patterns,
                   UnitAttr:$disable_transfer_permutation_map_lowering_patterns);
  let results = (outs TransformHandleTypeInterface:$transformed);

  let assemblyFormat =
      "$target attr-dict `:`"
      "functional-type(operands, results)";

  let builders = [
    OpBuilder<(ins "Value":$target,
               CArg<"bool", "false">:$vectorizePadding,
               CArg<"bool", "false">:$vectorizeNDExtract,
               CArg<"bool", "false">:$flatten1DDepthwise)>
  ];
  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::Operation *target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

def VectorizeOp : Op<Transform_Dialect, "structured.vectorize",
    [DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     TransformOpInterface, ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Vectorize the target ops, which must be Linalg ops.

    Use the optional vector sizes to specify exactly what configuration the
    vectorizer should use. It will then use masked vectors of the specified
    size to enforce this configuration ("masked vectorization"). If no vector
    sizes are specified, the vectorizer will infer the shapes to use from the
    target Linalg ops ("regular vectorization"). More specifically:

    ```mlir
    # Masked vectorization - vector sizes are specified explicitly
    transform.structured.vectorize %target vector_sizes [1, 4] : !transform.any_op
    # Regular vectorization - vector sizes are inferred from the target Op
    transform.structured.vectorize %target : !transform.any_op
    ```

    The vector sizes can be either static or dynamic (SSA values). In case of
    SSA values, the handle must be mapped to exactly one payload op with
    exactly one index-typed result.

    Note: The input vector sizes must be bigger than or equal to their
    counterpart iteration space sizes.

    Typically this operator should be applied to linalg operations that have
    already been tiled to the appropriate sizes.

    #### Return modes:

    This operation produces a silenceable failure if at least one target op is
    not a Linalg op or fails to vectorize. It produces a definite failure if
    the dynamic vector sizes (SSA values) do not satisfy the constraints
    mentioned above.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       Variadic<TransformAnyParamTypeOrAnyHandle>:$vector_sizes,
                       DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:
                          $static_vector_sizes,
                       OptionalAttr<UnitAttr>:$vectorize_nd_extract,
                       DefaultValuedOptionalAttr<DenseBoolArrayAttr, "{}">:
                          $scalable_sizes);

  let results = (outs);

  // We use oilist here to elide the optional `vector_sizes` when empty list
  // is passed.
  let assemblyFormat = [{
    $target oilist(
      `vector_sizes` custom<DynamicIndexList>(
        $vector_sizes,
        $static_vector_sizes,
        $scalable_sizes))
    attr-dict
    `:` type($target)(`,`type($vector_sizes)^)? 
  }];

  let hasVerifier = 1;

  let extraClassDeclaration = [{
    // TODO: applyToOne.
    ::mlir::DiagnosedSilenceableFailure apply(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::transform::TransformResults &transformResults,
        ::mlir::transform::TransformState &state);

    ::llvm::SmallVector<::mlir::OpFoldResult> getMixedVectorSizes();
  }];
}

//===----------------------------------------------------------------------===//
// HoistRedundantVectorTransfersOp
//===----------------------------------------------------------------------===//

def HoistRedundantVectorTransfersOp :
  Op<Transform_Dialect, "structured.hoist_redundant_vector_transfers",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     TransformEachOpTrait, TransformOpInterface,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Hoist vector.transfer_read / vector.transfer_write pairs out of immediately
    enclosing scf::ForOp iteratively, if the following conditions are true:
       1. The 2 ops access the same memref with the same indices.
       2. All operands are invariant under the enclosing scf::ForOp.
       3. No uses of the memref either dominate the transfer_read or are
       dominated by the transfer_write (i.e. no aliasing between the write and
       the read across the loop)

    WARNING: This hoisting does not model parallelism and is generally incorrect
    when used on distributed loops with memref semantics!
    TODO: obsolete and should be retired.

    #### Return modes:

    The operation always succeeds and returns a handle to the transformed
    function op.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                   UnitAttr:$verify_non_zero_trip);
  let results = (outs TransformHandleTypeInterface:$transformed);

  let assemblyFormat = "$target attr-dict `:` functional-type(operands, results) ";

  let builders = [
    OpBuilder<(ins "Value":$target,
               CArg<"bool", "false">:$verify_non_zero_trip)>,
  ];
  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
         ::mlir::transform::TransformRewriter &rewriter,
         ::mlir::func::FuncOp target,
         ::mlir::transform::ApplyToEachResultList &results,
         ::mlir::transform::TransformState &state);
   }];
}

//===----------------------------------------------------------------------===//
// HoistRedundantVectorBroadcastsOp
//===----------------------------------------------------------------------===//

def HoistRedundantVectorBroadcastsOp :
  Op<Transform_Dialect, "structured.hoist_redundant_vector_broadcasts",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     TransformEachOpTrait, TransformOpInterface,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Hoist vector.extract / vector.broadcasts pairs out of immediately
    enclosing scf::ForOp iteratively.

    #### Return modes:

    The operation always succeeds and returns a handle to the transformed
    function op.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs TransformHandleTypeInterface:$transformed);

  let assemblyFormat = "$target attr-dict `:` functional-type(operands, results) ";

  let builders = [
    OpBuilder<(ins "Value":$target)>,
  ];
  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
         ::mlir::transform::TransformRewriter &rewriter,
         ::mlir::Operation *target,
         ::mlir::transform::ApplyToEachResultList &results,
         ::mlir::transform::TransformState &state);
   }];
}

//===----------------------------------------------------------------------===//
// ConvertConv2DToImg2ColOp
//===----------------------------------------------------------------------===//

def ConvertConv2DToImg2ColOp : Op<Transform_Dialect,
    "structured.convert_conv2d_to_img2col",
    [FunctionalStyleTransformOpTrait,
     MemoryEffectsOpInterface,
     TransformOpInterface,
     TransformEachOpTrait,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Convert linalg.conv_2d_xxx into linalg.generic (for img2col packing)
    and linalg.matmul.

    A convolution operation can be written as a matrix-matrix multiplication by
    unfolding the cross-correlation between input and filter and explicitly copy
    overlapped sliding window inputs.

    Consider 2D input X with single channel input and output and 2x2 filter W:
    ```
    [x(0, 0)  , x(0, 1)  , ...,   x(0, n)  ]
    [x(1, 0)  , x(1, 1)  , ...,   x(1, n)  ]
    [.        ,  .       ,.   ,      .     ]            [w(0, 0), w(0, 1)]
    [.        ,  .       , .  ,      .     ]    (conv)  [w(1, 0), w(1, 1)]
    [.        ,  .       ,   .,      .     ]
    [x(n-1, 0), x(n-1, 1), ..., x(n-1, n-1)]
    ```

    The packed input data (img2col) is a matrix with |rows| = output spatial
    size, |columns| = filter spatial size. To compute the output Y(i, j) we need
    to calculate the dot product between filter window at input X(x, y)) and the
    filter which will look like the following where r.h.s is the img2col matrix
    and l.h.s is the flattned filter:
    ```
    [x(0,0), x(0,1), x(1,0), x(1,1)]
    [x(0,1), x(1,1), x(0,2), x(1,2)] (matmul) [w(0,0), w(0,1), w(1,0), w(1,1)]
    [x(0,1), x(1,1), x(0,2), x(1,2)]
    [   .  ,    .  ,    .  ,    .  ]
    ```

    In general for 2D case with (N, H, W, C) input and (Kh, Kw, C, D) filter
    and output (N, Ho, Wo, D) the convolution is the following matrix-matrix
    multiplication (Ho x Wo, Kh x Kw x C) * (Kh x Kw x C, D) for each input in
    the N input. For the case where N > 1 its a batched matrxi-matrix
    multplication.

    Returns two handles:
    - One on the operation that produces the img2col tensor.
    - One on the final operation of the sequence that replaces the original
      convolution.

    #### Return modes:

    Returns a definite failure if target is not isolated from above.
    Returns a silenceable failure if the pattern application failed.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs TransformHandleTypeInterface:$img2col_tensor,
                      TransformHandleTypeInterface:$transformed);

  let assemblyFormat =
    "$target attr-dict `:` functional-type($target, results)";

  let builders = [
    OpBuilder<(ins "Value":$target)>
  ];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::linalg::LinalgOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// FlattenElementwiseLinalgOp
//===----------------------------------------------------------------------===//

def FlattenElementwiseLinalgOp : Op<Transform_Dialect,
    "structured.flatten_elementwise",
    [FunctionalStyleTransformOpTrait,
     MemoryEffectsOpInterface,
     TransformOpInterface,
     TransformEachOpTrait,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Flattens the iteration space and (applicable) operands of elementwise
    linalg ops to a single dimension.

    Returns one handle:
    - Flattened linalg operation.

    #### Return modes:

    Returns a definite failure if target is not isolated from above.
    Returns a silenceable failure if the pattern application failed.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs TransformHandleTypeInterface:$transformed);

  let assemblyFormat =
    "$target attr-dict `:` functional-type($target, results)";

  let builders = [
    OpBuilder<(ins "Value":$target)>
  ];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::linalg::LinalgOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// Transpose Conv2D
//===----------------------------------------------------------------------===//

def TransposeConv2DOp : Op<Transform_Dialect,
    "structured.transpose_conv2d",
    [FunctionalStyleTransformOpTrait,
     MemoryEffectsOpInterface,
     TransformOpInterface,
     TransformEachOpTrait,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Convert linalg.conv_2d_nhwc_fhwc into linalg.conv_2d_nhwc_hwcf by introducing
    a linalg.transpose on the filter tensor/memref.

    Whilst the fhwc filter channel ordering can be desirable for certain targets
    and is a more direct mapping to higher level dialects such as TOSA (which only
    supports this ordering) hwcf is better suited for transformations such as
    img2col which can make use of optimized BLAS routines such as GEMM.

    Returns one handle:
    - The final operation of the sequence that replaces the original
      convolution.

    #### Return modes:

    Returns a definite failure if target is not isolated from above.
    Returns a silenceable failure if the pattern application failed.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs TransformHandleTypeInterface:$transformed);

  let assemblyFormat =
    "$target attr-dict `:` functional-type($target, results)";

  let builders = [
    OpBuilder<(ins "Value":$target)>
  ];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::linalg::LinalgOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// TransposeMatmulOp
//===----------------------------------------------------------------------===//

def TransposeMatmulOp : Op<Transform_Dialect,
    "structured.transpose_matmul",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     TransformOpInterface, TransformEachOpTrait,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Convert Linalg matmul ops to transposed variants.

    By default the LHS matrix is transposed. Specify `<rhs>` to instead
    transpose RHS matrix.

    #### Return modes:

    This operation fails if `target` is unsupported, i.e., not a
    `linalg.matmul` or `linalg.batch_matmul`. Otherwise, the operation succeeds
    and returns a handle to the transposed matmul op.
  }];

  let arguments = (ins
    TransformHandleTypeInterface:$target,
    DefaultValuedAttr<TransposeMatmulInput,
                      "TransposeMatmulInput::lhs">:$inputToTranspose);
  let results = (outs TransformHandleTypeInterface:$transformed);

  let assemblyFormat = [{
    $target (`<` $inputToTranspose^ `>`)?
    attr-dict `:` functional-type($target, results)
  }];

  let builders = [
    OpBuilder<(ins "Value":$target)>
  ];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::linalg::LinalgOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// InsertSliceToCopyOp
//===----------------------------------------------------------------------===//

def InsertSliceToCopyOp :
  Op<Transform_Dialect, "structured.insert_slice_to_copy",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     TransformEachOpTrait, TransformOpInterface]> {
  let description = [{
    Targeted rewrite of an tensor.insert_slice to linalg.copy.
    This is useful to materialize copies explicitly before bufferization and
    transform them, avoiding the need to rediscover them after bufferization.

    If the insert_slice source is already a linalg.copy, only return the source
    op (i.e. do not create an additional linalg.copy op).

    #### Return modes:

    The operation always succeeds and returns a handle to the relevant
    linalg.copy op.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs TransformHandleTypeInterface:$transformed);

  let assemblyFormat = "$target attr-dict `:` functional-type(operands, results) ";

  let builders = [
    OpBuilder<(ins "Value":$target)>,
  ];
  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::Operation *target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// MapCopyToThreadsOp
//===----------------------------------------------------------------------===//

def MapCopyToThreadsOp :
  Op<Transform_Dialect, "structured.gpu.map_copy_to_threads",
    [FunctionalStyleTransformOpTrait,
     MemoryEffectsOpInterface,
     TransformEachOpTrait,
     TransformOpInterface]> {
  let description = [{
    Targeted mapping of a linalg.copy / tensor.pad operation on tensors to a GPU
    thread mapping.

    This operation implements a greedy heuristic that determines a good
    distribution of threads to break down the copy/pad operation into.
    The heuristic is driven by considerations related to the underlying
    architecture for which good high-level decisions are needed assuming certain
    hardware features. Relevant features are exposed via first-class attributes
    to control the behavior of the transformation at a high level.

    For now, a single heuristic is implemented and can be extended on a per-need
    basis.

    #### Return modes

    This operation fails definitely if there is an unsupported op (i.e., not
    linalg.copy / tensor.pad) among the targeted op. Otherwise, the operation
    always succeeds and returns a handle to the relevant tiled linalg.copy /
    tensor.pad op and the enclosing scf.forall op.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       I64Attr:$total_num_threads,
                       I64Attr:$desired_bit_alignment);
  let results = (outs TransformHandleTypeInterface:$forall_op,
                      TransformHandleTypeInterface:$tiled_op);

  let assemblyFormat = [{
    $target
    `total_num_threads` `=` $total_num_threads
    `desired_bit_alignment` `=` $desired_bit_alignment
    attr-dict
    `:` functional-type(operands, results)
  }];

  let builders = [
    OpBuilder<(ins "Value":$target)>,
  ];
  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::Operation *target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);

    ::llvm::SmallVector<::mlir::OpFoldResult> getMixedNumThreads();
  }];
}

//===----------------------------------------------------------------------===//
// Winograd Conv2D
//===----------------------------------------------------------------------===//

def WinogradConv2DOp : Op<Transform_Dialect,
    "structured.winograd_conv2d",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     TransformOpInterface, TransformEachOpTrait,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Winograd Conv2D algorithm will convert linalg Conv2D operation into batched
    matrix multiply. Before the matrix multiply, it will convert filter and
    input into a format suitable for batched matrix multiply. After the matrix
    multiply, it will convert output to the final result tensor.

    The algorithm F(m x m, r x r) is

    Y = A^T x [(G x g x G^T) @ (B^T x d x B)] x A

    The size of output Y is m x m. The size of filter g is r x r. The size of
    input d is (m + r - 1) x (m + r - 1). A^T, A, G^T, G, B^T, and B are
    transformation matrices.

    #### Return modes:

    This operation produces a silenceable failure if `target` is unsupported.
    Otherwise, the operation succeeds and returns a handle of the sequence that
    replaces the original convolution.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       I64Attr:$m,
                       I64Attr:$r);
  let results = (outs TransformHandleTypeInterface:$transformed);

  let assemblyFormat =
    "$target attr-dict `:` functional-type($target, results)";

  let builders = [
    OpBuilder<(ins "Value":$target)>
  ];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::linalg::LinalgOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

def DecomposeWinogradOp : Op<Transform_Dialect,
    "structured.decompose_winograd_op",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     TransformOpInterface, TransformEachOpTrait,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Decompose winograd operations. It will convert filter, input and output
    transform operations into a combination of scf, tensor, and linalg
    equivalent operations. Before applying this transform operations, users
    need to tile winograd transform operations into supported sizes.

    #### Return modes:

    This operation fails if `target` is unsupported. Otherwise, the operation
    succeeds and returns a handle of the sequence that replaces the original
    operations.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs TransformHandleTypeInterface:$transformed);

  let assemblyFormat =
    "$target attr-dict `:` functional-type($target, results)";

  let builders = [
    OpBuilder<(ins "Value":$target)>
  ];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::Operation *target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

#endif // LINALG_TRANSFORM_OPS


//=== SparseTensorTransformOps.td ----------------------------*-tablegen *-===//
//
// Tablegen rules for Sparse Tensor transformation operations.
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef SPARSETENSOR_TRANSFORM_OPS
#define SPARSETENSOR_TRANSFORM_OPS

include "mlir/Dialect/Transform/Interfaces/MatchInterfaces.td"
include "mlir/Dialect/Transform/IR/TransformAttrs.td"
include "mlir/Dialect/Transform/IR/TransformDialect.td"
include "mlir/Dialect/Transform/IR/TransformTypes.td"
include "mlir/Interfaces/SideEffectInterfaces.td"

//===----------------------------------------------------------------------===//
// SparseTensorMatchOps
//===----------------------------------------------------------------------===//

def MatchSparseInOut : Op<Transform_Dialect, "sparse_tensor.match.sparse_inout", [
     MatchOpInterface,
     SingleOpMatcher,
     MemoryEffectsOpInterface]> {
  let description = [{
     Checks if the payload op has any sparse inputs and/or outputs.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs TransformHandleTypeInterface:$result);

  let assemblyFormat = [{
    $target attr-dict `:`
    custom<SemiFunctionType>(type($target), type($result), "false")
  }];
  let extraClassDeclaration = SingleOpMatcher.extraDeclaration # [{
    ::mlir::Value getOperandHandle() { return getTarget(); }
  }];
}

#endif // SPARSETENSOR_TRANSFORM_OPS


//===- LoopExtensionOps.td - Transform dialect operations --*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef MLIR_DIALECT_TRANSFORM_LOOPEXTENSION_LOOPEXTENSIONOPS
#define MLIR_DIALECT_TRANSFORM_LOOPEXTENSION_LOOPEXTENSIONOPS

include "mlir/Dialect/Transform/IR/TransformDialect.td"
include "mlir/Dialect/Transform/Interfaces/TransformInterfaces.td"
include "mlir/Interfaces/SideEffectInterfaces.td"

def HoistLoopInvariantSubsetsOp
    : TransformDialectOp<"loop.hoist_loop_invariant_subsets",
        [TransformOpInterface, TransformEachOpTrait,
         DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
         ReportTrackingListenerFailuresOpTrait]> {
  let summary = "Hoist loop invariant subset ops";
  let description = [{
    This transform hoists loop-invariant subset ops out of the targeted
    loop-like op. It looks for matching subset extraction/insertion op pairs and
    hoists them. The loop body operates on a newly introduced region iter_arg.

    Subset ops are hoisted only from the targeted op. If subset ops should be
    hoisted from an entire loop nest, this transformation must be applied to
    each loop-like op of the loop nest, starting with the innermost loop and
    ending with the outermost loop.

    Example:
    ```
    %r = scf.for ... iter_args(%t = %a) -> (tensor<?xf32>) {
      %0 = tensor.extract_slice %t[0][5][1] : tensor<?xf32> to tensor<5xf32>
      %1 = "test.foo"(%0) : (tensor<5xf32>) -> (tensor<5xf32>)
      %2 = tensor.insert_slice %1 into %t[0][5][1]
          : tensor<5xf32> into tensor<?xf32>
      scf.yield %2 : tensor<?xf32>
    }
    ```
    Is transformed to:
    ```
    %0 = tensor.extract_slice %a[0][5][1] : tensor<?xf32> to tensor<5xf32>
    %new_loop:2 = scf.for ... iter_args(%t = %a, %h = %0) -> (tensor<?xf32>) {
      %1 = "test.foo"(%h) : (tensor<5xf32>) -> (tensor<5xf32>)
      scf.yield %t, %2 : tensor<?xf32>, tensor<5xf32>
    }
    %r = tensor.insert_slice %new_loop#1 into %new_loop#0
        : tensor<5xf32> into tensor<?xf32>
    ```

    Subset ops are hoisted only if there are no conflicting subset ops. E.g.,
    if there were a second overlapping extraction in the above example, no ops
    could be hoisted safely.

    This transform reads the target handle and modifies the payload. This
    transform does not invalidate any handles, but loop-like ops are replaced
    with new loop-like ops when a subset op is hoisted. The transform rewriter
    updates all handles accordingly.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs);
  let assemblyFormat = "$target attr-dict `:` type($target)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
      ::mlir::transform::TransformRewriter &rewriter,
      ::mlir::LoopLikeOpInterface loopLikeOp,
      ::mlir::transform::ApplyToEachResultList &results,
      ::mlir::transform::TransformState &state);
  }];
}

#endif // MLIR_DIALECT_TRANSFORM_LOOPEXTENSION_LOOPEXTENSIONOPS


//===- BufferizationOps.td - Bufferization op definitions --*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef BUFFERIZATION_OPS
#define BUFFERIZATION_OPS

include "mlir/Dialect/Bufferization/IR/AllocationOpInterface.td"
include "mlir/Dialect/Bufferization/IR/BufferViewFlowOpInterface.td"
include "mlir/Dialect/Bufferization/IR/BufferizableOpInterface.td"
include "mlir/Dialect/Bufferization/IR/BufferizationBase.td"
include "mlir/Interfaces/DestinationStyleOpInterface.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/Interfaces/SubsetOpInterface.td"
include "mlir/Interfaces/CopyOpInterface.td"

class Bufferization_Op<string mnemonic, list<Trait> traits = []>
    : Op<Bufferization_Dialect, mnemonic, traits>;

//===----------------------------------------------------------------------===//
// AllocTensorOp
//===----------------------------------------------------------------------===//

def Bufferization_AllocTensorOp : Bufferization_Op<"alloc_tensor",
    [AttrSizedOperandSegments, BufferizableOpInterface,
     DeclareOpInterfaceMethods<ReifyRankedShapedTypeOpInterface>]> {
  let summary = "allocate buffer for a tensor";

  let description = [{
    `bufferization.alloc_tensor` materializes an uninitialized tensor with a
    given shape (dynamic or static). It always bufferizes to a new buffer
    allocation of the given shape. The optional `copy` operand specifies the
    contents of the tensors. If no `copy` operand is specified, reading from the
    result of an `alloc_tensor` op yields an undefined value.

    If `copy` is specified, no dynamic sizes should be passed, since they are
    the same as the dynamic sizes of the `copy` operand.

    `alloc_tensor` is a helper op for bufferization. The operation is provided
    as an anchor that marks the beginning of a new tensor SSA use-def chain. It
    can be used to control in-place bufferization decisions during One-Shot
    Bufferize: The bufferized result of a `bufferization.alloc_tensor` does not
    alias with any other buffer, so it can be used to resolve read-after-write
    conflicts that would have been introduced by the in-place bufferization of
    another op.

    The optional `memory_space` attribute specifies the memory space when
    bufferizing this op. The memory space is inferred from `copy` if specified.
    If neither `copy` nor `memory_space` is specified, the default memory space
    is used during bufferization.

    The optional `size_hint` operand specifies the number of non-zero elements
    for sparse tensors. The value of `size_hint` should be not less than 1 and
    not larger than the linear size of the corresponding dense tensor type. If
    this requirement is not met, the behavior of the operator is undefined.

    Both dense and sparse tensor types are supported. The result of a
    `bufferization.alloc_tensor` is a tensor value that can be used like any
    other tensor value. In practice, it is often used as the "out" operand of
    another op. Sparse tensor allocations should always be used in a local
    construction operation and never escape the function boundary directly.

    Example:

    ```mlir
    %c = bufferization.alloc_tensor(%d1, %d2) : tensor<?x?xf32, #SparseMatrix>
    %0 = linalg.matmul
      ins(%a, %b: tensor<?x?xf32, #SparseMatrix>, tensor<?x?xf32, #SparseMatrix>)
      outs(%c: tensor<?x?xf32, #SparseMatrix>) -> tensor<?x?xf32, #SparseMatrix>
    return %0 : tensor<?x?xf32, #SparseMatrix>
    ```

    ```mlir
    %c = bufferization.alloc_tensor(%d1, %d2) size_hint = %noe
      : tensor<?x?xf32, #SparseMatrix>
    ```

    Note: An `alloc_tensor` with a `copy` should also be expressed as an
    `alloc_tensor` without `copy`, followed by a `copy_tensor`.
  }];

  let arguments = (ins Variadic<Index>:$dynamic_sizes,
                       Optional<AnyTensor>:$copy,
                       Optional<Index>:$size_hint,
                       OptionalAttr<AnyAttr>:$memory_space);

  let results = (outs AnyTensor:$result);

  let extraClassDeclaration = [{
    LogicalResult bufferize(RewriterBase &rewriter,
                            const BufferizationOptions &options);

    bool resultBufferizesToMemoryWrite(OpResult opResult,
                                       const AnalysisState &state);

    bool bufferizesToAllocation(Value value) { return true; }

    bool bufferizesToMemoryRead(OpOperand &opOperand,
                                const AnalysisState &state);

    bool bufferizesToMemoryWrite(OpOperand &opOperand,
                                 const AnalysisState &state);

    AliasingValueList getAliasingValues(
        OpOperand &opOperand, const AnalysisState &state);

    FailureOr<BaseMemRefType> getBufferType(
        Value value, const BufferizationOptions &options,
        SmallVector<Value> &invocationStack);

    RankedTensorType getType() {
      return ::llvm::cast<RankedTensorType>(getResult().getType());
    }

    // Return true if the size of the tensor is dynamic at `idx`
    bool isDynamicDim(unsigned idx) {
      return getType().isDynamicDim(idx);
    }

    // Return the argument position that contains the dynamic size of
    // the tensor at dimension `idx`. Asserts that the shape is
    // dynamic at that `idx`.
    unsigned getIndexOfDynamicSize(unsigned idx) {
      assert(!getCopy() && "no dim sizes specified when copying a tensor");
      assert(isDynamicDim(idx) && "expected dynamic size");
      ArrayRef<int64_t> shape = getType().getShape();
      return std::count_if(
          shape.begin(), shape.begin() + idx,
          [&](int64_t size) { return ShapedType::isDynamic(size); });
    }

    // Return the Value of the dynamic size of the tensor at dimension
    // `idx`. Asserts that the shape is dynamic at that `idx.
    Value getDynamicSize(OpBuilder &b, unsigned idx);

    // Assert that the size of the result tensor is static at `idx`
    // and return the shape.
    int64_t getStaticSize(unsigned idx) {
      assert(!isDynamicDim(idx) && "expected static size");
      return getType().getShape()[idx];
    }
  }];

  let builders = [
    // Build an op without `copy` or `memory_space` or `size_hint`.
    OpBuilder<(ins "RankedTensorType":$type, "ValueRange":$dynamicSizes)>,

    // Build an op without `memory_space` or `size_hint`.
    OpBuilder<(ins "RankedTensorType":$type, "ValueRange":$dynamicSizes,
                   "Value":$copy)>,

    // Build an op without `size_hint`.
    OpBuilder<(ins "TensorType":$type, "ValueRange":$dynamicSizes,
                   "Value":$copy, "IntegerAttr":$memory_space)>,
  ];

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// CloneOp
//===----------------------------------------------------------------------===//

def Bufferization_CloneOp : Bufferization_Op<"clone", [
    CopyOpInterface,
    MemoryEffectsOpInterface,
    DeclareOpInterfaceMethods<AllocationOpInterface, ["buildDealloc", "buildClone"]>
  ]> {
  let builders = [
    OpBuilder<(ins "Value":$value), [{
      return build($_builder, $_state, value.getType(), value);
    }]>];

  let summary = "clone a memref";
  let description = [{
    Clones the data in the input view into an implicitly defined output view.

    Usage:

    ```mlir
    %arg1 = bufferization.clone %arg0 : memref<?xf32> to memref<?xf32>
    ```

    Valid implementations of this operation may alias the input and output
    views or create an actual copy. Mutating the source or result
    of the clone operation after the clone operation thus leads to undefined
    behavior.
  }];

  let arguments = (ins Arg<AnyRankedOrUnrankedMemRef, "",
                           [MemRead<DefaultResource>]>:$input);
  let results = (outs Res<AnyRankedOrUnrankedMemRef, "",
                          [MemWrite<DefaultResource>,
                           MemAlloc<DefaultResource>]>:$output);

  let extraClassDeclaration = [{
    Value getSource() { return getInput(); }
    Value getTarget() { return getOutput(); }
  }];

  let assemblyFormat = "$input attr-dict `:` type($input) `to` type($output)";

  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// MaterializeInDestinationOp
//===----------------------------------------------------------------------===//

def Bufferization_MaterializeInDestinationOp
    : Bufferization_Op<"materialize_in_destination",
        [AllElementTypesMatch<["source", "dest"]>,
         BufferizableOpInterface, DestinationStyleOpInterface,
         DeclareOpInterfaceMethods<ReifyRankedShapedTypeOpInterface>,
         DeclareOpInterfaceMethods<SubsetOpInterface,
            ["operatesOnEquivalentSubset", "operatesOnDisjointSubset"]>,
         DeclareOpInterfaceMethods<SubsetInsertionOpInterface,
            ["getSourceOperand", "getValuesNeededToBuildSubsetExtraction",
             "buildSubsetExtraction", "isEquivalentSubset"]>,
         DeclareOpInterfaceMethods<MemoryEffectsOpInterface, ["getEffects"]>]> {
  let summary = "copy a tensor";

  let description = [{
    This op indicates that the data of the `source` tensor is guaranteed to
    materialize in `dest`, which can be a tensor or a memref. In case of a
    tensor, `source` materializes in the future buffer of `dest` and a the
    updated destination tensor is returned. If this is not possible, e.g.,
    because the destination tensor is read-only or because its original
    contents are still read later, the input IR fails to bufferize. In case of a
    memref, `source` materializes in `dest`, which is already a buffer. The op
    has no results in that case.

    `source`, `dest` and `result` (if present) must have the same runtime shape
    and element type. If the op has a result, the types of `result` and `dest`
    must match exactly (e.g., including any tensor encodings).

    By default, this op bufferizes to a memcpy from the future buffer of the
    `source` tensor to the future buffer of the `dest` tensor or to the `dest`
    buffer. However, transformations such as "empty tensor elimination" may
    rewrite IR such that a computation is performed directly in `dest` and no
    memcpy is needed.

    If `dest` is a buffer, the `writable` attribute must be specified and the
    `restrict` keyword can be specified. These attributes have the same meaning
    as the respective attributes of `bufferization.to_tensor`.

    `writable` indicates that the `dest` buffer is considered writable. It does
    not make sense to materialize a computation in a read-only buffer, so
    `writable` is required.

    `restrict` indicates that there is no `bufferization.to_tensor` op and no
    other `bufferization.materialize_in_destination` op with `dest` (or an alias
    thereof) and "restrict". Only ops with this attribute are considered for
    "empty tensor elimination". As part of empty tensor elimination, a new
    `to_tensor` op with `dest` may be inserted and the `restrict` attribute is
    transferred from this op to the new `to_tensor` op. Having "restrict" on
    this op guarantees that performing empty tensor elimination would not create
    invalid IR (i.e., having multiple `to_tensor restrict` with aliasing
    buffers).

    Note: `writable` could be removed from this op because it must always be set
    for memref destinations. This op has that attribute to make clear the
    requirements on the `dest` operand in the op assembly format.

    Note: If `dest` is a tensor, `tensor.insert_slice` could be used for the
    same purpose, but since tensor dialect ops only indicate *what* should be
    computed but not *where*, it could fold away, causing the computation to
    materialize in a different buffer.
  }];

  let arguments = (ins AnyTensor:$source, AnyShaped:$dest,
                       UnitAttr:$restrict, UnitAttr:$writable);
  let results = (outs Optional<AnyTensor>:$result);

  let extraClassDeclaration = [{
    LogicalResult bufferize(RewriterBase &rewriter,
                            const BufferizationOptions &options);

    bool bufferizesToMemoryRead(OpOperand &opOperand,
                                const AnalysisState &state);

    bool bufferizesToMemoryWrite(OpOperand &opOperand,
                                 const AnalysisState &state);

    bool bufferizesToElementwiseAccess(const AnalysisState &state,
                                       ArrayRef<OpOperand *> opOperands);

    bool mustBufferizeInPlace(OpOperand &opOperand,
                              const AnalysisState &state);

    AliasingValueList getAliasingValues(
        OpOperand &opOperand, const AnalysisState &state);

    RankedTensorType getType() {
      return ::llvm::cast<RankedTensorType>(getResult().getType());
    }

    MutableOperandRange getDpsInitsMutable();

    bool isWritable(Value value, const AnalysisState &state);
  }];

  let builders = [
    // Builder that materializes a source tensor in a tensor destination.
    // Asserts that `dest` has tensor type. Infers the result type of this op
    // from the destination tensor.
    OpBuilder<(ins "Value":$source, "Value":$dest)>
  ];

  let assemblyFormat = [{
    $source `in` (`restrict` $restrict^)? (`writable` $writable^)? $dest
        attr-dict `:` functional-type(operands, results)
  }];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// DeallocTensorOp
//===----------------------------------------------------------------------===//

def Bufferization_DeallocTensorOp : Bufferization_Op<"dealloc_tensor",
    [BufferizableOpInterface]> {
  string summary = "release underlying storage format of given tensor";
  string description = [{
    `bufferization.dealloc_tensor` is a buffer deallocation in tensor land. This
    op can be used for manual buffer deallocation. Some bufferizations (such as
    One-Shot Bufferize) take care of buffer deallocation, in which case this op
    is usually not needed. Details can be found in the documentation of the
    respective bufferization passes.

    In case of a dense tensor, this op lowers to a `memref.dealloc` op during
    bufferization.

    In case of a sparse tensor, this op releases the underlying sparse storage
    format for a tensor that materialized earlier through a `new` operation, a
    `convert` operation with annotated destination tensor type (unless the
    convert is folded away), or a `bufferization.alloc_tensor` operation. The
    release operation should only be called once for any materialized tensor.
    After this operation, any subsequent `memref` querying operation on the
    tensor returns undefined results.

    Example:

    ```mlir
    bufferization.dealloc_tensor %tensor : tensor<1024x1024xf64, #CSR>
    ```
  }];

  let arguments = (ins AnyTensor:$tensor);
  let results = (outs);
  let assemblyFormat = "$tensor attr-dict `:` type($tensor)";

  let extraClassDeclaration = [{
    bool bufferizesToMemoryRead(OpOperand &opOperand,
                                const AnalysisState &state) const {
      return false;
    }

    bool bufferizesToMemoryWrite(OpOperand &opOperand,
                                 const AnalysisState &state) const {
      return false;
    }

    AliasingValueList getAliasingValues(
        OpOperand &opOperand, const AnalysisState &state) const {
      return {};
    }

    LogicalResult bufferize(RewriterBase &rewriter,
                            const BufferizationOptions &options);
  }];
}

//===----------------------------------------------------------------------===//
// ToTensorOp
//===----------------------------------------------------------------------===//

def Bufferization_ToTensorOp : Bufferization_Op<"to_tensor", [
    BufferizableOpInterface,
    SameOperandsAndResultShape,
    SameOperandsAndResultElementType,
    AllElementTypesMatch<["memref", "result"]>
  ]> {
  let summary = "create a tensor from a `memref`";
  let description = [{
    An operation that creates a tensor from a `memref`. The result value is a
    tensor whose shape and element type match the memref operand.

    The opposite of this op is `to_memref`. Together, these two ops are
    useful for source/target materializations when doing type conversions
    involving tensors and memrefs.

    Example:

    ```mlir
    // Produces a value of tensor<4x?xf32> type.
    %t = bufferization.to_tensor %m : memref<4x?xf32, #layout, 0> to tensor<4x?xf32>
    ```

    If the `writable` unit attribute is set, the produced tensor is considered
    "writable" during bufferization. Otherwise, every OpOperand that bufferizes
    to a write to the future buffer of the resulting tensor (or an alias
    thereof) will bufferize out-of-place to prevent emitting any writes to
    `memref` during bufferization.

    The `restrict` unit attribute (similar to the C `restrict` keyword)
    indicates that the produced tensor result is the only way for the tensor
    IR to gain access to the `memref` operand (or an alias thereof). E.g.,
    there must be no other `to_tensor` op with the same or with an aliasing
    `memref` operand.

    Note: Only `to_tensor` ops with the `restrict` unit attribute are supported
    by One-Shot Bufferize. Other IR is rejected. (To support `to_tensor`
    without `restrict`, One-Shot Bufferize would have to analyze memref IR.)
    Ops that have incorrect usage of `restrict` may bufferize incorrectly.

    Example:

    ```
    %t = bufferization.to_tensor %m restrict writable : memref<4xf32> to tensor<4xf32>

    // %t is writable, so the tensor.insert may bufferize in-place in the
    // absence of other conflicts.
    %r = tensor.insert %f into %t[%idx] : tensor<4xf32>
    ```

    `to_tensor` ops are not bufferized. They are expected to fold away after
    bufferization. If there are non-bufferizable ops in the IR and
    `allowUnknownOps` is set, they may be part of the resulting IR and not fold
    away. However, such IR is no longer bufferizable with One-Shot Bufferize.
  }];

  let arguments = (ins Arg<AnyRankedOrUnrankedMemRef,
                           "the reference to load from",
                           [MemReadAt<0, FullEffect>]>:$memref,
                       UnitAttr:$restrict, UnitAttr:$writable);
  let results = (outs AnyTensor:$result);

  let extraClassDeclaration = [{
    /// The result of a to_tensor is always a tensor.
    TensorType getType() {
      Type resultType = getResult().getType();
      if (::llvm::isa<TensorType>(resultType))
        return ::llvm::cast<TensorType>(resultType);
      return {};
    }

    //===------------------------------------------------------------------===//
    // BufferizableOpInterface implementation
    //===------------------------------------------------------------------===//

    LogicalResult bufferize(RewriterBase &rewriter,
                            const BufferizationOptions &options) const {
      // to_tensor/to_memref pairs fold away after bufferization.
      return success();
    }

    bool isWritable(Value value, const AnalysisState &state);

    FailureOr<BaseMemRefType> getBufferType(
        Value value, const BufferizationOptions &options,
        SmallVector<Value> &invocationStack) {
      return ::llvm::cast<BaseMemRefType>(getMemref().getType());
    }
  }];

  let assemblyFormat = [{
    $memref (`restrict` $restrict^)? (`writable` $writable^)? attr-dict
      `:` type($memref) `to` type($result)
  }];

  let builders = [
    OpBuilder<(ins "Value":$memref, CArg<"bool", "false">:$restrict, CArg<"bool", "false">:$writeable), [{
      auto rtt = memref::getTensorTypeFromMemRefType(memref.getType());
      build($_builder, $_state, rtt, memref, restrict, writeable);
    }]>
  ];

  let hasCanonicalizer = 1;
  let hasFolder = 1;
}


//===----------------------------------------------------------------------===//
// ToMemrefOp
//===----------------------------------------------------------------------===//

def Bufferization_ToMemrefOp : Bufferization_Op<"to_memref", [
    BufferizableOpInterface,
    SameOperandsAndResultShape,
    SameOperandsAndResultElementType,
    Pure,
    AllShapesMatch<["memref", "tensor"]>,
    AllElementTypesMatch<["memref", "tensor"]>
  ]> {
  let summary = "cast a tensor to memref";
  let description = [{
    An operation that returns the future buffer of a `tensor`.

    ```mlir
    // Result type is memref<4x?xf32, #layout, 0>
    %m = bufferization.to_memref %t : tensor<4x?xf32> to memref<4x?xf32, #layout, 0>
    ```

    This operation is a specialized variant of the built-in
    `unrealized_conversion_cast` and is used to make sure that the IR stays
    valid at any point during the bufferization.

    The `read_only` attribute can optionally be set, indicating to the
    bufferization that the buffer returned by this op (or an alias created from
    the returned buffer) will not be written to.
  }];

  let arguments = (ins AnyTensor:$tensor, UnitAttr:$read_only);
  let results = (outs AnyRankedOrUnrankedMemRef:$memref);

  let extraClassDeclaration = [{
    //===------------------------------------------------------------------===//
    // BufferizableOpInterface implementation
    //===------------------------------------------------------------------===//

    // Note: ToMemrefOp / ToTensorOp are temporary ops that are inserted at the
    // bufferization boundary. When One-Shot bufferization is complete, there
    // should be no such ops left over. If `allowUnknownOps` (or after running a
    // partial bufferization pass), such ops may be part of the resulting IR,
    // but such IR may no longer be analyzable by One-Shot analysis.

    bool bufferizesToMemoryRead(OpOperand &opOperand,
                                const AnalysisState &state) const {
      // It is unknown whether the resulting memref will be read or not.
      return true;
    }

    bool bufferizesToMemoryWrite(OpOperand &opOperand,
                                 const AnalysisState &state) {
      return !getReadOnly();
    }

    AliasingValueList getAliasingValues(
        OpOperand &opOperand, const AnalysisState &state) const {
      return {};
    }

    LogicalResult bufferize(RewriterBase &rewriter,
                            const BufferizationOptions &options);
  }];

  let assemblyFormat = [{
    $tensor (`read_only` $read_only^)? attr-dict `:` type($tensor) `to` type($memref)
  }];

  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

def Bufferization_DeallocOp : Bufferization_Op<"dealloc", [
    AttrSizedOperandSegments, DeclareOpInterfaceMethods<InferTypeOpInterface>
  ]> {
  let summary = "deallocates the given memrefs if no alias is retained";
  let description = [{
    This operation deallocates each of the given memrefs if there is no alias
    to that memref in the list of retained memrefs and the corresponding
    condition value is set. This condition can be used to indicate and pass on
    ownership of memref values (or in other words, the responsibility of
    deallocating that memref). If two memrefs alias each other, only one will be
    deallocated to avoid double free situations.

    The number of variadic `memref` operands (the memrefs to be deallocated)
    must equal the number of variadic `condition` operands and correspond to
    each other element-wise.

    The `memref` operands must be the originally allocated memrefs, however, the
    `retained` memref operands may be arbitrary memrefs.

    This operation returns a variadic number of `updatedConditions` operands,
    one updated condition per retained memref. An updated condition indicates
    the ownership of the respective retained memref. It is computed as the
    disjunction of all `conditions` operands where the corresponding to
    `memrefs` operand aliases with the retained memref. If the retained memref
    has no aliases among `memrefs`, the resulting updated condition is 'false'.
    This is because all memrefs that need to be deallocated within one basic
    block should be added to the same `bufferization.dealloc` operation at the
    end of the block; if no aliasing memref is present, then it does not have to
    be deallocated and thus we don't need to claim ownership. If the memrefs to
    be deallocated are split over multiple dealloc operations (e.g., to avoid
    aliasing checks at runtime between the `memref` operands), then the results
    have to be manually combined using an `arith.ori` operation and all of them
    still require the same list of `retained` memref operands unless the
    (potentially empty) set of aliasing memrefs can be determined statically. In
    that case, the `updatedCondition` operand can be replaced accordingly (e.g.,
    by a canonicalizer).

    Example:
    ```mlir
    %0:3 = bufferization.dealloc (%a0, %a1 : memref<2xf32>, memref<4xi32>)
      if (%cond0, %cond1) retain (%r0, %r1, %r2 : memref<?xf32>, memref<f64>,
      memref<2xi32>)
    ```
    Deallocation will be called on `%a0` if `%cond0` is 'true' and neither
    `%r0`, `%r1`, or `%r2` are aliases of `%a0`. `%a1` will be deallocated when
    `%cond1` is set to 'true' and none of `%r0`, `%r1`, `%r2`, and `%a0` are
    aliases.

    Note that this can be an expensive operation if there are many operands that
    cannot be optimized away. The runtime cost of this operation (assuming that
    nothing is optimized away) is `O(|memrefs|^2+|memrefs|*|retained|)`. The
    cost in terms of memory space is `O(|memrefs|+|retained|)`. As a result, it
    is recommended to place it carefully in the IR such that most operands can
    be optimized away by running the `buffer-deallocation-simplification` pass.
  }];

  let arguments = (ins Variadic<AnyRankedOrUnrankedMemRef>:$memrefs,
                       Variadic<I1>:$conditions,
                       Variadic<AnyRankedOrUnrankedMemRef>:$retained);
  let results = (outs Variadic<I1>:$updatedConditions);

  let assemblyFormat = [{
    (` ``(` $memrefs^ `:` type($memrefs) `)` `if` ` ` `(` $conditions `)` )?
    (`retain` ` ` `(` $retained^ `:` type($retained) `)` )? attr-dict
  }];

  let hasVerifier = 1;
  let hasCanonicalizer = 1;
}

#endif // BUFFERIZATION_OPS


//===- AsyncOps.td - Async operations definition -----------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This is the operation definition file for Async dialect operations.
//
//===----------------------------------------------------------------------===//

#ifndef ASYNC_OPS
#define ASYNC_OPS

include "mlir/Dialect/Async/IR/AsyncDialect.td"
include "mlir/Dialect/Async/IR/AsyncTypes.td"
include "mlir/Interfaces/ControlFlowInterfaces.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/Interfaces/CallInterfaces.td"
include "mlir/IR/SymbolInterfaces.td"
include "mlir/Interfaces/FunctionInterfaces.td"
include "mlir/IR/OpAsmInterface.td"


//===----------------------------------------------------------------------===//
// Async op definitions
//===----------------------------------------------------------------------===//

// Base class for the operation in this dialect
class Async_Op<string mnemonic, list<Trait> traits = []> :
    Op<AsyncDialect, mnemonic, traits>;

def Async_ExecuteOp :
  Async_Op<"execute", [SingleBlockImplicitTerminator<"YieldOp">,
                       DeclareOpInterfaceMethods<RegionBranchOpInterface,
                                                 ["getEntrySuccessorOperands",
                                                  "areTypesCompatible"]>,
                       AttrSizedOperandSegments,
                       AutomaticAllocationScope]> {
  let summary = "Asynchronous execute operation";
  let description = [{
    The `body` region attached to the `async.execute` operation semantically
    can be executed concurrently with the successor operation. In the followup
    example "compute0" can be executed concurrently with "compute1".

    The actual concurrency semantics depends on the dialect lowering to the
    executable format. Fully sequential execution ("compute0" completes before
    "compute1" starts) is a completely legal execution.

    Because concurrent execution is not guaranteed, it is illegal to create an
    implicit dependency from "compute1" to "compute0" (e.g. via shared global
    state). All dependencies must be made explicit with async execute arguments
    (`async.token` or `async.value`).

   `async.execute` operation takes `async.token` dependencies and `async.value`
    operands separately, and starts execution of the attached body region only
    when all tokens and values become ready.

    Example:

    ```mlir
    %dependency = ... : !async.token
    %value = ... : !async.value<f32>

    %token, %results =
      async.execute [%dependency](%value as %unwrapped: !async.value<f32>)
                 -> !async.value<!some.type>
      {
        %0 = "compute0"(%unwrapped): (f32) -> !some.type
        async.yield %0 : !some.type
      }

    %1 = "compute1"(...) : !some.type
    ```

    In the example above asynchronous execution starts only after dependency
    token and value argument become ready. Unwrapped value passed to the
    attached body region as an %unwrapped value of f32 type.
  }];

  let arguments = (ins Variadic<Async_TokenType>:$dependencies,
                       Variadic<Async_AnyValueOrTokenType>:$bodyOperands);

  let results = (outs Async_TokenType:$token,
                      Variadic<Async_ValueType>:$bodyResults);
  let regions = (region SizedRegion<1>:$bodyRegion);

  let hasCustomAssemblyFormat = 1;
  let skipDefaultBuilders = 1;
  let hasRegionVerifier = 1;
  let builders = [
    OpBuilder<(ins "TypeRange":$resultTypes, "ValueRange":$dependencies,
      "ValueRange":$operands,
      CArg<"function_ref<void(OpBuilder &, Location, ValueRange)>",
           "nullptr">:$bodyBuilder)>,
  ];

  let extraClassDeclaration = [{
    using BodyBuilderFn =
        function_ref<void(OpBuilder &, Location, ValueRange)>;

  }];
}

def Async_FuncOp : Async_Op<"func",
    [FunctionOpInterface, IsolatedFromAbove, OpAsmOpInterface]> {
  let summary = "async function operation";
  let description = [{
    An async function is like a normal function, but supports non-blocking
    await. Internally, async function is lowered to the LLVM coroutinue with
    async runtime intrinsic. It can return an async token and/or async values.
    The token represents the execution state of async function and can be used
    when users want to express dependencies on some side effects, e.g.,
    the token becomes available once every thing in the func body is executed.

    Example:

    ```mlir
    // Async function can't return void, it always must be some async thing.
    async.func @async.0() -> !async.token {
      return
    }

    // Function returns only async value.
    async.func @async.1() -> !async.value<i32> {
      %0 = arith.constant 42 : i32
      return %0 : i32
    }

    // Implicit token can be added to return types.
    async.func @async.2() -> !async.token, !async.value<i32> {
      %0 = arith.constant 42 : i32
      return %0 : i32
    }
    ```
  }];

  let arguments = (ins SymbolNameAttr:$sym_name,
                       TypeAttrOf<FunctionType>:$function_type,
                       OptionalAttr<StrAttr>:$sym_visibility,
                       OptionalAttr<DictArrayAttr>:$arg_attrs,
                       OptionalAttr<DictArrayAttr>:$res_attrs);

  let regions = (region AnyRegion:$body);

  let builders = [
    OpBuilder<(ins "StringRef":$name, "FunctionType":$type,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs,
      CArg<"ArrayRef<DictionaryAttr>", "{}">:$argAttrs)>
  ];

  let extraClassDeclaration = [{
    //===------------------------------------------------------------------===//
    // FunctionOpInterface Methods
    //===------------------------------------------------------------------===//

    /// Returns the region on the current operation that is callable. This may
    /// return null in the case of an external callable object, e.g. an external
    /// function.
    ::mlir::Region *getCallableRegion() { return isExternal() ? nullptr
                                                              : &getBody(); }

    /// Returns the argument types of this async function.
    ArrayRef<Type> getArgumentTypes() { return getFunctionType().getInputs(); }

    /// Returns the result types of this async function.
    ArrayRef<Type> getResultTypes() { return getFunctionType().getResults(); }

    /// Returns the number of results of this async function
    unsigned getNumResults() {return getResultTypes().size();}

    /// Is the async func stateful
    bool isStateful() { return isa<TokenType>(getFunctionType().getResult(0));}

    //===------------------------------------------------------------------===//
    // OpAsmOpInterface Methods
    //===------------------------------------------------------------------===//

    /// Allow the dialect prefix to be omitted.
    static StringRef getDefaultDialect() { return "async"; }

    //===------------------------------------------------------------------===//
    // SymbolOpInterface Methods
    //===------------------------------------------------------------------===//

    bool isDeclaration() { return isExternal(); }
  }];
  let hasCustomAssemblyFormat = 1;

  let hasVerifier = 1;
}

def Async_CallOp : Async_Op<"call",
    [CallOpInterface, DeclareOpInterfaceMethods<SymbolUserOpInterface>]> {
  let summary = "async call operation";
  let description = [{
    The `async.call` operation represents a direct call to an async function
    that is within the same symbol scope as the call. The operands and result
    types of the call must match the specified async function type. The callee
    is encoded as a symbol reference attribute named "callee".

    Example:

    ```mlir
    %2 = async.call @my_add(%0, %1) : (f32, f32) -> !async.value<f32>
    ```
  }];

  let arguments = (ins FlatSymbolRefAttr:$callee, Variadic<AnyType>:$operands);
  let results = (outs Variadic<Async_AnyValueOrTokenType>);

  let builders = [
    OpBuilder<(ins "FuncOp":$callee, CArg<"ValueRange", "{}">:$operands), [{
      $_state.addOperands(operands);
      $_state.addAttribute("callee", SymbolRefAttr::get(callee));
      $_state.addTypes(callee.getFunctionType().getResults());
    }]>,
    OpBuilder<(ins "SymbolRefAttr":$callee, "TypeRange":$results,
      CArg<"ValueRange", "{}">:$operands), [{
      $_state.addOperands(operands);
      $_state.addAttribute("callee", callee);
      $_state.addTypes(results);
    }]>,
    OpBuilder<(ins "StringAttr":$callee, "TypeRange":$results,
      CArg<"ValueRange", "{}">:$operands), [{
      build($_builder, $_state, SymbolRefAttr::get(callee), results, operands);
    }]>,
    OpBuilder<(ins "StringRef":$callee, "TypeRange":$results,
      CArg<"ValueRange", "{}">:$operands), [{
      build($_builder, $_state, StringAttr::get($_builder.getContext(), callee),
            results, operands);
    }]>
  ];

  let extraClassDeclaration = [{
    FunctionType getCalleeType();

    /// Get the argument operands to the called function.
    operand_range getArgOperands() {
      return {arg_operand_begin(), arg_operand_end()};
    }

    MutableOperandRange getArgOperandsMutable() {
      return getOperandsMutable();
    }

    operand_iterator arg_operand_begin() { return operand_begin(); }
    operand_iterator arg_operand_end() { return operand_end(); }

    /// Return the callee of this operation.
    CallInterfaceCallable getCallableForCallee() {
      return (*this)->getAttrOfType<SymbolRefAttr>("callee");
    }

    /// Set the callee for this operation.
    void setCalleeFromCallable(CallInterfaceCallable callee) {
      (*this)->setAttr("callee", callee.get<SymbolRefAttr>());
    }
  }];

  let assemblyFormat = [{
    $callee `(` $operands `)` attr-dict `:` functional-type($operands, results)
  }];
}

def Async_ReturnOp : Async_Op<"return",
    [Pure, HasParent<"FuncOp">, ReturnLike, Terminator]> {
  let summary = "Async function return operation";
  let description = [{
    The `async.return` is a special terminator operation for Async function.

    Example:

    ```mlir
    async.func @foo() : !async.token {
      return
    }
    ```
  }];

  let arguments = (ins Variadic<AnyType>:$operands);

  let builders = [OpBuilder<(ins), [{build($_builder, $_state, std::nullopt);}]>];

  let assemblyFormat = "attr-dict ($operands^ `:` type($operands))?";
  let hasVerifier = 1;
}

def Async_YieldOp :
    Async_Op<"yield", [
      HasParent<"ExecuteOp">, Pure, Terminator, ReturnLike]> {
  let summary = "terminator for Async execute operation";
  let description = [{
    The `async.yield` is a special terminator operation for the block inside
    `async.execute` operation.
  }];

  let arguments = (ins Variadic<AnyType>:$operands);
  let assemblyFormat = "($operands^ `:` type($operands))? attr-dict";
}

def Async_AwaitOp : Async_Op<"await"> {
  let summary = "waits for the argument to become ready";
  let description = [{
    The `async.await` operation waits until the argument becomes ready, and for
    the `async.value` arguments it unwraps the underlying value

    Example:

    ```mlir
    %0 = ... : !async.token
    async.await %0 : !async.token

    %1 = ... : !async.value<f32>
    %2 = async.await %1 : !async.value<f32>
    ```
  }];

  let arguments = (ins Async_AnyValueOrTokenType:$operand);
  let results = (outs Optional<AnyType>:$result);

  let skipDefaultBuilders = 1;
  let hasVerifier = 1;

  let builders = [
    OpBuilder<(ins "Value":$operand,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
  ];

  let extraClassDeclaration = [{
    std::optional<Type> getResultType() {
      if (getResultTypes().empty()) return std::nullopt;
      return getResultTypes()[0];
    }
  }];

  let assemblyFormat = [{
    $operand `:` custom<AwaitResultType>(
      type($operand), type($result)
    ) attr-dict
  }];
}

def Async_CreateGroupOp : Async_Op<"create_group", [Pure]> {
  let summary = "creates an empty async group";
  let description = [{
    The `async.create_group` allocates an empty async group. Async tokens or
    values can be added to this group later. The size of the group must be
    specified at construction time, and `await_all` operation will first
    wait until the number of added tokens or values reaches the group size.

    Example:

    ```mlir
    %size = ... : index
    %group = async.create_group %size : !async.group
    ...
    async.await_all %group
    ```
  }];

  let arguments = (ins Index:$size);
  let results = (outs Async_GroupType:$result);

  let hasCanonicalizeMethod = 1;

  let assemblyFormat = "$size `:` type($result) attr-dict";
}

def Async_AddToGroupOp : Async_Op<"add_to_group", []> {
  let summary = "adds and async token or value to the group";
  let description = [{
    The `async.add_to_group` adds an async token or value to the async group.
    Returns the rank of the added element in the group. This rank is fixed
    for the group lifetime.

    Example:

    ```mlir
    %0 = async.create_group %size : !async.group
    %1 = ... : !async.token
    %2 = async.add_to_group %1, %0 : !async.token
    ```
  }];

  let arguments = (ins Async_AnyValueOrTokenType:$operand,
                       Async_GroupType:$group);
  let results = (outs Index:$rank);

  let assemblyFormat = "$operand `,` $group `:` type($operand) attr-dict";
}

def Async_AwaitAllOp : Async_Op<"await_all", []> {
  let summary = "waits for the all async tokens or values in the group to "
                "become ready";
  let description = [{
    The `async.await_all` operation waits until all the tokens or values in the
    group become ready.

    Example:

    ```mlir
    %0 = async.create_group %size : !async.group

    %1 = ... : !async.token
    %2 = async.add_to_group %1, %0 : !async.token

    %3 = ... : !async.token
    %4 = async.add_to_group %2, %0 : !async.token

    async.await_all %0
    ```
  }];

  let arguments = (ins Async_GroupType:$operand);
  let results = (outs);

  let assemblyFormat = "$operand attr-dict";
}

//===----------------------------------------------------------------------===//
// Async Dialect LLVM Coroutines Operations.
//===----------------------------------------------------------------------===//

// Async to LLVM dialect lowering converts async tasks (regions inside async
// execute operations) to LLVM coroutines [1], and relies on switched-resume
// lowering [2] to produce an asynchronous executable.
//
// We define LLVM coro intrinsics in the async dialect to facilitate progressive
// lowering with verifiable and type-safe IR during the multi-step lowering
// pipeline. First we convert from high level async operations (e.g. execute) to
// the explicit calls to coro intrinsics and runtime API, and then finalize
// lowering to LLVM with a simple dialect conversion pass.
//
// [1] https://llvm.org/docs/Coroutines.html
// [2] https://llvm.org/docs/Coroutines.html#switched-resume-lowering

def Async_CoroIdOp : Async_Op<"coro.id"> {
  let summary = "returns a switched-resume coroutine identifier";
  let description = [{
    The `async.coro.id` returns a switched-resume coroutine identifier.
  }];

  let results = (outs Async_CoroIdType:$id);
  let assemblyFormat = "attr-dict";
}

def Async_CoroBeginOp : Async_Op<"coro.begin"> {
  let summary = "returns a handle to the coroutine";
  let description = [{
    The `async.coro.begin` allocates a coroutine frame and returns a handle to
    the coroutine.
  }];

  let arguments = (ins Async_CoroIdType:$id);
  let results = (outs Async_CoroHandleType:$handle);
  let assemblyFormat = "$id attr-dict";
}

def Async_CoroFreeOp : Async_Op<"coro.free"> {
  let summary = "deallocates the coroutine frame";
  let description = [{
    The `async.coro.free` deallocates the coroutine frame created by the
    async.coro.begin operation.
  }];

  let arguments = (ins Async_CoroIdType:$id,
                       Async_CoroHandleType:$handle);
  let assemblyFormat = "$id `,` $handle attr-dict";
}

def Async_CoroEndOp : Async_Op<"coro.end"> {
  let summary = "marks the end of the coroutine in the suspend block";
  let description = [{
    The `async.coro.end` marks the point where a coroutine needs to return
    control back to the caller if it is not an initial invocation of the
    coroutine. It the start part of the coroutine is is no-op.
  }];

  let arguments = (ins Async_CoroHandleType:$handle);
  let assemblyFormat = "$handle attr-dict";
}

def Async_CoroSaveOp : Async_Op<"coro.save"> {
  let summary = "saves the coroutine state";
  let description = [{
    The `async.coro.saves` saves the coroutine state.
  }];

  let arguments = (ins Async_CoroHandleType:$handle);
  let results = (outs Async_CoroStateType:$state);
  let assemblyFormat = "$handle attr-dict";
}

def Async_CoroSuspendOp : Async_Op<"coro.suspend", [Terminator]> {
  let summary = "suspends the coroutine";
  let description = [{
    The `async.coro.suspend` suspends the coroutine and transfers control to the
    `suspend` successor. If suspended coroutine later resumed it will transfer
    control to the `resume` successor. If it is destroyed it will transfer
    control to the the `cleanup` successor.

    In switched-resume lowering coroutine can be already in resumed state when
    suspend operation is called, in this case control will be transferred to the
    `resume` successor skipping the `suspend` successor.
  }];

  let arguments = (ins Async_CoroStateType:$state);
  let successors = (successor AnySuccessor:$suspendDest,
                              AnySuccessor:$resumeDest,
                              AnySuccessor:$cleanupDest);
  let assemblyFormat =
    "$state `,` $suspendDest `,` $resumeDest  `,` $cleanupDest attr-dict";
}

//===----------------------------------------------------------------------===//
// Async Dialect Runtime Operations.
//===----------------------------------------------------------------------===//

// The following operations are intermediate async dialect operations to help
// lowering from high level async operation like `async.execute` to the Async
// Runtime API defined in the `ExecutionEngine/AsyncRuntime.h`.

def Async_RuntimeCreateOp : Async_Op<"runtime.create"> {
  let summary = "creates an async runtime token or value";
  let description = [{
    The `async.runtime.create` operation creates an async dialect token or
    value. Tokens and values are created in the non-ready state.
  }];

  let results = (outs Async_AnyValueOrTokenType:$result);
  let assemblyFormat = "attr-dict `:` type($result)";
}

def Async_RuntimeCreateGroupOp : Async_Op<"runtime.create_group"> {
  let summary = "creates an async runtime group";
  let description = [{
    The `async.runtime.create_group` operation creates an async dialect group
    of the given size. Group created in the empty state.
  }];

  let arguments = (ins Index:$size);
  let results = (outs Async_GroupType:$result);
  let assemblyFormat = "$size `:` type($result) attr-dict ";
}

def Async_RuntimeSetAvailableOp : Async_Op<"runtime.set_available"> {
  let summary = "switches token or value to available state";
  let description = [{
    The `async.runtime.set_available` operation switches async token or value
    state to available.
  }];

  let arguments = (ins Async_AnyValueOrTokenType:$operand);
  let assemblyFormat = "$operand attr-dict `:` type($operand)";
}

def Async_RuntimeSetErrorOp : Async_Op<"runtime.set_error"> {
  let summary = "switches token or value to error state";
  let description = [{
    The `async.runtime.set_error` operation switches async token or value
    state to error.
  }];

  let arguments = (ins Async_AnyValueOrTokenType:$operand);
  let assemblyFormat = "$operand attr-dict `:` type($operand)";
}

def Async_RuntimeIsErrorOp : Async_Op<"runtime.is_error"> {
  let summary = "returns true if token, value or group is in error state";
  let description = [{
    The `async.runtime.is_error` operation returns true if the token, value or
    group (any of the async runtime values) is in the error state. It is the
    caller responsibility to check error state after the call to `await` or
    resuming after `await_and_resume`.
  }];

  let arguments = (ins Async_AnyAsyncType:$operand);
  let results = (outs I1:$is_error);

  let assemblyFormat = "$operand attr-dict `:` type($operand)";
}

def Async_RuntimeAwaitOp : Async_Op<"runtime.await"> {
  let summary = "blocks the caller thread until the operand becomes available";
  let description = [{
    The `async.runtime.await` operation blocks the caller thread until the
    operand becomes available or error.
  }];

  let arguments = (ins Async_AnyAsyncType:$operand);
  let assemblyFormat = "$operand attr-dict `:` type($operand)";
}

def Async_RuntimeResumeOp : Async_Op<"runtime.resume"> {
  let summary = "resumes the coroutine on a thread managed by the runtime";
  let description = [{
    The `async.runtime.resume` operation resumes the coroutine on a thread
    managed by the runtime.
  }];

  let arguments = (ins Async_CoroHandleType:$handle);
  let assemblyFormat = "$handle attr-dict";
}

def Async_RuntimeAwaitAndResumeOp : Async_Op<"runtime.await_and_resume"> {
  let summary = "awaits the async operand and resumes the coroutine";
  let description = [{
    The `async.runtime.await_and_resume` operation awaits for the operand to
    become available or error and resumes the coroutine on a thread managed by
    the runtime.
  }];

  let arguments = (ins Async_AnyAsyncType:$operand,
                       Async_CoroHandleType:$handle);
  let assemblyFormat = "$operand `,` $handle attr-dict `:` type($operand)";
}

def Async_RuntimeStoreOp : Async_Op<"runtime.store",
      [TypesMatchWith<"type of 'value' matches element type of 'storage'",
                     "storage", "value",
                     "::llvm::cast<ValueType>($_self).getValueType()">]> {
  let summary = "stores the value into the runtime async.value";
  let description = [{
    The `async.runtime.store` operation stores the value into the runtime
    async.value storage.
  }];

  let arguments = (ins AnyType:$value,
                       Async_ValueType:$storage);
  let assemblyFormat = "$value `,` $storage attr-dict `:` type($storage)";
}

def Async_RuntimeLoadOp : Async_Op<"runtime.load",
      [TypesMatchWith<"type of 'value' matches element type of 'storage'",
                     "storage", "result",
                     "::llvm::cast<ValueType>($_self).getValueType()">]> {
  let summary = "loads the value from the runtime async.value";
  let description = [{
    The `async.runtime.load` operation loads the value from the runtime
    async.value storage.
  }];

  let arguments = (ins Async_ValueType:$storage);
  let results = (outs AnyType:$result);
  let assemblyFormat = "$storage attr-dict `:` type($storage)";
}

def Async_RuntimeAddToGroupOp : Async_Op<"runtime.add_to_group", []> {
  let summary = "adds and async token or value to the group";
  let description = [{
    The `async.runtime.add_to_group` adds an async token or value to the async
    group. Returns the rank of the added element in the group.
  }];

  let arguments = (ins Async_AnyValueOrTokenType:$operand,
                       Async_GroupType:$group);
  let results = (outs Index:$rank);

  let assemblyFormat = "$operand `,` $group attr-dict `:` type($operand)";
}

// All async values (values, tokens, groups) are reference counted at runtime
// and automatically destructed when reference count drops to 0.
//
// All values are semantically created with a reference count of +1 and it is
// the responsibility of the last async value user to drop reference count.
//
// Async values created when:
//   1. Operation returns async result (e.g. the result of an `async.execute`).
//   2. Async value passed in as a block argument.
//
// It is the responsibility of the async value user to extend the lifetime by
// adding a +1 reference, if the reference counted value captured by the
// asynchronously executed region (`async.execute` operation), and drop it after
// the last nested use.
//
// Reference counting operations can be added to the IR using automatic
// reference count pass, that relies on liveness analysis to find the last uses
// of all reference counted values and automatically inserts
// `drop_ref` operations.
//
// See `AsyncRefCountingPass` documentation for the implementation details.

def Async_RuntimeAddRefOp : Async_Op<"runtime.add_ref"> {
  let summary = "adds a reference to async value";
  let description = [{
    The `async.runtime.add_ref` operation adds a reference(s) to async value
    (token, value or group).
  }];

  let arguments = (ins Async_AnyAsyncType:$operand,
                       ConfinedAttr<I64Attr, [IntPositive]>:$count);

  let assemblyFormat = [{
    $operand attr-dict `:` type($operand)
  }];
}

def Async_RuntimeDropRefOp : Async_Op<"runtime.drop_ref"> {
  let summary = "drops a reference to async value";
  let description = [{
    The `async.runtime.drop_ref` operation drops a reference(s) to async value
    (token, value or group).
  }];

  let arguments = (ins Async_AnyAsyncType:$operand,
                       ConfinedAttr<I64Attr, [IntPositive]>:$count);

  let assemblyFormat = [{
    $operand attr-dict `:` type($operand)
  }];
}

def Async_RuntimeNumWorkerThreadsOp :
  Async_Op<"runtime.num_worker_threads",
           [DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "gets the number of threads in the threadpool from the runtime";
  let description = [{
    The `async.runtime.num_worker_threads` operation gets the number of threads
    in the threadpool from the runtime.
  }];

  let results = (outs Index:$result);
  let assemblyFormat = "attr-dict `:` type($result)";
}

#endif // ASYNC_OPS


//===- AffineOps.td - Affine operation definitions ---------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// Defines MLIR affine operations.
//
//===----------------------------------------------------------------------===//

#ifndef AFFINE_OPS
#define AFFINE_OPS

include "mlir/Dialect/Arith/IR/ArithBase.td"
include "mlir/Dialect/Affine/IR/AffineMemoryOpInterfaces.td"
include "mlir/Interfaces/ControlFlowInterfaces.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/LoopLikeInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"

def Affine_Dialect : Dialect {
  let name = "affine";
  let cppNamespace = "::mlir::affine";
  let hasConstantMaterializer = 1;
  let dependentDialects = ["arith::ArithDialect", "ub::UBDialect"];
}

// Base class for Affine dialect ops.
class Affine_Op<string mnemonic, list<Trait> traits = []> :
    Op<Affine_Dialect, mnemonic, traits>;

// Require regions to have affine.yield.
def ImplicitAffineTerminator
    : SingleBlockImplicitTerminator<"AffineYieldOp">;

def AffineApplyOp : Affine_Op<"apply", [Pure]> {
  let summary = "affine apply operation";
  let description = [{
    The `affine.apply` operation applies an [affine mapping](#affine-maps)
    to a list of SSA values, yielding a single SSA value. The number of
    dimension and symbol arguments to `affine.apply` must be equal to the
    respective number of dimensional and symbolic inputs to the affine mapping;
    the affine mapping has to be one-dimensional, and so the `affine.apply`
    operation always returns one value. The input operands and result must all
    have ‘index’ type.

    Example:

    ```mlir
    #map10 = affine_map<(d0, d1) -> (d0 floordiv 8 + d1 floordiv 128)>
    ...
    %1 = affine.apply #map10 (%s, %t)

    // Inline example.
    %2 = affine.apply affine_map<(i)[s0] -> (i+s0)> (%42)[%n]
    ```
  }];
  let arguments = (ins AffineMapAttr:$map, Variadic<Index>:$mapOperands);
  let results = (outs Index);

  // TODO: The auto-generated builders should check to see if the return type
  // has a constant builder. That way we wouldn't need to explicitly specify the
  // result types here.
  let builders = [
    OpBuilder<(ins "ArrayRef<AffineExpr> ":$exprList,"ValueRange":$mapOperands),
    [{
      build($_builder, $_state, $_builder.getIndexType(),
            AffineMap::inferFromExprList(exprList, $_builder.getContext())
                                        .front(), mapOperands);
    }]>
  ];

  let extraClassDeclaration = [{
    /// Returns the affine map to be applied by this operation.
    AffineMap getAffineMap() { return getMap(); }

    /// Returns the affine value map computed from this operation.
    AffineValueMap getAffineValueMap();

    /// Returns true if the result of this operation can be used as dimension id
    /// in the region of the closest surrounding op with trait AffineScope.
    bool isValidDim();

    /// Returns true if the result of this operation can be used as dimension id
    /// within 'region', i.e., for all its uses with `region`.
    bool isValidDim(Region *region);

    /// Returns true if the result of this operation is a symbol in the region
    /// of the closest surrounding op that has the trait AffineScope.
    bool isValidSymbol();

    /// Returns true if the result of this operation is a symbol for all its
    /// uses in `region`.
    bool isValidSymbol(Region *region);

    /// Returns all dimension operands.
    ValueRange getDimOperands() {
      return OperandRange{getOperands().begin(),
                          getOperands().begin() + getMap().getNumDims()};
    }

    /// Returns all symbol operands.
    ValueRange getSymbolOperands() {
      return OperandRange{getOperands().begin() + getMap().getNumDims(),
                          getOperands().end()};
    }
  }];

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

def AffineForOp : Affine_Op<"for",
    [AttrSizedOperandSegments, AutomaticAllocationScope,
     ImplicitAffineTerminator, ConditionallySpeculatable,
     RecursiveMemoryEffects, DeclareOpInterfaceMethods<LoopLikeOpInterface,
     ["getLoopInductionVars", "getLoopLowerBounds", "getLoopSteps",
      "getLoopUpperBounds", "getYieldedValuesMutable",
      "replaceWithAdditionalYields"]>,
     DeclareOpInterfaceMethods<RegionBranchOpInterface,
     ["getEntrySuccessorOperands"]>]> {
  let summary = "for operation";
  let description = [{
    Syntax:

    ```
    operation   ::= `affine.for` ssa-id `=` lower-bound `to` upper-bound
                    (`step` integer-literal)? `{` op* `}`

    lower-bound ::= `max`? affine-map-attribute dim-and-symbol-use-list | shorthand-bound
    upper-bound ::= `min`? affine-map-attribute dim-and-symbol-use-list | shorthand-bound
    shorthand-bound ::= ssa-id | `-`? integer-literal
    ```

    The `affine.for` operation represents an affine loop nest. It has one region
    containing its body. This region must contain one block that terminates with
    [`affine.yield`](#affineyield-mliraffineyieldop). *Note:* when
    `affine.for` is printed in custom format, the terminator is omitted. The
    block has one argument of [`index`](Builtin.md/#indextype) type that
    represents the induction variable of the loop.

    The `affine.for` operation executes its body a number of times iterating
    from a lower bound to an upper bound by a stride. The stride, represented by
    `step`, is a positive constant integer which defaults to "1" if not present.
    The lower and upper bounds specify a half-open range: the range includes the
    lower bound but does not include the upper bound.

    The lower and upper bounds of a `affine.for` operation are represented as an
    application of an affine mapping to a list of SSA values passed to the map.
    The [same restrictions](#restrictions-on-dimensions-and-symbols) hold for
    these SSA values as for all bindings of SSA values to dimensions and
    symbols.

    The affine mappings for the bounds may return multiple results, in which
    case the `max`/`min` keywords are required (for the lower/upper bound
    respectively), and the bound is the maximum/minimum of the returned values.
    There is no semantic ambiguity, but MLIR syntax requires the use of these
    keywords to make things more obvious to human readers.

    Many upper and lower bounds are simple, so MLIR accepts two custom form
    syntaxes: the form that accepts a single 'ssa-id' (e.g. `%N`) is shorthand
    for applying that SSA value to a function that maps a single symbol to
    itself, e.g., `()[s]->(s)()[%N]`. The integer literal form (e.g. `-42`) is
    shorthand for a nullary mapping function that returns the constant value
    (e.g. `()->(-42)()`).

    Example showing reverse iteration of the inner loop:

    ```mlir
    #map57 = affine_map<(d0)[s0] -> (s0 - d0 - 1)>

    func.func @simple_example(%A: memref<?x?xf32>, %B: memref<?x?xf32>) {
      %N = dim %A, 0 : memref<?x?xf32>
      affine.for %i = 0 to %N step 1 {
        affine.for %j = 0 to %N {   // implicitly steps by 1
          %0 = affine.apply #map57(%j)[%N]
          %tmp = call @F1(%A, %i, %0) : (memref<?x?xf32>, index, index)->(f32)
          call @F2(%tmp, %B, %i, %0) : (f32, memref<?x?xf32>, index, index)->()
        }
      }
      return
    }
    ```
    `affine.for` can also operate on loop-carried variables (`iter_args`) and
    return the final values after loop termination. The initial values of the
    variables are passed as additional SSA operands to the `affine.for`
    following the operands for the loop's lower and upper bounds. The
    operation's region has equivalent arguments for each variable representing
    the value of the variable at the current iteration.

    The region must terminate with an `affine.yield` that passes all the current
    iteration variables to the next iteration, or to the `affine.for`'s results
    if at the last iteration. For `affine.for`'s that execute zero iterations, the
    initial values of the loop-carried variables (corresponding to the SSA
    operands) will be the op's results.

    For example, to sum-reduce a memref:

     ```mlir
    func.func @reduce(%buffer: memref<1024xf32>) -> (f32) {
      // Initial sum set to 0.
      %sum_0 = arith.constant 0.0 : f32
      // iter_args binds initial values to the loop's region arguments.
      %sum = affine.for %i = 0 to 10 step 2
          iter_args(%sum_iter = %sum_0) -> (f32) {
        %t = affine.load %buffer[%i] : memref<1024xf32>
        %sum_next = arith.addf %sum_iter, %t : f32
        // Yield current iteration sum to next iteration %sum_iter or to %sum
        // if final iteration.
        affine.yield %sum_next : f32
      }
      return %sum : f32
    }
    ```

    ```mlir
    %res:2 = affine.for %i = 0 to 128 iter_args(%arg0 = %init0, %arg1 = %init1)
               -> (index, index) {
      %y0 = arith.addi %arg0, %c1 : index
      %y1 = arith.addi %arg1, %c2 : index
      affine.yield %y0, %y1 : index, index
    }
    ```
    If the `affine.for` defines any values, a yield terminator must be
    explicitly present. The number and types of the "affine.for" results must
    match the initial values in the `iter_args` binding and the yield operands.
  }];
  let arguments = (ins Variadic<Index>:$lowerBoundOperands,
                       Variadic<Index>:$upperBoundOperands,
                       Variadic<AnyType>:$inits,
                       AffineMapAttr:$lowerBoundMap,
                       AffineMapAttr:$upperBoundMap,
                       IndexAttr:$step);
  let results = (outs Variadic<AnyType>:$results);
  let regions = (region SizedRegion<1>:$region);

  let skipDefaultBuilders = 1;
  let builders = [
    OpBuilder<(ins "int64_t":$lowerBound, "int64_t":$upperBound,
      CArg<"int64_t", "1">:$step, CArg<"ValueRange", "std::nullopt">:$iterArgs,
      CArg<"function_ref<void(OpBuilder &, Location, Value, ValueRange)>",
           "nullptr">:$bodyBuilder)>,
    OpBuilder<(ins "ValueRange":$lbOperands, "AffineMap":$lbMap,
      "ValueRange":$ubOperands, "AffineMap":$ubMap, CArg<"int64_t", "1">:$step,
      CArg<"ValueRange", "std::nullopt">:$iterArgs,
      CArg<"function_ref<void(OpBuilder &, Location, Value, ValueRange)>",
           "nullptr">:$bodyBuilder)>
  ];

  let extraClassDeclaration = [{
    /// Defining the function type we use for building the body of affine.for.
    using BodyBuilderFn =
        function_ref<void(OpBuilder &, Location, Value, ValueRange)>;

    BlockArgument getInductionVar() { return getBody()->getArgument(0); }
    Block::BlockArgListType getRegionIterArgs() {
      return getBody()->getArguments().drop_front();
    }

    /// Returns operands for the lower and upper bound maps with the operands
    /// for the lower bound map in front of those for the upper bound map.
    operand_range getControlOperands();

    /// Returns information about the lower bound as a single object.
    AffineBound getLowerBound();

    /// Returns information about the upper bound as a single object.
    AffineBound getUpperBound();

    /// Returns loop step.
    int64_t getStepAsInt() { return getStep().getSExtValue(); }

    /// Set lower bound. The new bound must have the same number of operands as
    /// the current bound map. Otherwise, 'replaceForLowerBound' should be used.
    void setLowerBound(ValueRange operands, AffineMap map);
    /// Set upper bound. The new bound must not have more operands than the
    /// current bound map. Otherwise, 'replaceForUpperBound' should be used.
    void setUpperBound(ValueRange operands, AffineMap map);

    /// Set loop step.
    void setStep(int64_t step) {
      assert(step > 0 && "step has to be a positive integer constant");
      setStep(APInt(/*numBits=*/64, step, /*isSigned=*/true));
    }

    /// Returns number of region arguments for loop-carried values.
    unsigned getNumRegionIterArgs() {
      return getBody()->getNumArguments() - 1;
    }

    /// Number of operands controlling the loop: lb and ub.
    unsigned getNumControlOperands() {
      return getOperation()->getNumOperands() - getNumIterOperands();
    }

    /// Get the number of loop-carried values.
    unsigned getNumIterOperands();

    /// Returns true if the lower bound is constant.
    bool hasConstantLowerBound();
    /// Returns true if the upper bound is constant.
    bool hasConstantUpperBound();
    /// Returns true if both bounds are constant.
    bool hasConstantBounds() {
      return hasConstantLowerBound() && hasConstantUpperBound();
    }
    /// Returns the value of the constant lower bound.
    /// Fails assertion if the bound is non-constant.
    int64_t getConstantLowerBound();
    /// Returns the value of the constant upper bound. The upper bound is
    /// exclusive. Fails assertion if the bound is non-constant.
    int64_t getConstantUpperBound();
    /// Sets the lower bound to the given constant value.
    void setConstantLowerBound(int64_t value);
    /// Sets the upper bound to the given constant value.
    void setConstantUpperBound(int64_t value);

    /// Returns true if both the lower and upper bound have the same operand
    /// lists (same operands in the same order).
    bool matchingBoundOperandList();

    /// Interface method for ConditionallySpeculatable.
    Speculation::Speculatability getSpeculatability();
  }];

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasFolder = 1;
  let hasRegionVerifier = 1;
}

def AffineIfOp : Affine_Op<"if",
                           [ImplicitAffineTerminator, RecursivelySpeculatable,
                            RecursiveMemoryEffects, NoRegionArguments,
                            DeclareOpInterfaceMethods<RegionBranchOpInterface>
                           ]> {
  let summary = "if-then-else operation";
  let description = [{
    Syntax:

    ```
    operation  ::= `affine.if` if-op-cond `{` op* `}` (`else` `{` op* `}`)?
    if-op-cond ::= integer-set-attr dim-and-symbol-use-list
    ```

    The `affine.if` operation restricts execution to a subset of the loop
    iteration space defined by an integer set (a conjunction of affine
    constraints). A single `affine.if` may end with an optional `else` clause.

    The condition of the `affine.if` is represented by an
    [integer set](#integer-sets) (a conjunction of affine constraints),
    and the SSA values bound to the dimensions and symbols in the integer set.
    The [same restrictions](#restrictions-on-dimensions-and-symbols) hold for
    these SSA values as for all bindings of SSA values to dimensions and
    symbols.

    The `affine.if` operation contains two regions for the "then" and "else"
    clauses.  `affine.if` may return results that are defined in its regions.
    The values defined are determined by which execution path is taken.  Each
    region of the `affine.if` must contain a single block with no arguments,
    and be terminated by `affine.yield`.  If `affine.if` defines no values,
    the `affine.yield` can be left out, and will be inserted implicitly.
    Otherwise, it must be explicit.  If no values are defined, the else block
    may be empty (i.e. contain no blocks).

    Example:

    ```mlir
    #set = affine_set<(d0, d1)[s0]: (d0 - 10 >= 0, s0 - d0 - 9 >= 0,
                                     d1 - 10 >= 0, s0 - d1 - 9 >= 0)>
    func.func @reduced_domain_example(%A, %X, %N) : (memref<10xi32>, i32, i32) {
      affine.for %i = 0 to %N {
         affine.for %j = 0 to %N {
           %0 = affine.apply #map42(%j)
           %tmp = call @S1(%X, %i, %0)
           affine.if #set(%i, %j)[%N] {
              %1 = affine.apply #map43(%i, %j)
              call @S2(%tmp, %A, %i, %1)
           }
        }
      }
      return
    }
    ```

    Example with an explicit yield (initialization with edge padding):

    ```mlir
    #interior = affine_set<(i, j) : (i - 1 >= 0, j - 1 >= 0,  10 - i >= 0, 10 - j >= 0)> (%i, %j)
    func.func @pad_edges(%I : memref<10x10xf32>) -> (memref<12x12xf32) {
      %O = alloc memref<12x12xf32>
      affine.parallel (%i, %j) = (0, 0) to (12, 12) {
        %1 = affine.if #interior (%i, %j) {
          %2 = load %I[%i - 1, %j - 1] : memref<10x10xf32>
          affine.yield %2
        } else {
          %2 = arith.constant 0.0 : f32
          affine.yield %2 : f32
        }
        affine.store %1, %O[%i, %j] : memref<12x12xf32>
      }
      return %O
    }
    ```
  }];
  let arguments = (ins Variadic<AnyType>,
                       IntegerSetAttr:$condition);
  let results = (outs Variadic<AnyType>:$results);
  let regions = (region SizedRegion<1>:$thenRegion, AnyRegion:$elseRegion);

  let skipDefaultBuilders = 1;

  let builders = [
    OpBuilder<(ins "IntegerSet":$set, "ValueRange":$args,
      "bool":$withElseRegion)>,
    OpBuilder<(ins "TypeRange":$resultTypes, "IntegerSet":$set,
      "ValueRange":$args, "bool":$withElseRegion)>,
  ];

  let extraClassDeclaration = [{
    static StringRef getConditionAttrStrName() { return "condition"; }

    IntegerSet getIntegerSet();
    void setIntegerSet(IntegerSet newSet);

    /// Sets the integer set with its operands.
    void setConditional(IntegerSet set, ValueRange operands);

    /// Returns true if an else block exists.
    bool hasElse() { return !getElseRegion().empty(); }

    Block *getThenBlock() {
      assert(!getThenRegion().empty() && "Unexpected empty 'then' region.");
      return &getThenRegion().front();
    }

    Block *getElseBlock() {
      assert(hasElse() && "Empty 'else' region.");
      return &getElseRegion().front();
    }

    OpBuilder getThenBodyBuilder() {
      assert(!getThenRegion().empty() && "Unexpected empty 'then' region.");
      Block &body = getThenRegion().front();
      return OpBuilder(&body, std::prev(body.end()));
    }
    OpBuilder getElseBodyBuilder() {
      assert(hasElse() && "No 'else' block");
      Block &body = getElseRegion().front();
      return OpBuilder(&body, std::prev(body.end()));
    }
  }];

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

class AffineLoadOpBase<string mnemonic, list<Trait> traits = []> :
    Affine_Op<mnemonic, !listconcat(traits,
        [DeclareOpInterfaceMethods<AffineReadOpInterface>,
        DeclareOpInterfaceMethods<AffineMapAccessInterface>,
        MemRefsNormalizable])> {
  let arguments = (ins Arg<AnyMemRef, "the reference to load from",
      [MemRead]>:$memref,
      Variadic<Index>:$indices,
      AffineMapAttr:$map);

  code extraClassDeclarationBase = [{
    /// Returns the operand index of the memref.
    unsigned getMemRefOperandIndex() { return 0; }

    void setMemRef(Value value) { setOperand(getMemRefOperandIndex(), value); }

    /// Returns the affine map used to index the memref for this operation.
    AffineMapAttr getAffineMapAttr() {
      return getProperties().map;
    }

    static StringRef getMapAttrStrName() { return "map"; }
  }];
}

def AffineLoadOp : AffineLoadOpBase<"load"> {
  let summary = "affine load operation";
  let description = [{
    Syntax:

    ```
    operation ::= ssa-id `=` `affine.load` ssa-use `[` multi-dim-affine-map-of-ssa-ids `]` `:` memref-type
    ```

    The `affine.load` op reads an element from a memref, where the index
    for each memref dimension is an affine expression of loop induction
    variables and symbols. The output of `affine.load` is a new value with the
    same type as the elements of the memref. An affine expression of loop IVs
    and symbols must be specified for each dimension of the memref. The keyword
    `symbol` can be used to indicate SSA identifiers which are symbolic.

    Example 1:

    ```mlir
    %1 = affine.load %0[%i0 + 3, %i1 + 7] : memref<100x100xf32>
    ```

    Example 2: Uses `symbol` keyword for symbols `%n` and `%m`.

    ```mlir
    %1 = affine.load %0[%i0 + symbol(%n), %i1 + symbol(%m)] : memref<100x100xf32>
    ```
  }];

  let results = (outs AnyType:$result);

  let builders = [
    /// Builds an affine load op with the specified map and operands.
    OpBuilder<(ins "AffineMap":$map, "ValueRange":$operands)>,
    /// Builds an affine load op with an identity map and operands.
    OpBuilder<(ins "Value":$memref, CArg<"ValueRange", "{}">:$indices)>,
    /// Builds an affine load op with the specified map and its operands.
    OpBuilder<(ins "Value":$memref, "AffineMap":$map,
      "ValueRange":$mapOperands)>
  ];

  let extraClassDeclaration = extraClassDeclarationBase;

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

class AffineMinMaxOpBase<string mnemonic, list<Trait> traits = []> :
    Op<Affine_Dialect, mnemonic, traits> {
  let arguments = (ins AffineMapAttr:$map, Variadic<Index>:$operands);
  let results = (outs Index);

  let extraClassDeclaration = [{
    static StringRef getMapAttrStrName() { return "map"; }
    AffineMap getAffineMap() { return getMap(); }
    ValueRange getMapOperands() { return getOperands(); }
    ValueRange getDimOperands() {
      return OperandRange{getOperands().begin(),
                          getOperands().begin() + getMap().getNumDims()};
    }
    ValueRange getSymbolOperands() {
      return OperandRange{getOperands().begin() + getMap().getNumDims(),
                          getOperands().end()};
    }
  }];
  let hasCustomAssemblyFormat = 1;
  let hasFolder = 1;
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

def AffineMinOp : AffineMinMaxOpBase<"min", [Pure]> {
  let summary = "min operation";
  let description = [{
    Syntax:

    ```
    operation ::= ssa-id `=` `affine.min` affine-map-attribute dim-and-symbol-use-list
    ```

    The `affine.min` operation applies an [affine mapping](#affine-expressions)
    to a list of SSA values, and returns the minimum value of all result
    expressions. The number of dimension and symbol arguments to `affine.min`
    must be equal to the respective number of dimensional and symbolic inputs to
    the affine mapping; the `affine.min` operation always returns one value. The
    input operands and result must all have 'index' type.

    Example:

    ```mlir
    %0 = affine.min affine_map<(d0)[s0] -> (1000, d0 + 512, s0)> (%arg0)[%arg1]
    ```
  }];
}

def AffineMaxOp : AffineMinMaxOpBase<"max", [Pure]> {
  let summary = "max operation";
  let description = [{
    The `affine.max` operation computes the maximum value result from a multi-result
    affine map.

    Example:

    ```mlir
    %0 = affine.max (d0) -> (1000, d0 + 512) (%i0) : index
    ```
  }];
}

def AffineParallelOp : Affine_Op<"parallel",
    [AutomaticAllocationScope, ImplicitAffineTerminator, RecursivelySpeculatable,
     RecursiveMemoryEffects, DeclareOpInterfaceMethods<LoopLikeOpInterface>,
     MemRefsNormalizable]> {
  let summary = "multi-index parallel band operation";
  let description = [{
    The `affine.parallel` operation represents a hyper-rectangular affine
    parallel band, defining zero or more SSA values for its induction variables.
    It has one region capturing the parallel band body. The induction variables
    are represented as arguments of this region. These SSA values always have
    type index, which is the size of the machine word. The strides, represented
    by steps, are positive constant integers which defaults to "1" if not
    present. The lower and upper bounds specify a half-open range: the range
    includes the lower bound but does not include the upper bound. The body
    region must contain exactly one block that terminates with `affine.yield`.

    The lower and upper bounds of a parallel operation are represented as an
    application of an affine mapping to a list of SSA values passed to the map.
    The same restrictions hold for these SSA values as for all bindings of SSA
    values to dimensions and symbols. The list of expressions in each map is
    interpreted according to the respective bounds group attribute. If a single
    expression belongs to the group, then the result of this expression is taken
    as a lower(upper) bound of the corresponding loop induction variable. If
    multiple expressions belong to the group, then the lower(upper) bound is the
    max(min) of these values obtained from these expressions. The loop band has
    as many loops as elements in the group bounds attributes.

    Each value yielded by `affine.yield` will be accumulated/reduced via one of
    the reduction methods defined in the AtomicRMWKind enum.  The order of
    reduction is unspecified, and lowering may produce any valid ordering.
    Loops with a 0 trip count will produce as a result the identity value
    associated with each reduction (i.e. 0.0 for addf, 1.0 for mulf).  Assign
    reductions for loops with a trip count != 1 produces undefined results.

    Note: Calling `AffineParallelOp::build` will create the required region and
    block, and insert the required terminator if it is trivial (i.e. no values
    are yielded).  Parsing will also create the required region, block, and
    terminator, even when they are missing from the textual representation.

    Example (3x3 valid convolution):

    ```mlir
    func.func @conv_2d(%D : memref<100x100xf32>, %K : memref<3x3xf32>) -> (memref<98x98xf32>) {
      %O = memref.alloc() : memref<98x98xf32>
      affine.parallel (%x, %y) = (0, 0) to (98, 98) {
        %0 = affine.parallel (%kx, %ky) = (0, 0) to (2, 2) reduce ("addf") -> f32 {
          %1 = affine.load %D[%x + %kx, %y + %ky] : memref<100x100xf32>
          %2 = affine.load %K[%kx, %ky] : memref<3x3xf32>
          %3 = arith.mulf %1, %2 : f32
          affine.yield %3 : f32
        }
        affine.store %0, %O[%x, %y] : memref<98x98xf32>
      }
      return %O : memref<98x98xf32>
    }
    ```

    Example (tiling by potentially imperfectly dividing sizes):

    ```mlir
    affine.parallel (%ii, %jj) = (0, 0) to (%N, %M) step (32, 32) {
      affine.parallel (%i, %j) = (%ii, %jj)
                              to (min(%ii + 32, %N), min(%jj + 32, %M)) {
        call @f(%i, %j) : (index, index) -> ()
      }
    }
    ```
  }];

  let arguments = (ins
     TypedArrayAttrBase<AtomicRMWKindAttr, "Reduction ops">:$reductions,
     AffineMapAttr:$lowerBoundsMap,
     I32ElementsAttr:$lowerBoundsGroups,
     AffineMapAttr:$upperBoundsMap,
     I32ElementsAttr:$upperBoundsGroups,
     I64SmallVectorArrayAttr:$steps,
     Variadic<Index>:$mapOperands);
  let results = (outs Variadic<AnyType>:$results);
  let regions = (region SizedRegion<1>:$region);

  let builders = [
    OpBuilder<(ins "TypeRange":$resultTypes,
      "ArrayRef<arith::AtomicRMWKind>":$reductions, "ArrayRef<int64_t>":$ranges)>,
    OpBuilder<(ins "TypeRange":$resultTypes,
      "ArrayRef<arith::AtomicRMWKind>":$reductions, "ArrayRef<AffineMap>":$lbMaps,
      "ValueRange":$lbArgs, "ArrayRef<AffineMap>":$ubMaps, "ValueRange":$ubArgs,
      "ArrayRef<int64_t>":$steps)>
  ];

  let extraClassDeclaration = [{
    /// Get the number of dimensions.
    unsigned getNumDims();

    /// Get ranges as constants, may fail in dynamic case.
    std::optional<SmallVector<int64_t, 8>> getConstantRanges();

    Block *getBody();
    OpBuilder getBodyBuilder();
    MutableArrayRef<BlockArgument> getIVs() {
      return getBody()->getArguments();
    }

    /// Returns elements of the loop lower bound.
    AffineMap getLowerBoundMap(unsigned pos);
    operand_range getLowerBoundsOperands();
    AffineValueMap getLowerBoundsValueMap();

    /// Sets elements of the loop lower bound.
    void setLowerBounds(ValueRange operands, AffineMap map);

    /// Returns elements of the loop upper bound.
    AffineMap getUpperBoundMap(unsigned pos);
    operand_range getUpperBoundsOperands();
    AffineValueMap getUpperBoundsValueMap();

    /// Sets elements fo the loop upper bound.
    void setUpperBounds(ValueRange operands, AffineMap map);

    void setSteps(ArrayRef<int64_t> newSteps);

    /// Returns attribute names to use in op construction. Not expected to be
    /// used directly.
    static StringRef getReductionsAttrStrName() { return "reductions"; }
    static StringRef getLowerBoundsMapAttrStrName() { return "lowerBoundsMap"; }
    static StringRef getLowerBoundsGroupsAttrStrName() {
      return "lowerBoundsGroups";
    }
    static StringRef getUpperBoundsMapAttrStrName() { return "upperBoundsMap"; }
    static StringRef getUpperBoundsGroupsAttrStrName() {
      return "upperBoundsGroups";
    }
    static StringRef getStepsAttrStrName() { return "steps"; }

    /// Returns `true` if the loop bounds have min/max expressions.
    bool hasMinMaxBounds() {
      return getLowerBoundsMap().getNumResults() != getNumDims() ||
             getUpperBoundsMap().getNumResults() != getNumDims();
    }
  }];

  let hasCustomAssemblyFormat = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

def AffinePrefetchOp : Affine_Op<"prefetch",
  [DeclareOpInterfaceMethods<AffineMapAccessInterface>,
   MemRefsNormalizable]> {
  let summary = "affine prefetch operation";
  let description = [{
    The `affine.prefetch` op prefetches data from a memref location described
    with an affine subscript similar to affine.load, and has three attributes:
    a read/write specifier, a locality hint, and a cache type specifier as shown
    below:

    ```mlir
    affine.prefetch %0[%i, %j + 5], read, locality<3>, data : memref<400x400xi32>
    ```

    The read/write specifier is either 'read' or 'write', the locality hint
    specifier ranges from locality<0> (no locality) to locality<3> (extremely
    local keep in cache). The cache type specifier is either 'data' or 'instr'
    and specifies whether the prefetch is performed on data cache or on
    instruction cache.
  }];

  let arguments = (ins AnyMemRef:$memref, Variadic<Index>:$indices,
                   BoolAttr:$isWrite,
                   ConfinedAttr<I32Attr, [IntMinValue<0>,
                     IntMaxValue<3>]>:$localityHint,
                   BoolAttr:$isDataCache,
                   AffineMapAttr:$map);

  let builders = [
    OpBuilder<(ins "Value":$memref, "AffineMap":$map,
      "ArrayRef<Value>":$mapOperands, "bool":$isWrite, "unsigned":$localityHint,
      "bool":$isDataCache),
    [{
      assert(map.getNumInputs() == mapOperands.size()
             && "inconsistent index info");
      auto localityHintAttr = $_builder.getI32IntegerAttr(localityHint);
      auto isWriteAttr = $_builder.getBoolAttr(isWrite);
      auto isDataCacheAttr = $_builder.getBoolAttr(isDataCache);
      $_state.addOperands(memref);
      $_state.addOperands(mapOperands);
      Properties &prop = $_state.getOrAddProperties<Properties>();
      prop.map = AffineMapAttr::get(map);
      prop.localityHint = localityHintAttr;
      prop.isWrite = isWriteAttr;
      prop.isDataCache = isDataCacheAttr;
    }]>];

  let extraClassDeclaration = [{
    MemRefType getMemRefType() {
      return ::llvm::cast<MemRefType>(getMemref().getType());
    }

    /// Returns the affine map used to index the memref for this operation.
    AffineMap getAffineMap() { return getAffineMapAttr().getValue(); }
    AffineMapAttr getAffineMapAttr() {
      return getProperties().map;
    }

    /// Implements the AffineMapAccessInterface.
    /// Returns the AffineMapAttr associated with 'memref'.
    NamedAttribute getAffineMapAttrForMemRef(Value mref) {
      assert(mref == getMemref() &&
             "Expected mref argument to match memref operand");
      return {StringAttr::get(getContext(), getMapAttrStrName()),
        getAffineMapAttr()};
    }

    /// Get affine map operands.
    operand_range getMapOperands() {
      return {operand_begin() + 1, operand_end()};
    }

    static StringRef getMapAttrStrName() { return "map"; }
    static StringRef getLocalityHintAttrStrName() { return "localityHint"; }
    static StringRef getIsWriteAttrStrName() { return "isWrite"; }
    static StringRef getIsDataCacheAttrStrName() { return "isDataCache"; }
  }];

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

class AffineStoreOpBase<string mnemonic, list<Trait> traits = []> :
    Affine_Op<mnemonic, !listconcat(traits,
    [DeclareOpInterfaceMethods<AffineWriteOpInterface>,
    DeclareOpInterfaceMethods<AffineMapAccessInterface>,
    MemRefsNormalizable])> {
  code extraClassDeclarationBase = [{
    /// Returns the operand index of the value to be stored.
    unsigned getStoredValOperandIndex() { return 0; }

    /// Returns the operand index of the memref.
    unsigned getMemRefOperandIndex() { return 1; }

    void setMemRef(Value value) { setOperand(getMemRefOperandIndex(), value); }

    /// Returns the affine map used to index the memref for this operation.
    AffineMapAttr getAffineMapAttr() {
      return getProperties().map;
    }

    static StringRef getMapAttrStrName() { return "map"; }
  }];
}

def AffineStoreOp : AffineStoreOpBase<"store"> {
  let summary = "affine store operation";
  let description = [{
    Syntax:

    ```
    operation ::= `affine.store` ssa-use, ssa-use `[` multi-dim-affine-map-of-ssa-ids `]` `:` memref-type
    ```

    The `affine.store` op writes an element to a memref, where the index
    for each memref dimension is an affine expression of loop induction
    variables and symbols. The `affine.store` op stores a new value which is the
    same type as the elements of the memref. An affine expression of loop IVs
    and symbols must be specified for each dimension of the memref. The keyword
    `symbol` can be used to indicate SSA identifiers which are symbolic.

    Example 1:

    ```mlir
    affine.store %v0, %0[%i0 + 3, %i1 + 7] : memref<100x100xf32>
    ```

    Example 2: Uses `symbol` keyword for symbols `%n` and `%m`.

    ```mlir
    affine.store %v0, %0[%i0 + symbol(%n), %i1 + symbol(%m)] : memref<100x100xf32>
    ```
  }];
  let arguments = (ins AnyType:$value,
      Arg<AnyMemRef, "the reference to store to",
      [MemWrite]>:$memref,
      Variadic<Index>:$indices,
      AffineMapAttr:$map);

  let skipDefaultBuilders = 1;
  let builders = [
    OpBuilder<(ins "Value":$valueToStore, "Value":$memref,
      "ValueRange":$indices)>,
    OpBuilder<(ins "Value":$valueToStore, "Value":$memref, "AffineMap":$map,
      "ValueRange":$mapOperands)>
  ];

  let extraClassDeclaration = extraClassDeclarationBase;

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

def AffineYieldOp : Affine_Op<"yield", [Pure, Terminator, ReturnLike,
    MemRefsNormalizable]> {
  let summary = "Yield values to parent operation";
  let description = [{
    The `affine.yield` yields zero or more SSA values from an affine op region and
    terminates the region. The semantics of how the values yielded are used
    is defined by the parent operation.
    If `affine.yield` has any operands, the operands must match the parent
    operation's results.
    If the parent operation defines no values, then the `affine.yield` may be
    left out in the custom syntax and the builders will insert one implicitly.
    Otherwise, it has to be present in the syntax to indicate which values are
    yielded.
  }];

  let arguments = (ins Variadic<AnyType>:$operands);

  let builders = [
    OpBuilder<(ins), [{ build($_builder, $_state, std::nullopt); }]>
  ];

  let assemblyFormat = "attr-dict ($operands^ `:` type($operands))?";
  let hasVerifier = 1;
}

def AffineVectorLoadOp : AffineLoadOpBase<"vector_load"> {
  let summary = "affine vector load operation";
  let description = [{
    The `affine.vector_load` is the vector counterpart of
    [affine.load](#affineload-mliraffineloadop). It reads a slice from a
    [MemRef](Builtin.md/#memreftype), supplied as its first operand,
    into a [vector](Builtin.md/#vectortype) of the same base elemental type.
    The index for each memref dimension is an affine expression of loop induction
    variables and symbols. These indices determine the start position of the read
    within the memref. The shape of the return vector type determines the shape of
    the slice read from the memref. This slice is contiguous along the respective
    dimensions of the shape. Strided vector loads will be supported in the future.
    An affine expression of loop IVs and symbols must be specified for each
    dimension of the memref. The keyword `symbol` can be used to indicate SSA
    identifiers which are symbolic.

    Example 1: 8-wide f32 vector load.

    ```mlir
    %1 = affine.vector_load %0[%i0 + 3, %i1 + 7] : memref<100x100xf32>, vector<8xf32>
    ```

    Example 2: 4-wide f32 vector load. Uses `symbol` keyword for symbols `%n` and `%m`.

    ```mlir
    %1 = affine.vector_load %0[%i0 + symbol(%n), %i1 + symbol(%m)] : memref<100x100xf32>, vector<4xf32>
    ```

    Example 3: 2-dim f32 vector load.

    ```mlir
    %1 = affine.vector_load %0[%i0, %i1] : memref<100x100xf32>, vector<2x8xf32>
    ```

    TODOs:
    * Add support for strided vector loads.
    * Consider adding a permutation map to permute the slice that is read from memory
    (see [vector.transfer_read](../Vector/#vectortransfer_read-mlirvectortransferreadop)).
  }];

  let results = (outs AnyVectorOfNonZeroRank:$result);

  let builders = [
    /// Builds an affine vector load op with the specified map and operands.
    OpBuilder<(ins "VectorType":$resultType, "AffineMap":$map,
      "ValueRange":$operands)>,
    /// Builds an affine vector load op with an identity map and operands.
    OpBuilder<(ins "VectorType":$resultType, "Value":$memref,
      CArg<"ValueRange", "{}">:$indices)>,
    /// Builds an affine vector load op with the specified map and its operands.
    OpBuilder<(ins "VectorType":$resultType, "Value":$memref,
      "AffineMap":$map, "ValueRange":$mapOperands)>
  ];

  let extraClassDeclaration = extraClassDeclarationBase # [{
    VectorType getVectorType() {
      return ::llvm::cast<VectorType>(getResult().getType());
    }
  }];

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

def AffineVectorStoreOp : AffineStoreOpBase<"vector_store"> {
  let summary = "affine vector store operation";
  let description = [{
    The `affine.vector_store` is the vector counterpart of
    [affine.store](#affinestore-mliraffinestoreop). It writes a
    [vector](Builtin.md/#vectortype), supplied as its first operand,
    into a slice within a [MemRef](Builtin.md/#memreftype) of the same base
    elemental type, supplied as its second operand.
    The index for each memref dimension is an affine expression of loop
    induction variables and symbols. These indices determine the start position
    of the write within the memref. The shape of th input vector determines the
    shape of the slice written to the memref. This slice is contiguous along the
    respective dimensions of the shape. Strided vector stores will be supported
    in the future.
    An affine expression of loop IVs and symbols must be specified for each
    dimension of the memref. The keyword `symbol` can be used to indicate SSA
    identifiers which are symbolic.

    Example 1: 8-wide f32 vector store.

    ```mlir
    affine.vector_store %v0, %0[%i0 + 3, %i1 + 7] : memref<100x100xf32>, vector<8xf32>
    ```

    Example 2: 4-wide f32 vector store. Uses `symbol` keyword for symbols `%n` and `%m`.

    ```mlir
    affine.vector_store %v0, %0[%i0 + symbol(%n), %i1 + symbol(%m)] : memref<100x100xf32>, vector<4xf32>
    ```

    Example 3: 2-dim f32 vector store.

    ```mlir
    affine.vector_store %v0, %0[%i0, %i1] : memref<100x100xf32>, vector<2x8xf32>
    ```

    TODOs:
    * Add support for strided vector stores.
    * Consider adding a permutation map to permute the slice that is written to memory
    (see [vector.transfer_write](../Vector/#vectortransfer_write-mlirvectortransferwriteop)).
  }];

  let arguments = (ins AnyVectorOfNonZeroRank:$value,
      Arg<AnyMemRef, "the reference to store to",
      [MemWrite]>:$memref,
      Variadic<Index>:$indices,
      AffineMapAttr:$map);

  let skipDefaultBuilders = 1;
  let builders = [
    OpBuilder<(ins "Value":$valueToStore, "Value":$memref,
      "ValueRange":$indices)>,
    OpBuilder<(ins "Value":$valueToStore, "Value":$memref, "AffineMap":$map,
      "ValueRange":$mapOperands)>
  ];

  let extraClassDeclaration = extraClassDeclarationBase # [{
    VectorType getVectorType() {
      return ::llvm::cast<VectorType>(getValue().getType());
    }
  }];

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// AffineDelinearizeIndexOp
//===----------------------------------------------------------------------===//

def AffineDelinearizeIndexOp : Affine_Op<"delinearize_index", [Pure]> {
  let summary = "delinearize an index";
  let description = [{
    The `affine.delinearize_index` operation takes a single index value and
    calculates the multi-index according to the given basis.

    Example:

    ```
    %indices:3 = affine.delinearize_index %linear_index into (%c16, %c224, %c224) : index, index, index
    ```

    In the above example, `%indices:3` conceptually holds the following:

    ```
    #map0 = affine_map<()[s0] -> (s0 floordiv 50176)>
    #map1 = affine_map<()[s0] -> ((s0 mod 50176) floordiv 224)>
    #map2 = affine_map<()[s0] -> (s0 mod 224)>
    %indices_0 = affine.apply #map0()[%linear_index]
    %indices_1 = affine.apply #map1()[%linear_index]
    %indices_2 = affine.apply #map2()[%linear_index]
    ```

    The basis may either contain `N` or `N-1` elements, where `N` is the number of results.
    If there are N basis elements, the first one will not be used during computations,
    but may be used during analysis and canonicalization to eliminate terms from
    the `affine.delinearize_index` or to enable conclusions about the total size of
    `%linear_index`.

    If the basis is fully provided, the delinearize_index operation is said to "have
    an outer bound". The builders assume that an `affine.delinearize_index` has
    an outer bound by default, as this is how the operation was initially defined.

    That is, the example above could also have been written
    ```mlir
    %0:3 = affine.delinearize_index %linear_index into (244, 244) : index, index
    ```

    Note that, due to the constraints of affine maps, all the basis elements must
    be strictly positive. A dynamic basis element being 0 or negative causes
    undefined behavior.
  }];

  let arguments = (ins Index:$linear_index,
    Variadic<Index>:$dynamic_basis,
    DenseI64ArrayAttr:$static_basis);
  let results = (outs Variadic<Index>:$multi_index);

  let assemblyFormat = [{
    $linear_index `into`
    custom<DynamicIndexList>($dynamic_basis, $static_basis, "::mlir::AsmParser::Delimiter::Paren")
    attr-dict `:` type($multi_index)
  }];

  let builders = [
    OpBuilder<(ins "Value":$linear_index, "ValueRange":$dynamic_basis, "ArrayRef<int64_t>":$static_basis, CArg<"bool", "true">:$hasOuterBound)>,
    OpBuilder<(ins "Value":$linear_index, "ValueRange":$basis, CArg<"bool", "true">:$hasOuterBound)>,
    OpBuilder<(ins "Value":$linear_index, "ArrayRef<OpFoldResult>":$basis, CArg<"bool", "true">:$hasOuterBound)>,
    OpBuilder<(ins "Value":$linear_index, "ArrayRef<int64_t>":$basis, CArg<"bool", "true">:$hasOuterBound)>
  ];

  let extraClassDeclaration = [{
    /// Return true if the basis includes a bound on the first index input.
    bool hasOuterBound() {
      return getMultiIndex().size() == getStaticBasis().size();
    }

    /// Returns a vector with all the static and dynamic basis values.
    SmallVector<OpFoldResult> getMixedBasis() {
      OpBuilder builder(getContext());
      return ::mlir::getMixedValues(getStaticBasis(), getDynamicBasis(), builder);
    }

    /// Return a vector that contains the basis of the operation, removing
    /// the outer bound if one is present.
    SmallVector<OpFoldResult> getEffectiveBasis();
  }];

  let hasVerifier = 1;
  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// AffineLinearizeIndexOp
//===----------------------------------------------------------------------===//
def AffineLinearizeIndexOp : Affine_Op<"linearize_index",
    [Pure, AttrSizedOperandSegments]> {
  let summary = "linearize an index";
  let description = [{
    The `affine.linearize_index` operation takes a sequence of index values and a
    basis of the same length and linearizes the indices using that basis.

    That is, for indices `%idx_0` to `%idx_{N-1}` and basis elements `b_0`
    (or `b_1`) up to `b_{N-1}` it computes

    ```
    sum(i = 0 to N-1) %idx_i * product(j = i + 1 to N-1) B_j
    ```

    The basis may either have `N` or `N-1` elements, where `N` is the number of
    inputs to linearize_index. If `N` inputs are provided, the first one is not used
    in computation, but may be used during analysis or canonicalization as a bound
    on `%idx_0`.

    If all `N` basis elements are provided, the linearize_index operation is said to
    "have an outer bound".

    If the `disjoint` property is present, this is an optimization hint that,
    for all `i`, `0 <= %idx_i < B_i` - that is, no index affects any other index,
    except that `%idx_0` may be negative to make the index as a whole negative.

    Note that the outputs of `affine.delinearize_index` are, by definition, `disjoint`.

    Example:

    ```mlir
    %linear_index = affine.linearize_index [%index_0, %index_1, %index_2] by (2, 3, 5) : index
    // Same effect
    %linear_index = affine.linearize_index [%index_0, %index_1, %index_2] by (3, 5) : index
    ```

    In the above example, `%linear_index` conceptually holds the following:

    ```mlir
    #map = affine_map<()[s0, s1, s2] -> (s0 * 15 + s1 * 5 + s2)>
    %linear_index = affine.apply #map()[%index_0, %index_1, %index_2]
    ```
  }];

  let arguments = (ins Variadic<Index>:$multi_index,
    Variadic<Index>:$dynamic_basis,
    DenseI64ArrayAttr:$static_basis,
    UnitProperty:$disjoint);
  let results = (outs Index:$linear_index);

  let assemblyFormat = [{
    (`disjoint` $disjoint^)? ` `
    `[` $multi_index `]` `by`
    custom<DynamicIndexList>($dynamic_basis, $static_basis, "::mlir::AsmParser::Delimiter::Paren")
    attr-dict `:` type($linear_index)
  }];

  let builders = [
    OpBuilder<(ins "ValueRange":$multi_index, "ValueRange":$basis, CArg<"bool", "false">:$disjoint)>,
    OpBuilder<(ins "ValueRange":$multi_index, "ArrayRef<OpFoldResult>":$basis, CArg<"bool", "false">:$disjoint)>,
    OpBuilder<(ins "ValueRange":$multi_index, "ArrayRef<int64_t>":$basis, CArg<"bool", "false">:$disjoint)>
  ];

  let extraClassDeclaration = [{
    /// Return true if the basis includes a bound on the first index input.
    bool hasOuterBound() {
      return getMultiIndex().size() == getStaticBasis().size();
    }

    /// Return a vector with all the static and dynamic basis values.
    SmallVector<OpFoldResult> getMixedBasis() {
      OpBuilder builder(getContext());
      return ::mlir::getMixedValues(getStaticBasis(), getDynamicBasis(), builder);
    }

    /// Return a vector that contains the basis of the operation, removing
    /// the outer bound if one is present.
    SmallVector<OpFoldResult> getEffectiveBasis();
  }];

  let hasVerifier = 1;
  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

#endif // AFFINE_OPS


//===- TransformAttrs.td - Transform dialect attributes ----*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef MLIR_DIALECT_TRANSFORM_IR_TRANSFORMATTRS
#define MLIR_DIALECT_TRANSFORM_IR_TRANSFORMATTRS

include "mlir/IR/EnumAttr.td"

def PropagateFailuresCase : I32EnumAttrCase<"Propagate", 1, "propagate">;
def SuppressFailuresCase : I32EnumAttrCase<"Suppress", 2, "suppress">;

def FailurePropagationMode : I32EnumAttr<
    "FailurePropagationMode", "Silenceable error propagation policy",
    [PropagateFailuresCase, SuppressFailuresCase]> {
  let cppNamespace = "::mlir::transform";
}

def MatchCmpIPredicateAttr : I32EnumAttr<
    "MatchCmpIPredicate", "",
    [
      I32EnumAttrCase<"eq", 0>,
      I32EnumAttrCase<"ne", 1>,
      I32EnumAttrCase<"lt", 2>,
      I32EnumAttrCase<"le", 3>,
      I32EnumAttrCase<"gt", 4>,
      I32EnumAttrCase<"ge", 5>,
    ]> {
  let cppNamespace = "::mlir::transform";
}

#endif  // MLIR_DIALECT_TRANSFORM_IR_TRANSFORMATTRS


//===- BuiltinOps.td - Builtin operation definitions -------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// Defines the set of builtin MLIR operations, or the set of operations
// necessary for the validity of and defining the IR.
//
//===----------------------------------------------------------------------===//

#ifndef BUILTIN_OPS
#define BUILTIN_OPS

include "mlir/IR/BuiltinDialect.td"
include "mlir/IR/OpAsmInterface.td"
include "mlir/IR/RegionKindInterface.td"
include "mlir/IR/SymbolInterfaces.td"
include "mlir/Interfaces/CastInterfaces.td"
include "mlir/Interfaces/DataLayoutInterfaces.td"
include "mlir/Interfaces/SideEffectInterfaces.td"

// Base class for Builtin dialect ops.
class Builtin_Op<string mnemonic, list<Trait> traits = []> :
    Op<Builtin_Dialect, mnemonic, traits>;

//===----------------------------------------------------------------------===//
// ModuleOp
//===----------------------------------------------------------------------===//

def ModuleOp : Builtin_Op<"module", [
    AffineScope, IsolatedFromAbove, NoRegionArguments, SymbolTable, Symbol,
    OpAsmOpInterface
  ] # GraphRegionNoTerminator.traits> {
  let summary = "A top level container operation";
  let description = [{
    A `module` represents a top-level container operation. It contains a single
    [graph region](../LangRef.md#control-flow-and-ssacfg-regions) containing a single block
    which can contain any operations and does not have a terminator. Operations
    within this region cannot implicitly capture values defined outside the module,
    i.e. Modules are [IsolatedFromAbove](../Traits.md#isolatedfromabove). Modules have
    an optional [symbol name](../SymbolsAndSymbolTables.md) which can be used to refer
    to them in operations.

    Example:

    ```mlir
    module {
      func.func @foo()
    }
    ```
  }];

  let arguments = (ins OptionalAttr<SymbolNameAttr>:$sym_name,
                       OptionalAttr<StrAttr>:$sym_visibility);
  let regions = (region SizedRegion<1>:$bodyRegion);

  let assemblyFormat = "($sym_name^)? attr-dict-with-keyword $bodyRegion";
  let builders = [OpBuilder<(ins CArg<"std::optional<StringRef>", "{}">:$name)>];
  let extraClassDeclaration = [{
    /// Construct a module from the given location with an optional name.
    static ModuleOp create(Location loc, std::optional<StringRef> name = std::nullopt);

    /// Return the name of this module if present.
    std::optional<StringRef> getName() { return getSymName(); }

    //===------------------------------------------------------------------===//
    // SymbolOpInterface Methods
    //===------------------------------------------------------------------===//

    /// A ModuleOp may optionally define a symbol.
    bool isOptionalSymbol() { return true; }

    //===------------------------------------------------------------------===//
    // DataLayoutOpInterface Methods
    //===------------------------------------------------------------------===//

    DataLayoutSpecInterface getDataLayoutSpec();
    TargetSystemSpecInterface getTargetSystemSpec();

    //===------------------------------------------------------------------===//
    // OpAsmOpInterface Methods
    //===------------------------------------------------------------------===//

    static ::llvm::StringRef getDefaultDialect() {
      return "builtin";
    }
  }];
  let hasVerifier = 1;

  // We need to ensure the block inside the region is properly terminated;
  // the auto-generated builders do not guarantee that.
  let skipDefaultBuilders = 1;
}

//===----------------------------------------------------------------------===//
// UnrealizedConversionCastOp
//===----------------------------------------------------------------------===//

def UnrealizedConversionCastOp : Builtin_Op<"unrealized_conversion_cast", [
    Pure
  ]> {
  let summary = "An unrealized conversion from one set of types to another";
  let description = [{
    An `unrealized_conversion_cast` operation represents an unrealized
    conversion from one set of types to another, that is used to enable the
    inter-mixing of different type systems. This operation should not be
    attributed any special representational or execution semantics, and is
    generally only intended to be used to satisfy the temporary intermixing of
    type systems during the conversion of one type system to another.

    This operation may produce results of arity 1-N, and accept as input
    operands of arity 0-N.

    Example:

    ```mlir
    // An unrealized 0-1 conversion. These types of conversions are useful in
    // cases where a type is removed from the type system, but not all uses have
    // been converted. For example, imagine we have a tuple type that is
    // expanded to its element types. If only some uses of an empty tuple type
    // instance are converted we still need an instance of the tuple type, but
    // have no inputs to the unrealized conversion.
    %result = unrealized_conversion_cast to !bar.tuple_type<>

    // An unrealized 1-1 conversion.
    %result1 = unrealized_conversion_cast %operand : !foo.type to !bar.lowered_type

    // An unrealized 1-N conversion.
    %results2:2 = unrealized_conversion_cast %tuple_operand : !foo.tuple_type<!foo.type, !foo.type> to !foo.type, !foo.type

    // An unrealized N-1 conversion.
    %result3 = unrealized_conversion_cast %operand, %operand : !foo.type, !foo.type to !bar.tuple_type<!foo.type, !foo.type>
    ```
  }];

  let arguments = (ins Variadic<AnyType>:$inputs);
  let results = (outs Variadic<AnyType>:$outputs);
  let assemblyFormat = [{
    ($inputs^ `:` type($inputs))? `to` type($outputs) attr-dict
  }];
  let hasFolder = 1;
  let hasVerifier = 1;
}

#endif // BUILTIN_OPS


//===-- SPIRVOps.td - MLIR SPIR-V Op Definitions Spec ------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This is the main operation definition specification file for SPIR-V
// operations.
//
//===----------------------------------------------------------------------===//

// Note that for each op in this file and the included files for specific op
// categories, we use a tool to automatically generate certain sections in its
// definition: basic structure, summary, description. So modifications to these
// sections will not be respected. Modifications to op traits, arguments,
// results, and sections after the results are retained. Besides, ops must be
// separated via the '// -----' marker.

#ifndef MLIR_DIALECT_SPIRV_IR_OPS
#define MLIR_DIALECT_SPIRV_IR_OPS

include "mlir/Dialect/SPIRV/IR/SPIRVBase.td"
include "mlir/Dialect/SPIRV/IR/SPIRVArithmeticOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVAtomicOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVBarrierOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVBitOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVCastOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVCompositeOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVControlFlowOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVCooperativeMatrixOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVIntelExtOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVGLOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVGroupOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVImageOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVIntegerDotProductOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVLogicalOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVMatrixOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVMemoryOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVMiscOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVNonUniformOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVCLOps.td"
include "mlir/Dialect/SPIRV/IR/SPIRVStructureOps.td"
include "mlir/Interfaces/SideEffectInterfaces.td"

#endif // MLIR_DIALECT_SPIRV_IR_OPS


include "mlir/IR/EnumAttr.td"

def MatchInterfaceEnum : I32EnumAttr<"MatchInterfaceEnum", "An interface to match",
    [
      I32EnumAttrCase<"LinalgOp", 0>,
      I32EnumAttrCase<"TilingInterface", 1>,
      I32EnumAttrCase<"LoopLikeInterface", 2>,
    ]>{
  let cppNamespace = "mlir::transform";
}

def TransposeMatmulInput : I32EnumAttr<"TransposeMatmulInput",
    "Input to transpose when converting matmul ops to transposed variants",
    [
      I32EnumAttrCase<"lhs", 0>,
      I32EnumAttrCase<"rhs", 1>,
    ]>{
  let cppNamespace = "mlir::transform";
}


//===- MathOps.td - Math op definitions --------------------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef MATH_OPS
#define MATH_OPS

include "mlir/Dialect/Arith/IR/ArithBase.td"
include "mlir/Dialect/Arith/IR/ArithOpsInterfaces.td"
include "mlir/Dialect/Math/IR/MathBase.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/VectorInterfaces.td"
include "mlir/Interfaces/SideEffectInterfaces.td"

// Base class for math dialect ops. Ops in this dialect have no side effects and
// can be applied element-wise to vectors and tensors.
class Math_Op<string mnemonic, list<Trait> traits = []> :
    Op<Math_Dialect, mnemonic, traits # [Pure,
    DeclareOpInterfaceMethods<VectorUnrollOpInterface>] #
    ElementwiseMappable.traits>;

// Base class for unary math operations on integer types. Require an operand
// and result of the same type. This type can be an integer type, vector or
// tensor thereof.
class Math_IntegerUnaryOp<string mnemonic, list<Trait> traits = []> :
    Math_Op<mnemonic, traits # [SameOperandsAndResultType]> {
  let arguments = (ins SignlessIntegerLike:$operand);
  let results = (outs SignlessIntegerLike:$result);

  let assemblyFormat = "$operand attr-dict `:` type($result)";
}

// Base class for unary math operations on floating point types. Require an
// operand and result of the same type. This type can be a floating point type,
// vector or tensor thereof.
class Math_FloatUnaryOp<string mnemonic, list<Trait> traits = []> :
    Math_Op<mnemonic,
        traits # [SameOperandsAndResultType,
                  DeclareOpInterfaceMethods<ArithFastMathInterface>]> {
  let arguments = (ins FloatLike:$operand,
      DefaultValuedAttr<Arith_FastMathAttr,
                        "::mlir::arith::FastMathFlags::none">:$fastmath);
  let results = (outs FloatLike:$result);

  let assemblyFormat = [{ $operand (`fastmath` `` $fastmath^)?
                          attr-dict `:` type($result) }];
}

// Base class for binary math operations on integer types. Require two
// operands and one result of the same type. This type can be an integer
// type, vector or tensor thereof.
class Math_IntegerBinaryOp<string mnemonic, list<Trait> traits = []> :
    Math_Op<mnemonic, traits # [SameOperandsAndResultType]> {
  let arguments = (ins SignlessIntegerLike:$lhs, SignlessIntegerLike:$rhs);
  let results = (outs SignlessIntegerLike:$result);

  let assemblyFormat = "$lhs `,` $rhs attr-dict `:` type($result)";
}

// Base class for binary math operations on floating point types. Require two
// operands and one result of the same type. This type can be a floating point
// type, vector or tensor thereof.
class Math_FloatBinaryOp<string mnemonic, list<Trait> traits = []> :
    Math_Op<mnemonic,
        traits # [SameOperandsAndResultType,
                  DeclareOpInterfaceMethods<ArithFastMathInterface>]> {
  let arguments = (ins FloatLike:$lhs, FloatLike:$rhs,
      DefaultValuedAttr<Arith_FastMathAttr,
                        "::mlir::arith::FastMathFlags::none">:$fastmath);
  let results = (outs FloatLike:$result);

  let assemblyFormat = [{ $lhs `,` $rhs (`fastmath` `` $fastmath^)?
                          attr-dict `:` type($result) }];
}

// Base class for floating point ternary operations. Require three operands and
// one result of the same type. This type can be a floating point type, vector
// or tensor thereof.
class Math_FloatTernaryOp<string mnemonic, list<Trait> traits = []> :
    Math_Op<mnemonic,
        traits # [SameOperandsAndResultType,
                  DeclareOpInterfaceMethods<ArithFastMathInterface>]> {
  let arguments = (ins FloatLike:$a, FloatLike:$b, FloatLike:$c,
      DefaultValuedAttr<Arith_FastMathAttr,
                        "::mlir::arith::FastMathFlags::none">:$fastmath);
  let results = (outs FloatLike:$result);

  let assemblyFormat = [{ $a `,` $b `,` $c (`fastmath` `` $fastmath^)?
                          attr-dict `:` type($result) }];
}

//===----------------------------------------------------------------------===//
// AbsFOp
//===----------------------------------------------------------------------===//

def Math_AbsFOp : Math_FloatUnaryOp<"absf"> {
  let summary = "floating point absolute-value operation";
  let description = [{
    The `absf` operation computes the absolute value. It takes one operand of
    floating point type (i.e., scalar, tensor or vector) and returns one result
    of the same type.

    Example:

    ```mlir
    // Scalar absolute value.
    %a = math.absf %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// AbsIOp
//===----------------------------------------------------------------------===//

def Math_AbsIOp : Math_IntegerUnaryOp<"absi"> {
  let summary = "integer absolute-value operation";
  let description = [{
    The `absi` operation computes the absolute value. It takes one operand of
    integer type (i.e., scalar, tensor or vector) and returns one result of the
    same type.

    Example:

    ```mlir
    // Scalar absolute value.
    %a = math.absi %b : i64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// AcoshOp
//===----------------------------------------------------------------------===//

def Math_AcoshOp : Math_FloatUnaryOp<"acosh">{
  let summary = "Hyperbolic arcus cosine of the given value";
  let description = [{
    Syntax:

    ```
    operation ::= ssa-id `=` `math.acosh` ssa-use `:` type
    ```

    The `acosh` operation computes the arcus cosine of a given value.  It takes
    one operand of floating point type (i.e., scalar, tensor or vector) and returns
    one result of the same type. It has no standard attributes.

    Example:

    ```mlir
    // Hyperbolic arcus cosine of scalar value.
    %a = math.acosh %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// AsinOp
//===----------------------------------------------------------------------===//

def Math_AsinOp : Math_FloatUnaryOp<"asin">{
  let summary = "arcus sine of the given value";
  let description = [{
    Syntax:

    ```
    operation ::= ssa-id `=` `math.asin` ssa-use `:` type
    ```

    The `asin` operation computes the arcus sine of a given value.  It takes
    one operand of floating point type (i.e., scalar, tensor or vector) and returns
    one result of the same type. It has no standard attributes.

    Example:

    ```mlir
    // Arcus sine of scalar value.
    %a = math.asin %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// AsinhOp
//===----------------------------------------------------------------------===//

def Math_AsinhOp : Math_FloatUnaryOp<"asinh">{
  let summary = "hyperbolic arcus sine of the given value";
  let description = [{
    Syntax:

    ```
    operation ::= ssa-id `=` `math.asinh` ssa-use `:` type
    ```

    The `asinh` operation computes the hyperbolic arcus sine of a given value.  It takes
    one operand of floating point type (i.e., scalar, tensor or vector) and returns
    one result of the same type. It has no standard attributes.

    Example:

    ```mlir
    // Hyperbolic arcus sine of scalar value.
    %a = math.asinh %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// AtanOp
//===----------------------------------------------------------------------===//

def Math_AtanOp : Math_FloatUnaryOp<"atan">{
  let summary = "arcus tangent of the given value";
  let description = [{
    The `atan` operation computes the arcus tangent of a given value.  It takes
    one operand of floating point type (i.e., scalar, tensor or vector) and returns
    one result of the same type. It has no standard attributes.

    Example:

    ```mlir
    // Arcus tangent of scalar value.
    %a = math.atan %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// AtanhOp
//===----------------------------------------------------------------------===//

def Math_AtanhOp : Math_FloatUnaryOp<"atanh">{
  let summary = "hyperbolic arcus tangent of the given value";
  let description = [{
    Syntax:

    ```
    operation ::= ssa-id `=` `math.atanh` ssa-use `:` type
    ```

    The `atanh` operation computes the hyperbolic arcus tangent of a given value.  It takes
    one operand of floating point type (i.e., scalar, tensor or vector) and returns
    one result of the same type. It has no standard attributes.

    Example:

    ```mlir
    // Hyperbolic arcus tangent of scalar value.
    %a = math.atanh %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Atan2Op
//===----------------------------------------------------------------------===//

def Math_Atan2Op : Math_FloatBinaryOp<"atan2">{
  let summary = "2-argument arcus tangent of the given values";
  let description = [{
    The `atan2` operation takes two operands and returns one result, all of
    which must be of the same type.  The operands must be of floating point type
    (i.e., scalar, tensor or vector).

    The 2-argument arcus tangent `atan2(y, x)` returns the angle in the
    Euclidian plane between the positive x-axis and the ray through the point
    (x, y).  It is a generalization of the 1-argument arcus tangent which
    returns the angle on the basis of the ratio y/x.

    See also https://en.wikipedia.org/wiki/Atan2

    Example:

    ```mlir
    // Scalar variant.
    %a = math.atan2 %b, %c : f32
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// CbrtOp
//===----------------------------------------------------------------------===//

def Math_CbrtOp : Math_FloatUnaryOp<"cbrt"> {
  let summary = "cube root of the specified value";
  let description = [{
    The `cbrt` operation computes the cube root. It takes one operand of
    floating point type (i.e., scalar, tensor or vector) and returns one result
    of the same type. It has no standard attributes.

    Example:

    ```mlir
    // Scalar cube root value.
    %a = math.cbrt %b : f64
    ```

    Note: This op is not equivalent to powf(..., 1/3.0).
  }];
}

//===----------------------------------------------------------------------===//
// CeilOp
//===----------------------------------------------------------------------===//

def Math_CeilOp : Math_FloatUnaryOp<"ceil"> {
  let summary = "ceiling of the specified value";
  let description = [{
    The `ceil` operation computes the ceiling of a given value. It takes one
    operand of floating point type (i.e., scalar, tensor or vector) and returns one
    result of the same type.  It has no standard attributes.

    Example:

    ```mlir
    // Scalar ceiling value.
    %a = math.ceil %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// CopySignOp
//===----------------------------------------------------------------------===//

def Math_CopySignOp : Math_FloatBinaryOp<"copysign"> {
  let summary = "A copysign operation";
  let description = [{
    The `copysign` returns a value with the magnitude of the first operand and
    the sign of the second operand. It takes two operands and returns one result of
    the same type. The operands must be of floating point type (i.e., scalar,
    tensor or vector). It has no standard attributes.

    Example:

    ```mlir
    // Scalar copysign value.
    %a = math.copysign %b, %c : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// CosOp
//===----------------------------------------------------------------------===//

def Math_CosOp : Math_FloatUnaryOp<"cos"> {
  let summary = "cosine of the specified value";
  let description = [{
    The `cos` operation computes the cosine of a given value. It takes one
    operand of floating point type (i.e., scalar, tensor or vector) and returns one
    result of the same type.  It has no standard attributes.

    Example:

    ```mlir
    // Scalar cosine value.
    %a = math.cos %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// AcosOp
//===----------------------------------------------------------------------===//

def Math_AcosOp : Math_FloatUnaryOp<"acos"> {
  let summary = "arcus cosine of the specified value";
  let description = [{
    The `acos` operation computes the arcus cosine of a given value. It takes one
    operand of floating point type (i.e., scalar, tensor or vector) and returns one
    result of the same type.  It has no standard attributes.

    Example:

    ```mlir
    // Scalar arcus cosine value.
    %a = math.acos %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// CoshOp
//===----------------------------------------------------------------------===//

def Math_CoshOp : Math_FloatUnaryOp<"cosh"> {
  let summary = "hyperbolic cosine of the specified value";
  let description = [{
    The `cosh` operation computes the hyperbolic cosine. It takes one operand
    of floating point type (i.e., scalar, tensor or vector) and returns one
    result of the same type. It has no standard attributes.

    Example:

    ```mlir
    // Scalar hyperbolic cosine value.
    %a = math.cosh %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// SinOp
//===----------------------------------------------------------------------===//

def Math_SinOp : Math_FloatUnaryOp<"sin"> {
  let summary = "sine of the specified value";
  let description = [{
    The `sin` operation computes the sine of a given value. It takes one
    operand of floating point type (i.e., scalar, tensor or vector) and returns one
    result of the same type.  It has no standard attributes.

    Example:

    ```mlir
    // Scalar sine value.
    %a = math.sin %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// SinhOp
//===----------------------------------------------------------------------===//

def Math_SinhOp : Math_FloatUnaryOp<"sinh"> {
  let summary = "hyperbolic sine of the specified value";
  let description = [{
    The `sinh` operation computes the hyperbolic sine. It takes one operand
    of floating point type (i.e., scalar, tensor or vector) and returns one
    result of the same type. It has no standard attributes.

    Example:

    ```mlir
    // Scalar hyperbolic sine value.
    %a = math.sinh %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// CountLeadingZerosOp
//===----------------------------------------------------------------------===//

def Math_CountLeadingZerosOp : Math_IntegerUnaryOp<"ctlz"> {
  let summary = "counts the leading zeros an integer value";
  let description = [{
    The `ctlz` operation computes the number of leading zeros of an integer value.
    It operates on scalar, tensor or vector.

    Example:

    ```mlir
    // Scalar ctlz function value.
    %a = math.ctlz %b : i32
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// CountTrailingZerosOp
//===----------------------------------------------------------------------===//

def Math_CountTrailingZerosOp : Math_IntegerUnaryOp<"cttz"> {
  let summary = "counts the trailing zeros an integer value";
  let description = [{
    The `cttz` operation computes the number of trailing zeros of an integer value.
    It operates on scalar, tensor or vector.

    Example:

    ```mlir
    // Scalar cttz function value.
    %a = math.cttz %b : i32
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// CtPopOp
//===----------------------------------------------------------------------===//

def Math_CtPopOp : Math_IntegerUnaryOp<"ctpop"> {
  let summary = "counts the number of set bits of an integer value";
  let description = [{
    The `ctpop` operation computes the number of set bits of an integer value.
    It operates on scalar, tensor or vector.

    Example:

    ```mlir
    // Scalar ctpop function value.
    %a = math.ctpop %b : i32
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// ErfOp
//===----------------------------------------------------------------------===//

def Math_ErfOp : Math_FloatUnaryOp<"erf"> {
  let summary = "error function of the specified value";
  let description = [{
    The `erf` operation computes the error function. It takes one operand of
    floating point type (i.e., scalar, tensor or vector) and returns one result of
    the same type. It has no standard attributes.

    Example:

    ```mlir
    // Scalar error function value.
    %a = math.erf %b : f64
    ```
  }];
  let hasFolder = 1;
}


//===----------------------------------------------------------------------===//
// ExpOp
//===----------------------------------------------------------------------===//

def Math_ExpOp : Math_FloatUnaryOp<"exp"> {
  let summary = "base-e exponential of the specified value";
  let description = [{
    The `exp` operation takes one operand of floating point type (i.e., scalar,
    tensor or vector) and returns one result of the same type. It has no standard
    attributes.

    Example:

    ```mlir
    // Scalar natural exponential.
    %a = math.exp %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Exp2Op
//===----------------------------------------------------------------------===//

def Math_Exp2Op : Math_FloatUnaryOp<"exp2"> {
  let summary = "base-2 exponential of the specified value";

  let description = [{
    The `exp` operation takes one operand of floating point type (i.e., scalar,
    tensor or vector) and returns one result of the same type. It has no standard
    attributes.

    Example:

    ```mlir
    // Scalar natural exponential.
    %a = math.exp2 %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// ExpM1Op
//===----------------------------------------------------------------------===//

def Math_ExpM1Op : Math_FloatUnaryOp<"expm1"> {
  let summary = "base-e exponential of the specified value minus 1";
  let description = [{
    expm1(x) := exp(x) - 1

    The `expm1` operation takes one operand of floating point type (i.e.,
    scalar, tensor or vector) and returns one result of the same type. It has no
    standard attributes.

    Example:

    ```mlir
    // Scalar natural exponential minus 1.
    %a = math.expm1 %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// FloorOp
//===----------------------------------------------------------------------===//

def Math_FloorOp : Math_FloatUnaryOp<"floor"> {
  let summary = "floor of the specified value";
  let description = [{
    The `floor` operation computes the floor of a given value. It takes one
    operand of floating point type (i.e., scalar, tensor or vector) and returns one
    result of the same type.  It has no standard attributes.

    Example:

    ```mlir
    // Scalar floor value.
    %a = math.floor %b : f64
    ```
  }];

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// FmaOp
//===----------------------------------------------------------------------===//

def Math_FmaOp : Math_FloatTernaryOp<"fma"> {
  let summary = "floating point fused multipy-add operation";
  let description = [{
    The `fma` operation takes three operands and returns one result, each of
    these is required to be the same type. Operands must be of floating point type
    (i.e., scalar, tensor or vector).

    Example:

    ```mlir
    // Scalar fused multiply-add: d = a*b + c
    %d = math.fma %a, %b, %c : f64
    ```

    The semantics of the operation correspond to those of the `llvm.fma`
    [intrinsic](https://llvm.org/docs/LangRef.html#llvm-fma-intrinsic). In the
    particular case of lowering to LLVM, this is guaranteed to lower
    to the `llvm.fma.*` intrinsic.
  }];
}

//===----------------------------------------------------------------------===//
// IPowIOp
//===----------------------------------------------------------------------===//

def Math_IPowIOp : Math_IntegerBinaryOp<"ipowi"> {
  let summary = "signed integer raised to the power of operation";
  let description = [{
    The `ipowi` operation takes two operands of integer type (i.e., scalar,
    tensor or vector) and returns one result of the same type. Operands
    must have the same type.

    Example:

    ```mlir
    // Scalar signed integer exponentiation.
    %a = math.ipowi %b, %c : i32
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// LogOp
//===----------------------------------------------------------------------===//

def Math_LogOp : Math_FloatUnaryOp<"log"> {
  let summary = "base-e logarithm of the specified value";

  let description = [{
    Computes the base-e logarithm of the given value. It takes one operand of
    floating point type (i.e., scalar, tensor or vector) and returns one result of
    the same type.

    Example:

    ```mlir
    // Scalar log operation.
    %y = math.log %x : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Log10Op
//===----------------------------------------------------------------------===//

def Math_Log10Op : Math_FloatUnaryOp<"log10"> {
  let summary = "base-10 logarithm of the specified value";

  let description = [{
    Computes the base-10 logarithm of the given value. It takes one operand of
    floating point type (i.e., scalar, tensor or vector) and returns one result of
    the same type.

    Example:

    ```mlir
    // Scalar log10 operation.
    %y = math.log10 %x : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Log1pOp
//===----------------------------------------------------------------------===//

def Math_Log1pOp : Math_FloatUnaryOp<"log1p"> {
  let summary = "Computes the natural logarithm of one plus the given value";

  let description = [{
    Computes the base-e logarithm of one plus the given value. It takes one
    operand of floating point type (i.e., scalar, tensor or vector) and returns one
    result of the same type.

    log1p(x) := log(1 + x)

    Example:

    ```mlir
    // Scalar log1p operation.
    %y = math.log1p %x : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Log2Op
//===----------------------------------------------------------------------===//

def Math_Log2Op : Math_FloatUnaryOp<"log2"> {
  let summary = "base-2 logarithm of the specified value";

  let description = [{
    Computes the base-2 logarithm of the given value. It takes one operand of
    floating point type (i.e., scalar, tensor or vector) and returns one result of
    the same type.

    Example:

    ```mlir
    // Scalar log2 operation.
    %y = math.log2 %x : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// PowFOp
//===----------------------------------------------------------------------===//

def Math_PowFOp : Math_FloatBinaryOp<"powf"> {
  let summary = "floating point raised to the power of operation";
  let description = [{
    The `powf` operation takes two operands of floating point type (i.e.,
    scalar, tensor or vector) and returns one result of the same type. Operands
    must have the same type.

    Example:

    ```mlir
    // Scalar exponentiation.
    %a = math.powf %b, %c : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// RsqrtOp
//===----------------------------------------------------------------------===//

def Math_RsqrtOp : Math_FloatUnaryOp<"rsqrt"> {
  let summary = "reciprocal of sqrt (1 / sqrt of the specified value)";
  let description = [{
    The `rsqrt` operation computes the reciprocal of the square root. It takes
    one operand of floating point type (i.e., scalar, tensor or vector) and returns
    one result of the same type. It has no standard attributes.

    Example:

    ```mlir
    // Scalar reciprocal square root value.
    %a = math.rsqrt %b : f64
    ```
  }];
}

//===----------------------------------------------------------------------===//
// SqrtOp
//===----------------------------------------------------------------------===//

def Math_SqrtOp : Math_FloatUnaryOp<"sqrt"> {
  let summary = "sqrt of the specified value";
  let description = [{
    The `sqrt` operation computes the square root. It takes one operand of
    floating point type (i.e., scalar, tensor or vector) and returns one result of
    the same type. It has no standard attributes.

    Example:

    ```mlir
    // Scalar square root value.
    %a = math.sqrt %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// TanOp
//===----------------------------------------------------------------------===//

def Math_TanOp : Math_FloatUnaryOp<"tan"> {
  let summary = "tangent of the specified value";
  let description = [{
    The `tan` operation computes the tangent. It takes one operand
    of floating point type (i.e., scalar, tensor or vector) and returns one
    result of the same type. It has no standard attributes.

    Example:

    ```mlir
    // Scalar tangent value.
    %a = math.tan %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// TanhOp
//===----------------------------------------------------------------------===//

def Math_TanhOp : Math_FloatUnaryOp<"tanh"> {
  let summary = "hyperbolic tangent of the specified value";
  let description = [{
    The `tanh` operation computes the hyperbolic tangent. It takes one operand
    of floating point type (i.e., scalar, tensor or vector) and returns one
    result of the same type. It has no standard attributes.

    Example:

    ```mlir
    // Scalar hyperbolic tangent value.
    %a = math.tanh %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// RoundEvenOp
//===----------------------------------------------------------------------===//

def Math_RoundEvenOp : Math_FloatUnaryOp<"roundeven"> {
  let summary = "round of the specified value with halfway cases to even";
  let description = [{
    The `roundeven` operation returns the operand rounded to the nearest integer
    value in floating-point format. It takes one operand of floating point type
    (i.e., scalar, tensor or vector) and produces one result of the same type.  The
    operation rounds the argument to the nearest integer value in floating-point
    format, rounding halfway cases to even, regardless of the current
    rounding direction.

    Example:

    ```mlir
    // Scalar round operation.
    %a = math.roundeven %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// RoundOp
//===----------------------------------------------------------------------===//

def Math_RoundOp : Math_FloatUnaryOp<"round"> {
  let summary = "round of the specified value";
  let description = [{
    The `round` operation returns the operand rounded to the nearest integer
    value in floating-point format. It takes one operand of floating point type
    (i.e., scalar, tensor or vector) and produces one result of the same type.  The
    operation rounds the argument to the nearest integer value in floating-point
    format, rounding halfway cases away from zero, regardless of the current
    rounding direction.

    Example:

    ```mlir
    // Scalar round operation.
    %a = math.round %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// TruncOp
//===----------------------------------------------------------------------===//

def Math_TruncOp : Math_FloatUnaryOp<"trunc"> {
  let summary = "trunc of the specified value";
  let description = [{
    The `trunc` operation returns the operand rounded to the nearest integer
    value in floating-point format. It takes one operand of floating point type
    (i.e., scalar, tensor or vector) and produces one result of the same type.
    The operation always rounds to the nearest integer not larger in magnitude
    than the operand, regardless of the current rounding direction.

    Example:

    ```mlir
    // Scalar trunc operation.
    %a = math.trunc %b : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// FPowIOp
//===----------------------------------------------------------------------===//

def Math_FPowIOp : Math_Op<"fpowi",
    [SameOperandsAndResultShape, AllTypesMatch<["lhs", "result"]>,
     DeclareOpInterfaceMethods<ArithFastMathInterface>]> {
  let summary = "floating point raised to the signed integer power";
  let description = [{
    The `fpowi` operation takes a `base` operand of floating point type
    (i.e. scalar, tensor or vector) and a `power` operand of integer type
    (also scalar, tensor or vector) and returns one result of the same type
    as `base`. The result is `base` raised to the power of `power`.
    The operation is elementwise for non-scalars, e.g.:

    ```mlir
    %v = math.fpowi %base, %power : vector<2xf32>, vector<2xi32
    ```

    The result is a vector of:

    ```
    [<math.fpowi %base[0], %power[0]>, <math.fpowi %base[1], %power[1]>]
    ```

    Example:

    ```mlir
    // Scalar exponentiation.
    %a = math.fpowi %base, %power : f64, i32
    ```
  }];

  let arguments = (ins FloatLike:$lhs, SignlessIntegerLike:$rhs,
      DefaultValuedAttr<Arith_FastMathAttr,
                        "::mlir::arith::FastMathFlags::none">:$fastmath);
  let results = (outs FloatLike:$result);
  let assemblyFormat = [{ $lhs `,` $rhs (`fastmath` `` $fastmath^)?
                          attr-dict `:` type($lhs) `,` type($rhs) }];

  // TODO: add a constant folder using pow[f] for cases, when
  //       the power argument is exactly representable in floating
  //       point type of the base.
}

#endif // MATH_OPS


//===-- NVGPU.td - NVGPU dialect operation definitions *- tablegen -*------===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This file defines the basic operations for the NVGPU dialect.
//
// This NVGPU provides a bridge between the target agnostic GPU and Vector
// dialects and lower level NVVM dialect. This allow representing PTX specific
// operations while using MLIR high level concepts like memref and 2-D vector.
//
// Ops semantic are going to be based on vendor specific PTX defintion:
// https://docs.nvidia.com/cuda/parallel-thread-execution/index.html
//
//===----------------------------------------------------------------------===//

#ifndef NVGPU
#define NVGPU

include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/IR/AttrTypeBase.td"
include "mlir/IR/OpBase.td"
include "mlir/IR/EnumAttr.td"

def NVGPU_Dialect : Dialect {
  let name = "nvgpu";
  let cppNamespace = "::mlir::nvgpu";
  let description = [{
    The `NVGPU` dialect provides a bridge between higher-level target-agnostic
    dialects (GPU and Vector) and the lower-level target-specific dialect
    (LLVM IR based NVVM dialect) for NVIDIA GPUs. This allow representing PTX
    specific operations while using MLIR high level dialects such as Memref
    and Vector for memory and target-specific register operands, respectively.
  }];

  let useDefaultTypePrinterParser = 1;
  let useDefaultAttributePrinterParser = 1;
  
  let extraClassDeclaration = [{
    /// Return true if the given MemRefType has an integer address
    /// space that matches the NVVM shared memory address space or
    /// is a gpu::AddressSpaceAttr attribute with value 'workgroup`.
    static bool hasSharedMemoryAddressSpace(MemRefType type);

    /// Return true if the given Attribute has an integer address
    /// space that matches the NVVM shared memory address space or
    /// is a gpu::AddressSpaceAttr attribute with value 'workgroup`.
    static bool isSharedMemoryAddressSpace(Attribute type);

    /// Defines the MemRef memory space attribute numeric value that indicates
    /// a memref is located in global memory. This should correspond to the
    /// value used in NVVM.
    static constexpr unsigned kGlobaldMemoryAddressSpace = 1;

    /// Defines the MemRef memory space attribute numeric value that indicates
    /// a memref is located in shared memory. This should correspond to the
    /// value used in NVVM.
    static constexpr unsigned kSharedMemoryAddressSpace = 3;
  }];
}

//===----------------------------------------------------------------------===//
// NVGPU Attribute Definitions
//===----------------------------------------------------------------------===//

def TensorMapSwizzleNone : I32EnumAttrCase<"SWIZZLE_NONE", 0, "none">;
def TensorMapSwizzle32B  : I32EnumAttrCase<"SWIZZLE_32B", 1, "swizzle_32b">;
def TensorMapSwizzle64B  : I32EnumAttrCase<"SWIZZLE_64B", 2, "swizzle_64b">;
def TensorMapSwizzle128B : I32EnumAttrCase<"SWIZZLE_128B", 3, "swizzle_128b">;
def TensorMapSwizzleKind : I32EnumAttr<"TensorMapSwizzleKind", 
                                "Tensor map swizzling mode of shared memory banks",
  [ TensorMapSwizzleNone, TensorMapSwizzle32B, TensorMapSwizzle64B, 
    TensorMapSwizzle128B]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::nvgpu";
}

def TensorMapL2PromoNone : I32EnumAttrCase<"L2PROMO_NONE", 0, "none">;
def TensorMapL2Promo64B  : I32EnumAttrCase<"L2PROMO_64B", 1, "l2promo_64b">;
def TensorMapL2Promo128B : I32EnumAttrCase<"L2PROMO_128B", 2, "l2promo_128b">;
def TensorMapL2Promo256B : I32EnumAttrCase<"L2PROMO_256B", 3, "l2promo_256b">;
def TensorMapL2PromoKind : I32EnumAttr<"TensorMapL2PromoKind", 
                                "Tensor map L2 promotion type",
  [ TensorMapL2PromoNone, TensorMapL2Promo64B, TensorMapL2Promo128B, 
    TensorMapL2Promo256B]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::nvgpu";
}

def TensorMapOOBZero : I32EnumAttrCase<"OOB_ZERO", 0, "zero">;
def TensorMapOOBNaN  : I32EnumAttrCase<"OOB_NAN", 1, "nan">;
def TensorMapOOBKind : I32EnumAttr<"TensorMapOOBKind", 
                                "Tensor map out-of-bounds fill type",
  [ TensorMapOOBZero, TensorMapOOBNaN]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::nvgpu";
}

def TensorMapInterleaveNone : I32EnumAttrCase<"INTERLEAVE_NONE", 0, "none">;
def TensorMapInterleave16B  : I32EnumAttrCase<"INTERLEAVE_16B", 1, "interleave_16b">;
def TensorMapInterleave32B  : I32EnumAttrCase<"INTERLEAVE_32B", 2, "interleave_32b">;
def TensorMapInterleaveKind : I32EnumAttr<"TensorMapInterleaveKind", 
                                "Tensor map interleave layout type",
  [ TensorMapInterleaveNone, TensorMapInterleave16B, TensorMapInterleave32B]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::nvgpu";
}

def RcpApprox : I32EnumAttrCase<"APPROX", 0, "approx">;
def RcpRN     : I32EnumAttrCase<"RN", 1, "rn">;
def RcpRZ     : I32EnumAttrCase<"RZ", 2, "rz">;
def RcpRM     : I32EnumAttrCase<"RM", 3, "rm">;
def RcpRP     : I32EnumAttrCase<"RP", 4, "rp">;
def RcpRoundingMode   : I32EnumAttr<"RcpRoundingMode", "Rounding mode of rcp",
  [RcpApprox, RcpRN, RcpRZ, RcpRM, RcpRP]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::nvgpu";
}

def TensorMapSwizzleAttr : EnumAttr<NVGPU_Dialect, TensorMapSwizzleKind, "swizzle">;
def TensorMapL2PromoAttr : EnumAttr<NVGPU_Dialect, TensorMapL2PromoKind, "l2promo">;
def TensorMapOOBAttr : EnumAttr<NVGPU_Dialect, TensorMapOOBKind, "oob">;
def TensorMapInterleaveAttr : EnumAttr<NVGPU_Dialect, TensorMapInterleaveKind, "interleave">;
def RcpRoundingModeAttr : EnumAttr<NVGPU_Dialect, RcpRoundingMode, "rcp_rounding_mode">;

//===----------------------------------------------------------------------===//
// NVGPU Type Definitions
//===----------------------------------------------------------------------===//

class NVGPU_Type<string name, string typeMnemonic,
        list<Trait> traits = []> : TypeDef<NVGPU_Dialect, name, traits> {
  let mnemonic = typeMnemonic;
}

def NVGPU_DeviceAsyncToken : NVGPU_Type<"DeviceAsyncToken",
                                        "device.async.token", []> {
  let summary = "device async token type";
  let description = [{
    `nvgpu.device.async.token` is a type returned by an asynchronous operation
    that runs on the GPU (device). It is used to establish an SSA-based link
    between the async operation (e.g. DeviceAsyncCopy) and operations that
    group or synchronize the async operations (e.g. DeviceAsyncCreateGroupOp,
    DeviceAsyncWaitOp).
  }];
}

def NVGPU_MBarrierGroup : NVGPU_Type<"MBarrierGroup", "mbarrier.group", []> {
  let summary = "mbarrier barrier type";
  let description = [{
    This is the type for one or more mbarrier object in shared memory that is 
    used to synchronize a variable number of threads.

    If `num_barriers` is not set, the number of mbarrier objects is 1.

    A mbarrier object is 64 bit with 8 byte alignment. The mbarrier object 
    can be initiated and invalidated.

    [See for more details in PTX ISA](https://docs.nvidia.com/cuda/parallel-thread-execution/#size-and-alignment-of-mbarrier-object)
  }];    
  let parameters = (ins "Attribute":$memorySpace, DefaultValuedParameter<"unsigned", "1">:$num_barriers);
  let assemblyFormat = "`<` struct(params) `>`";
  let builders = [
    TypeBuilder<(ins "Attribute":$memorySpace), [{
      return $_get($_ctxt, memorySpace, 1);
    }]>
  ];
}

def NVGPU_MBarrierToken : NVGPU_Type<"MBarrierToken", "mbarrier.token", []> { }

// https://docs.nvidia.com/cuda/parallel-thread-execution/#tensor-map
def NVGPU_TensorMapDescriptor : NVGPU_Type<"TensorMapDescriptor", "tensormap.descriptor", []> {
  let summary = "TensorMap descriptor";
  let parameters = (ins "MemRefType":$tensor,
                        EnumParameter<TensorMapSwizzleKind>:$swizzle,
                        EnumParameter<TensorMapL2PromoKind>:$l2promo,
                        EnumParameter<TensorMapOOBKind>:$oob,
                        EnumParameter<TensorMapInterleaveKind>:$interleave);
  let description = [{
    `nvgpu.tma.descriptor` is a type that represents a TMA descriptor. It is 
    128-byte object either in constant space or kernel paramater.    
  }];
  let assemblyFormat = "`<` struct(params) `>`";
}

def NVGPU_WarpgroupMatrixDescriptor : NVGPU_Type<"WarpgroupMatrixDescriptor", "warpgroup.descriptor", []> {
  let summary = "Warpgroup matrix descriptor type";
  let description = [{
  The descriptor specifies the properties of the matrix in shared memory that 
  is a multiplicand in the matrix multiply and accumulate operation. 
  
  The descriptor is a 64-bit value contained in a register with the following:
  ```
  +---------+-----+-----------+-----+-----------+-----+-----+-----------+-----+
  |   0-13  |14-15|   16-29   |30-31|   32-45   |46-48|49-51|   52-61   |62-63|
  +---------+-----+-----------+-----+-----------+-----+-----+-----------+-----+
  |  14bits |2bits|   14bits  |2bits|   14bits  |2bits|3bits|   10bits  |2bits|
  +---------+-----+-----------+-----+-----------+-----+-----+-----------+-----+
  | BaseAddr|  0  | LeadingDim|  0  |   Stride  |  0  |Offst|     0     |Swzle|
  +---------+-----+-----------+-----+-----------+-----+-----+-----------+-----+
  ```
   
  [See for more details in PTX ISA](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-shared-memory-layout-matrix-descriptor) 
  
  }];  
  let parameters = (ins "MemRefType":$tensor);
  let assemblyFormat = "`<` struct(params) `>`";
}

def NVGPU_WarpgroupAccumulator : NVGPU_Type<"WarpgroupAccumulator", "warpgroup.accumulator", []> {
  let parameters = (ins "VectorType":$fragmented);
  let assemblyFormat = "`<` struct(params) `>`";
  let description = [{
    This type represents the result matrix obtained from `nvgpu.warpgroup.mma`. 
    The `$fragmented` type signifies the distributed or fragmented result 
    vector that is collectively owned by all the threads in the warp-group 
    that executed `nvgpu.warpgroup.mma`.
    [See the details of register fragment layout for accumulator matrix D]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#wgmma-64n16-d) 
  }];
}

//===----------------------------------------------------------------------===//
// NVGPU Op Definitions
//===----------------------------------------------------------------------===//

class NVGPU_Op<string mnemonic, list<Trait> traits = []> :
  Op<NVGPU_Dialect, mnemonic, traits> {}

def NVGPU_LdMatrixOp : NVGPU_Op<"ldmatrix", [
                                MemoryEffects<[MemRead]>,
                                PredOpTrait<"srcMemref and res have same element type",
                                            TCresVTEtIsSameAsOp<0, 0>>]> {
  let description = [{
    The `nvgpu.ldmatrix` op represents loading a matrix fragment from
    memory to registers. The source and result type must be compatible
    with lowering to the `nvvm.ldmatrix` instruction. This op represents
    the distributed version of a `vector.transfer_read` as an intermediate
    step between lowering from `vector.transfer_read` to `nvvm.ldmatrix`.

    This operation is meant to follow the semantic of described here:
    https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix

    Example:
    ```mlir
    %0 = nvgpu.ldmatrix %sm[%c0, %c0] {numTiles = 4 : i32, transpose = false} :
      memref<?x?xf16, 3> -> vector<4x2xf16>
    ```
  }];

  let arguments = (ins Arg<AnyMemRef, "", [MemReadAt<0, FullEffect>]>:$srcMemref,
                           Variadic<Index>:$indices, BoolAttr:$transpose,
                           I32Attr:$numTiles);
  let results = (outs AnyVectorOfNonZeroRank:$res);
  let assemblyFormat = [{
    $srcMemref`[` $indices `]` attr-dict `:` type($srcMemref) `->` type($res)
  }];

  let hasVerifier = 1;
}

class NVGPU_MmaSyncOp<string mnemonic> :
        NVGPU_Op<mnemonic,  [Pure,
                             PredOpTrait<"matrixA and matrixB have same element type",
                                         TCopVTEtIsSameAs<0, 1>>]> {
  code extraBaseClassDeclaration = [{
    std::array<int64_t, 3> getMmaShapeAsArray() {
      ArrayAttr mmaShape = this->getMmaShape();
      assert(mmaShape.size() == 3 && "mmaShape should be three integers");
      return {::llvm::cast<IntegerAttr>(mmaShape[0]).getInt(),
              ::llvm::cast<IntegerAttr>(mmaShape[1]).getInt(),
              ::llvm::cast<IntegerAttr>(mmaShape[2]).getInt()};
    }
  }];

  let hasVerifier = 1;
}

def NVGPU_MmaSyncOp : NVGPU_MmaSyncOp<"mma.sync"> {
  let description = [{
    The `nvgpu.mma.sync` op represents the warp-level matrix-multiply-and-
    accumulate (mma) operation that is compatible with `nvvm.mma.sync`.
    The operands and results vector sizes are thread-level onwership to
    the warp-level mma operation shape. `mmaShape` attribute holds the
    warp-level matrix-multiply shape.

    The `nvgpu.mma.sync` op serves as an intermediate point between lowering from
    `vector.contract` to `nvvm.mma.sync`.

    This operation is meant to follow the semantic of described here:
      https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma

    Example:

    ```mlir
    %res = nvgpu.mma.sync (%matrixA, %matrixB, %matrixC) {mmaShape = [16, 8, 16]} :
        (vector<4x2xf16>, vector<2x2xf16>, vector<2x2xf32>) -> vector<2x2xf32>
    ```
  }];
  let arguments = (ins AnyVectorOfNonZeroRank:$matrixA,
                       AnyVectorOfNonZeroRank:$matrixB,
                       AnyVectorOfNonZeroRank:$matrixC,
                       I64ArrayAttr:$mmaShape,
                       OptionalAttr<UnitAttr>:$tf32Enabled);

  let results = (outs AnyVectorOfNonZeroRank:$res);

  let builders = [
    OpBuilder<(ins "Value":$matrixA,
                   "Value":$matrixB,
                   "Value":$matrixC,
                   "ArrayAttr":$mmaShape)>,
    OpBuilder<(ins "Value":$matrixA,
                   "Value":$matrixB,
                   "Value":$matrixC,
                   "ArrayRef<int64_t>":$mmaShape,
                   CArg<"bool", "false">:$tf32Enabled)>
  ];

  let assemblyFormat = [{
    `(` $matrixA`,` $matrixB`,` $matrixC `)` attr-dict
    `:` `(` type($matrixA) `,` type($matrixB) `,` type($matrixC) `)` `->` type($res)
  }];

  let extraClassDeclaration = extraBaseClassDeclaration;
}

def NVGPU_MmaSparseSyncMetadataType : FixedVectorOfLengthAndType<[2], [I16]>,
                        BuildableType<"::mlir::VectorType::get("
                          "{2},$_builder.getI16Type())">;

def NVGPU_MmaSparseSyncOp : NVGPU_MmaSyncOp<"mma.sp.sync"> {
  let description = [{
  The `nvgu.mma.sp.sync` operation performs a warp-distributed MMA operation
  where operand A is "structured sparse". In this case, the `matrixA` operand
  represents the (warp-distributed) non-zero values of operand A, and the
  `sparse_metadata` operand provides the indices.

  The full description of the sparsity storage format and distribution scheme is
  described in the PTX docs. This operation is meant to follow the semantic
  described in the PTX documentation here:
  https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-for-sparse-mma

  The way the indices are distributed among the threads in a warp is controlled
  by the optional `sparsity_selector` operand, which is `0` by default. For
  more information, please consult the PTX documentation linked above.

  Example (targetingthe f16 16x8x32 `mma.sp` PTX instruction):

  ```mlir
  nvgpu.mma.sp.sync (%a, %b, %c) metadata (%meta) {mmaShape = [16, 8, 32]} :
    (vector<4x2xf16>, vector<2x2xf16>, vector<2x2xf16>) -> vector<2x2xf16>
  ```
  }];

  let arguments = (ins AnyVectorOfNonZeroRank:$matrixA,
                       AnyVectorOfNonZeroRank:$matrixB,
                       AnyVectorOfNonZeroRank:$matrixC,
                       NVGPU_MmaSparseSyncMetadataType:$sparseMetadata,
                       I64ArrayAttr:$mmaShape,
                       DefaultValuedAttr<I32Attr, "0">:$sparsitySelector,
                       OptionalAttr<UnitAttr>:$tf32Enabled
                       );

  let results = (outs AnyVectorOfNonZeroRank:$res);

  let builders = [
    OpBuilder<(ins "Value":$matrixA,
                   "Value":$matrixB,
                   "Value":$matrixC,
                   "Value":$sparseMetadata,
                   "ArrayRef<int64_t>":$mmaShape)>
  ];

  let assemblyFormat = [{
    `(` $matrixA`,` $matrixB`,` $matrixC `)` `metadata` `(` $sparseMetadata `)` attr-dict
    `:` `(` type($matrixA) `,` type($matrixB) `,` type($matrixC) `)` `->` type($res)
  }];

  let extraClassDeclaration = extraBaseClassDeclaration;
}

def NVGPU_DeviceAsyncCopyOp : NVGPU_Op<"device_async_copy", [
                                       AttrSizedOperandSegments]> {
  let summary = "device-side asynchronous copy";
  let description = [{
    The `nvgpu.device_async_copy` op initiates an asynchronous copy operation of
    elements from source (global memory) to the destination (shared memory)
    without blocking the thread. The async copy is added to a group.

    This op is meant to be used with `nvgpu.device_async_create_group` and
    `nvgpu.device_async_wait` to synchronize copies as explained in those ops
    descriptions.

    `bypassL1` attribute is hint to the hardware to bypass the L1 cache during
    async copy, this hint may be ignored by the hardware.

    `dstElements` attribute is the total number of elements written to
    destination (shared memory).

    `srcElements` argument is the total number of elements read from
    source (global memory).

    `srcElements` is an optional argument and when present the op only reads
    `srcElements` number of elements from the source (global memory) and zero fills
    the rest of the elements in the destination (shared memory).

    In order to do a copy and wait for the result we need the following
    combination:
    ```
    // copy 1.
    %cp1 = nvgpu.device_async_copy %A[%c0], %B[%c0], 4 :memref<16xf32> to memref<16xf32, 3>
    // copy 2.
    %cp2 = nvgpu.device_async_copy %C[%c0], %D[%c0], 4 : memref<16xf32> to memref<16xf32, 3>
    // group 1 contains copy 1 and copy 2.
    %token1 = nvgpu.device_async_create_group %cp1, %cp2
    // copy 3.
    %cp3 = nvgpu.device_async_copy %E[%c0], %F[%c0], 4 : memref<16xf32> to memref<16xf32, 3>
    // group 2 contains copy 3.
    %token2 = nvgpu.device_async_create_group %cp3
    // after the wait copy 1 and copy 2 are complete.
    nvgpu.device_async_wait %token1
    // after the wait copy 3 is complete.
    nvgpu.device_async_wait %token2
    ```

    Example:

    ```mlir
    %0 = nvgpu.device_async_copy %src[%c0, %c0], %dst[%c0, %c0, %c0], 4 :
      memref<4x5xf32> to memref<2x7x5xf32, 3>
    ```
  }];
  let results = (outs NVGPU_DeviceAsyncToken:$asyncToken);
  let arguments = (ins Arg<AnyMemRef, "", [MemWriteAt<0, FullEffect>]>:$dst,
                       Variadic<Index>:$dstIndices,
                       Arg<AnyMemRef, "", [MemReadAt<0, FullEffect>]>:$src,
                       Variadic<Index>:$srcIndices,
                       IndexAttr:$dstElements,
                       Optional<Index>:$srcElements,
                       OptionalAttr<UnitAttr>:$bypassL1);
  let assemblyFormat = [{
    $src `[` $srcIndices `]` `,` $dst `[` $dstIndices `]` `,` $dstElements (`,` $srcElements^)?
      attr-dict `:` type($src) `to` type($dst)
  }];
  let hasVerifier = 1;
}

def NVGPU_DeviceAsyncCreateGroupOp : NVGPU_Op<"device_async_create_group", []> {
  let summary = "device side asynchronous create group operation";
  let description = [{
    The `nvgpu.device_async_create_group` op creates a group of memory accesses
    containing all the pending `device_async_copy` operations associated with
    argument tokens. Each token can only be part of one group.

    It returns a token that can be use to wait until the group fully completes.

    This is meant to be used with `nvgpu.device_async_wait` to synchronize copies
    as explained in those ops descriptions.

    Groups are executed in the order they are created.

    Example:

    ```mlir
    %0 = nvgpu.device_async_create_group
  ```
  }];
  let results = (outs NVGPU_DeviceAsyncToken:$asyncToken);
  let arguments = (ins Variadic<NVGPU_DeviceAsyncToken>:$inputTokens);
  let assemblyFormat = [{
    $inputTokens attr-dict
  }];
}

def NVGPU_DeviceAsyncWaitOp : NVGPU_Op<"device_async_wait", []> {
  let summary = "Wait for async gpu ops to complete.";
  let description = [{
    The `nvgpu.device_async_wait` op will block the execution thread until the group
    associated with the source token is fully completed.

    The optional `$numGroups` attribute gives an upper bound of the number of
    groups uncompleted when the wait can unblock the thread. For example,  if
    16 async groups are pushe and `$numGroups` is set to 12, then the thread
    will unblock when 12 groups or fewer are in flight (4 groups have
    completed).

    Example:

    ```mlir
    nvgpu.device_async_wait %0
    ```
  }];
  let arguments = (ins NVGPU_DeviceAsyncToken:$asyncDependencies,
                       OptionalAttr<I32Attr>:$numGroups);
  let assemblyFormat = [{
    $asyncDependencies attr-dict
  }];
}

def NVGPU_MBarrierCreateOp : NVGPU_Op<"mbarrier.create", []> {
  let summary = "Creates a `nvgpu.mbarrier` object.";
  let description = [{
    The Op generates one or more `mbarrier` object, which is a barrier created in 
    shared memory and supports various synchronization behaviors for threads.

    The `mbarrier` object has the following type and alignment requirements:
      Type: .b64, Alignment: 8, Memory space: .shared
    
    Example:
    ```mlir
      %barrier = nvgpu.mbarrier.create -> !nvgpu.mbarrier.barrier<memorySpace = #gpu.address_space<workgroup>>
    ```
    }];
  let arguments = (ins);
  let results = (outs NVGPU_MBarrierGroup:$barriers);
  let assemblyFormat = [{
     attr-dict `->` type($barriers)
  }];
}

def NVGPU_MBarrierInitOp : NVGPU_Op<"mbarrier.init", []> {
  let summary = "Initialize the `nvgpu.mbarrier`.";
  let description = [{
    The Op initializes the `mbarrier` object with the given number of threads.

    Example:
    ```mlir
      %num_threads = gpu.block_dim x
      %barrier = nvgpu.mbarrier.create -> !nvgpu.mbarrier.barrier<memorySpace = #gpu.address_space<workgroup>>
      nvgpu.mbarrier.init %barrier, %num_threads : !nvgpu.mbarrier.barrier<memorySpace = #gpu.address_space<workgroup>>
    ```
  }];
  let arguments = (ins NVGPU_MBarrierGroup:$barriers, Index:$count, Index:$mbarId, Optional<I1>:$predicate);
  let assemblyFormat = "$barriers `[` $mbarId `]` `,` $count (`,` `predicate` `=` $predicate^)? attr-dict `:` type($barriers)";
}

def NVGPU_MBarrierTestWaitOp : NVGPU_Op<"mbarrier.test.wait", []> {
  let summary = "Checks if the `nvgpu.mbarrier` has completed its current phase.";
  let description = [{
    Checks whether the mbarrier object has completed the phase. It is is a 
    non-blocking instruction which tests for the completion of the phase.

    Example:
    ```mlir
      %isComplete = nvgpu.mbarrier.test.wait %barrier, %token : !nvgpu.mbarrier.barrier<memorySpace = #gpu.address_space<workgroup>>, !nvgpu.mbarrier.token
    ```
  }];
  let arguments = (ins NVGPU_MBarrierGroup:$barriers, NVGPU_MBarrierToken:$token, Index:$mbarId);
  let results = (outs I1:$waitComplete);
  let assemblyFormat = "$barriers `[` $mbarId `]` `,` $token attr-dict `:` type($barriers) `,` type($token)";
}

def NVGPU_MBarrierArriveOp : NVGPU_Op<"mbarrier.arrive", []> {
  let summary = "Performs arrive operation on the `nvgpu.mbarrier.arrive`.";
  let description = [{
    The Op performs arrive-on operation on the `mbarrier` object and returns a 
    `nvgpu.mbarrier.token`.

    For more information, see
    https://docs.nvidia.com/cuda/parallel-thread-execution/#arrive-on-operation-on-mbarrier-object

    Example:
    ```mlir
      %token = nvgpu.mbarrier.arrive %barrier : !nvgpu.mbarrier.barrier<memorySpace = #gpu.address_space<workgroup>> -> !nvgpu.mbarrier.token
    ```
  }];
  let arguments = (ins NVGPU_MBarrierGroup:$barriers, Index:$mbarId);
  let results = (outs NVGPU_MBarrierToken:$token);
let assemblyFormat = "$barriers `[` $mbarId `]` attr-dict `:` type($barriers) `->` type($token)";
}

def NVGPU_MBarrierArriveNoCompleteOp : NVGPU_Op<"mbarrier.arrive.nocomplete", []> {
  let summary = "Performs arrive operation on the `nvgpu.mbarrier.arrive.nocomplete` as non-blocking.";
  let description = [{
    The Op performs arrive-on operation on the `mbarrier` object and returns a 
    `nvgpu.mbarrier.token`.

    The Op does not cause the `nvgpu.mbarrier` to complete its current phase.

    Example:
    ```mlir
      %token = nvgpu.mbarrier.arrive.noComplete %barrier, %count : !nvgpu.mbarrier.barrier<memorySpace = #gpu.address_space<workgroup>> -> !nvgpu.mbarrier.token
    ```
  }];
  let arguments = (ins NVGPU_MBarrierGroup:$barriers, Index:$mbarId,
                       Index:$count);
  let results = (outs NVGPU_MBarrierToken:$token);
  let assemblyFormat = "$barriers `[` $mbarId `]` `,` $count attr-dict `:` type($barriers) `->` type($token)";
}

def NVGPU_MBarrierArriveExpectTxOp : NVGPU_Op<"mbarrier.arrive.expect_tx", []> {
  let summary = "Performs expect_tx operation on the `nvgpu.mbarrier.arrive`";
  let description = [{
    A thread executing the Op performs an expect-tx operation on the mbarrier 
    object at the location specified by the address operand $barrier. The 
    expect-tx operation, with an $txcount argument, increases the tx-count of 
    an mbarrier object by the value specified by $txcount. This makes the 
    current phase of the mbarrier object to expect and track the completion of 
    additional asynchronous transactions.
    
    The `$txCount` specifies the number of element to the expect-tx operation.

    Example:
    ```mlir
      nvgpu.mbarrier.arrive.expect_tx %barrier, %ic0 : !nvgpu.mbarrier.barrier<memorySpace = #gpu.address_space<workgroup>>
    ```
  }];
  let arguments = (ins NVGPU_MBarrierGroup:$barriers, Index:$txcount, Index:$mbarId, Optional<I1>:$predicate);
  let assemblyFormat = "$barriers `[` $mbarId `]` `,` $txcount  (`,` `predicate` `=` $predicate^)? attr-dict `:` type($barriers)";
}

def NVGPU_MBarrierTryWaitParityOp : NVGPU_Op<"mbarrier.try_wait.parity", []> {
  let summary = "Waits for the `nvgpu.mbarrier` to complete its current phase.";
  let description = [{
    Checks whether the mbarrier object has completed the phase. It is is a 
    potentially blocking instruction which tests for the completion of the 
    phase. Suspended thread resumes execution when the specified phase completes 
    OR before the phase completes following a system-dependent time limit. 

    The `$phaseParity` specifies either even phase (0) or odd phase (1) to 
    wait.

    Example:
    ```mlir
      nvgpu.mbarrier.try_wait.parity %barrier, %phaseParity, %ticks : !nvgpu.mbarrier.barrier<memorySpace = #gpu.address_space<workgroup>>
    ```
  }];
  let arguments = (ins NVGPU_MBarrierGroup:$barriers, I1:$phaseParity, Index:$ticks, Index:$mbarId);
  let assemblyFormat = "$barriers `[` $mbarId `]` `,` $phaseParity `,` $ticks attr-dict `:` type($barriers)";  
}

def NVGPU_TmaPrefetchOp : NVGPU_Op<"tma.prefetch.descriptor", []> {
  let summary = "Prefetch given `nvgpu.tensormap.descriptor` ";
  let description = [{
    The Op brings the cache line containing the given `$tmaDescriptor` for 
    subsequent use by the `tma.async.load` instruction.
  }];
  let arguments = (ins NVGPU_TensorMapDescriptor:$tensorMapDescriptor, Optional<I1>:$predicate);
  let assemblyFormat = [{
    $tensorMapDescriptor (`,` `predicate` `=` $predicate^)? attr-dict `:` type($tensorMapDescriptor)
  }];
}

def NVGPU_TmaAsyncLoadOp : NVGPU_Op<"tma.async.load", [AttrSizedOperandSegments]> {
  let summary = "TMA asynchronous load";
  let description = [{
    The Op loads a tile memory region from global memory to shared memory by 
    Tensor Memory Access (TMA).
    
    `$tensorMapDescriptor` is tensor map descriptor which has information about
    tile shape. The descriptor is created by `nvgpu.tma.create.descriptor`

    The Op uses `$barrier` mbarrier based completion mechanism. 
  }];  
  let arguments = (ins  Arg<AnyMemRef, "", [MemWriteAt<0, FullEffect>]>:$dst,
                        NVGPU_MBarrierGroup:$barriers,
                        NVGPU_TensorMapDescriptor:$tensorMapDescriptor,
                        Variadic<Index>:$coordinates, 
                        Index:$mbarId,
                        Optional<I16>:$multicastMask,
                        Optional<I1>:$predicate);
  let assemblyFormat = [{
    $tensorMapDescriptor `[` $coordinates `]` `,` $barriers `[` $mbarId `]` 
      `to` $dst
      (`multicast_mask` `=` $multicastMask^ )?
      (`,` `predicate` `=` $predicate^)?
      attr-dict `:` type($tensorMapDescriptor) `,` type($barriers) 
      `->` type($dst)
  }];
  let hasVerifier = 1;

}

def NVGPU_TmaAsyncStoreOp : NVGPU_Op<"tma.async.store", [AttrSizedOperandSegments]> {
  let summary = "TMA asynchronous store";
  let description = [{
    The Op store a tile memory region from global memory to shared memory by 
    Tensor Memory Access (TMA).
    
    `$tensorMapDescriptor` is tensor map descriptor which has information about
    tile shape. The descriptor is created by `nvgpu.tma.create.descriptor`
  }];  
  let arguments = (ins  Arg<AnyMemRef, "", [MemReadAt<0, FullEffect>]>:$src,
                        Arg<NVGPU_TensorMapDescriptor, "", [MemWriteAt<0, FullEffect>]>:$tensorMapDescriptor,
                        Variadic<Index>:$coordinates, 
                        Optional<I1>:$predicate);
  let assemblyFormat = [{
      $src `to` $tensorMapDescriptor `[` $coordinates `]`
      (`,` `predicate` `=` $predicate^)?
      attr-dict `:` type($src)
      `->` type($tensorMapDescriptor)
  }];
  let hasVerifier = 1;
}

def NVGPU_TmaCreateDescriptorOp : NVGPU_Op<"tma.create.descriptor", []> {
  let summary = "TMA create descriptor";
  let description = [{
    The Op creates a tensor map descriptor object representing tiled memory 
    region. To do that it calls CUDA Driver's `cuTensorMapEncodeTiled`. The 
    descriptor is used by Tensor Memory Access (TMA).

    The `tensor` is the source tensor to be tiled. 

    The `boxDimensions` is the size of the tiled memory region in each dimension.

    For more information see below:
    https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html
  }];

  let arguments = (ins AnyUnrankedMemRef:$tensor,
                       Variadic<Index>:$boxDimensions);
  let results = (outs NVGPU_TensorMapDescriptor:$tensorMap);
  let assemblyFormat = [{
         $tensor `box` `[` $boxDimensions `]` attr-dict `:` type($tensor) `->` type($tensorMap)
  }];
  let hasVerifier = 1;
}

def NVGPU_WarpgroupGenerateDescriptorOp : NVGPU_Op<"warpgroup.generate.descriptor", []> {
  let summary = "Generate a warpgroup matrix descriptor";
  let description = [{
  This Op builds a `nvgpu.warpgroup.descriptor` that is used by 
  `nvgpu.warpgroup.mma` to perform warpgroup-level matrix multiply and 
  accumulate.

  The descriptor specifies the properties of the matrix in shared memory that 
  is a multiplicand in the matrix multiply and accumulate operation. 
  }];  
  let results = (outs NVGPU_WarpgroupMatrixDescriptor:$descriptor);
  let arguments = (ins Arg<AnyMemRef, "", [MemRead]>:$tensor, 
                       NVGPU_TensorMapDescriptor:$tensorMap);
  let assemblyFormat = [{$tensor `,` $tensorMap attr-dict `:` type($tensor) `,` type($tensorMap) `->` type($descriptor)}];
  let hasVerifier = 1;
}

def NVGPU_WarpgroupMmaOp : NVGPU_Op<"warpgroup.mma"> {
  let description = [{
    The `nvgpu.warpgroup.mma` op performs the warpgroup-level (4 warps) 
    matrix-multiply-and-accumulate (mma) operation that results in 
    `nvvm.wgmma.mma_async`. 
    
    The operands are `descriptorA` and `descriptorB` that are wgmma matrix 
    descriptors that shows the properties of the matrix in shared memory. The 
    results are thread-level ownership to the warpgroup-level mma operation 
    shape. The shape is deduced from the descriptor types and output vector.

    The Op encapsulates multiple `nvvm.wgmma.mma_async` operations to complete 
    the given shape. As `nvvm.wgmma.async` Op, or its corresponding PTX 
    instruction, is asynchronous, this Op groups the `nvvm.wgmma.async` and 
    surrounds them between `wgmma.fence.aligned` and 
    `wgmma.commit.group.sync.aligned`, `wgmma.wait.group.sync.aligned` Ops.

    Example:
    ```mlir
      %r1,%r2 = nvgpu.warpgroup.mma %descA, %descB, %acc1, %acc2: 
                 !nvgpu.warpgroup.descriptor<tensor = memref<128x64xf16, 3>>, 
                 !nvgpu.warpgroup.descriptor<tensor = memref<64x128xf16, 3>>, 
                 !nvgpu.warpgroup.accumulator<fragmented = vector<64x128xf32>>,
                 !nvgpu.warpgroup.accumulator<fragmented = vector<64x128xf32>>
                 -> 
                 !nvgpu.warpgroup.accumulator<fragmented = vector<64x128xf32>>,
                 !nvgpu.warpgroup.accumulator<fragmented = vector<64x128xf32>>
    ```
  }];

  let arguments = (ins NVGPU_WarpgroupMatrixDescriptor:$descriptorA, 
                       NVGPU_WarpgroupMatrixDescriptor:$descriptorB,                                               
                       DefaultValuedOptionalAttr<I32Attr, "1">:$waitGroup,
                       OptionalAttr<UnitAttr>:$transposeA,
                       OptionalAttr<UnitAttr>:$transposeB,
                       NVGPU_WarpgroupAccumulator:$matrixC);
  let results = (outs NVGPU_WarpgroupAccumulator:$matrixD);
  let assemblyFormat = [{    
    $descriptorA`,` $descriptorB`,` $matrixC attr-dict
    `:` type($descriptorA) `,` type($descriptorB) `,` type($matrixC) `->` type($matrixD)
  }];
  let hasVerifier = 1;
}

def NVGPU_WarpgroupMmaStoreOp : NVGPU_Op<"warpgroup.mma.store"> {
  let description = [{
    The `nvgpu.warpgroup.mma.store` op performs the store of fragmented result 
    in $matrixD to given memref. 

    [See the details of register fragment layout for accumulator matrix D]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#wgmma-64n16-d) 

    Note that, the op must be run with warp group.
  }];

  let arguments = (ins NVGPU_WarpgroupAccumulator:$matrixD,
                       Arg<AnyMemRef, "", [MemWrite]>:$dstMemref);
  
  let assemblyFormat = [{
    $matrixD `,` $dstMemref attr-dict `:` type($matrixD) `to` type($dstMemref)
  }];
  let hasVerifier = 1;
}

def NVGPU_WarpgroupMmaInitAccumulatorOp : NVGPU_Op<"warpgroup.mma.init.accumulator"> {  
  let summary = "Initializes the accumulator matrix";

  let description = [{
    This Op generates and initializes the accumulator matrix for 
    `nvgpu.warpgroup.mma` op to perform matrix-multiply-and-accumulate.
  }];
  let results = (outs NVGPU_WarpgroupAccumulator:$matrixC);
  let assemblyFormat = "attr-dict `->` type($matrixC)";
  let hasVerifier = 1;
}

def NVGPU_RcpOp : NVGPU_Op<"rcp", [Pure,
                                   SameOperandsAndResultType]> {
  let summary = "The reciprocal calculation for vector types";
  let description = [{
    Reciprocal calculation for `vector` types using `nvvm.rcp` OPs.

    Currently, only the `approx` rounding mode and `ftz` are supported, and only for the `f32` type.

    The input and output must be of the same vector type and shape.
  }];
  let arguments = (ins VectorOfNonZeroRankOf<[F32]>:$in,
                       DefaultValuedAttr<RcpRoundingModeAttr, "RcpRoundingMode::APPROX">:$rounding,
                       UnitAttr:$ftz);
  let results = (outs VectorOfNonZeroRankOf<[F32]>:$out);
  let assemblyFormat = [{
    $in `{` `rounding` `=` $rounding (`,` `ftz` $ftz^)? `}` 
    attr-dict `:` type($out)
  }];
  let hasVerifier = 1;
}
#endif // NVGPU


//===-- LLVMOps.td - LLVM IR dialect op definition file ----*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This is the LLVM IR operation definition file.
//
//===----------------------------------------------------------------------===//

#ifndef LLVMIR_OPS
#define LLVMIR_OPS

include "mlir/Dialect/LLVMIR/LLVMAttrDefs.td"
include "mlir/Dialect/LLVMIR/LLVMEnums.td"
include "mlir/Dialect/LLVMIR/LLVMOpBase.td"
include "mlir/IR/EnumAttr.td"
include "mlir/Interfaces/FunctionInterfaces.td"
include "mlir/IR/SymbolInterfaces.td"
include "mlir/Interfaces/CallInterfaces.td"
include "mlir/Interfaces/ControlFlowInterfaces.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/MemorySlotInterfaces.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/Interfaces/ViewLikeInterface.td"

class LLVM_Builder<string builder> {
  string llvmBuilder = builder;
}

// Base class for LLVM terminator operations.  All terminator operations have
// zero results and an optional list of successors.
class LLVM_TerminatorOp<string mnemonic, list<Trait> traits = []> :
    LLVM_Op<mnemonic, !listconcat(traits, [Terminator])>;

// Class for arithmetic binary operations.
class LLVM_ArithmeticOpBase<Type type, string mnemonic,
                            string instName, list<Trait> traits = []> :
    LLVM_Op<mnemonic,
           !listconcat([Pure, SameOperandsAndResultType], traits)>,
    LLVM_Builder<"$res = builder.Create" # instName # "($lhs, $rhs);"> {
  dag commonArgs = (ins LLVM_ScalarOrVectorOf<type>:$lhs,
                    LLVM_ScalarOrVectorOf<type>:$rhs);
  let results = (outs LLVM_ScalarOrVectorOf<type>:$res);
  let builders = [LLVM_OneResultOpBuilder];
  let assemblyFormat = "$lhs `,` $rhs attr-dict `:` type($res)";
  string llvmInstName = instName;
}
class LLVM_IntArithmeticOp<string mnemonic, string instName,
                           list<Trait> traits = []> :
    LLVM_ArithmeticOpBase<AnySignlessInteger, mnemonic, instName, traits> {
  let arguments = commonArgs;
  string mlirBuilder = [{
    $res = $_builder.create<$_qualCppClassName>($_location, $lhs, $rhs);
  }];
}
class LLVM_IntArithmeticOpWithOverflowFlag<string mnemonic, string instName,
                                   list<Trait> traits = []> :
    LLVM_ArithmeticOpBase<AnySignlessInteger, mnemonic, instName,
    !listconcat([DeclareOpInterfaceMethods<IntegerOverflowFlagsInterface>], traits)> {
  dag iofArg = (ins EnumProperty<"IntegerOverflowFlags", "", "IntegerOverflowFlags::none">:$overflowFlags);
  let arguments = !con(commonArgs, iofArg);

  string mlirBuilder = [{
    auto op = $_builder.create<$_qualCppClassName>($_location, $lhs, $rhs);
    moduleImport.setIntegerOverflowFlags(inst, op);
    $res = op;
  }];
  let assemblyFormat = [{
    $lhs `,` $rhs `` custom<OverflowFlags>($overflowFlags) attr-dict `:` type($res)
  }];
  string llvmBuilder =
    "$res = builder.Create" # instName #
    "($lhs, $rhs, /*Name=*/\"\", op.hasNoUnsignedWrap(), op.hasNoSignedWrap());";
}
class LLVM_IntArithmeticOpWithExactFlag<string mnemonic, string instName,
                                   list<Trait> traits = []> :
    LLVM_ArithmeticOpBase<AnySignlessInteger, mnemonic, instName,
    !listconcat([DeclareOpInterfaceMethods<ExactFlagInterface>], traits)> {
  let arguments = !con(commonArgs, (ins UnitAttr:$isExact));

  string mlirBuilder = [{
    auto op = $_builder.create<$_qualCppClassName>($_location, $lhs, $rhs);
    moduleImport.setExactFlag(inst, op);
    $res = op;
  }];
  let assemblyFormat = [{
    (`exact` $isExact^)? $lhs `,` $rhs attr-dict `:` type($res)
  }];
  string llvmBuilder =
    "$res = builder.Create" # instName #
    "($lhs, $rhs, /*Name=*/\"\", op.getIsExact());";
}
class LLVM_IntArithmeticOpWithDisjointFlag<string mnemonic, string instName,
                                   list<Trait> traits = []> :
    LLVM_ArithmeticOpBase<AnySignlessInteger, mnemonic, instName,
    !listconcat([DeclareOpInterfaceMethods<DisjointFlagInterface>], traits)> {
  let arguments = !con(commonArgs, (ins UnitAttr:$isDisjoint));

  string mlirBuilder = [{
    auto op = $_builder.create<$_qualCppClassName>($_location, $lhs, $rhs);
    moduleImport.setDisjointFlag(inst, op);
    $res = op;
  }];
  let assemblyFormat = [{
    (`disjoint` $isDisjoint^)? $lhs `,` $rhs attr-dict `:` type($res)
  }];
  string llvmBuilder = [{
    auto inst = builder.Create}] # instName # [{($lhs, $rhs, /*Name=*/"");
    moduleTranslation.setDisjointFlag(op, inst);
    $res = inst;
  }];
}
class LLVM_FloatArithmeticOp<string mnemonic, string instName,
                             list<Trait> traits = []> :
    LLVM_ArithmeticOpBase<LLVM_AnyFloat, mnemonic, instName,
    !listconcat([DeclareOpInterfaceMethods<FastmathFlagsInterface>], traits)> {
  dag fmfArg = (
    ins DefaultValuedAttr<LLVM_FastmathFlagsAttr, "{}">:$fastmathFlags);
  let arguments = !con(commonArgs, fmfArg);
  string mlirBuilder = [{
    auto op = $_builder.create<$_qualCppClassName>($_location, $lhs, $rhs);
    moduleImport.setFastmathFlagsAttr(inst, op);
    $res = op;
  }];
}

// Class for arithmetic unary operations.
class LLVM_UnaryFloatArithmeticOp<Type type, string mnemonic,
                                  string instName, list<Trait> traits = []> :
    LLVM_Op<mnemonic,
           !listconcat([Pure, SameOperandsAndResultType, DeclareOpInterfaceMethods<FastmathFlagsInterface>], traits)>,
    LLVM_Builder<"$res = builder.Create" # instName # "($operand);"> {
  let arguments = (
    ins type:$operand,
    DefaultValuedAttr<LLVM_FastmathFlagsAttr, "{}">:$fastmathFlags);
  let results = (outs type:$res);
  let builders = [LLVM_OneResultOpBuilder];
  let assemblyFormat = "$operand attr-dict `:` type($res)";
  string llvmInstName = instName;
  string mlirBuilder = [{
    auto op = $_builder.create<$_qualCppClassName>($_location, $operand);
    moduleImport.setFastmathFlagsAttr(inst, op);
    $res = op;
   }];
}

// Integer binary operations.
def LLVM_AddOp : LLVM_IntArithmeticOpWithOverflowFlag<"add", "Add",
    [Commutative]>;
def LLVM_SubOp : LLVM_IntArithmeticOpWithOverflowFlag<"sub", "Sub", []>;
def LLVM_MulOp : LLVM_IntArithmeticOpWithOverflowFlag<"mul", "Mul",
    [Commutative]>;
def LLVM_UDivOp : LLVM_IntArithmeticOpWithExactFlag<"udiv", "UDiv">;
def LLVM_SDivOp : LLVM_IntArithmeticOpWithExactFlag<"sdiv", "SDiv">;
def LLVM_URemOp : LLVM_IntArithmeticOp<"urem", "URem">;
def LLVM_SRemOp : LLVM_IntArithmeticOp<"srem", "SRem">;
def LLVM_AndOp : LLVM_IntArithmeticOp<"and", "And">;
def LLVM_OrOp : LLVM_IntArithmeticOpWithDisjointFlag<"or", "Or"> {
  let hasFolder = 1;
}
def LLVM_XOrOp : LLVM_IntArithmeticOp<"xor", "Xor">;
def LLVM_ShlOp : LLVM_IntArithmeticOpWithOverflowFlag<"shl", "Shl", []> {
  let hasFolder = 1;
}
def LLVM_LShrOp : LLVM_IntArithmeticOpWithExactFlag<"lshr", "LShr">;
def LLVM_AShrOp : LLVM_IntArithmeticOpWithExactFlag<"ashr", "AShr">;

// Base class for compare operations. A compare operation takes two operands
// of the same type and returns a boolean result. If the operands are
// vectors, then the result has to be a boolean vector of the same shape.
class LLVM_ArithmeticCmpOp<string mnemonic, list<Trait> traits = []> :
    LLVM_Op<mnemonic, traits # [SameTypeOperands, TypesMatchWith<
    "result type has i1 element type and same shape as operands",
    "lhs", "res", "::getI1SameShape($_self)">]> {
  let results = (outs LLVM_ScalarOrVectorOf<I1>:$res);
}

// Other integer operations.
def LLVM_ICmpOp : LLVM_ArithmeticCmpOp<"icmp", [Pure]> {
  let arguments = (ins ICmpPredicate:$predicate,
                   AnyTypeOf<[LLVM_ScalarOrVectorOf<AnySignlessInteger>,
                              LLVM_ScalarOrVectorOf<LLVM_AnyPointer>]>:$lhs,
                   AnyTypeOf<[LLVM_ScalarOrVectorOf<AnySignlessInteger>,
                              LLVM_ScalarOrVectorOf<LLVM_AnyPointer>]>:$rhs);
  let hasCustomAssemblyFormat = 1;
  string llvmInstName = "ICmp";
  string llvmBuilder = [{
    $res = builder.CreateICmp(
            convertICmpPredicateToLLVM($predicate), $lhs, $rhs);
  }];
  string mlirBuilder = [{
    auto *iCmpInst = cast<llvm::ICmpInst>(inst);
    $res = $_builder.create<$_qualCppClassName>($_location,
            convertICmpPredicateFromLLVM(iCmpInst->getPredicate()), $lhs, $rhs);
  }];
  // Set the $predicate index to -1 to indicate there is no matching operand
  // and decrement the following indices.
  list<int> llvmArgIndices = [-1, 0, 1];
  let hasFolder = 1;
}

// Other floating-point operations.
def LLVM_FCmpOp : LLVM_ArithmeticCmpOp<"fcmp", [
    Pure, DeclareOpInterfaceMethods<FastmathFlagsInterface>]> {
  let arguments = (ins FCmpPredicate:$predicate,
                   LLVM_ScalarOrVectorOf<LLVM_AnyFloat>:$lhs,
                   LLVM_ScalarOrVectorOf<LLVM_AnyFloat>:$rhs,
                   DefaultValuedAttr<LLVM_FastmathFlagsAttr,
                                     "{}">:$fastmathFlags);
  let hasCustomAssemblyFormat = 1;
  string llvmInstName = "FCmp";
  string llvmBuilder = [{
    $res = builder.CreateFCmp(convertFCmpPredicateToLLVM($predicate), $lhs, $rhs);
  }];
  string mlirBuilder = [{
    auto *fCmpInst = cast<llvm::FCmpInst>(inst);
    auto op = $_builder.create<$_qualCppClassName>(
      $_location, convertFCmpPredicateFromLLVM(fCmpInst->getPredicate()), $lhs, $rhs);
    moduleImport.setFastmathFlagsAttr(inst, op);
    $res = op;
  }];
  // Set the $predicate index to -1 to indicate there is no matching operand
  // and decrement the following indices.
  list<int> llvmArgIndices = [-1, 0, 1, 2];
}

// Floating point binary operations.
def LLVM_FAddOp : LLVM_FloatArithmeticOp<"fadd", "FAdd">;
def LLVM_FSubOp : LLVM_FloatArithmeticOp<"fsub", "FSub">;
def LLVM_FMulOp : LLVM_FloatArithmeticOp<"fmul", "FMul">;
def LLVM_FDivOp : LLVM_FloatArithmeticOp<"fdiv", "FDiv">;
def LLVM_FRemOp : LLVM_FloatArithmeticOp<"frem", "FRem">;
def LLVM_FNegOp : LLVM_UnaryFloatArithmeticOp<
  LLVM_ScalarOrVectorOf<LLVM_AnyFloat>, "fneg", "FNeg">;

// Memory-related operations.
def LLVM_AllocaOp : LLVM_Op<"alloca",
    [DeclareOpInterfaceMethods<PromotableAllocationOpInterface>,
     DeclareOpInterfaceMethods<DestructurableAllocationOpInterface>]>,
  LLVM_MemOpPatterns {
  let arguments = (ins AnySignlessInteger:$arraySize,
                   OptionalAttr<I64Attr>:$alignment,
                   TypeAttr:$elem_type,
                   UnitAttr:$inalloca);
  let results = (outs Res<LLVM_AnyPointer, "",
                          [MemAlloc<AutomaticAllocationScopeResource>]>:$res);
  string llvmInstName = "Alloca";
  string llvmBuilder = [{
    auto addrSpace = $_resultType->getPointerAddressSpace();
    llvm::Type *elementType = moduleTranslation.convertType($elem_type);
    auto *inst = builder.CreateAlloca(elementType, addrSpace, $arraySize);
    }] # setAlignmentCode # [{
    inst->setUsedWithInAlloca($inalloca);
    $res = inst;
  }];
  string mlirBuilder = [{
    auto *allocaInst = cast<llvm::AllocaInst>(inst);
    Type allocatedType =
      moduleImport.convertType(allocaInst->getAllocatedType());
    unsigned alignment = allocaInst->getAlign().value();
    $res = $_builder.create<LLVM::AllocaOp>(
      $_location, $_resultType, $arraySize,
      alignment == 0 ? IntegerAttr() : $_builder.getI64IntegerAttr(alignment),
      allocatedType, allocaInst->isUsedWithInAlloca());
  }];
  let builders = [
    OpBuilder<(ins "Type":$resultType, "Type":$elementType, "Value":$arraySize,
               CArg<"unsigned", "0">:$alignment),
    [{
      build($_builder, $_state, resultType, arraySize,
            alignment == 0 ? IntegerAttr()
                           : $_builder.getI64IntegerAttr(alignment),
            elementType, false);

    }]>
    ];
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

def LLVM_GEPOp : LLVM_Op<"getelementptr", [Pure,
    DeclareOpInterfaceMethods<PromotableOpInterface>,
    DeclareOpInterfaceMethods<SafeMemorySlotAccessOpInterface>,
    DeclareOpInterfaceMethods<DestructurableAccessorOpInterface>,
    DeclareOpInterfaceMethods<ViewLikeOpInterface>
    ]> {
  let arguments = (ins LLVM_ScalarOrVectorOf<LLVM_AnyPointer>:$base,
                   Variadic<LLVM_ScalarOrVectorOf<AnySignlessInteger>>:$dynamicIndices,
                   DenseI32ArrayAttr:$rawConstantIndices,
                   TypeAttr:$elem_type,
                   UnitAttr:$inbounds);
  let results = (outs LLVM_ScalarOrVectorOf<LLVM_AnyPointer>:$res);
  let skipDefaultBuilders = 1;

  let description = [{
    This operation mirrors LLVM IRs 'getelementptr' operation that is used to
    perform pointer arithmetic.

    Like in LLVM IR, it is possible to use both constants as well as SSA values
    as indices. In the case of indexing within a structure, it is required to
    either use constant indices directly, or supply a constant SSA value.

    An optional 'inbounds' attribute specifies the low-level pointer arithmetic
    overflow behavior that LLVM uses after lowering the operation to LLVM IR.

    Examples:

    ```mlir
    // GEP with an SSA value offset
    %0 = llvm.getelementptr %1[%2] : (!llvm.ptr, i64) -> !llvm.ptr, f32

    // GEP with a constant offset and the inbounds attribute set
    %0 = llvm.getelementptr inbounds %1[3] : (!llvm.ptr) -> !llvm.ptr, f32

    // GEP with constant offsets into a structure
    %0 = llvm.getelementptr %1[0, 1]
       : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i32, f32)>
    ```
  }];

  let builders = [
    OpBuilder<(ins "Type":$resultType, "Type":$elementType, "Value":$basePtr,
               "ValueRange":$indices, CArg<"bool", "false">:$inbounds,
               CArg<"ArrayRef<NamedAttribute>", "{}">:$attributes)>,
    OpBuilder<(ins "Type":$resultType, "Type":$elementType, "Value":$basePtr,
               "ArrayRef<GEPArg>":$indices, CArg<"bool", "false">:$inbounds,
               CArg<"ArrayRef<NamedAttribute>", "{}">:$attributes)>,
  ];
  let llvmBuilder = [{
    SmallVector<llvm::Value *> indices;
    indices.reserve($rawConstantIndices.size());
    GEPIndicesAdaptor<decltype($dynamicIndices)>
        gepIndices(op.getRawConstantIndicesAttr(), $dynamicIndices);
    for (PointerUnion<IntegerAttr, llvm::Value*> valueOrAttr : gepIndices) {
      if (llvm::Value* value = ::llvm::dyn_cast<llvm::Value*>(valueOrAttr))
        indices.push_back(value);
      else
        indices.push_back(
            builder.getInt32(valueOrAttr.get<IntegerAttr>().getInt()));
    }
    Type baseElementType = op.getElemType();
    llvm::Type *elementType = moduleTranslation.convertType(baseElementType);
    $res = builder.CreateGEP(elementType, $base, indices, "", $inbounds);
  }];
  let assemblyFormat = [{
    (`inbounds` $inbounds^)?
    $base `[` custom<GEPIndices>($dynamicIndices, $rawConstantIndices) `]` attr-dict
    `:` functional-type(operands, results) `,` $elem_type
  }];

  let extraClassDeclaration = [{
    constexpr static int32_t kDynamicIndex = std::numeric_limits<int32_t>::min();

    GEPIndicesAdaptor<ValueRange> getIndices();
  }];
  let hasFolder = 1;
  let hasVerifier = 1;
}

def LLVM_LoadOp : LLVM_MemAccessOpBase<"load",
    [DeclareOpInterfaceMethods<DestructurableAccessorOpInterface>,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     DeclareOpInterfaceMethods<PromotableMemOpInterface>,
     DeclareOpInterfaceMethods<SafeMemorySlotAccessOpInterface>]> {
  dag args = (ins LLVM_AnyPointer:$addr,
              OptionalAttr<I64Attr>:$alignment,
              UnitAttr:$volatile_,
              UnitAttr:$nontemporal,
              UnitAttr:$invariant,
              UnitAttr:$invariantGroup,
              DefaultValuedAttr<
                AtomicOrdering, "AtomicOrdering::not_atomic">:$ordering,
              OptionalAttr<StrAttr>:$syncscope);
  // Append the aliasing related attributes defined in LLVM_MemAccessOpBase.
  let arguments = !con(args, aliasAttrs);
  let results = (outs LLVM_LoadableType:$res);
  string llvmInstName = "Load";
  let description = [{
    The `load` operation is used to read from memory. A load may be marked as
    atomic, volatile, and/or nontemporal, and takes a number of optional
    attributes that specify aliasing information.

    An atomic load only supports a limited set of pointer, integer, and
    floating point types, and requires an explicit alignment.

    Examples:
    ```mlir
    // A volatile load of a float variable.
    %0 = llvm.load volatile %ptr : !llvm.ptr -> f32

    // A nontemporal load of a float variable.
    %0 = llvm.load %ptr {nontemporal} : !llvm.ptr -> f32

    // An atomic load of an integer variable.
    %0 = llvm.load %ptr atomic monotonic {alignment = 8 : i64}
        : !llvm.ptr -> i64
    ```

    See the following link for more details:
    https://llvm.org/docs/LangRef.html#load-instruction
  }];
  let assemblyFormat = [{
    (`volatile` $volatile_^)? $addr
    (`atomic` (`syncscope` `(` $syncscope^ `)`)? $ordering^)?
    (`invariant` $invariant^)?
    (`invariant_group` $invariantGroup^)?
    attr-dict `:` qualified(type($addr)) `->` type($res)
  }];
  string llvmBuilder = [{
    auto *inst = builder.CreateLoad($_resultType, $addr, $volatile_);
    $res = inst;
    if ($invariant) {
      llvm::MDNode *metadata = llvm::MDNode::get(inst->getContext(), std::nullopt);
      inst->setMetadata(llvm::LLVMContext::MD_invariant_load, metadata);
    }
  }] # setOrderingCode
     # setSyncScopeCode
     # setAlignmentCode
     # setNonTemporalMetadataCode
     # setInvariantGroupCode
     # setAccessGroupsMetadataCode
     # setAliasAnalysisMetadataCode;
  string mlirBuilder = [{
    auto *loadInst = cast<llvm::LoadInst>(inst);
    unsigned alignment = loadInst->getAlign().value();
    $res = $_builder.create<LLVM::LoadOp>($_location, $_resultType, $addr,
        alignment, loadInst->isVolatile(),
        loadInst->hasMetadata(llvm::LLVMContext::MD_nontemporal),
        loadInst->hasMetadata(llvm::LLVMContext::MD_invariant_load),
        loadInst->hasMetadata(llvm::LLVMContext::MD_invariant_group),
        convertAtomicOrderingFromLLVM(loadInst->getOrdering()),
        getLLVMSyncScope(loadInst));
  }];
  let builders = [
    OpBuilder<(ins "Type":$type, "Value":$addr,
      CArg<"unsigned", "0">:$alignment, CArg<"bool", "false">:$isVolatile,
      CArg<"bool", "false">:$isNonTemporal, CArg<"bool", "false">:$isInvariant,
      CArg<"bool", "false">:$isInvariantGroup,
      CArg<"AtomicOrdering", "AtomicOrdering::not_atomic">:$ordering,
      CArg<"StringRef", "StringRef()">:$syncscope)>
  ];
  let hasVerifier = 1;
}

def LLVM_StoreOp : LLVM_MemAccessOpBase<"store",
    [DeclareOpInterfaceMethods<DestructurableAccessorOpInterface>,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     DeclareOpInterfaceMethods<PromotableMemOpInterface>,
     DeclareOpInterfaceMethods<SafeMemorySlotAccessOpInterface>]> {
  dag args = (ins LLVM_LoadableType:$value,
              LLVM_AnyPointer:$addr,
              OptionalAttr<I64Attr>:$alignment,
              UnitAttr:$volatile_,
              UnitAttr:$nontemporal,
              UnitAttr:$invariantGroup,
              DefaultValuedAttr<
                AtomicOrdering, "AtomicOrdering::not_atomic">:$ordering,
              OptionalAttr<StrAttr>:$syncscope);
  // Append the aliasing related attributes defined in LLVM_MemAccessOpBase.
  let arguments = !con(args, aliasAttrs);
  string llvmInstName = "Store";
  let description = [{
    The `store` operation is used to write to memory. A store may be marked as
    atomic, volatile, and/or nontemporal, and takes a number of optional
    attributes that specify aliasing information.

    An atomic store only supports a limited set of pointer, integer, and
    floating point types, and requires an explicit alignment.

    Examples:
    ```mlir
    // A volatile store of a float variable.
    llvm.store volatile %val, %ptr : f32, !llvm.ptr

    // A nontemporal store of a float variable.
    llvm.store %val, %ptr {nontemporal} : f32, !llvm.ptr

    // An atomic store of an integer variable.
    llvm.store %val, %ptr atomic monotonic {alignment = 8 : i64}
        : i64, !llvm.ptr
    ```

    See the following link for more details:
    https://llvm.org/docs/LangRef.html#store-instruction
  }];
  let assemblyFormat = [{
    (`volatile` $volatile_^)? $value `,` $addr
    (`atomic` (`syncscope` `(` $syncscope^ `)`)? $ordering^)?
    (`invariant_group` $invariantGroup^)?
    attr-dict `:` type($value) `,` qualified(type($addr))
  }];
  string llvmBuilder = [{
    auto *inst = builder.CreateStore($value, $addr, $volatile_);
  }] # setOrderingCode
     # setSyncScopeCode
     # setAlignmentCode
     # setNonTemporalMetadataCode
     # setInvariantGroupCode
     # setAccessGroupsMetadataCode
     # setAliasAnalysisMetadataCode;
  string mlirBuilder = [{
    auto *storeInst = cast<llvm::StoreInst>(inst);
    unsigned alignment = storeInst->getAlign().value();
    $_op = $_builder.create<LLVM::StoreOp>($_location, $value, $addr,
        alignment, storeInst->isVolatile(),
        storeInst->hasMetadata(llvm::LLVMContext::MD_nontemporal),
        storeInst->hasMetadata(llvm::LLVMContext::MD_invariant_group),
        convertAtomicOrderingFromLLVM(storeInst->getOrdering()),
        getLLVMSyncScope(storeInst));
  }];
  let builders = [
    OpBuilder<(ins "Value":$value, "Value":$addr,
      CArg<"unsigned", "0">:$alignment, CArg<"bool", "false">:$isVolatile,
      CArg<"bool", "false">:$isNonTemporal,
      CArg<"bool", "false">:$isInvariantGroup,
      CArg<"AtomicOrdering", "AtomicOrdering::not_atomic">:$ordering,
      CArg<"StringRef", "StringRef()">:$syncscope)>
  ];
  let hasVerifier = 1;
}

// Casts.
class LLVM_CastOp<string mnemonic, string instName, Type type,
                  Type resultType, list<Trait> traits = []> :
    LLVM_Op<mnemonic, !listconcat([Pure], traits)>,
    LLVM_Builder<"$res = builder.Create" # instName # "($arg, $_resultType);"> {
  let arguments = (ins type:$arg);
  let results = (outs resultType:$res);
  let builders = [LLVM_OneResultOpBuilder];
  let assemblyFormat = "$arg attr-dict `:` type($arg) `to` type($res)";
  string llvmInstName = instName;
  string mlirBuilder = [{
    $res = $_builder.create<$_qualCppClassName>(
      $_location, $_resultType, $arg);
  }];
}
class LLVM_CastOpWithNNegFlag<string mnemonic, string instName, Type type,
                  Type resultType, list<Trait> traits = []> :
    LLVM_Op<mnemonic, !listconcat([Pure], [DeclareOpInterfaceMethods<NonNegFlagInterface>], traits)>,
    LLVM_Builder<"$res = builder.Create" # instName # "($arg, $_resultType, /*Name=*/\"\", op.getNonNeg());"> {
  let arguments = (ins type:$arg, UnitAttr:$nonNeg);
  let results = (outs resultType:$res);
  let builders = [LLVM_OneResultOpBuilder];
  let assemblyFormat = "(`nneg` $nonNeg^)? $arg attr-dict `:` type($arg) `to` type($res)";
  string llvmInstName = instName;
  string mlirBuilder = [{
    auto op = $_builder.create<$_qualCppClassName>(
      $_location, $_resultType, $arg);
    moduleImport.setNonNegFlag(inst, op);
    $res = op;
  }];
}

class LLVM_CastOpWithOverflowFlag<string mnemonic, string instName, Type type,
                  Type resultType, list<Trait> traits = []> :
    LLVM_Op<mnemonic, !listconcat([Pure], [DeclareOpInterfaceMethods<IntegerOverflowFlagsInterface>], traits)>,
    LLVM_Builder<"$res = builder.Create" # instName # "($arg, $_resultType, /*Name=*/\"\", op.hasNoUnsignedWrap(), op.hasNoSignedWrap());"> {
  let arguments = (ins type:$arg, EnumProperty<"IntegerOverflowFlags", "", "IntegerOverflowFlags::none">:$overflowFlags);
  let results = (outs resultType:$res);
  let builders = [LLVM_OneResultOpBuilder];
  let assemblyFormat = "$arg `` custom<OverflowFlags>($overflowFlags) attr-dict `:` type($arg) `to` type($res)";
  string llvmInstName = instName;
  string mlirBuilder = [{
    auto op = $_builder.create<$_qualCppClassName>(
      $_location, $_resultType, $arg);
    moduleImport.setIntegerOverflowFlags(inst, op);
    $res = op;
  }];
}

def LLVM_BitcastOp : LLVM_CastOp<"bitcast", "BitCast", LLVM_AnyNonAggregate,
    LLVM_AnyNonAggregate, [DeclareOpInterfaceMethods<PromotableOpInterface>]> {
  let hasFolder = 1;
  let hasVerifier = 1;
}
def LLVM_AddrSpaceCastOp : LLVM_CastOp<"addrspacecast", "AddrSpaceCast",
    LLVM_ScalarOrVectorOf<LLVM_AnyPointer>,
    LLVM_ScalarOrVectorOf<LLVM_AnyPointer>,
    [DeclareOpInterfaceMethods<PromotableOpInterface>,
     DeclareOpInterfaceMethods<ViewLikeOpInterface>]> {
  let hasFolder = 1;
}
def LLVM_IntToPtrOp : LLVM_CastOp<"inttoptr", "IntToPtr",
                                  LLVM_ScalarOrVectorOf<AnySignlessInteger>,
                                  LLVM_ScalarOrVectorOf<LLVM_AnyPointer>>;
def LLVM_PtrToIntOp : LLVM_CastOp<"ptrtoint", "PtrToInt",
                                  LLVM_ScalarOrVectorOf<LLVM_AnyPointer>,
                                  LLVM_ScalarOrVectorOf<AnySignlessInteger>>;
def LLVM_SExtOp : LLVM_CastOp<"sext", "SExt",
                              LLVM_ScalarOrVectorOf<AnySignlessInteger>,
                              LLVM_ScalarOrVectorOf<AnySignlessInteger>> {
  let hasVerifier = 1;
}
def LLVM_ZExtOp : LLVM_CastOpWithNNegFlag<"zext", "ZExt",
                              LLVM_ScalarOrVectorOf<AnySignlessInteger>,
                              LLVM_ScalarOrVectorOf<AnySignlessInteger>> {
  let hasFolder = 1;
  let hasVerifier = 1;
}
def LLVM_TruncOp : LLVM_CastOpWithOverflowFlag<"trunc", "Trunc",
                               LLVM_ScalarOrVectorOf<AnySignlessInteger>,
                               LLVM_ScalarOrVectorOf<AnySignlessInteger>>;
def LLVM_SIToFPOp : LLVM_CastOp<"sitofp", "SIToFP",
                                LLVM_ScalarOrVectorOf<AnySignlessInteger>,
                                LLVM_ScalarOrVectorOf<LLVM_AnyFloat>>;
def LLVM_UIToFPOp : LLVM_CastOpWithNNegFlag<"uitofp", "UIToFP",
                                LLVM_ScalarOrVectorOf<AnySignlessInteger>,
                                LLVM_ScalarOrVectorOf<LLVM_AnyFloat>>;
def LLVM_FPToSIOp : LLVM_CastOp<"fptosi", "FPToSI",
                                LLVM_ScalarOrVectorOf<LLVM_AnyFloat>,
                                LLVM_ScalarOrVectorOf<AnySignlessInteger>>;
def LLVM_FPToUIOp : LLVM_CastOp<"fptoui", "FPToUI",
                                LLVM_ScalarOrVectorOf<LLVM_AnyFloat>,
                                LLVM_ScalarOrVectorOf<AnySignlessInteger>>;
def LLVM_FPExtOp : LLVM_CastOp<"fpext", "FPExt",
                                LLVM_ScalarOrVectorOf<LLVM_AnyFloat>,
                                LLVM_ScalarOrVectorOf<LLVM_AnyFloat>>;
def LLVM_FPTruncOp : LLVM_CastOp<"fptrunc", "FPTrunc",
                                 LLVM_ScalarOrVectorOf<LLVM_AnyFloat>,
                                 LLVM_ScalarOrVectorOf<LLVM_AnyFloat>>;

// Call-related operations.
def LLVM_InvokeOp : LLVM_Op<"invoke", [
                      AttrSizedOperandSegments,
                      DeclareOpInterfaceMethods<BranchOpInterface>,
                      DeclareOpInterfaceMethods<CallOpInterface>,
                      DeclareOpInterfaceMethods<BranchWeightOpInterface>,
                      Terminator]> {
  let arguments = (ins
                   OptionalAttr<TypeAttrOf<LLVM_FunctionType>>:$var_callee_type,
                   OptionalAttr<FlatSymbolRefAttr>:$callee,
                   Variadic<LLVM_Type>:$callee_operands,
                   Variadic<LLVM_Type>:$normalDestOperands,
                   Variadic<LLVM_Type>:$unwindDestOperands,
                   OptionalAttr<DenseI32ArrayAttr>:$branch_weights,
                   DefaultValuedAttr<CConv, "CConv::C">:$CConv,
                   VariadicOfVariadic<LLVM_Type,
                                      "op_bundle_sizes">:$op_bundle_operands,
                   DenseI32ArrayAttr:$op_bundle_sizes,
                   OptionalAttr<ArrayAttr>:$op_bundle_tags);
  let results = (outs Optional<LLVM_Type>:$result);
  let successors = (successor AnySuccessor:$normalDest,
                              AnySuccessor:$unwindDest);

  let builders = [
    OpBuilder<(ins "LLVMFuncOp":$func,
      "ValueRange":$ops, "Block*":$normal, "ValueRange":$normalOps,
      "Block*":$unwind, "ValueRange":$unwindOps)>,
    OpBuilder<(ins "TypeRange":$tys, "FlatSymbolRefAttr":$callee,
      "ValueRange":$ops, "Block*":$normal, "ValueRange":$normalOps,
      "Block*":$unwind, "ValueRange":$unwindOps)>,
    OpBuilder<(ins "LLVMFunctionType":$calleeType, "FlatSymbolRefAttr":$callee,
      "ValueRange":$ops, "Block*":$normal, "ValueRange":$normalOps,
      "Block*":$unwind, "ValueRange":$unwindOps)>];
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
  let extraClassDeclaration = [{
    /// Returns the callee function type.
    LLVMFunctionType getCalleeFunctionType();
  }];
}

def LLVM_LandingpadOp : LLVM_Op<"landingpad"> {
  let arguments = (ins UnitAttr:$cleanup, Variadic<LLVM_Type>);
  let results = (outs LLVM_Type:$res);
  let builders = [LLVM_OneResultOpBuilder];
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// VAArgOp
//===----------------------------------------------------------------------===//

def LLVM_VaArgOp : LLVM_Op<"va_arg"> {
  let arguments = (ins LLVM_AnyPointer:$arg);
  let results = (outs LLVM_Type:$res);

  let builders = [LLVM_OneResultOpBuilder];

  let llvmBuilder = " $res = builder.CreateVAArg($arg, $_resultType); ";
  let assemblyFormat = "$arg attr-dict `:` functional-type($arg, $res)";

  string llvmInstName = "VAArg";
  string mlirBuilder = [{
    $res = $_builder.create<mlir::LLVM::VaArgOp>(
      $_location, $_resultType, $arg);
  }];
}

//===----------------------------------------------------------------------===//
// CallOp
//===----------------------------------------------------------------------===//

def LLVM_CallOp : LLVM_MemAccessOpBase<"call",
                    [AttrSizedOperandSegments,
                     DeclareOpInterfaceMethods<FastmathFlagsInterface>,
                     DeclareOpInterfaceMethods<CallOpInterface>,
                     DeclareOpInterfaceMethods<SymbolUserOpInterface>,
                     DeclareOpInterfaceMethods<BranchWeightOpInterface>]> {
  let summary = "Call to an LLVM function.";
  let description = [{
    In LLVM IR, functions may return either 0 or 1 value. LLVM IR dialect
    implements this behavior by providing a variadic `call` operation for 0- and
    1-result functions. Even though MLIR supports multi-result functions, LLVM
    IR dialect disallows them.

    The `call` instruction supports both direct and indirect calls. Direct calls
    start with a function name (`@`-prefixed) and indirect calls start with an
    SSA value (`%`-prefixed). The direct callee, if present, is stored as a
    function attribute `callee`. For indirect calls, the callee is of `!llvm.ptr` type
    and is stored as the first value in `callee_operands`. If and only if the
    callee is a variadic function, the `var_callee_type` attribute must carry
    the variadic LLVM function type. The trailing type list contains the
    optional indirect callee type and the MLIR function type, which differs from
    the LLVM function type that uses an explicit void type to model functions
    that do not return a value.

    Examples:

    ```mlir
    // Direct call without arguments and with one result.
    %0 = llvm.call @foo() : () -> (f32)

    // Direct call with arguments and without a result.
    llvm.call @bar(%0) : (f32) -> ()

    // Indirect call with an argument and without a result.
    %1 = llvm.mlir.addressof @foo : !llvm.ptr
    llvm.call %1(%0) : !llvm.ptr, (f32) -> ()

    // Direct variadic call.
    llvm.call @printf(%0, %1) vararg(!llvm.func<i32 (ptr, ...)>) : (!llvm.ptr, i32) -> i32

    // Indirect variadic call
    llvm.call %1(%0) vararg(!llvm.func<void (...)>) : !llvm.ptr, (i32) -> ()
    ```
  }];

  dag args = (ins OptionalAttr<TypeAttrOf<LLVM_FunctionType>>:$var_callee_type,
                  OptionalAttr<FlatSymbolRefAttr>:$callee,
                  Variadic<LLVM_Type>:$callee_operands,
                  DefaultValuedAttr<LLVM_FastmathFlagsAttr,
                                   "{}">:$fastmathFlags,
                  OptionalAttr<DenseI32ArrayAttr>:$branch_weights,
                  DefaultValuedAttr<CConv, "CConv::C">:$CConv,
                  DefaultValuedAttr<TailCallKind, "TailCallKind::None">:$TailCallKind,
                  OptionalAttr<LLVM_MemoryEffectsAttr>:$memory_effects,
                  OptionalAttr<UnitAttr>:$convergent,
                  OptionalAttr<UnitAttr>:$no_unwind,
                  OptionalAttr<UnitAttr>:$will_return,
                  VariadicOfVariadic<LLVM_Type,
                                     "op_bundle_sizes">:$op_bundle_operands,
                  DenseI32ArrayAttr:$op_bundle_sizes,
                  OptionalAttr<ArrayAttr>:$op_bundle_tags);
  // Append the aliasing related attributes defined in LLVM_MemAccessOpBase.
  let arguments = !con(args, aliasAttrs);
  let results = (outs Optional<LLVM_Type>:$result);
  let builders = [
    OpBuilder<(ins "LLVMFuncOp":$func, "ValueRange":$args)>,
    OpBuilder<(ins "LLVMFunctionType":$calleeType, "ValueRange":$args)>,
    OpBuilder<(ins "TypeRange":$results, "StringAttr":$callee,
                   CArg<"ValueRange", "{}">:$args)>,
    OpBuilder<(ins "TypeRange":$results, "FlatSymbolRefAttr":$callee,
                   CArg<"ValueRange", "{}">:$args)>,
    OpBuilder<(ins "TypeRange":$results, "StringRef":$callee,
                   CArg<"ValueRange", "{}">:$args)>,
    OpBuilder<(ins "LLVMFunctionType":$calleeType, "StringAttr":$callee,
                   CArg<"ValueRange", "{}">:$args)>,
    OpBuilder<(ins "LLVMFunctionType":$calleeType, "FlatSymbolRefAttr":$callee,
                   CArg<"ValueRange", "{}">:$args)>,
    OpBuilder<(ins "LLVMFunctionType":$calleeType, "StringRef":$callee,
                   CArg<"ValueRange", "{}">:$args)>
  ];
  let hasVerifier = 1;
  let hasCustomAssemblyFormat = 1;
  let extraClassDeclaration = [{
    /// Returns the callee function type.
    LLVMFunctionType getCalleeFunctionType();
  }];
}

//===----------------------------------------------------------------------===//
// ExtractElementOp
//===----------------------------------------------------------------------===//

def LLVM_ExtractElementOp : LLVM_Op<"extractelement", [Pure,
    TypesMatchWith<"result type matches vector element type", "vector", "res",
                   "LLVM::getVectorElementType($_self)">]> {
  let summary = "Extract an element from an LLVM vector.";

  let arguments = (ins LLVM_AnyVector:$vector, AnySignlessInteger:$position);
  let results = (outs LLVM_Type:$res);

  let assemblyFormat = [{
    $vector `[` $position `:` type($position) `]` attr-dict `:` type($vector)
  }];

  string llvmInstName = "ExtractElement";
  string llvmBuilder = [{
    $res = builder.CreateExtractElement($vector, $position);
  }];
  string mlirBuilder = [{
    $res = $_builder.create<LLVM::ExtractElementOp>(
      $_location, $vector, $position);
  }];
}

//===----------------------------------------------------------------------===//
// ExtractValueOp
//===----------------------------------------------------------------------===//

def LLVM_ExtractValueOp : LLVM_Op<"extractvalue", [Pure]> {
  let summary = "Extract a value from an LLVM struct.";

  let arguments = (ins LLVM_AnyAggregate:$container, DenseI64ArrayAttr:$position);
  let results = (outs LLVM_Type:$res);

  let builders = [
    OpBuilder<(ins "Value":$container, "ArrayRef<int64_t>":$position)>
  ];

  let assemblyFormat = [{
    $container `` $position attr-dict `:` type($container)
    custom<InsertExtractValueElementType>(type($res), ref(type($container)),
                                          ref($position))
  }];

  let hasFolder = 1;
  let hasVerifier = 1;

  string llvmInstName = "ExtractValue";
  string llvmBuilder = [{
    $res = builder.CreateExtractValue($container, extractPosition($position));
  }];
  string mlirBuilder = [{
    auto *evInst = cast<llvm::ExtractValueInst>(inst);
    $res = $_builder.create<LLVM::ExtractValueOp>($_location,
      $container, getPositionFromIndices(evInst->getIndices()));
  }];
}

//===----------------------------------------------------------------------===//
// InsertElementOp
//===----------------------------------------------------------------------===//

def LLVM_InsertElementOp : LLVM_Op<"insertelement", [Pure,
    TypesMatchWith<"argument type matches vector element type", "vector",
                   "value", "LLVM::getVectorElementType($_self)">,
    AllTypesMatch<["res", "vector"]>]> {
  let summary = "Insert an element into an LLVM vector.";

  let arguments = (ins LLVM_AnyVector:$vector, LLVM_PrimitiveType:$value,
                       AnySignlessInteger:$position);
  let results = (outs LLVM_AnyVector:$res);

  let builders = [LLVM_OneResultOpBuilder];

  let assemblyFormat = [{
    $value `,` $vector `[` $position `:` type($position) `]` attr-dict `:`
    type($vector)
  }];

  string llvmInstName = "InsertElement";
  string llvmBuilder = [{
    $res = builder.CreateInsertElement($vector, $value, $position);
  }];
  string mlirBuilder = [{
    $res = $_builder.create<LLVM::InsertElementOp>(
      $_location, $vector, $value, $position);
  }];
}

//===----------------------------------------------------------------------===//
// InsertValueOp
//===----------------------------------------------------------------------===//

def LLVM_InsertValueOp : LLVM_Op<
    "insertvalue", [Pure, AllTypesMatch<["container", "res"]>]> {
  let summary = "Insert a value into an LLVM struct.";

  let arguments = (ins LLVM_AnyAggregate:$container, LLVM_PrimitiveType:$value,
                       DenseI64ArrayAttr:$position);
  let results = (outs LLVM_AnyAggregate:$res);

  let assemblyFormat = [{
    $value `,` $container `` $position attr-dict `:` type($container)
    custom<InsertExtractValueElementType>(type($value), ref(type($container)),
                                          ref($position))
  }];

  let hasVerifier = 1;

  string llvmInstName = "InsertValue";
  string llvmBuilder = [{
    $res = builder.CreateInsertValue($container, $value,
                                     extractPosition($position));
  }];
  string mlirBuilder = [{
    auto *ivInst = cast<llvm::InsertValueInst>(inst);
    $res = $_builder.create<LLVM::InsertValueOp>($_location,
      $container, $value, getPositionFromIndices(ivInst->getIndices()));
  }];
}

//===----------------------------------------------------------------------===//
// ShuffleVectorOp
//===----------------------------------------------------------------------===//

def LLVM_ShuffleVectorOp : LLVM_Op<"shufflevector",
    [Pure, AllTypesMatch<["v1", "v2"]>]> {
  let summary = "Construct a permutation of two vectors.";

  let arguments = (ins LLVM_AnyVector:$v1, LLVM_AnyVector:$v2,
                       DenseI32ArrayAttr:$mask);
  let results = (outs LLVM_AnyVector:$res);

  let builders = [
    OpBuilder<(ins "Value":$v1, "Value":$v2, "DenseI32ArrayAttr":$mask,
                   CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    OpBuilder<(ins "Value":$v1, "Value":$v2, "ArrayRef<int32_t>":$mask)>
  ];

  let assemblyFormat = [{
    $v1 `,` $v2 $mask attr-dict `:` type($v1)
    custom<ShuffleType>(ref(type($v1)), type($res), ref($mask))
  }];

  let hasVerifier = 1;

  string llvmInstName = "ShuffleVector";
  string llvmBuilder = [{
    $res = builder.CreateShuffleVector($v1, $v2, $mask);
  }];
  string mlirBuilder = [{
    auto *svInst = cast<llvm::ShuffleVectorInst>(inst);
    SmallVector<int32_t> mask(svInst->getShuffleMask());
    $res = $_builder.create<LLVM::ShuffleVectorOp>(
      $_location, $v1, $v2, mask);
  }];
}

// Misc operations.
def LLVM_SelectOp
    : LLVM_Op<"select",
          [Pure, AllTypesMatch<["trueValue", "falseValue", "res"]>,
           DeclareOpInterfaceMethods<FastmathFlagsInterface>,
           DeclareOpInterfaceMethods<SelectLikeOpInterface>]>,
      LLVM_Builder<
          "$res = builder.CreateSelect($condition, $trueValue, $falseValue);"> {
  let arguments = (ins LLVM_ScalarOrVectorOf<I1>:$condition,
                   LLVM_Type:$trueValue, LLVM_Type:$falseValue,
                   DefaultValuedAttr<LLVM_FastmathFlagsAttr,
                                     "{}">:$fastmathFlags);
  let results = (outs LLVM_Type:$res);
  let assemblyFormat = "operands attr-dict `:` type($condition) `,` type($res)";
  string llvmInstName = "Select";
  string mlirBuilder = [{
    auto op = $_builder.create<LLVM::SelectOp>(
      $_location, $_resultType, $condition, $trueValue, $falseValue);
    moduleImport.setFastmathFlagsAttr(inst, op);
    $res = op;
  }];
}
def LLVM_FreezeOp : LLVM_Op<"freeze", [Pure, SameOperandsAndResultType]> {
  let arguments = (ins LLVM_Type:$val);
  let results = (outs LLVM_Type:$res);
  let builders = [LLVM_OneResultOpBuilder];
  let assemblyFormat = "$val attr-dict `:` type($val)";
  string llvmInstName = "Freeze";
  string llvmBuilder = "$res = builder.CreateFreeze($val);";
  string mlirBuilder = [{
    $res = $_builder.create<LLVM::FreezeOp>($_location, $val);
  }];
}

// Terminators.
def LLVM_BrOp : LLVM_TerminatorOp<"br",
    [DeclareOpInterfaceMethods<BranchOpInterface>, Pure]> {
  let arguments = (ins
    Variadic<LLVM_Type>:$destOperands,
    OptionalAttr<LoopAnnotationAttr>:$loop_annotation
  );
  let successors = (successor AnySuccessor:$dest);
  let assemblyFormat = [{
    $dest (`(` $destOperands^ `:` type($destOperands) `)`)? attr-dict
  }];
  let builders = [
    OpBuilder<(ins "Block *":$dest), [{
      build($_builder, $_state, ValueRange(), dest);
    }]>,
    OpBuilder<(ins "ValueRange":$operands, "Block *":$dest), [{
      build($_builder, $_state, operands, /*loop_annotation=*/{}, dest);
    }]>,
    LLVM_TerminatorPassthroughOpBuilder
  ];
}
def LLVM_CondBrOp : LLVM_TerminatorOp<"cond_br",
    [AttrSizedOperandSegments,
     DeclareOpInterfaceMethods<BranchOpInterface>,
     DeclareOpInterfaceMethods<BranchWeightOpInterface>,
     Pure]> {
  let arguments = (ins I1:$condition,
                   Variadic<LLVM_Type>:$trueDestOperands,
                   Variadic<LLVM_Type>:$falseDestOperands,
                   OptionalAttr<DenseI32ArrayAttr>:$branch_weights,
                   OptionalAttr<LoopAnnotationAttr>:$loop_annotation);
  let successors = (successor AnySuccessor:$trueDest, AnySuccessor:$falseDest);
  let assemblyFormat = [{
    $condition ( `weights` `(` $branch_weights^ `)` )? `,`
    $trueDest (`(` $trueDestOperands^ `:` type($trueDestOperands) `)`)? `,`
    $falseDest (`(` $falseDestOperands^ `:` type($falseDestOperands) `)`)?
    attr-dict
  }];

  let builders = [
    OpBuilder<(ins "Value":$condition, "Block *":$trueDest,
      "ValueRange":$trueOperands, "Block *":$falseDest,
      "ValueRange":$falseOperands,
      CArg<"std::optional<std::pair<uint32_t, uint32_t>>", "{}">:$weights)>,
  OpBuilder<(ins "Value":$condition, "Block *":$trueDest,
    "Block *":$falseDest, CArg<"ValueRange", "{}">:$falseOperands),
  [{
      build($_builder, $_state, condition, trueDest, ValueRange(), falseDest,
            falseOperands);
  }]>,
  OpBuilder<(ins "Value":$condition, "ValueRange":$trueOperands, "ValueRange":$falseOperands,
    "DenseI32ArrayAttr":$branchWeights, "Block *":$trueDest, "Block *":$falseDest),
  [{
      build($_builder, $_state, condition, trueOperands, falseOperands, branchWeights,
      {}, trueDest, falseDest);
  }]>, LLVM_TerminatorPassthroughOpBuilder];
}

//===----------------------------------------------------------------------===//
// ReturnOp
//===----------------------------------------------------------------------===//

def LLVM_ReturnOp : LLVM_TerminatorOp<"return", [Pure, ReturnLike]> {
  let arguments = (ins Optional<LLVM_Type>:$arg);
  let assemblyFormat = "attr-dict ($arg^ `:` type($arg))?";

  let builders = [
    OpBuilder<(ins "ValueRange":$args), [{
      build($_builder, $_state, TypeRange(), args);
    }]>
  ];

  let hasVerifier = 1;

  string llvmInstName = "Ret";
  string llvmBuilder = [{
    if ($_numOperands != 0)
      builder.CreateRet($arg);
    else
      builder.CreateRetVoid();
  }];
  string mlirBuilder = [{
    FailureOr<SmallVector<Value>> mlirOperands =
      moduleImport.convertValues(llvmOperands);
    if (failed(mlirOperands))
      return failure();
    $_op = $_builder.create<LLVM::ReturnOp>($_location, *mlirOperands);
  }];
}

def LLVM_ResumeOp : LLVM_TerminatorOp<"resume"> {
  let arguments = (ins LLVM_Type:$value);
  let assemblyFormat = "$value attr-dict `:` type($value)";
  // Consistency of llvm.resume value types is checked in LLVMFuncOp::verify().
  let hasVerifier = false;
  string llvmInstName = "Resume";
  string llvmBuilder = [{ builder.CreateResume($value); }];
  string mlirBuilder = [{
    $_op = $_builder.create<LLVM::ResumeOp>($_location, $value);
  }];
}
def LLVM_UnreachableOp : LLVM_TerminatorOp<"unreachable"> {
  let assemblyFormat = "attr-dict";
  string llvmInstName = "Unreachable";
  string llvmBuilder = [{ builder.CreateUnreachable(); }];
  string mlirBuilder = [{
    $_op = $_builder.create<LLVM::UnreachableOp>($_location);
  }];
}

def LLVM_SwitchOp : LLVM_TerminatorOp<"switch",
    [AttrSizedOperandSegments,
     DeclareOpInterfaceMethods<BranchOpInterface>,
     DeclareOpInterfaceMethods<BranchWeightOpInterface>,
     Pure]> {
  let arguments = (ins
    AnySignlessInteger:$value,
    Variadic<AnyType>:$defaultOperands,
    VariadicOfVariadic<AnyType, "case_operand_segments">:$caseOperands,
    OptionalAttr<AnyIntElementsAttr>:$case_values,
    DenseI32ArrayAttr:$case_operand_segments,
    OptionalAttr<DenseI32ArrayAttr>:$branch_weights
  );
  let successors = (successor
    AnySuccessor:$defaultDestination,
    VariadicSuccessor<AnySuccessor>:$caseDestinations
  );

  let assemblyFormat = [{
    $value `:` type($value) `,`
    $defaultDestination (`(` $defaultOperands^ `:` type($defaultOperands) `)`)?
    custom<SwitchOpCases>(ref(type($value)), $case_values, $caseDestinations,
                                   $caseOperands, type($caseOperands))
    attr-dict
  }];
  let hasVerifier = 1;

  let builders = [
    OpBuilder<(ins "Value":$value,
      "Block *":$defaultDestination,
      "ValueRange":$defaultOperands,
      CArg<"ArrayRef<APInt>", "{}">:$caseValues,
      CArg<"BlockRange", "{}">:$caseDestinations,
      CArg<"ArrayRef<ValueRange>", "{}">:$caseOperands,
      CArg<"ArrayRef<int32_t>", "{}">:$branchWeights)>,
    OpBuilder<(ins "Value":$value,
      "Block *":$defaultDestination,
      "ValueRange":$defaultOperands,
      CArg<"ArrayRef<int32_t>", "{}">:$caseValues,
      CArg<"BlockRange", "{}">:$caseDestinations,
      CArg<"ArrayRef<ValueRange>", "{}">:$caseOperands,
      CArg<"ArrayRef<int32_t>", "{}">:$branchWeights)>,
    OpBuilder<(ins "Value":$value,
      "Block *":$defaultDestination,
      "ValueRange":$defaultOperands,
      CArg<"DenseIntElementsAttr", "{}">:$caseValues,
      CArg<"BlockRange", "{}">:$caseDestinations,
      CArg<"ArrayRef<ValueRange>", "{}">:$caseOperands,
      CArg<"ArrayRef<int32_t>", "{}">:$branchWeights)>,
    LLVM_TerminatorPassthroughOpBuilder
  ];

  let extraClassDeclaration = [{
    /// Return the operands for the case destination block at the given index.
    OperandRange getCaseOperands(unsigned index) {
      return getCaseOperands()[index];
    }

    /// Return a mutable range of operands for the case destination block at the
    /// given index.
    MutableOperandRange getCaseOperandsMutable(unsigned index) {
      return getCaseOperandsMutable()[index];
    }
  }];
}

////////////////////////////////////////////////////////////////////////////////
// Auxiliary operations (do not appear in LLVM IR but necessary for the dialect
// to work correctly).
////////////////////////////////////////////////////////////////////////////////

def LLVM_AddressOfOp : LLVM_Op<"mlir.addressof",
    [Pure, ConstantLike, DeclareOpInterfaceMethods<SymbolUserOpInterface>]> {
  let arguments = (ins FlatSymbolRefAttr:$global_name);
  let results = (outs LLVM_AnyPointer:$res);

  let summary = "Creates a pointer pointing to a global or a function";

  let description = [{
    Creates an SSA value containing a pointer to a global variable or constant
    defined by `llvm.mlir.global`. The global value can be defined after its
    first referenced. If the global value is a constant, storing into it is not
    allowed.

    Examples:

    ```mlir
    func @foo() {
      // Get the address of a global variable.
      %0 = llvm.mlir.addressof @const : !llvm.ptr

      // Use it as a regular pointer.
      %1 = llvm.load %0 : !llvm.ptr -> i32

      // Get the address of a function.
      %2 = llvm.mlir.addressof @foo : !llvm.ptr

      // The function address can be used for indirect calls.
      llvm.call %2() : !llvm.ptr, () -> ()
    }

    // Define the global.
    llvm.mlir.global @const(42 : i32) : i32
    ```
  }];

  let builders = [
    OpBuilder<(ins "GlobalOp":$global,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs),
    [{
      build($_builder, $_state,
            LLVM::LLVMPointerType::get($_builder.getContext(), global.getAddrSpace()),
            global.getSymName());
      $_state.addAttributes(attrs);
    }]>,
    OpBuilder<(ins "LLVMFuncOp":$func,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs),
    [{
      build($_builder, $_state,
            LLVM::LLVMPointerType::get($_builder.getContext()), func.getName());
      $_state.addAttributes(attrs);
    }]>
  ];

  let extraClassDeclaration = [{
    /// Return the llvm.mlir.global operation that defined the value referenced
    /// here.
    GlobalOp getGlobal(SymbolTableCollection &symbolTable);

    /// Return the llvm.func operation that is referenced here.
    LLVMFuncOp getFunction(SymbolTableCollection &symbolTable);
  }];

  let assemblyFormat = "$global_name attr-dict `:` qualified(type($res))";

  let hasFolder = 1;
}

def LLVM_GlobalOp : LLVM_Op<"mlir.global",
    [IsolatedFromAbove, SingleBlockImplicitTerminator<"ReturnOp">, Symbol]> {
  let arguments = (ins
    TypeAttr:$global_type,
    UnitAttr:$constant,
    StrAttr:$sym_name,
    Linkage:$linkage,
    UnitAttr:$dso_local,
    UnitAttr:$thread_local_,
    UnitAttr:$externally_initialized,
    OptionalAttr<AnyAttr>:$value,
    OptionalAttr<I64Attr>:$alignment,
    DefaultValuedAttr<ConfinedAttr<I32Attr, [IntNonNegative]>, "0">:$addr_space,
    OptionalAttr<UnnamedAddr>:$unnamed_addr,
    OptionalAttr<StrAttr>:$section,
    OptionalAttr<SymbolRefAttr>:$comdat,
    OptionalAttr<DIGlobalVariableExpressionArrayAttr>:$dbg_exprs,
    DefaultValuedAttr<Visibility, "mlir::LLVM::Visibility::Default">:$visibility_
  );
  let summary = "LLVM dialect global.";
  let description = [{
    Since MLIR allows for arbitrary operations to be present at the top level,
    global variables are defined using the `llvm.mlir.global` operation. Both
    global constants and variables can be defined, and the value may also be
    initialized in both cases.

    There are two forms of initialization syntax. Simple constants that can be
    represented as MLIR attributes can be given in-line:

    ```mlir
    llvm.mlir.global @variable(32.0 : f32) : f32
    ```

    This initialization and type syntax is similar to `llvm.mlir.constant` and
    may use two types: one for MLIR attribute and another for the LLVM value.
    These types must be compatible.

    More complex constants that cannot be represented as MLIR attributes can be
    given in an initializer region:

    ```mlir
    // This global is initialized with the equivalent of:
    //   i32* getelementptr (i32* @g2, i32 2)
    llvm.mlir.global constant @int_gep() : !llvm.ptr {
      %0 = llvm.mlir.addressof @g2 : !llvm.ptr
      %1 = llvm.mlir.constant(2 : i32) : i32
      %2 = llvm.getelementptr %0[%1]
         : (!llvm.ptr, i32) -> !llvm.ptr, i32
      // The initializer region must end with `llvm.return`.
      llvm.return %2 : !llvm.ptr
    }
    ```

    Only one of the initializer attribute or initializer region may be provided.

    `llvm.mlir.global` must appear at top-level of the enclosing module. It uses
    an @-identifier for its value, which will be uniqued by the module with
    respect to other @-identifiers in it.

    Examples:

    ```mlir
    // Global values use @-identifiers.
    llvm.mlir.global constant @cst(42 : i32) : i32

    // Non-constant values must also be initialized.
    llvm.mlir.global @variable(32.0 : f32) : f32

    // Strings are expected to be of wrapped LLVM i8 array type and do not
    // automatically include the trailing zero.
    llvm.mlir.global @string("abc") : !llvm.array<3 x i8>

    // For strings globals, the trailing type may be omitted.
    llvm.mlir.global constant @no_trailing_type("foo bar")

    // A complex initializer is constructed with an initializer region.
    llvm.mlir.global constant @int_gep() : !llvm.ptr {
      %0 = llvm.mlir.addressof @g2 : !llvm.ptr
      %1 = llvm.mlir.constant(2 : i32) : i32
      %2 = llvm.getelementptr %0[%1]
         : (!llvm.ptr, i32) -> !llvm.ptr, i32
      llvm.return %2 : !llvm.ptr
    }
    ```

    Similarly to functions, globals have a linkage attribute. In the custom
    syntax, this attribute is placed between `llvm.mlir.global` and the optional
    `constant` keyword. If the attribute is omitted, `external` linkage is
    assumed by default.

    Examples:

    ```mlir
    // A constant with internal linkage will not participate in linking.
    llvm.mlir.global internal constant @cst(42 : i32) : i32

    // By default, "external" linkage is assumed and the global participates in
    // symbol resolution at link-time.
    llvm.mlir.global @glob(0 : f32) : f32

    // Alignment is optional
    llvm.mlir.global private constant @y(dense<1.0> : tensor<8xf32>) : !llvm.array<8 x f32>
    ```

    Like global variables in LLVM IR, globals can have an (optional)
    alignment attribute using keyword `alignment`. The integer value of the
    alignment must be a positive integer that is a power of 2.

    Examples:

    ```mlir
    // Alignment is optional
    llvm.mlir.global private constant @y(dense<1.0> : tensor<8xf32>) { alignment = 32 : i64 } : !llvm.array<8 x f32>
    ```

  }];
  let regions = (region AnyRegion:$initializer);

  let builders = [
    OpBuilder<(ins "Type":$type, "bool":$isConstant, "Linkage":$linkage,
      "StringRef":$name, "Attribute":$value,
      CArg<"uint64_t", "0">:$alignment,
      CArg<"unsigned", "0">:$addrSpace,
      CArg<"bool", "false">:$dsoLocal,
      CArg<"bool", "false">:$thread_local_,
      CArg<"SymbolRefAttr", "{}">:$comdat,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs,
      CArg<"ArrayRef<Attribute>", "{}">:$dbgExprs)>
  ];

  let extraClassDeclaration = [{
    /// Return the LLVM type of the global.
    Type getType() {
      return getGlobalType();
    }
    /// Return the initializer attribute if it exists, or a null attribute.
    Attribute getValueOrNull() {
      return getValue().value_or(Attribute());
    }
    /// Return the initializer region. This may be empty, but if it is not it
    /// terminates in an `llvm.return` op with the initializer value.
    Region &getInitializerRegion() {
      return getOperation()->getRegion(0);
    }
    /// Return the initializer block. If the initializer region is empty this
    /// is nullptr. If it is not nullptr, it terminates with an `llvm.return`
    /// op with the initializer value.
    Block *getInitializerBlock() {
      return getInitializerRegion().empty() ?
        nullptr : &getInitializerRegion().front();
    }
  }];

  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
  let hasRegionVerifier = 1;
}

def LLVM_GlobalCtorsOp : LLVM_Op<"mlir.global_ctors", [
                           DeclareOpInterfaceMethods<SymbolUserOpInterface>]> {
  let arguments = (ins FlatSymbolRefArrayAttr
                   : $ctors, I32ArrayAttr
                   : $priorities);
  let summary = "LLVM dialect global_ctors.";
  let description = [{
    Specifies a list of constructor functions and priorities. The functions
    referenced by this array will be called in ascending order of priority (i.e.
    lowest first) when the module is loaded. The order of functions with the
    same priority is not defined. This operation is translated to LLVM's
    global_ctors global variable. The initializer functions are run at load
    time. The `data` field present in LLVM's global_ctors variable is not
    modeled here.

    Examples:

    ```mlir
    llvm.mlir.global_ctors {@ctor}

    llvm.func @ctor() {
      ...
      llvm.return
    }
    ```

  }];
  let assemblyFormat = "attr-dict";
  let hasVerifier = 1;
}

def LLVM_GlobalDtorsOp : LLVM_Op<"mlir.global_dtors", [
                           DeclareOpInterfaceMethods<SymbolUserOpInterface>]> {
  let arguments = (ins
    FlatSymbolRefArrayAttr:$dtors,
    I32ArrayAttr:$priorities
  );
  let summary = "LLVM dialect global_dtors.";
  let description = [{
    Specifies a list of destructor functions and priorities. The functions
    referenced by this array will be called in descending order of priority (i.e.
    highest first) when the module is unloaded. The order of functions with the
    same priority is not defined. This operation is translated to LLVM's
    global_dtors global variable. The `data` field present in LLVM's
    global_dtors variable is not modeled here.

    Examples:

    ```mlir
    llvm.func @dtor() {
      llvm.return
    }
    llvm.mlir.global_dtors {@dtor}
    ```

  }];
  let assemblyFormat = "attr-dict";
  let hasVerifier = 1;
}

def LLVM_ComdatSelectorOp : LLVM_Op<"comdat_selector", [Symbol]> {
  let arguments = (ins
    SymbolNameAttr:$sym_name,
    Comdat:$comdat
  );

  let summary = "LLVM dialect comdat selector declaration";

  let description = [{
    Provides access to object file COMDAT section/group functionality.

    Examples:
    ```mlir
    llvm.comdat @__llvm_comdat {
      llvm.comdat_selector @any any
    }
    llvm.mlir.global internal constant @has_any_comdat(1 : i64) comdat(@__llvm_comdat::@any) : i64
    ```
  }];
  let assemblyFormat = "$sym_name $comdat attr-dict";
}

def LLVM_ComdatOp : LLVM_Op<"comdat", [NoTerminator, NoRegionArguments, SymbolTable, Symbol]> {
  let arguments = (ins
    SymbolNameAttr:$sym_name
  );
  let summary = "LLVM dialect comdat region";

  let description = [{
    Provides access to object file COMDAT section/group functionality.

    Examples:
    ```mlir
    llvm.comdat @__llvm_comdat {
      llvm.comdat_selector @any any
    }
    llvm.mlir.global internal constant @has_any_comdat(1 : i64) comdat(@__llvm_comdat::@any) : i64
    ```
  }];
  let regions = (region SizedRegion<1>:$body);


  let skipDefaultBuilders = 1;
  let builders = [OpBuilder<(ins "StringRef":$symName)>];

  let assemblyFormat = "$sym_name $body attr-dict";
  let hasRegionVerifier = 1;
}

def LLVM_LLVMFuncOp : LLVM_Op<"func", [
    AutomaticAllocationScope, IsolatedFromAbove, FunctionOpInterface
  ]> {
  let summary = "LLVM dialect function.";

  let description = [{
    MLIR functions are defined by an operation that is not built into the IR
    itself. The LLVM dialect provides an `llvm.func` operation to define
    functions compatible with LLVM IR. These functions have LLVM dialect
    function type but use MLIR syntax to express it. They are required to have
    exactly one result type. LLVM function operation is intended to capture
    additional properties of LLVM functions, such as linkage and calling
    convention, that may be modeled differently by the built-in MLIR function.

    ```mlir
    // The type of @bar is !llvm<"i64 (i64)">
    llvm.func @bar(%arg0: i64) -> i64 {
      llvm.return %arg0 : i64
    }

    // Type type of @foo is !llvm<"void (i64)">
    // !llvm.void type is omitted
    llvm.func @foo(%arg0: i64) {
      llvm.return
    }

    // A function with `internal` linkage.
    llvm.func internal @internal_func() {
      llvm.return
    }
    ```
  }];

  let arguments = (ins
    StrAttr:$sym_name,
    OptionalAttr<StrAttr>:$sym_visibility,
    TypeAttrOf<LLVM_FunctionType>:$function_type,
    DefaultValuedAttr<Linkage, "Linkage::External">:$linkage,
    UnitAttr:$dso_local,
    DefaultValuedAttr<CConv, "CConv::C">:$CConv,
    OptionalAttr<SymbolRefAttr>:$comdat,
    OptionalAttr<UnitAttr>:$convergent,
    OptionalAttr<FlatSymbolRefAttr>:$personality,
    OptionalAttr<StrAttr>:$garbageCollector,
    OptionalAttr<ArrayAttr>:$passthrough,
    OptionalAttr<DictArrayAttr>:$arg_attrs,
    OptionalAttr<DictArrayAttr>:$res_attrs,
    OptionalAttr<I64Attr>:$function_entry_count,
    OptionalAttr<LLVM_MemoryEffectsAttr>:$memory_effects,
    DefaultValuedAttr<Visibility, "mlir::LLVM::Visibility::Default">:$visibility_,
    OptionalAttr<UnitAttr>:$arm_streaming,
    OptionalAttr<UnitAttr>:$arm_locally_streaming,
    OptionalAttr<UnitAttr>:$arm_streaming_compatible,
    OptionalAttr<UnitAttr>:$arm_new_za,
    OptionalAttr<UnitAttr>:$arm_in_za,
    OptionalAttr<UnitAttr>:$arm_out_za,
    OptionalAttr<UnitAttr>:$arm_inout_za,
    OptionalAttr<UnitAttr>:$arm_preserves_za,
    OptionalAttr<StrAttr>:$section,
    OptionalAttr<UnnamedAddr>:$unnamed_addr,
    OptionalAttr<I64Attr>:$alignment,
    OptionalAttr<LLVM_VScaleRangeAttr>:$vscale_range,
    OptionalAttr<FramePointerKindAttr>:$frame_pointer,
    OptionalAttr<StrAttr>:$target_cpu,
    OptionalAttr<StrAttr>:$tune_cpu,
    OptionalAttr<LLVM_TargetFeaturesAttr>:$target_features,
    OptionalAttr<BoolAttr>:$unsafe_fp_math,
    OptionalAttr<BoolAttr>:$no_infs_fp_math,
    OptionalAttr<BoolAttr>:$no_nans_fp_math,
    OptionalAttr<BoolAttr>:$approx_func_fp_math,
    OptionalAttr<BoolAttr>:$no_signed_zeros_fp_math,
    OptionalAttr<StrAttr>:$denormal_fp_math,
    OptionalAttr<StrAttr>:$denormal_fp_math_f32,
    OptionalAttr<StrAttr>:$fp_contract,
    OptionalAttr<UnitAttr>:$no_inline,
    OptionalAttr<UnitAttr>:$always_inline,
    OptionalAttr<UnitAttr>:$no_unwind,
    OptionalAttr<UnitAttr>:$will_return,
    OptionalAttr<UnitAttr>:$optimize_none,
    OptionalAttr<LLVM_VecTypeHintAttr>:$vec_type_hint,
    OptionalAttr<DenseI32ArrayAttr>:$work_group_size_hint,
    OptionalAttr<DenseI32ArrayAttr>:$reqd_work_group_size,
    OptionalAttr<I32Attr>:$intel_reqd_sub_group_size
  );

  let regions = (region AnyRegion:$body);

  let skipDefaultBuilders = 1;

  let builders = [
    OpBuilder<(ins "StringRef":$name, "Type":$type,
      CArg<"Linkage", "Linkage::External">:$linkage,
      CArg<"bool", "false">:$dsoLocal,
      CArg<"CConv", "CConv::C">:$cconv,
      CArg<"SymbolRefAttr", "{}">:$comdat,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs,
      CArg<"ArrayRef<DictionaryAttr>", "{}">:$argAttrs,
      CArg<"std::optional<uint64_t>", "{}">:$functionEntryCount)>
  ];

  let extraClassDeclaration = [{
    // Add an entry block to an empty function, and set up the block arguments
    // to match the signature of the function.
    Block *addEntryBlock(OpBuilder &builder);

    bool isVarArg() { return getFunctionType().isVarArg(); }

    /// Returns the argument types of this function.
    ArrayRef<Type> getArgumentTypes() { return getFunctionType().getParams(); }

    /// Returns the result types of this function.
    ArrayRef<Type> getResultTypes() {
      if (::llvm::isa<LLVM::LLVMVoidType>(getFunctionType().getReturnType()))
        return {};
      return getFunctionType().getReturnTypes();
    }

    /// Returns the callable region, which is the function body. If the function
    /// is external, returns null.
    Region *getCallableRegion();

    /// Returns true if the `no_inline` attribute is set, false otherwise.
    bool isNoInline() { return bool(getNoInlineAttr()); }

    /// Returns true if the `always_inline` attribute is set, false otherwise.
    bool isAlwaysInline() { return bool(getAlwaysInlineAttr()); }

    /// Returns true if the `optimize_none` attribute is set, false otherwise.
    bool isOptimizeNone() { return bool(getOptimizeNoneAttr()); }
  }];

  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
  let hasRegionVerifier = 1;
}

def LLVM_NoneTokenOp
    : LLVM_Op<"mlir.none", [Pure]> {
  let summary = "Defines a value containing an empty token to LLVM type.";
  let description = [{
    Unlike LLVM IR, MLIR does not have first-class token values. They must be
    explicitly created as SSA values using `llvm.mlir.none`. This operation has
    no operands or attributes, and returns a none token value of a wrapped LLVM IR
    pointer type.

    Examples:

    ```mlir
    %0 = llvm.mlir.none : !llvm.token
    ```
  }];

  string llvmBuilder = [{
    $res = llvm::ConstantTokenNone::get(builder.getContext());
  }];

  let results = (outs LLVM_TokenType:$res);
  let builders = [LLVM_OneResultOpBuilder];
  let assemblyFormat = "attr-dict `:` type($res)";
}

def LLVM_UndefOp : LLVM_Op<"mlir.undef", [Pure, ConstantLike]>,
                   LLVM_Builder<"$res = llvm::UndefValue::get($_resultType);"> {
  let summary = "Creates an undefined value of LLVM dialect type.";
  let description = [{
    Unlike LLVM IR, MLIR does not have first-class undefined values. Such values
    must be created as SSA values using `llvm.mlir.undef`. This operation has no
    operands or attributes. It creates an undefined value of the specified LLVM
    IR dialect type.

    Example:

    ```mlir
    // Create a structure with a 32-bit integer followed by a float.
    %0 = llvm.mlir.undef : !llvm.struct<(i32, f32)>
    ```
  }];
  let results = (outs LLVM_Type:$res);
  let builders = [LLVM_OneResultOpBuilder];
  let assemblyFormat = "attr-dict `:` type($res)";
  let hasFolder = 1;
}

def LLVM_PoisonOp : LLVM_Op<"mlir.poison", [Pure, ConstantLike]>,
                    LLVM_Builder<"$res = llvm::PoisonValue::get($_resultType);"> {
  let summary = "Creates a poison value of LLVM dialect type.";
  let description = [{
    Unlike LLVM IR, MLIR does not have first-class poison values. Such values
    must be created as SSA values using `llvm.mlir.poison`. This operation has
    no operands or attributes. It creates a poison value of the specified LLVM
    IR dialect type.

    Example:

    ```mlir
    // Create a poison value for a structure with a 32-bit integer followed
    // by a float.
    %0 = llvm.mlir.poison : !llvm.struct<(i32, f32)>
    ```
  }];
  let results = (outs LLVM_Type:$res);
  let builders = [LLVM_OneResultOpBuilder];
  let assemblyFormat = "attr-dict `:` type($res)";
  let hasFolder = 1;
}

def LLVM_ZeroOp
    : LLVM_Op<"mlir.zero", [Pure, ConstantLike]>,
      LLVM_Builder<"$res = llvm::Constant::getNullValue($_resultType);">
{
  let summary = "Creates a zero-initialized value of LLVM dialect type.";
  let description = [{
    Unlike LLVM IR, MLIR does not have first-class zero-initialized values.
    Such values must be created as SSA values using `llvm.mlir.zero`. This
    operation has no operands or attributes. It creates a zero-initialized
    value of the specified LLVM IR dialect type.

    Example:

    ```mlir
    // Create a zero-initialized value for a structure with a 32-bit integer
    // followed by a float.
    %0 = llvm.mlir.zero : !llvm.struct<(i32, f32)>
    ```
  }];
  let results = (outs LLVM_Type:$res);
  let builders = [LLVM_OneResultOpBuilder];
  let assemblyFormat = "attr-dict `:` type($res)";
  let hasVerifier = 1;
  let hasFolder = 1;
}

def LLVM_ConstantOp
    : LLVM_Op<"mlir.constant", [Pure, ConstantLike]>,
      LLVM_Builder<[{$res = getLLVMConstant($_resultType, $value, $_location,
                                            moduleTranslation);}]>
{
  let summary = "Defines a constant of LLVM type.";
  let description = [{
    Unlike LLVM IR, MLIR does not have first-class constant values. Therefore,
    all constants must be created as SSA values before being used in other
    operations. `llvm.mlir.constant` creates such values for scalars, vectors,
    strings, and structs. It has a mandatory `value` attribute whose type
    depends on the type of the constant value. The type of the constant value
    must correspond to the attribute type converted to LLVM IR type.

    When creating constant scalars, the `value` attribute must be either an
    integer attribute or a floating point attribute. The type of the attribute
    may be omitted for `i64` and `f64` types that are implied.

    When creating constant vectors, the `value` attribute must be either an
    array attribute, a dense attribute, or a sparse attribute that contains
    integers or floats. The number of elements in the result vector must match
    the number of elements in the attribute.

    When creating constant strings, the `value` attribute must be a string
    attribute. The type of the constant must be an LLVM array of `i8`s, and the
    length of the array must match the length of the attribute.

    When creating constant structs, the `value` attribute must be an array
    attribute that contains integers or floats. The type of the constant must be
    an LLVM struct type. The number of fields in the struct must match the
    number of elements in the attribute, and the type of each LLVM struct field
    must correspond to the type of the corresponding attribute element converted
    to LLVM IR.

    Examples:

    ```mlir
    // Integer constant, internal i32 is mandatory
    %0 = llvm.mlir.constant(42 : i32) : i32

    // It's okay to omit i64.
    %1 = llvm.mlir.constant(42) : i64

    // Floating point constant.
    %2 = llvm.mlir.constant(42.0 : f32) : f32

    // Splat dense vector constant.
    %3 = llvm.mlir.constant(dense<1.0> : vector<4xf32>) : vector<4xf32>
    ```
  }];

  let arguments = (ins AnyAttr:$value);
  let results = (outs LLVM_Type:$res);

  let assemblyFormat = "`(` $value `)` attr-dict `:` type($res)";

  let builders = [
    LLVM_OneResultOpBuilder,
    OpBuilder<(ins "Type":$type, "int64_t":$value), [{
      build($_builder, $_state, type, $_builder.getIntegerAttr(type, value));
    }]>,
    OpBuilder<(ins "Type":$type, "const APInt &":$value), [{
      build($_builder, $_state, type, $_builder.getIntegerAttr(type, value));
    }]>,
    OpBuilder<(ins "Type":$type, "const APFloat &":$value), [{
      build($_builder, $_state, type, $_builder.getFloatAttr(type, value));
    }]>,
    OpBuilder<(ins "TypedAttr":$value), [{
      build($_builder, $_state, value.getType(), value);
    }]>
  ];

  let extraClassDeclaration = [{
    /// Whether the constant op can be constructed with a particular value and
    /// type.
    static bool isBuildableWith(Attribute value, Type type);

    /// Build the constant op with `value` and `type` if possible, otherwise
    /// returns null.
    static ConstantOp materialize(OpBuilder &builder, Attribute value,
                                  Type type, Location loc);
  }];

  let hasFolder = 1;
  let hasVerifier = 1;
}

// Atomic operations.
//

def LLVM_AtomicRMWType
    : AnyTypeOf<[LLVM_AnyFloat, LLVM_AnyPointer, AnySignlessInteger, LLVM_AnyFixedVector]>;

def LLVM_AtomicRMWOp : LLVM_MemAccessOpBase<"atomicrmw", [
      TypesMatchWith<"result #0 and operand #1 have the same type",
                     "val", "res", "$_self">]> {
  dag args = (ins AtomicBinOp:$bin_op,
              LLVM_AnyPointer:$ptr,
              LLVM_AtomicRMWType:$val, AtomicOrdering:$ordering,
              OptionalAttr<StrAttr>:$syncscope,
              OptionalAttr<I64Attr>:$alignment,
              UnitAttr:$volatile_);
  // Append the aliasing related attributes defined in LLVM_MemAccessOpBase.
  let arguments = !con(args, aliasAttrs);
  let results = (outs LLVM_AtomicRMWType:$res);
  let assemblyFormat = [{
    (`volatile` $volatile_^)? $bin_op $ptr `,` $val
    (`syncscope` `(` $syncscope^ `)`)? $ordering attr-dict `:`
    qualified(type($ptr)) `,` type($val)
  }];
  string llvmInstName = "AtomicRMW";
  string llvmBuilder = [{
    auto *inst = builder.CreateAtomicRMW(
        convertAtomicBinOpToLLVM($bin_op), $ptr, $val, llvm::MaybeAlign(),
        convertAtomicOrderingToLLVM($ordering));
    $res = inst;
  }] # setVolatileCode
     # setSyncScopeCode
     # setAlignmentCode
     # setAccessGroupsMetadataCode
     # setAliasAnalysisMetadataCode;
  string mlirBuilder = [{
    auto *atomicInst = cast<llvm::AtomicRMWInst>(inst);
    unsigned alignment = atomicInst->getAlign().value();
    $res = $_builder.create<LLVM::AtomicRMWOp>($_location,
        convertAtomicBinOpFromLLVM(atomicInst->getOperation()), $ptr, $val,
        convertAtomicOrderingFromLLVM(atomicInst->getOrdering()),
        getLLVMSyncScope(atomicInst), alignment, atomicInst->isVolatile());
  }];
  list<int> llvmArgIndices = [-1, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1];
  let builders = [
    OpBuilder<(ins "LLVM::AtomicBinOp":$binOp, "Value":$ptr, "Value":$val,
      "LLVM::AtomicOrdering":$ordering,
      CArg<"StringRef", "StringRef()">:$syncscope,
      CArg<"unsigned", "0">:$alignment, CArg<"bool", "false">:$isVolatile
    )>
  ];
  let hasVerifier = 1;
}

def LLVM_AtomicCmpXchgType : AnyTypeOf<[AnySignlessInteger, LLVM_AnyPointer]>;

def LLVM_AtomicCmpXchgOp : LLVM_MemAccessOpBase<"cmpxchg", [
      TypesMatchWith<"operand #1 and operand #2 have the same type",
                     "val", "cmp", "$_self">,
      TypesMatchWith<"result #0 has an LLVM struct type consisting of "
                     "the type of operand #2 and a bool", "val", "res",
                     "getValAndBoolStructType($_self)">]> {
  dag args = (ins LLVM_AnyPointer:$ptr,
              LLVM_AtomicCmpXchgType:$cmp, LLVM_AtomicCmpXchgType:$val,
              AtomicOrdering:$success_ordering,
              AtomicOrdering:$failure_ordering,
              OptionalAttr<StrAttr>:$syncscope,
              OptionalAttr<I64Attr>:$alignment,
              UnitAttr:$weak,
              UnitAttr:$volatile_);
  // Append the aliasing related attributes defined in LLVM_MemAccessOpBase.
  let arguments = !con(args, aliasAttrs);
  let results = (outs LLVM_AnyStruct:$res);
  let assemblyFormat = [{
    (`weak` $weak^)? (`volatile` $volatile_^)? $ptr `,` $cmp `,` $val
    (`syncscope` `(` $syncscope^ `)`)? $success_ordering $failure_ordering
    attr-dict `:` qualified(type($ptr)) `,` type($val)
  }];
  string llvmInstName = "AtomicCmpXchg";
  string llvmBuilder = [{
    auto *inst = builder.CreateAtomicCmpXchg($ptr, $cmp, $val,
        llvm::MaybeAlign(), convertAtomicOrderingToLLVM($success_ordering),
        convertAtomicOrderingToLLVM($failure_ordering));
    $res = inst;
    inst->setWeak($weak);
  }] # setVolatileCode
     # setSyncScopeCode
     # setAlignmentCode
     # setAccessGroupsMetadataCode
     # setAliasAnalysisMetadataCode;
  string mlirBuilder = [{
    auto *cmpXchgInst = cast<llvm::AtomicCmpXchgInst>(inst);
    unsigned alignment = cmpXchgInst->getAlign().value();
    $res = $_builder.create<LLVM::AtomicCmpXchgOp>(
      $_location, $ptr, $cmp, $val,
      convertAtomicOrderingFromLLVM(cmpXchgInst->getSuccessOrdering()),
      convertAtomicOrderingFromLLVM(cmpXchgInst->getFailureOrdering()),
      getLLVMSyncScope(cmpXchgInst), alignment, cmpXchgInst->isWeak(),
      cmpXchgInst->isVolatile());
  }];
  let builders = [
    OpBuilder<(ins "Value":$ptr, "Value":$cmp, "Value":$val,
      "LLVM::AtomicOrdering":$successOrdering,
      "LLVM::AtomicOrdering":$failureOrdering,
      CArg<"StringRef", "StringRef()">:$syncscope,
      CArg<"unsigned", "0">:$alignment, CArg<"bool", "false">:$isWeak,
      CArg<"bool", "false">:$isVolatile
    )>
  ];
  let hasVerifier = 1;
}

def LLVM_FenceOp : LLVM_Op<"fence">, LLVM_MemOpPatterns {
  let arguments = (ins AtomicOrdering:$ordering,
                   OptionalAttr<StrAttr>:$syncscope);
  let assemblyFormat = "(`syncscope` `(` $syncscope^ `)`)? $ordering attr-dict";
  string llvmInstName = "Fence";
  let llvmBuilder = [{
    auto *inst = builder.CreateFence(convertAtomicOrderingToLLVM($ordering));
  }] # setSyncScopeCode;
  string mlirBuilder = [{
    llvm::FenceInst *fenceInst = cast<llvm::FenceInst>(inst);
    $_op = $_builder.create<LLVM::FenceOp>(
      $_location,
      convertAtomicOrderingFromLLVM(fenceInst->getOrdering()),
      getLLVMSyncScope(fenceInst));
  }];
  let builders = [
    LLVM_VoidResultTypeOpBuilder,
    LLVM_ZeroResultOpBuilder,
    OpBuilder<(ins "LLVM::AtomicOrdering":$ordering,
      CArg<"StringRef", "StringRef()">:$syncscope)>
  ];
  let hasVerifier = 1;
}

def LLVM_InlineAsmOp : LLVM_Op<"inline_asm", [DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
  let description = [{
    The InlineAsmOp mirrors the underlying LLVM semantics with a notable
    exception: the embedded `asm_string` is not allowed to define or reference
    any symbol or any global variable: only the operands of the op may be read,
    written, or referenced.
    Attempting to define or reference any symbol or any global behavior is
    considered undefined behavior at this time.
  }];
  let arguments = (
    ins Variadic<LLVM_Type>:$operands,
        StrAttr:$asm_string,
        StrAttr:$constraints,
        UnitAttr:$has_side_effects,
        UnitAttr:$is_align_stack,
        OptionalAttr<
          DefaultValuedAttr<AsmATTOrIntel, "AsmDialect::AD_ATT">>:$asm_dialect,
        OptionalAttr<ArrayAttr>:$operand_attrs);

  let results = (outs Optional<LLVM_Type>:$res);

  let assemblyFormat = [{
    (`has_side_effects` $has_side_effects^)?
    (`is_align_stack` $is_align_stack^)?
    (`asm_dialect` `=` $asm_dialect^)?
    (`operand_attrs` `=` $operand_attrs^)?
    attr-dict
    $asm_string `,` $constraints
    operands `:` functional-type(operands, results)
   }];

  let extraClassDeclaration = [{
    static StringRef getElementTypeAttrName() {
      return "elementtype";
    }
  }];
}

//===--------------------------------------------------------------------===//
// CallIntrinsicOp
//===--------------------------------------------------------------------===//

def LLVM_CallIntrinsicOp
    : LLVM_Op<"call_intrinsic",
              [AttrSizedOperandSegments,
               DeclareOpInterfaceMethods<FastmathFlagsInterface>]> {
  let summary = "Call to an LLVM intrinsic function.";
  let description = [{
    Call the specified llvm intrinsic. If the intrinsic is overloaded, use
    the MLIR function type of this op to determine which intrinsic to call.
  }];
  let arguments = (ins StrAttr:$intrin, Variadic<LLVM_Type>:$args,
                       DefaultValuedAttr<LLVM_FastmathFlagsAttr,
                                         "{}">:$fastmathFlags,
                       VariadicOfVariadic<LLVM_Type,
                                          "op_bundle_sizes">:$op_bundle_operands,
                       DenseI32ArrayAttr:$op_bundle_sizes,
                       OptionalAttr<ArrayAttr>:$op_bundle_tags);
  let results = (outs Optional<LLVM_Type>:$results);
  let llvmBuilder = [{
    return convertCallLLVMIntrinsicOp(op, builder, moduleTranslation);
  }];
  let assemblyFormat = [{
    $intrin `(` $args `)`
    ( custom<OpBundles>($op_bundle_operands, type($op_bundle_operands),
                        $op_bundle_tags)^ )?
    `:` functional-type($args, $results)
    attr-dict
  }];

  let builders = [
    OpBuilder<(ins "StringAttr":$intrin, "ValueRange":$args)>,
    OpBuilder<(ins "StringAttr":$intrin, "ValueRange":$args, "FastmathFlagsAttr":$fastMathFlags)>,
    OpBuilder<(ins "Type": $resultType, "StringAttr":$intrin, "ValueRange":$args)>,
    OpBuilder<(ins "TypeRange": $resultTypes, "StringAttr":$intrin, "ValueRange":$args, "FastmathFlagsAttr":$fastMathFlags)>
  ];

  let hasVerifier = 1;
}

def LLVM_LinkerOptionsOp
    : LLVM_Op<"linker_options"> {
  let summary = "Options to pass to the linker when the object file is linked";
  let description = [{
    Pass the given options to the linker when the resulting object file is linked.
    This is used extensively on Windows to determine the C runtime that the object
    files should link against.

    Examples:
    ```mlir
    // Link against the MSVC static threaded CRT.
    llvm.linker_options ["/DEFAULTLIB:", "libcmt"]

    // Link against aarch64 compiler-rt builtins
    llvm.linker_options ["-l", "clang_rt.builtins-aarch64"]
    ```
  }];
  let arguments  = (ins StrArrayAttr:$options);
  let assemblyFormat = [{
    $options attr-dict
  }];

  let llvmBuilder = [{
    convertLinkerOptionsOp($options, builder, moduleTranslation);
  }];

  let hasVerifier = 1;
}

#endif // LLVMIR_OPS


//===- NVGPUTransformOps.td - NVGPU transform ops ----------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef NVGPU_TRANSFORM_OPS
#define NVGPU_TRANSFORM_OPS

include "mlir/Dialect/Transform/IR/TransformAttrs.td"
include "mlir/Dialect/Transform/IR/TransformDialect.td"
include "mlir/Dialect/Transform/Interfaces/TransformInterfaces.td"
include "mlir/Dialect/Transform/IR/TransformTypes.td"
include "mlir/Interfaces/SideEffectInterfaces.td"

//===----------------------------------------------------------------------===//
// Apply...ConversionPatternsOp
//===----------------------------------------------------------------------===//

def ApplyNVGPUToNVVMConversionPatternsOp : Op<Transform_Dialect,
    "apply_conversion_patterns.nvgpu.nvgpu_to_nvvm",
    [DeclareOpInterfaceMethods<ConversionPatternDescriptorOpInterface,
                               ["verifyTypeConverter"]>]> {
  let description = [{
    Collects patterns that convert NVGPU dialect ops to NVVM dialect ops. These
    patterns require an "LLVMTypeConverter".
  }];
  let assemblyFormat = "attr-dict";
}

//===----------------------------------------------------------------------===//
// CreateAsyncGroupsOp
//===----------------------------------------------------------------------===//

def CreateAsyncGroupsOp :
  Op<Transform_Dialect, "nvgpu.create_async_groups",
    [DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     TransformEachOpTrait,
     TransformOpInterface,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Look for global to shared memory copies within the targeted op in the form
    of vector transfer ops and convert them to async copies when possible.
    Consecutive copies are put into the same group. A "wait" operation is
    inserted right at the of end the group.

    `bypass_l1` specifies whether `bypassL1` attributes should be added to
    the async copies. `bypass_l1` is a compiler hint: only 16 byte transfers
    can bypass the L1 cache, so this attribute is not set for any other transfer
    sizes.

    #### Return modes

    This op consumes the `target` handle and produces the `result` handle, which
    is mapped to the same payload operations as the `target` handle. The op
    modifies the payload.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                   UnitAttr:$bypass_l1);
  let results = (outs TransformHandleTypeInterface:$result);

  let assemblyFormat = [{
    $target attr-dict `:` functional-type(operands, results)
  }];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::Operation *target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// PipelineSharedMemoryCopiesOp
//===----------------------------------------------------------------------===//

def PipelineSharedMemoryCopiesOp :
  Op<Transform_Dialect, "nvgpu.pipeline_shared_memory_copies",
    [FunctionalStyleTransformOpTrait,
     MemoryEffectsOpInterface,
     TransformEachOpTrait,
     TransformOpInterface,
     ReportTrackingListenerFailuresOpTrait]> {
  let summary =
    "Applies software pipelining to a given loop with shared memory copies";

  let description = [{
    Applies software pipelining to a given scf.for loop. The pipelining
    strategy will look for a load into shared memory and pipeline it to overlap
    it with the rest of the loop.
    
    NOTE: It is user responsibility to ensure that there are no dependency
    between `depth` iterations of the loop by using multi-buffering. It is
    also user responsibility to ensure a sufficient amount of shared memory
    is allocated to cover eventual writes by `depth-1` speculative
    iterations.

    `depth` will indicate how many stages the software pipeline should have.
    `peel_epilogue` allows to force the epilogue to be peeled out instead of
    potentially using predicated operations for the epilogue phase.

    #### Return modes

    Consumes the operand handle and produces a result handle pointing to the
    loop, which may or may not have been pipelined. Produces a definite failure
    if the loop pipeliner mutated the IR before failing to pipeline, in
    particular if `peel_epilogue` is not set and the loop body doesn't support
    predication. If failure propagation mode is set to "propagate", produces a
    silenceable failure when pipelining preconditions, e.g., loop bound being
    static, are not met or when the loop wasn't pipelined because due to the
    lack of loads into shared memory. If the failure propagation mode is set
    to "suppress" (default), succeeds in these case and associates the result
    handle with the original loop.

    TODO: the shared memory part and behavior specific to NVGPU should be
    made orthogonal to pipelining so that `transform.loop.pipeline` becomes
    usable here.
  }];

  let arguments = (ins TransformHandleTypeInterface:$for_op,
                   I64Attr:$depth,
                   UnitAttr:$peel_epilogue,
                   DefaultValuedAttr<FailurePropagationMode,
                      "::mlir::transform::FailurePropagationMode::Suppress">
                     :$failure_propagation_mode);
  let results = (outs TransformHandleTypeInterface:$result);

  let assemblyFormat = [{ 
    `failures` `(` $failure_propagation_mode `)`
    $for_op
    attr-dict 
    `:` functional-type(operands, results)
  }];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::scf::ForOp forOp,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// RewriteMatmulAsMmaSyncOp
//===----------------------------------------------------------------------===//

def RewriteMatmulAsMmaSyncOp :
  Op<Transform_Dialect, "nvgpu.rewrite_matmul_as_mma_sync",
    [FunctionalStyleTransformOpTrait, 
     MemoryEffectsOpInterface,
     TransformEachOpTrait, 
     TransformOpInterface,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Rewrite a matmul operation on memref to an mma.sync operation on vectors.

    Memory copies with the required access patterns are automatically inserted.
    Operations that do not have a 1-1 mapping to mma.sync operations are left
    unchanged.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs);

  let assemblyFormat = "$target attr-dict `:` functional-type(operands, results) ";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::linalg::LinalgOp linalgOp,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// RewriteCopyAsTmaOp
//===----------------------------------------------------------------------===//

def RewriteCopyAsTmaOp :
  Op<Transform_Dialect, "nvgpu.rewrite_copy_as_tma",
    [FunctionalStyleTransformOpTrait,
     MemoryEffectsOpInterface,
     TransformEachOpTrait,
     TransformOpInterface,
     ReportTrackingListenerFailuresOpTrait]> {
  let description = [{
    Rewrite a copy operation on memref to tma operations that transit through
    shared memory.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs);

  let assemblyFormat = "$target attr-dict `:` functional-type(operands, results) ";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure apply(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::transform::TransformResults &transformResults,
        ::mlir::transform::TransformState &state);
  }];
}

#endif // NVGPU_TRANSFORM_OPS


//===- MemRefTransformOps.td - MemRef transformation ops --*- tablegen -*--===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef MEMREF_TRANSFORM_OPS
#define MEMREF_TRANSFORM_OPS

include "mlir/Dialect/Transform/IR/TransformDialect.td"
include "mlir/Dialect/Transform/Interfaces/TransformInterfaces.td"
include "mlir/Dialect/Transform/IR/TransformTypes.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/IR/OpBase.td"

def MemrefToLLVMTypeConverterOp : Op<Transform_Dialect,
    "apply_conversion_patterns.memref.memref_to_llvm_type_converter",
    [DeclareOpInterfaceMethods<TypeConverterBuilderOpInterface,
                               ["getTypeConverter",
                                "getTypeConverterType"]>]> {
  let description = [{
    This operation provides an "LLVMTypeConverter" that lowers memref types to
    LLVM types.

    The type converter can be customized as follows:
    - `use_aligned_alloc`: Use aligned_alloc in place of malloc for heap
      allocations.
    - `index_bitwidth`: Bitwidth of the index type, "0" indicates the size of a
      machine word.
    - `use_generic_functions`: Use generic allocation and deallocation functions
      instead of the classic "malloc", "aligned_alloc" and "free" functions.
    // TODO: the following two options don't really make sense for 
    // memref_to_llvm_type_converter specifically.
    // We should have a single to_llvm_type_converter.
    - `use_bare_ptr_call_conv`: Replace FuncOp's MemRef arguments with bare 
      pointers to the MemRef element types.
    - `data-layout`: String description (LLVM format) of the data layout that is
      expected on the produced module.
  }];

  let arguments = (ins
      DefaultValuedOptionalAttr<BoolAttr, "false">:$use_aligned_alloc,
      DefaultValuedOptionalAttr<I64Attr, "64">:$index_bitwidth,
      DefaultValuedOptionalAttr<BoolAttr, "false">:$use_generic_functions,
      DefaultValuedOptionalAttr<BoolAttr, "false">:$use_bare_ptr_call_conv,
      OptionalAttr<StrAttr>:$data_layout);
  let assemblyFormat = "attr-dict";
}

def ApplyAllocToAllocaOp : Op<Transform_Dialect,
    "apply_patterns.memref.alloc_to_alloca",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface, ["populatePatternsWithState"]>]> {
  let description = [{
    Collects patterns to rewrite scoped dynamic allocation (`alloc`/`dealloc`
    pairs) into automatic allocation (`alloca`) in the same scope, for memrefs
    of static shape.

    The `size_limit` attribute controls the maximum allocated memory (in bytes,
    subject to data layout) for which the pattern applies.
  }];

  let arguments = (ins
    OptionalAttr<I64Attr>:$size_limit);
  let assemblyFormat = "(`size_limit` `(` $size_limit^ `)`)? attr-dict";
}

def ApplyExpandOpsPatternsOp : Op<Transform_Dialect,
    "apply_patterns.memref.expand_ops",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Collects patterns to rewrite ops within the memref dialect.

    - Converts `atomic_rmw` that cannot be lowered to a simple atomic op with
      AtomicRMWOpLowering pattern, e.g. with "minf" or "maxf" attributes, to
      `memref.generic_atomic_rmw` with the expanded code.
    - Converts `memref.reshape` that has a target shape of a statically-known
      size to `memref.reinterpret_cast`.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyExpandStridedMetadataPatternsOp : Op<Transform_Dialect,
    "apply_patterns.memref.expand_strided_metadata",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Collects patterns for expanding memref operations that modify the metadata
    (sizes, offset, strides) of a memref into easier to analyze constructs.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyExtractAddressComputationsPatternsOp : Op<Transform_Dialect,
    "apply_patterns.memref.extract_address_computations",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Collects patterns for extracting address computations from operations
    with memory accesses such that these memory accesses use only a base
    pointer.

    For instance,
    ```mlir
    memref.load %base[%off0, ...]
    ```

    Will be rewritten in:
    ```mlir
    %new_base = memref.subview %base[%off0,...][1,...][1,...]
    memref.load %new_base[%c0,...]
    ```
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyFoldMemrefAliasOpsPatternsOp : Op<Transform_Dialect,
    "apply_patterns.memref.fold_memref_alias_ops",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Collects patterns for folding memref aliasing ops (memref.subview) into
    consumer load/store ops (affine.load, memref.load, nvgpu.ldmatrix,
    vector.load, vector.transfer_read, affine.store, memref.store, etc.) and
    other ops (e.g., memref.subview).
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyResolveRankedShapedTypeResultDimsPatternsOp : Op<Transform_Dialect,
    "apply_patterns.memref.resolve_ranked_shaped_type_result_dims",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Collects patterns that resolve `memref.dim` operations with values that are
    defined by operations that implement the `ReifyRankedShapedTypeOpInterface`,
    in terms of shapes of its input operands.
  }];

  let assemblyFormat = "attr-dict";
}

def Transform_MemRefAllocOp : Transform_ConcreteOpType<"memref.alloc">;
def Transform_MemRefAllocaOp : Transform_ConcreteOpType<"memref.alloca">;

def MemRefAllocaToGlobalOp :
  Op<Transform_Dialect, "memref.alloca_to_global",
     [TransformOpInterface,
      DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
      DeclareOpInterfaceMethods<TransformOpInterface>]> {
  let description = [{
    Inserts a new `memref.global` for each provided `memref.alloca` into the
    nearest symbol table (e.g., a `builtin.module`) and replaces it with a
    `memref.get_global`. This is useful, for example, for allocations that
    should reside in the shared memory of a GPU, which have to be declared as
    globals.

    #### Example

    Consider the following transform op:

    ```mlir
    %get_global, %global =
        transform.memref.alloca_to_global %alloca
          : (!transform.op<"memref.alloca">)
            -> (!transform.any_op, !transform.any_op)
    ```

    and the following input payload:

    ```mlir
    module {
      func.func @func() {
        %alloca = memref.alloca() : memref<2x32xf32>
        // usages of %alloca...
      }
    }
    ```

    then applying the transform op to the payload would result in the following
    output IR:

    ```mlir
    module {
      memref.global "private" @alloc : memref<2x32xf32>
      func.func @func() {
        %alloca = memref.get_global @alloc : memref<2x32xf32>
        // usages of %alloca...
      }
    }
    ```

    #### Return modes

    Succeeds always. The returned handles refer to the `memref.get_global` and
    `memref.global` ops that were inserted by the transformation.
  }];

  let arguments = (ins Transform_MemRefAllocaOp:$alloca);
  let results = (outs TransformHandleTypeInterface:$getGlobal,
                  TransformHandleTypeInterface:$global);

  let assemblyFormat = [{
    $alloca attr-dict `:` functional-type(operands, results)
  }];
}

def MemRefMultiBufferOp : Op<Transform_Dialect, "memref.multibuffer",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     DeclareOpInterfaceMethods<TransformOpInterface>]> {
  let summary = "Multibuffers an allocation";
  let description = [{
     Transformation to do multi-buffering/array expansion to remove
     dependencies on the temporary allocation between consecutive loop
     iterations. This transform expands the size of an allocation by
     a given multiplicative factor and fixes up any users of the
     multibuffered allocation.
     If skip analysis is not set the transformation will only apply
     if it can prove that there is no data being carried across loop
     iterations.

     #### Return modes

     This operation returns the new allocation if multi-buffering
     succeeds, and failure otherwise.
  }];

  let arguments =
      (ins Transform_MemRefAllocOp:$target,
           ConfinedAttr<I64Attr, [IntPositive]>:$factor,
           UnitAttr:$skip_analysis);

  let results = (outs TransformHandleTypeInterface:$transformed);

  let assemblyFormat =
    "$target attr-dict `:` functional-type(operands, results)";
}

def MemRefEraseDeadAllocAndStoresOp
    : Op<Transform_Dialect, "memref.erase_dead_alloc_and_stores", [
      TransformEachOpTrait, TransformOpInterface,
      DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
      ReportTrackingListenerFailuresOpTrait
    ]> {
  let description = [{
    This applies memory optimization on memref. In particular it does store to
    load forwarding, dead store elimination and dead alloc elimination.

    #### Return modes

    This operation applies a set of memory optimization on the whole region of
    the operand.

    The transformation does not consume the target handle. It modifies the
    payload. Dead allocations, loads and stores are silently dropped from all
    mappings.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs);

  let assemblyFormat = "$target attr-dict `:` functional-type($target, results)";

  let skipDefaultBuilders = 1;
  let builders = [
    OpBuilder<(ins "Value":$target)>
  ];
  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::Operation *target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

def MemRefMakeLoopIndependentOp
    : Op<Transform_Dialect, "memref.make_loop_independent",
         [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
          TransformOpInterface, TransformEachOpTrait]> {
  let description = [{
    Rewrite the targeted ops such that their index-typed operands no longer
    depend on any loop induction variable of the `num_loop` enclosing `scf.for`
    loops. I.e., compute an upper bound that is independent of any such loop IV
    for every tensor dimension. The transformed op could then be hoisted from
    the `num_loop` enclosing loops. To preserve the original semantics, place a
    `memref.subview` inside the loop.

    Currently supported operations are:
    - memref.alloca: Replaced with a new memref.alloca with upper bound sizes,
      followed by a memref.subview.

    #### Return modes

    This operation fails if at least one induction variable could not be
    eliminated. In case the targeted op is already independent of induction
    variables, this transform succeeds and returns the unmodified target op.

    Otherwise, the returned handle points to a subset of the produced ops:
    - memref.alloca: The returned handle points to the memref.subview op.

    This transform op consumes the target handle and produces a result handle.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target, I64Attr:$num_loops);
  let results = (outs TransformHandleTypeInterface:$transformed);
  let assemblyFormat =
      "$target attr-dict `:` functional-type($target, $transformed)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::Operation *target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

#endif // MEMREF_TRANSFORM_OPS


//===-- OpenMPOps.td - OpenMP dialect operation definitions *- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This file defines the basic operations for the OpenMP dialect.
//
//===----------------------------------------------------------------------===//


#ifndef OPENMP_OPS
#define OPENMP_OPS

include "mlir/Dialect/LLVMIR/LLVMOpBase.td"
include "mlir/Dialect/OpenACCMPCommon/Interfaces/AtomicInterfaces.td"
include "mlir/Dialect/OpenACCMPCommon/Interfaces/OpenACCMPOpsInterfaces.td"
include "mlir/Dialect/OpenMP/OpenMPClauses.td"
include "mlir/Dialect/OpenMP/OpenMPOpBase.td"
include "mlir/Interfaces/ControlFlowInterfaces.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/IR/EnumAttr.td"
include "mlir/IR/OpBase.td"
include "mlir/IR/SymbolInterfaces.td"

//===----------------------------------------------------------------------===//
// 2.19.4 Data-Sharing Attribute Clauses
//===----------------------------------------------------------------------===//

def PrivateClauseOp : OpenMP_Op<"private", [IsolatedFromAbove, RecipeInterface]> {
  let summary = "Provides declaration of [first]private logic.";
  let description = [{
    This operation provides a declaration of how to implement the
    [first]privatization of a variable. The dialect users should provide
    information about how to create an instance of the type in the alloc region,
    how to initialize the copy from the original item in the copy region, and if
    needed, how to deallocate allocated memory in the dealloc region.

    Examples:

    * `private(x)` would be emitted as:
    ```mlir
    omp.private {type = private} @x.privatizer : !fir.ref<i32> alloc {
    ^bb0(%arg0: !fir.ref<i32>):
    %0 = ... allocate proper memory for the private clone ...
    omp.yield(%0 : !fir.ref<i32>)
    }
    ```

    * `firstprivate(x)` would be emitted as:
    ```mlir
    omp.private {type = firstprivate} @x.privatizer : !fir.ref<i32> alloc {
    ^bb0(%arg0: !fir.ref<i32>):
    %0 = ... allocate proper memory for the private clone ...
    omp.yield(%0 : !fir.ref<i32>)
    } copy {
    ^bb0(%arg0: !fir.ref<i32>, %arg1: !fir.ref<i32>):
    // %arg0 is the original host variable. Same as for `alloc`.
    // %arg1 represents the memory allocated in `alloc`.
    ... copy from host to the privatized clone ....
    omp.yield(%arg1 : !fir.ref<i32>)
    }
    ```

    * `private(x)` for "allocatables" would be emitted as:
    ```mlir
    omp.private {type = private} @x.privatizer : !some.type alloc {
    ^bb0(%arg0: !some.type):
    %0 = ... allocate proper memory for the private clone ...
    omp.yield(%0 : !fir.ref<i32>)
    } dealloc {
    ^bb0(%arg0: !some.type):
    ... deallocate allocated memory ...
    omp.yield
    }
    ```

    There are no restrictions on the body except for:
    - The `alloc` & `dealloc` regions have a single argument.
    - The `copy` region has 2 arguments.
    - All three regions are terminated by `omp.yield` ops.
    The above restrictions and other obvious restrictions (e.g. verifying the
    type of yielded values) are verified by the custom op verifier. The actual
    contents of the blocks inside all regions are not verified.

    Instances of this op would then be used by ops that model directives that
    accept data-sharing attribute clauses.

    The $sym_name attribute provides a symbol by which the privatizer op can be
    referenced by other dialect ops.

    The $type attribute is the type of the value being privatized.

    The $data_sharing_type attribute specifies whether privatizer corresponds
    to a `private` or a `firstprivate` clause.
  }];

  let arguments = (ins SymbolNameAttr:$sym_name,
                       TypeAttrOf<AnyType>:$type,
                       DataSharingClauseTypeAttr:$data_sharing_type);

  let regions = (region MinSizedRegion<1>:$alloc_region,
                        AnyRegion:$copy_region,
                        AnyRegion:$dealloc_region);

  let assemblyFormat = [{
    $data_sharing_type $sym_name `:` $type
      `alloc` $alloc_region
      (`copy` $copy_region^)?
      (`dealloc` $dealloc_region^)?
      attr-dict
  }];

  let builders = [
    OpBuilder<(ins CArg<"TypeRange">:$result,
                   CArg<"StringAttr">:$sym_name,
                   CArg<"TypeAttr">:$type)>
  ];

  let extraClassDeclaration = [{
    BlockArgument getAllocMoldArg() {
      return getAllocRegion().getArgument(0);
    }
    BlockArgument getCopyMoldArg() {
      auto &region = getCopyRegion();
      return region.empty() ? nullptr : region.getArgument(0);
    }
    BlockArgument getCopyPrivateArg() {
      auto &region = getCopyRegion();
      return region.empty() ? nullptr : region.getArgument(1);
    }
    BlockArgument getDeallocMoldArg() {
      auto &region = getDeallocRegion();
      return region.empty() ? nullptr : region.getArgument(0);
    }
  }];

  let hasRegionVerifier = 1;
}

//===----------------------------------------------------------------------===//
// 2.6 parallel Construct
//===----------------------------------------------------------------------===//

def ParallelOp : OpenMP_Op<"parallel", traits = [
    AttrSizedOperandSegments, AutomaticAllocationScope,
    DeclareOpInterfaceMethods<ComposableOpInterface>,
    DeclareOpInterfaceMethods<OutlineableOpenMPOpInterface>,
    RecursiveMemoryEffects
  ], clauses = [
    OpenMP_AllocateClause, OpenMP_IfClause, OpenMP_NumThreadsClause,
    OpenMP_PrivateClause, OpenMP_ProcBindClause, OpenMP_ReductionClause
  ], singleRegion = true> {
  let summary = "parallel construct";
  let description = [{
    The parallel construct includes a region of code which is to be executed
    by a team of threads.

    The optional `if_expr` parameter specifies a boolean result of a conditional
    check. If this value is 1 or is not provided then the parallel region runs
    as normal, if it is 0 then the parallel region is executed with one thread.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"ArrayRef<NamedAttribute>", "{}">:$attributes)>,
    OpBuilder<(ins CArg<"const ParallelOperands &">:$clauses)>
  ];

  let assemblyFormat = clausesAssemblyFormat # [{
    custom<PrivateReductionRegion>($region, $private_vars, type($private_vars),
        $private_syms, $reduction_vars, type($reduction_vars), $reduction_byref,
        $reduction_syms) attr-dict
  }];

  let hasVerifier = 1;
  let hasRegionVerifier = 1;
}

def TerminatorOp : OpenMP_Op<"terminator", [Terminator, Pure]> {
  let summary = "terminator for OpenMP regions";
  let description = [{
    A terminator operation for regions that appear in the body of OpenMP
    operation.  These regions are not expected to return any value so the
    terminator takes no operands. The terminator op returns control to the
    enclosing op.
  }];

  let assemblyFormat = "attr-dict";
}

//===----------------------------------------------------------------------===//
// 2.7 teams Construct
//===----------------------------------------------------------------------===//
def TeamsOp : OpenMP_Op<"teams", traits = [
    AttrSizedOperandSegments, RecursiveMemoryEffects
  ], clauses = [
    OpenMP_AllocateClause, OpenMP_IfClause, OpenMP_NumTeamsClause,
    OpenMP_PrivateClause, OpenMP_ReductionClause, OpenMP_ThreadLimitClause
  ], singleRegion = true> {
  let summary = "teams construct";
  let description = [{
    The teams construct defines a region of code that triggers the creation of a
    league of teams. Once created, the number of teams remains constant for the
    duration of its code region.

    If the `if_expr` is present and it evaluates to `false`, the number of teams
    created is one.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const TeamsOperands &">:$clauses)>
  ];

  let assemblyFormat = clausesAssemblyFormat # [{
    custom<PrivateReductionRegion>($region, $private_vars, type($private_vars),
        $private_syms, $reduction_vars, type($reduction_vars), $reduction_byref,
        $reduction_syms) attr-dict
  }];

  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// 2.8.1 Sections Construct
//===----------------------------------------------------------------------===//

def SectionOp : OpenMP_Op<"section", traits = [
    BlockArgOpenMPOpInterface, HasParent<"SectionsOp">
  ], singleRegion = true> {
  let summary = "section directive";
  let description = [{
    A section operation encloses a region which represents one section in a
    sections construct. A section op should always be surrounded by an
    `omp.sections` operation. The section operation may have block args
    which corespond to the block arguments of the surrounding `omp.sections`
    operation. This is done to reflect situations where these block arguments
    represent variables private to each section.
  }];
  let extraClassDeclaration = [{
    // Override BlockArgOpenMPOpInterface methods based on the parent
    // omp.sections operation. Only forward-declare here because SectionsOp is
    // not completely defined at this point.
    unsigned numPrivateBlockArgs();
    unsigned numReductionBlockArgs();
  }] # clausesExtraClassDeclaration;
  let assemblyFormat = "$region attr-dict";
}

def SectionsOp : OpenMP_Op<"sections", traits = [
    AttrSizedOperandSegments
  ], clauses = [
    OpenMP_AllocateClause, OpenMP_NowaitClause, OpenMP_PrivateClause,
    OpenMP_ReductionClause
  ], singleRegion = true> {
  let summary = "sections construct";
  let description = [{
    The sections construct is a non-iterative worksharing construct that
    contains `omp.section` operations. The `omp.section` operations are to be
    distributed among and executed by the threads in a team. Each `omp.section`
    is executed once by one of the threads in the team in the context of its
    implicit task.
    Block arguments for reduction variables should be mirrored in enclosed
    `omp.section` operations.
  }] # clausesDescription;

  // Override region definition.
  let regions = (region SizedRegion<1>:$region);

  let builders = [
    OpBuilder<(ins CArg<"const SectionsOperands &">:$clauses)>
  ];

  let assemblyFormat = clausesAssemblyFormat # [{
    custom<PrivateReductionRegion>($region, $private_vars, type($private_vars),
        $private_syms, $reduction_vars, type($reduction_vars), $reduction_byref,
        $reduction_syms) attr-dict
  }];

  let hasVerifier = 1;
  let hasRegionVerifier = 1;
}

//===----------------------------------------------------------------------===//
// 2.8.2 Single Construct
//===----------------------------------------------------------------------===//

def SingleOp : OpenMP_Op<"single", traits = [
    AttrSizedOperandSegments
  ], clauses = [
    OpenMP_AllocateClause, OpenMP_CopyprivateClause, OpenMP_NowaitClause,
    OpenMP_PrivateClause
  ], singleRegion = true> {
  let summary = "single directive";
  let description = [{
    The single construct specifies that the associated structured block is
    executed by only one of the threads in the team (not necessarily the
    master thread), in the context of its implicit task. The other threads
    in the team, which do not execute the block, wait at an implicit barrier
    at the end of the single construct.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const SingleOperands &">:$clauses)>
  ];

  let assemblyFormat = clausesAssemblyFormat # [{
    custom<PrivateRegion>($region, $private_vars, type($private_vars),
        $private_syms) attr-dict
  }];

  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// 2.8.3 Workshare Construct
//===----------------------------------------------------------------------===//

def WorkshareOp : OpenMP_Op<"workshare", traits = [
    RecursiveMemoryEffects,
  ], clauses = [
    OpenMP_NowaitClause,
  ], singleRegion = true> {
  let summary = "workshare directive";
  let description = [{
    The workshare construct divides the execution of the enclosed structured
    block into separate units of work, and causes the threads of the team to
    share the work such that each unit is executed only once by one thread, in
    the context of its implicit task

    This operation is used for the intermediate representation of the workshare
    block before the work gets divided between the threads. See the flang
    LowerWorkshare pass for details.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const WorkshareOperands &">:$clauses)>
  ];
}

def WorkshareLoopWrapperOp : OpenMP_Op<"workshare.loop_wrapper", traits = [
    DeclareOpInterfaceMethods<LoopWrapperInterface>, NoTerminator,
    RecursiveMemoryEffects, SingleBlock
  ], singleRegion = true> {
  let summary = "contains loop nests to be parallelized by workshare";
  let description = [{
    This operation wraps a loop nest that is marked for dividing into units of
    work by an encompassing omp.workshare operation.
  }];

  let builders = [
    OpBuilder<(ins), [{ build($_builder, $_state, {}); }]>
  ];
  let assemblyFormat = "$region attr-dict";
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Loop Nest
//===----------------------------------------------------------------------===//

def LoopNestOp : OpenMP_Op<"loop_nest", traits = [
    RecursiveMemoryEffects, SameVariadicOperandSize
  ], clauses = [
    OpenMP_LoopRelatedClause
  ], singleRegion = true> {
  let summary = "rectangular loop nest";
  let description = [{
    This operation represents a collapsed rectangular loop nest. For each
    rectangular loop of the nest represented by an instance of this operation,
    lower and upper bounds, as well as a step variable, must be defined.

    The lower and upper bounds specify a half-open range: the range includes the
    lower bound but does not include the upper bound. If the `loop_inclusive`
    attribute is specified then the upper bound is also included.

    The body region can contain any number of blocks. The region is terminated
    by an `omp.yield` instruction without operands. The induction variables,
    represented as entry block arguments to the loop nest operation's single
    region, match the types of the `loop_lower_bounds`, `loop_upper_bounds` and
    `loop_steps` arguments.

    ```mlir
    omp.loop_nest (%i1, %i2) : i32 = (%c0, %c0) to (%c10, %c10) step (%c1, %c1) {
      %a = load %arrA[%i1, %i2] : memref<?x?xf32>
      %b = load %arrB[%i1, %i2] : memref<?x?xf32>
      %sum = arith.addf %a, %b : f32
      store %sum, %arrC[%i1, %i2] : memref<?x?xf32>
      omp.yield
    }
    ```

    This is a temporary simplified definition of a loop based on existing OpenMP
    loop operations intended to serve as a stopgap solution until the long-term
    representation of canonical loops is defined. Specifically, this operation
    is intended to serve as a unique source for loop information during the
    transition to making `omp.distribute`, `omp.simd`, `omp.taskloop` and
    `omp.wsloop` wrapper operations. It is not intended to help with the
    addition of support for loop transformations, non-rectangular loops and
    non-perfectly nested loops.
  }];

  let builders = [
    OpBuilder<(ins CArg<"const LoopNestOperands &">:$clauses)>
  ];

  let extraClassDeclaration = [{
    /// Returns the induction variables of the loop nest.
    ArrayRef<BlockArgument> getIVs() { return getRegion().getArguments(); }

    /// Fills a list of wrapper operations around this loop nest. Wrappers
    /// in the resulting vector will be sorted from innermost to outermost.
    void gatherWrappers(SmallVectorImpl<LoopWrapperInterface> &wrappers);
  }] # clausesExtraClassDeclaration;

  // Disable inherited clause-based declarative assembly format and instead
  // enable using the custom parser-printer implemented in C++.
  let assemblyFormat = ?;
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// 2.9.2 Workshare Loop Construct
//===----------------------------------------------------------------------===//

def LoopOp : OpenMP_Op<"loop", traits = [
    AttrSizedOperandSegments, DeclareOpInterfaceMethods<LoopWrapperInterface>,
    NoTerminator, SingleBlock
  ], clauses = [
    OpenMP_BindClause, OpenMP_PrivateClause, OpenMP_OrderClause,
    OpenMP_ReductionClause
  ], singleRegion = true> {
  let summary = "loop construct";
  let description = [{
    A loop construct specifies that the logical iterations of the associated loops
    may execute concurrently and permits the encountering threads to execute the
    loop accordingly. A loop construct can have 3 different types of binding:
      1. teams: in which case the binding region is the innermost enclosing `teams`
         region.
      2. parallel: in which case the binding region is the innermost enclosing `parallel`
         region.
      3. thread: in which case the binding region is not defined.

    The body region can only contain a single block which must contain a single
    operation, this operation must be an `omp.loop_nest`.

    ```
    omp.loop <clauses> {
      omp.loop_nest (%i1, %i2) : index = (%c0, %c0) to (%c10, %c10) step (%c1, %c1) {
        %a = load %arrA[%i1, %i2] : memref<?x?xf32>
        %b = load %arrB[%i1, %i2] : memref<?x?xf32>
        %sum = arith.addf %a, %b : f32
        store %sum, %arrC[%i1, %i2] : memref<?x?xf32>
        omp.yield
      }
    }
    ```
  }] # clausesDescription;

  let assemblyFormat = clausesAssemblyFormat # [{
    custom<PrivateReductionRegion>($region, $private_vars, type($private_vars),
        $private_syms, $reduction_vars, type($reduction_vars), $reduction_byref,
        $reduction_syms) attr-dict
  }];

  let builders = [
    OpBuilder<(ins CArg<"const LoopOperands &">:$clauses)>
  ];

  let hasVerifier = 1;
  let hasRegionVerifier = 1;
}

def WsloopOp : OpenMP_Op<"wsloop", traits = [
    AttrSizedOperandSegments, DeclareOpInterfaceMethods<ComposableOpInterface>,
    DeclareOpInterfaceMethods<LoopWrapperInterface>, NoTerminator,
    RecursiveMemoryEffects, SingleBlock
  ], clauses = [
    OpenMP_AllocateClause, OpenMP_LinearClause, OpenMP_NowaitClause,
    OpenMP_OrderClause, OpenMP_OrderedClause, OpenMP_PrivateClause,
    OpenMP_ReductionClause, OpenMP_ScheduleClause
  ], singleRegion = true> {
  let summary = "worksharing-loop construct";
  let description = [{
    The worksharing-loop construct specifies that the iterations of the loop(s)
    will be executed in parallel by threads in the current context. These
    iterations are spread across threads that already exist in the enclosing
    parallel region.

    The body region can only contain a single block which must contain a single
    operation. This operation must be another compatible loop wrapper or an
    `omp.loop_nest`.

    ```
    omp.wsloop <clauses> {
      omp.loop_nest (%i1, %i2) : index = (%c0, %c0) to (%c10, %c10) step (%c1, %c1) {
        %a = load %arrA[%i1, %i2] : memref<?x?xf32>
        %b = load %arrB[%i1, %i2] : memref<?x?xf32>
        %sum = arith.addf %a, %b : f32
        store %sum, %arrC[%i1, %i2] : memref<?x?xf32>
        omp.yield
      }
    }
    ```
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"ArrayRef<NamedAttribute>", "{}">:$attributes)>,
    OpBuilder<(ins CArg<"const WsloopOperands &">:$clauses)>
  ];

  let assemblyFormat = clausesAssemblyFormat # [{
    custom<PrivateReductionRegion>($region, $private_vars, type($private_vars),
        $private_syms, $reduction_vars, type($reduction_vars), $reduction_byref,
        $reduction_syms) attr-dict
  }];

  let hasVerifier = 1;
  let hasRegionVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Simd construct [2.9.3.1]
//===----------------------------------------------------------------------===//

def SimdOp : OpenMP_Op<"simd", traits = [
    AttrSizedOperandSegments, DeclareOpInterfaceMethods<ComposableOpInterface>,
    DeclareOpInterfaceMethods<LoopWrapperInterface>, NoTerminator,
    RecursiveMemoryEffects, SingleBlock
  ], clauses = [
    OpenMP_AlignedClause, OpenMP_IfClause, OpenMP_LinearClause,
    OpenMP_NontemporalClause, OpenMP_OrderClause, OpenMP_PrivateClause,
    OpenMP_ReductionClause, OpenMP_SafelenClause, OpenMP_SimdlenClause
  ], singleRegion = true> {
  let summary = "simd construct";
  let description = [{
    The simd construct can be applied to a loop to indicate that the loop can be
    transformed into a SIMD loop (that is, multiple iterations of the loop can
    be executed concurrently using SIMD instructions).

    The body region can only contain a single block which must contain a single
    operation. This operation must be another compatible loop wrapper or an
    `omp.loop_nest`.

    ```
    omp.simd <clauses> {
      omp.loop_nest (%i1, %i2) : index = (%c0, %c0) to (%c10, %c10) step (%c1, %c1) {
        %a = load %arrA[%i1, %i2] : memref<?x?xf32>
        %b = load %arrB[%i1, %i2] : memref<?x?xf32>
        %sum = arith.addf %a, %b : f32
        store %sum, %arrC[%i1, %i2] : memref<?x?xf32>
        omp.yield
      }
    }
    ```

    When an if clause is present and evaluates to false, the preferred number of
    iterations to be executed concurrently is one, regardless of whether
    a simdlen clause is specified.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const SimdOperands &">:$clauses)>
  ];

  let assemblyFormat = clausesAssemblyFormat # [{
    custom<PrivateReductionRegion>($region, $private_vars, type($private_vars),
        $private_syms, $reduction_vars, type($reduction_vars), $reduction_byref,
        $reduction_syms) attr-dict
  }];

  let hasVerifier = 1;
  let hasRegionVerifier = 1;
}


def YieldOp : OpenMP_Op<"yield",
    [Pure, ReturnLike, Terminator,
     ParentOneOf<["AtomicUpdateOp", "DeclareReductionOp", "LoopNestOp",
                  "PrivateClauseOp"]>]> {
  let summary = "loop yield and termination operation";
  let description = [{
    "omp.yield" yields SSA values from the OpenMP dialect op region and
    terminates the region. The semantics of how the values are yielded is
    defined by the parent operation.
  }];

  let arguments = (ins Variadic<AnyType>:$results);

  let builders = [
    OpBuilder<(ins), [{ build($_builder, $_state, {}); }]>
  ];

  let assemblyFormat = "( `(` $results^ `:` type($results) `)` )? attr-dict";
}

//===----------------------------------------------------------------------===//
// Distribute construct [2.9.4.1]
//===----------------------------------------------------------------------===//
def DistributeOp : OpenMP_Op<"distribute", traits = [
    AttrSizedOperandSegments, DeclareOpInterfaceMethods<ComposableOpInterface>,
    DeclareOpInterfaceMethods<LoopWrapperInterface>, NoTerminator,
    RecursiveMemoryEffects, SingleBlock
  ], clauses = [
    OpenMP_AllocateClause, OpenMP_DistScheduleClause, OpenMP_OrderClause,
    OpenMP_PrivateClause
  ], singleRegion = true> {
  let summary = "distribute construct";
  let description = [{
    The distribute construct specifies that the iterations of one or more loops
    (optionally specified using collapse clause) will be executed by the
    initial teams in the context of their implicit tasks. The loops that the
    distribute op is associated with starts with the outermost loop enclosed by
    the distribute op region and going down the loop nest toward the innermost
    loop. The iterations are distributed across the initial threads of all
    initial teams that execute the teams region to which the distribute region
    binds.

    The distribute loop construct specifies that the iterations of the loop(s)
    will be executed in parallel by threads in the current context. These
    iterations are spread across threads that already exist in the enclosing
    region.
    
    The body region can only contain a single block which must contain a single
    operation. This operation must be another compatible loop wrapper or an
    `omp.loop_nest`.

    ```mlir
    omp.distribute <clauses> {
      omp.loop_nest (%i1, %i2) : index = (%c0, %c0) to (%c10, %c10) step (%c1, %c1) {
        %a = load %arrA[%i1, %i2] : memref<?x?xf32>
        %b = load %arrB[%i1, %i2] : memref<?x?xf32>
        %sum = arith.addf %a, %b : f32
        store %sum, %arrC[%i1, %i2] : memref<?x?xf32>
        omp.yield
      }
    }
    ```
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const DistributeOperands &">:$clauses)>
  ];

  let assemblyFormat = clausesAssemblyFormat # [{
    custom<PrivateRegion>($region, $private_vars, type($private_vars),
        $private_syms) attr-dict
  }];

  let hasVerifier = 1;
  let hasRegionVerifier = 1;
}

//===----------------------------------------------------------------------===//
// 2.10.1 task Construct
//===----------------------------------------------------------------------===//

def TaskOp
    : OpenMP_Op<"task",
                traits = [AttrSizedOperandSegments, AutomaticAllocationScope,
                          OutlineableOpenMPOpInterface],
                clauses = [
                    // TODO: Complete clause list (affinity, detach).
                    OpenMP_AllocateClause, OpenMP_DependClause,
                    OpenMP_FinalClause, OpenMP_IfClause,
                    OpenMP_InReductionClause, OpenMP_MergeableClause,
                    OpenMP_PriorityClause, OpenMP_PrivateClause,
                    OpenMP_UntiedClause, OpenMP_DetachClause],
                singleRegion = true> {
  let summary = "task construct";
  let description = [{
    The task construct defines an explicit task.

    For definitions of "undeferred task", "included task", "final task" and
    "mergeable task", please check OpenMP Specification.

    When an `if` clause is present on a task construct, and the value of
    `if_expr` evaluates to `false`, an "undeferred task" is generated, and the
    encountering thread must suspend the current task region, for which
    execution cannot be resumed until execution of the structured block that is
    associated with the generated task is completed.

    The `in_reduction` clause specifies that this particular task (among all the
    tasks in current taskgroup, if any) participates in a reduction.
    `in_reduction_byref` indicates whether each reduction variable should
    be passed by value or by reference.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const TaskOperands &">:$clauses)>
  ];

  let assemblyFormat = clausesAssemblyFormat # [{
    custom<InReductionPrivateRegion>(
        $region, $in_reduction_vars, type($in_reduction_vars),
        $in_reduction_byref, $in_reduction_syms, $private_vars,
        type($private_vars), $private_syms) attr-dict
  }];

  let hasVerifier = 1;
}

def TaskloopOp : OpenMP_Op<"taskloop", traits = [
    AttrSizedOperandSegments, AutomaticAllocationScope,
    DeclareOpInterfaceMethods<ComposableOpInterface>,
    DeclareOpInterfaceMethods<LoopWrapperInterface>, NoTerminator,
    RecursiveMemoryEffects, SingleBlock
  ], clauses = [
    OpenMP_AllocateClause, OpenMP_FinalClause, OpenMP_GrainsizeClause,
    OpenMP_IfClause, OpenMP_InReductionClauseSkip<extraClassDeclaration = true>,
    OpenMP_MergeableClause, OpenMP_NogroupClause, OpenMP_NumTasksClause,
    OpenMP_PriorityClause, OpenMP_PrivateClause,
    OpenMP_ReductionClauseSkip<extraClassDeclaration = true>,
    OpenMP_UntiedClause
  ], singleRegion = true> {
  let summary = "taskloop construct";
  let description = [{
    The taskloop construct specifies that the iterations of one or more
    associated loops will be executed in parallel using explicit tasks. The
    iterations are distributed across tasks generated by the construct and
    scheduled to be executed.

    The body region can only contain a single block which must contain a single
    operation. This operation must be another compatible loop wrapper or an
    `omp.loop_nest`.

    ```
    omp.taskloop <clauses> {
      omp.loop_nest (%i1, %i2) : index = (%c0, %c0) to (%c10, %c10) step (%c1, %c1) {
        %a = load %arrA[%i1, %i2] : memref<?x?xf32>
        %b = load %arrB[%i1, %i2] : memref<?x?xf32>
        %sum = arith.addf %a, %b : f32
        store %sum, %arrC[%i1, %i2] : memref<?x?xf32>
        omp.yield
      }
    }
    ```

    For definitions of "undeferred task", "included task", "final task" and
    "mergeable task", please check OpenMP Specification.

    When an `if` clause is present on a taskloop construct, and if the `if`
    clause expression evaluates to `false`, undeferred tasks are generated. The
    use of a variable in an `if` clause expression of a taskloop construct
    causes an implicit reference to the variable in all enclosing constructs.
  }] # clausesDescription # [{
    If an `in_reduction` clause is present on the taskloop construct, the
    behavior is as if each generated task was defined by a task construct on
    which an `in_reduction` clause with the same reduction operator and list
    items is present. Thus, the generated tasks are participants of a reduction
    previously defined by a reduction scoping clause. In this case, accumulator
    variables are specified in `in_reduction_vars`, symbols referring to
    reduction declarations in `in_reduction_syms` and `in_reduction_byref`
    indicate for each reduction variable whether it should be passed by value or
    by reference.

    If a `reduction` clause is present on the taskloop construct, the behavior
    is as if a `task_reduction` clause with the same reduction operator and list
    items was applied to the implicit taskgroup construct enclosing the taskloop
    construct. The taskloop construct executes as if each generated task was
    defined by a task construct on which an `in_reduction` clause with the same
    reduction operator and list items is present. Thus, the generated tasks are
    participants of the reduction defined by the `task_reduction` clause that
    was applied to the implicit taskgroup construct.
  }];

  let builders = [
    OpBuilder<(ins CArg<"const TaskloopOperands &">:$clauses)>
  ];

  let assemblyFormat = clausesAssemblyFormat # [{
    custom<InReductionPrivateReductionRegion>(
        $region, $in_reduction_vars, type($in_reduction_vars),
        $in_reduction_byref, $in_reduction_syms, $private_vars,
        type($private_vars), $private_syms, $reduction_vars,
        type($reduction_vars), $reduction_byref, $reduction_syms) attr-dict
  }];

  let extraClassDeclaration = [{
    /// Returns the reduction variables
    SmallVector<Value> getAllReductionVars();

    // Define BlockArgOpenMPOpInterface methods here because they are not
    // inherited from the respective clauses.
    unsigned numInReductionBlockArgs() { return getInReductionVars().size(); }
    unsigned numReductionBlockArgs() { return getReductionVars().size(); }

    void getEffects(SmallVectorImpl<MemoryEffects::EffectInstance> &effects);
  }] # clausesExtraClassDeclaration;

  let hasVerifier = 1;
  let hasRegionVerifier = 1;
}

def TaskgroupOp : OpenMP_Op<"taskgroup", traits = [
    AttrSizedOperandSegments, AutomaticAllocationScope
  ], clauses = [
    OpenMP_AllocateClause, OpenMP_TaskReductionClause
  ], singleRegion = true> {
  let summary = "taskgroup construct";
  let description = [{
    The taskgroup construct specifies a wait on completion of child tasks of the
    current task and their descendent tasks.

    When a thread encounters a taskgroup construct, it starts executing the
    region. All child tasks generated in the taskgroup region and all of their
    descendants that bind to the same parallel region as the taskgroup region
    are part of the taskgroup set associated with the taskgroup region. There is
    an implicit task scheduling point at the end of the taskgroup region. The
    current task is suspended at the task scheduling point until all tasks in
    the taskgroup set complete execution.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const TaskgroupOperands &">:$clauses)>
  ];

  let assemblyFormat = clausesAssemblyFormat # [{
    custom<TaskReductionRegion>(
        $region, $task_reduction_vars, type($task_reduction_vars),
        $task_reduction_byref, $task_reduction_syms) attr-dict
  }];

  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// 2.10.4 taskyield Construct
//===----------------------------------------------------------------------===//

def TaskyieldOp : OpenMP_Op<"taskyield"> {
  let summary = "taskyield construct";
  let description = [{
    The taskyield construct specifies that the current task can be suspended
    in favor of execution of a different task.
  }];

  let assemblyFormat = "attr-dict";
}

//===----------------------------------------------------------------------===//
// 2.13.7 flush Construct
//===----------------------------------------------------------------------===//
def FlushOp : OpenMP_Op<"flush", clauses = [
    // TODO: Complete clause list (memory_order).
  ]> {
  let summary = "flush construct";
  let description = [{
    The flush construct executes the OpenMP flush operation. This operation
    makes a thread's temporary view of memory consistent with memory and
    enforces an order on the memory operations of the variables explicitly
    specified or implied.
  }] # clausesDescription;

  let arguments = !con((ins Variadic<OpenMP_PointerLikeType>:$varList),
                       clausesArgs);

  // Override inherited assembly format to include `varList`.
  let assemblyFormat = "( `(` $varList^ `:` type($varList) `)` )? attr-dict";

  let extraClassDeclaration = [{
    /// The number of variable operands.
    unsigned getNumVariableOperands() {
      return getOperation()->getNumOperands();
    }
    /// The i-th variable operand passed.
    Value getVariableOperand(unsigned i) {
      return getOperand(i);
    }
  }] # clausesExtraClassDeclaration;
}

//===----------------------------------------------------------------------===//
// Map related constructs
//===----------------------------------------------------------------------===//

def MapBoundsOp : OpenMP_Op<"map.bounds",
    [AttrSizedOperandSegments, NoMemoryEffect]> {
  let summary = "Represents normalized bounds information for map clauses.";

  let description = [{
    This operation is a variation on the OpenACC dialects DataBoundsOp. Within
    the OpenMP dialect it stores the bounds/range of data to be mapped to a
    device specified by map clauses on target directives. Within
    the OpenMP dialect, the MapBoundsOp is associated with MapInfoOp,
    helping to store bounds information for the mapped variable.

    It is used to support OpenMP array sectioning, Fortran pointer and
    allocatable mapping and pointer/allocatable member of derived types.
    In all cases the MapBoundsOp holds information on the section of
    data to be mapped. Such as the upper bound and lower bound of the
    section of data to be mapped. This information is currently
    utilised by the LLVM-IR lowering to help generate instructions to
    copy data to and from the device when processing target operations.

    The example below copys a section of a 10-element array; all except the
    first element, utilising OpenMP array sectioning syntax where array
    subscripts are provided to specify the bounds to be mapped to device.
    To simplify the examples, the constants are used directly, in reality
    they will be MLIR SSA values.

    C++:
    ```
    int array[10];
    #pragma target map(array[1:9])
    ```
    =>
    ```mlir
    omp.map.bounds lower_bound(1) upper_bound(9) extent(9) start_idx(0)
    ```

    Fortran:
    ```
    integer :: array(1:10)
    !$target map(array(2:10))
    ```
    =>
    ```mlir
    omp.map.bounds lower_bound(1) upper_bound(9) extent(9) start_idx(1)
    ```

    For Fortran pointers and allocatables (as well as those that are
    members of derived types) the bounds information is provided by
    the Fortran compiler and runtime through descriptor information.

    A basic pointer example can be found below (constants again
    provided for simplicity, where in reality SSA values will be
    used, in this case that point to data yielded by Fortran's
    descriptors):

    Fortran:
    ```
    integer, pointer :: ptr(:)
    allocate(ptr(10))
    !$target map(ptr)
    ```
    =>
    ```mlir
    omp.map.bounds lower_bound(0) upper_bound(9) extent(10) start_idx(1)
    ```

    This operation records the bounds information in a normalized fashion
    (zero-based). This works well with the `PointerLikeType`
    requirement in data clauses - since a `lower_bound` of 0 means looking
    at data at the zero offset from pointer.

    This operation must have an `upper_bound` or `extent` (or both are allowed -
    but not checked for consistency). When the source language's arrays are
    not zero-based, the `start_idx` must specify the zero-position index.
  }];

  let arguments = (ins Optional<IntLikeType>:$lower_bound,
                       Optional<IntLikeType>:$upper_bound,
                       Optional<IntLikeType>:$extent,
                       Optional<IntLikeType>:$stride,
                       DefaultValuedAttr<BoolAttr, "false">:$stride_in_bytes,
                       Optional<IntLikeType>:$start_idx);
  let results = (outs OpenMP_MapBoundsType:$result);

  let assemblyFormat = [{
    oilist(
        `lower_bound` `(` $lower_bound `:` type($lower_bound) `)`
      | `upper_bound` `(` $upper_bound `:` type($upper_bound) `)`
      | `extent` `(` $extent `:` type($extent) `)`
      | `stride` `(` $stride `:` type($stride) `)`
      | `start_idx` `(` $start_idx `:` type($start_idx) `)`
    ) attr-dict
  }];

  let extraClassDeclaration = [{
    /// The number of variable operands.
    unsigned getNumVariableOperands() {
      return getNumOperands();
    }

    /// The i-th variable operand passed.
    Value getVariableOperand(unsigned i) {
      return getOperands()[i];
    }
  }];

  let hasVerifier = 1;
}

def MapInfoOp : OpenMP_Op<"map.info", [AttrSizedOperandSegments]> {
  let arguments = (ins OpenMP_PointerLikeType:$var_ptr,
                       TypeAttr:$var_type,
                       Optional<OpenMP_PointerLikeType>:$var_ptr_ptr,
                       Variadic<OpenMP_PointerLikeType>:$members,
                       OptionalAttr<IndexListArrayAttr>:$members_index,
                       Variadic<OpenMP_MapBoundsType>:$bounds, /* rank-0 to rank-{n-1} */
                       OptionalAttr<UI64Attr>:$map_type,
                       OptionalAttr<VariableCaptureKindAttr>:$map_capture_type,
                       OptionalAttr<StrAttr>:$name,
                       DefaultValuedAttr<BoolAttr, "false">:$partial_map);
  let results = (outs OpenMP_PointerLikeType:$omp_ptr);

  let description = [{
    The MapInfoOp captures information relating to individual OpenMP map clauses
    that are applied to certain OpenMP directives such as Target and Target Data.

    For example, the map type modifier; such as from, tofrom and to, the variable
    being captured or the bounds of an array section being mapped.

    It can be used to capture both implicit and explicit map information, where
    explicit is an argument directly specified to an OpenMP map clause or implicit
    where a variable is utilised in a target region but is defined externally to
    the target region.

    This map information is later used to aid the lowering of the target operations
    they are attached to providing argument input and output context for kernels
    generated or the target data mapping environment.

    Example (Fortran):

    ```
    integer :: index
    !$target map(to: index)
    ```
    =>
    ```mlir
    omp.map.info var_ptr(%index_ssa) map_type(to) map_capture_type(ByRef)
      name(index)
    ```

    Description of arguments:
    - `var_ptr`: The address of variable to copy.
    - `var_type`: The type of the variable to copy.
    - `var_ptr_ptr`: Used when the variable copied is a member of a class, structure
      or derived type and refers to the originating struct.
    - `members`: Used to indicate mapped child members for the current MapInfoOp,
       represented as other MapInfoOp's, utilised in cases where a parent structure
       type and members of the structure type are being mapped at the same time.
       For example: map(to: parent, parent->member, parent->member2[:10])
    - `members_index`: Used to indicate the ordering of members within the containing
       parent (generally a record type such as a structure, class or derived type),
       e.g. struct {int x, float y, double z}, x would be 0, y would be 1, and z
       would be 2. This aids the mapping.
    - `bounds`: Used when copying slices of array's, pointers or pointer members of
       objects (e.g. derived types or classes), indicates the bounds to be copied
       of the variable. When it's an array slice it is in rank order where rank 0
       is the inner-most dimension.
    - 'map_type': OpenMP map type for this map capture, for example: from, to and
       always. It's a bitfield composed of the OpenMP runtime flags stored in
       OpenMPOffloadMappingFlags.
    - 'map_capture_type': Capture type for the variable e.g. this, byref, byvalue, byvla
       this can affect how the variable is lowered.
    - `name`: Holds the name of variable as specified in user clause (including bounds).
    - `partial_map`: The record type being mapped will not be mapped in its entirety,
       it may be used however, in a mapping to bind it's mapped components together.
  }];

  let assemblyFormat = [{
    `var_ptr` `(` $var_ptr `:` type($var_ptr) `,` $var_type `)`
    oilist(
        `var_ptr_ptr` `(` $var_ptr_ptr `:` type($var_ptr_ptr) `)`
      | `map_clauses` `(` custom<MapClause>($map_type) `)`
      | `capture` `(` custom<CaptureType>($map_capture_type) `)`
      | `members` `(` $members `:` custom<MembersIndex>($members_index) `:` type($members) `)`
      | `bounds` `(` $bounds `)`
    ) `->` type($omp_ptr) attr-dict
  }];

  let extraClassDeclaration = [{
    /// The number of variable operands.
    unsigned getNumVariableOperands() {
      return getNumOperands();
    }

    /// The i-th variable operand passed.
    Value getVariableOperand(unsigned i) {
      return getOperands()[i];
    }
  }];
}

//===---------------------------------------------------------------------===//
// 2.14.2 target data Construct
//===---------------------------------------------------------------------===//

def TargetDataOp: OpenMP_Op<"target_data", traits = [
    AttrSizedOperandSegments
  ], clauses = [
    OpenMP_DeviceClause, OpenMP_IfClause, OpenMP_MapClause,
    OpenMP_UseDeviceAddrClause, OpenMP_UseDevicePtrClause
  ], singleRegion = true> {
  let summary = "target data construct";
  let description = [{
    Map variables to a device data environment for the extent of the region.

    The omp target data directive maps variables to a device data
    environment, and defines the lexical scope of the data environment
    that is created. The omp target data directive can reduce data copies
    to and from the offloading device when multiple target regions are using
    the same data.

    The optional `if_expr` parameter specifies a boolean result of a conditional
    check. If this value is 1 or is not provided then the target region runs on
    a device, if it is 0 then the target region is executed on the host device.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const TargetDataOperands &">:$clauses)>
  ];

  let assemblyFormat = clausesAssemblyFormat # [{
    custom<UseDeviceAddrUseDevicePtrRegion>(
        $region, $use_device_addr_vars, type($use_device_addr_vars),
        $use_device_ptr_vars, type($use_device_ptr_vars)) attr-dict
  }];

  let hasVerifier = 1;
}

//===---------------------------------------------------------------------===//
// 2.14.3 target enter data Construct
//===---------------------------------------------------------------------===//

def TargetEnterDataOp: OpenMP_Op<"target_enter_data", traits = [
    AttrSizedOperandSegments
  ], clauses = [
    OpenMP_DependClause, OpenMP_DeviceClause, OpenMP_IfClause, OpenMP_MapClause,
    OpenMP_NowaitClause
  ]> {
  let summary = "target enter data construct";
  let description = [{
    The target enter data directive specifies that variables are mapped to
    a device data environment. The target enter data directive is a
    stand-alone directive.

    The optional `if_expr` parameter specifies a boolean result of a conditional
    check. If this value is 1 or is not provided then the target region runs on
    a device, if it is 0 then the target region is executed on the host device.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const TargetEnterExitUpdateDataOperands &">:$clauses)>
  ];

  let hasVerifier = 1;
}

//===---------------------------------------------------------------------===//
// 2.14.4 target exit data Construct
//===---------------------------------------------------------------------===//

def TargetExitDataOp: OpenMP_Op<"target_exit_data", traits = [
    AttrSizedOperandSegments
  ], clauses = [
    OpenMP_DependClause, OpenMP_DeviceClause, OpenMP_IfClause, OpenMP_MapClause,
    OpenMP_NowaitClause
  ]> {
  let summary = "target exit data construct";
  let description = [{
    The target exit data directive specifies that variables are mapped to a
    device data environment. The target exit data directive is
    a stand-alone directive.

    The optional `if_expr` parameter specifies a boolean result of a conditional
    check. If this value is 1 or is not provided then the target region runs on
    a device, if it is 0 then the target region is executed on the host device.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const TargetEnterExitUpdateDataOperands &">:$clauses)>
  ];

  let hasVerifier = 1;
}

//===---------------------------------------------------------------------===//
// 2.14.6 target update Construct
//===---------------------------------------------------------------------===//

def TargetUpdateOp: OpenMP_Op<"target_update", traits = [
    AttrSizedOperandSegments
  ], clauses = [
    OpenMP_DependClause, OpenMP_DeviceClause, OpenMP_IfClause, OpenMP_MapClause,
    OpenMP_NowaitClause
  ]> {
  let summary = "target update construct";
  let description = [{
    The target update directive makes the corresponding list items in the device
    data environment consistent with their original list items, according to the
    specified motion clauses. The target update construct is a stand-alone
    directive.

    The optional `if_expr` parameter specifies a boolean result of a conditional
    check. If this value is 1 or is not provided then the target region runs on
    a device, if it is 0 then the target region is executed on the host device.

    We use `MapInfoOp` to model the motion clauses and their modifiers. Even
    though the spec differentiates between map-types & map-type-modifiers vs.
    motion-clauses & motion-modifiers, the motion clauses and their modifiers
    are a subset of map types and their modifiers. The subset relation is
    handled in during verification to make sure the restrictions for target
    update are respected.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const TargetEnterExitUpdateDataOperands &">:$clauses)>
  ];

  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// 2.14.5 target construct
//===----------------------------------------------------------------------===//

def TargetOp : OpenMP_Op<"target", traits = [
    AttrSizedOperandSegments, BlockArgOpenMPOpInterface, IsolatedFromAbove,
    OutlineableOpenMPOpInterface
  ], clauses = [
    // TODO: Complete clause list (defaultmap, uses_allocators).
    OpenMP_AllocateClause, OpenMP_DependClause, OpenMP_DeviceClause,
    OpenMP_HasDeviceAddrClause, OpenMP_IfClause, OpenMP_InReductionClause,
    OpenMP_IsDevicePtrClause, OpenMP_MapClauseSkip<assemblyFormat = true>,
    OpenMP_NowaitClause, OpenMP_PrivateClause, OpenMP_ThreadLimitClause
  ], singleRegion = true> {
  let summary = "target construct";
  let description = [{
    The target construct includes a region of code which is to be executed
    on a device.

    The optional `if_expr` parameter specifies a boolean result of a conditional
    check. If this value is 1 or is not provided then the target region runs on
    a device, if it is 0 then the target region is executed on the host device.

    The `private_maps` attribute connects `private` operands to their corresponding
    `map` operands. For `private` operands that require a map, the value of the
    corresponding element in the attribute is the index of the `map` operand
    (relative to other `map` operands not the whole operands of the operation). For
    `private` opernads that do not require a map, this value is -1 (which is omitted
    from the assembly foramt printing).
  }] # clausesDescription;

  let arguments = !con(clausesArgs,
                       (ins OptionalAttr<DenseI64ArrayAttr>:$private_maps));

  let builders = [
    OpBuilder<(ins CArg<"const TargetOperands &">:$clauses)>
  ];

  let extraClassDeclaration = [{
    unsigned numMapBlockArgs() { return getMapVars().size(); }

    mlir::Value getMappedValueForPrivateVar(unsigned privVarIdx) {
      std::optional<DenseI64ArrayAttr> privateMapIdices = getPrivateMapsAttr();

      if (!privateMapIdices.has_value())
        return {};

      int64_t mapInfoOpIdx = (*privateMapIdices)[privVarIdx];

      if (mapInfoOpIdx == -1)
        return {};

      return getMapVars()[mapInfoOpIdx];
    }
  }] # clausesExtraClassDeclaration;

  let assemblyFormat = clausesAssemblyFormat # [{
    custom<InReductionMapPrivateRegion>(
        $region, $in_reduction_vars, type($in_reduction_vars),
        $in_reduction_byref, $in_reduction_syms, $map_vars, type($map_vars),
        $private_vars, type($private_vars), $private_syms, $private_maps)
        attr-dict
  }];

  let hasVerifier = 1;
}


//===----------------------------------------------------------------------===//
// 2.16 master Construct
//===----------------------------------------------------------------------===//
def MasterOp : OpenMP_Op<"master", singleRegion = true> {
  let summary = "master construct";
  let description = [{
    The master construct specifies a structured block that is executed by
    the master thread of the team.
  }];

  let assemblyFormat = "$region attr-dict";
}

//===----------------------------------------------------------------------===//
// 2.17.1 critical Construct
//===----------------------------------------------------------------------===//
def CriticalDeclareOp : OpenMP_Op<"critical.declare", clauses = [
    OpenMP_CriticalNameClause, OpenMP_HintClause
  ]> {
  let summary = "declares a named critical section.";
  let description = [{
    Declares a named critical section.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const CriticalDeclareOperands &">:$clauses)>
  ];

  let hasVerifier = 1;
}


def CriticalOp : OpenMP_Op<"critical", [
    DeclareOpInterfaceMethods<SymbolUserOpInterface>
  ], singleRegion = 1> {
  let summary = "critical construct";
  let description = [{
    The critical construct imposes a restriction on the associated structured
    block (region) to be executed by only a single thread at a time.

    The optional `name` argument of critical constructs is used to identify
    them. Unnamed critical constructs behave as though an identical name was
    specified.
  }];

  let arguments = (ins OptionalAttr<FlatSymbolRefAttr>:$name);

  let assemblyFormat = [{
    (`(` $name^ `)`)? $region attr-dict
  }];
}

//===----------------------------------------------------------------------===//
// 2.17.2 barrier Construct
//===----------------------------------------------------------------------===//

def BarrierOp : OpenMP_Op<"barrier"> {
  let summary = "barrier construct";
  let description = [{
    The barrier construct specifies an explicit barrier at the point at which
    the construct appears.
  }];

  let assemblyFormat = "attr-dict";
}

//===----------------------------------------------------------------------===//
// [5.1] 2.19.9 ordered Construct
//===----------------------------------------------------------------------===//

def OrderedOp : OpenMP_Op<"ordered", clauses = [OpenMP_DoacrossClause]> {
  let summary = "ordered construct without region";
  let description = [{
    The ordered construct without region is a stand-alone directive that
    specifies cross-iteration dependencies in a doacross loop nest.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const OrderedOperands &">:$clauses)>
  ];

  let hasVerifier = 1;
}

def OrderedRegionOp : OpenMP_Op<"ordered.region", clauses = [
    OpenMP_ParallelizationLevelClause
  ], singleRegion = true> {
  let summary = "ordered construct with region";
  let description = [{
    The ordered construct with region specifies a structured block in a
    worksharing-loop, SIMD, or worksharing-loop SIMD region that is executed in
    the order of the loop iterations.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const OrderedRegionOperands &">:$clauses)>
  ];

  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// 2.17.5 taskwait Construct
//===----------------------------------------------------------------------===//

def TaskwaitOp : OpenMP_Op<"taskwait", clauses = [
    OpenMP_DependClause, OpenMP_NowaitClause
  ]> {
  let summary = "taskwait construct";
  let description = [{
    The taskwait construct specifies a wait on the completion of child tasks
    of the current task.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const TaskwaitOperands &">:$clauses)>
  ];
}

//===----------------------------------------------------------------------===//
// 2.17.7 atomic construct
//===----------------------------------------------------------------------===//

// In the OpenMP Specification, atomic construct has an `atomic-clause` which
// can take the values `read`, `write`, `update` and `capture`. These four
// kinds of atomic constructs are fundamentally independent and are handled
// separately while lowering. Having four separate operations (one for each
// value of the clause) here decomposes handling of this construct into a
// two-step process.

def AtomicReadOp : OpenMP_Op<"atomic.read", traits = [
    AtomicReadOpInterface
  ], clauses = [
    OpenMP_HintClause, OpenMP_MemoryOrderClause
  ]> {
  let summary = "performs an atomic read";
  let description = [{
    This operation performs an atomic read.

    The operand `x` is the address from where the value is atomically read.
    The operand `v` is the address where the value is stored after reading.
  }] # clausesDescription;

  let arguments = !con((ins OpenMP_PointerLikeType:$x,
                            OpenMP_PointerLikeType:$v,
                            TypeAttr:$element_type), clausesArgs);

  // Override clause-based assemblyFormat.
  let assemblyFormat = "$v `=` $x" # clausesReqAssemblyFormat # " oilist(" #
    clausesOptAssemblyFormat #
    ") `:` type($v) `,` type($x) `,` $element_type attr-dict";

  let extraClassDeclaration = [{
    /// The number of variable operands.
    unsigned getNumVariableOperands() {
      assert(getX() && "expected 'x' operand");
      assert(getV() && "expected 'v' operand");
      return 2;
    }

    /// The i-th variable operand passed.
    Value getVariableOperand(unsigned i) {
      assert(i < 2 && "invalid index position for an operand");
      return i == 0 ? getX() : getV();
    }
  }] # clausesExtraClassDeclaration;

  let hasVerifier = 1;
}

def AtomicWriteOp : OpenMP_Op<"atomic.write", traits = [
    AtomicWriteOpInterface
  ], clauses = [
    OpenMP_HintClause, OpenMP_MemoryOrderClause
  ]> {
  let summary = "performs an atomic write";
  let description = [{
    This operation performs an atomic write.

    The operand `x` is the address to where the `expr` is atomically
    written w.r.t. multiple threads. The evaluation of `expr` need not be
    atomic w.r.t. the write to address. In general, the type(x) must
    dereference to type(expr).
  }] # clausesDescription;

  let arguments = !con((ins OpenMP_PointerLikeType:$x,
                            AnyType:$expr), clausesArgs);

  // Override clause-based assemblyFormat.
  let assemblyFormat = "$x `=` $expr" # clausesReqAssemblyFormat # " oilist(" #
    clausesOptAssemblyFormat # ") `:` type($x) `,` type($expr) attr-dict";

  let extraClassDeclaration = [{
    /// The number of variable operands.
    unsigned getNumVariableOperands() {
      assert(getX() && "expected address operand");
      assert(getExpr() && "expected value operand");
      return 2;
    }

    /// The i-th variable operand passed.
    Value getVariableOperand(unsigned i) {
      assert(i < 2 && "invalid index position for an operand");
      return i == 0 ? getX() : getExpr();
    }
  }] # clausesExtraClassDeclaration;

  let hasVerifier = 1;
}

def AtomicUpdateOp : OpenMP_Op<"atomic.update", traits = [
    AtomicUpdateOpInterface, RecursiveMemoryEffects,
    SingleBlockImplicitTerminator<"YieldOp">
  ], clauses = [
    OpenMP_HintClause, OpenMP_MemoryOrderClause
  ], singleRegion = 1> {
  let summary = "performs an atomic update";
  let description = [{
    This operation performs an atomic update.

    The operand `x` is exactly the same as the operand `x` in the OpenMP
    Standard (OpenMP 5.0, section 2.17.7). It is the address of the variable
    that is being updated. `x` is atomically read/written.

    The region describes how to update the value of `x`. It takes the value at
    `x` as an input and must yield the updated value. Only the update to `x` is
    atomic. Generally the region must have only one instruction, but can
    potentially have more than one instructions too. The update is sematically
    similar to a compare-exchange loop based atomic update.

    The syntax of atomic update operation is different from atomic read and
    atomic write operations. This is because only the host dialect knows how to
    appropriately update a value. For example, while generating LLVM IR, if
    there are no special `atomicrmw` instructions for the operation-type
    combination in atomic update, a compare-exchange loop is generated, where
    the core update operation is directly translated like regular operations by
    the host dialect. The front-end must handle semantic checks for allowed
    operations.
  }] # clausesDescription;

  let arguments = !con((ins Arg<OpenMP_PointerLikeType,
                                "Address of variable to be updated",
                                [MemRead, MemWrite]>:$x), clausesArgs);

  // Override region definition.
  let regions = (region SizedRegion<1>:$region);

  // Override clause-based assemblyFormat.
  let assemblyFormat = clausesAssemblyFormat #
    "$x `:` type($x) $region attr-dict";

  let extraClassDeclaration = [{
    /// The number of variable operands.
    unsigned getNumVariableOperands() {
      assert(getX() && "expected 'x' operand");
      return 1;
    }

    /// The i-th variable operand passed.
    Value getVariableOperand(unsigned i) {
      assert(i == 0 && "invalid index position for an operand");
      return getX();
    }
  }] # clausesExtraClassDeclaration;

  let hasVerifier = 1;
  let hasRegionVerifier = 1;
  let hasCanonicalizeMethod = 1;
}

def AtomicCaptureOp : OpenMP_Op<"atomic.capture", traits = [
    AtomicCaptureOpInterface, RecursiveMemoryEffects,
    SingleBlockImplicitTerminator<"TerminatorOp">
  ], clauses = [
    OpenMP_HintClause, OpenMP_MemoryOrderClause
  ], singleRegion = 1> {
  let summary = "performs an atomic capture";
  let description = [{
    This operation performs an atomic capture.

    The region has the following allowed forms:
    ```
      omp.atomic.capture {
        omp.atomic.update ...
        omp.atomic.read ...
        omp.terminator
      }

      omp.atomic.capture {
        omp.atomic.read ...
        omp.atomic.update ...
        omp.terminator
      }

      omp.atomic.capture {
        omp.atomic.read ...
        omp.atomic.write ...
        omp.terminator
      }
    ```
  }] # clausesDescription;

  // Override region definition.
  let regions = (region SizedRegion<1>:$region);

  let extraClassDeclaration = [{
    /// Returns the `atomic.read` operation inside the region, if any.
    /// Otherwise, it returns nullptr.
    AtomicReadOp getAtomicReadOp();

    /// Returns the `atomic.write` operation inside the region, if any.
    /// Otherwise, it returns nullptr.
    AtomicWriteOp getAtomicWriteOp();

    /// Returns the `atomic.update` operation inside the region, if any.
    /// Otherwise, it returns nullptr.
    AtomicUpdateOp getAtomicUpdateOp();
  }] # clausesExtraClassDeclaration;

  let hasRegionVerifier = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// [5.1] 2.21.2 threadprivate Directive
//===----------------------------------------------------------------------===//

def ThreadprivateOp : OpenMP_Op<"threadprivate",
                                [AllTypesMatch<["sym_addr", "tls_addr"]>]> {
  let summary = "threadprivate directive";
  let description = [{
    The threadprivate directive specifies that variables are replicated, with
    each thread having its own copy.

    The current implementation uses the OpenMP runtime to provide thread-local
    storage (TLS). Using the TLS feature of the LLVM IR will be supported in
    future.

    This operation takes in the address of a symbol that represents the original
    variable and returns the address of its TLS. All occurrences of
    threadprivate variables in a parallel region should use the TLS returned by
    this operation.

    The `sym_addr` refers to the address of the symbol, which is a pointer to
    the original variable.
  }];

  let arguments = (ins OpenMP_PointerLikeType:$sym_addr);
  let results = (outs OpenMP_PointerLikeType:$tls_addr);
  let assemblyFormat = [{
    $sym_addr `:` type($sym_addr) `->` type($tls_addr) attr-dict
  }];
  let extraClassDeclaration = [{
    /// The number of variable operands.
    unsigned getNumVariableOperands() {
      assert(getSymAddr() && "expected one variable operand");
      return 1;
    }

    /// The i-th variable operand passed.
    Value getVariableOperand(unsigned i) {
      assert(i == 0 && "invalid index position for an operand");
      return getSymAddr();
    }
  }];
}

//===----------------------------------------------------------------------===//
// 2.18.1 Cancel Construct
//===----------------------------------------------------------------------===//
def CancelOp : OpenMP_Op<"cancel", clauses = [
    OpenMP_CancelDirectiveNameClause, OpenMP_IfClause
  ]> {
  let summary = "cancel directive";
  let description = [{
    The cancel construct activates cancellation of the innermost enclosing
    region of the type specified.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const CancelOperands &">:$clauses)>
  ];

  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// 2.18.2 Cancellation Point Construct
//===----------------------------------------------------------------------===//
def CancellationPointOp : OpenMP_Op<"cancellation_point", clauses = [
    OpenMP_CancelDirectiveNameClause
  ]> {
  let summary = "cancellation point directive";
  let description = [{
    The cancellation point construct introduces a user-defined cancellation
    point at which implicit or explicit tasks check if cancellation of the
    innermost enclosing region of the type specified has been activated.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const CancellationPointOperands &">:$clauses)>
  ];

  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// 2.19.5.7 declare reduction Directive
//===----------------------------------------------------------------------===//

def DeclareReductionOp : OpenMP_Op<"declare_reduction", [IsolatedFromAbove,
                                                         RecipeInterface,
                                                         Symbol]> {
  let summary = "declares a reduction kind";
  let description = [{
    Declares an OpenMP reduction kind. This requires two mandatory and three
    optional regions.

      1. The optional alloc region specifies how to allocate the thread-local
         reduction value. This region should not contain control flow and all
         IR should be suitable for inlining straight into an entry block. In
         the common case this is expected to contain only allocas. It is
         expected to `omp.yield` the allocated value on all control paths.
         If allocation is conditional (e.g. only allocate if the mold is
         allocated), this should be done in the initilizer region and this
         region not included. The alloc region is not used for by-value
         reductions (where allocation is implicit).
      2. The initializer region specifies how to initialize the thread-local
         reduction value. This is usually the neutral element of the reduction.
         For convenience, the region has an argument that contains the value
         of the reduction accumulator at the start of the reduction. If an alloc
         region is specified, there is a second block argument containing the
         address of the allocated memory. The initializer region is expected to
         `omp.yield` the new value on all control flow paths.
      3. The reduction region specifies how to combine two values into one, i.e.
         the reduction operator. It accepts the two values as arguments and is
         expected to `omp.yield` the combined value on all control flow paths.
      4. The atomic reduction region is optional and specifies how two values
         can be combined atomically given local accumulator variables. It is
         expected to store the combined value in the first accumulator variable.
      5. The cleanup region is optional and specifies how to clean up any memory
         allocated by the initializer region. The region has an argument that
         contains the value of the thread-local reduction accumulator. This will
         be executed after the reduction has completed.

    Note that the MLIR type system does not allow for type-polymorphic
    reductions. Separate reduction declarations should be created for different
    element and accumulator types.

    For initializer and reduction regions, the operand to `omp.yield` must
    match the parent operation's results.
  }];

  let arguments = (ins SymbolNameAttr:$sym_name,
                       TypeAttr:$type);

  let regions = (region MaxSizedRegion<1>:$allocRegion,
                        AnyRegion:$initializerRegion,
                        AnyRegion:$reductionRegion,
                        AnyRegion:$atomicReductionRegion,
                        AnyRegion:$cleanupRegion);

  let assemblyFormat = "$sym_name `:` $type attr-dict-with-keyword "
                       "( `alloc` $allocRegion^ )? "
                       "`init` $initializerRegion "
                       "`combiner` $reductionRegion "
                       "( `atomic` $atomicReductionRegion^ )? "
                       "( `cleanup` $cleanupRegion^ )? ";

  let extraClassDeclaration = [{
    BlockArgument getAllocMoldArg() {
      auto &region = getAllocRegion();
      return region.empty() ? nullptr : region.getArgument(0);
    }
    BlockArgument getInitializerMoldArg() {
      return getInitializerRegion().getArgument(0);
    }
    BlockArgument getInitializerAllocArg() {
      return getAllocRegion().empty() ?
          nullptr : getInitializerRegion().getArgument(1);
    }
    BlockArgument getReductionLhsArg() {
      return getReductionRegion().getArgument(0);
    }
    BlockArgument getReductionRhsArg() {
      return getReductionRegion().getArgument(1);
    }
    BlockArgument getAtomicReductionLhsArg() {
      auto &region = getAtomicReductionRegion();
      return region.empty() ? nullptr : region.getArgument(0);
    }
    BlockArgument getAtomicReductionRhsArg() {
      auto &region = getAtomicReductionRegion();
      return region.empty() ? nullptr : region.getArgument(1);
    }
    BlockArgument getCleanupAllocArg() {
      auto &region = getCleanupRegion();
      return region.empty() ? nullptr : region.getArgument(0);
    }

    PointerLikeType getAccumulatorType() {
      if (getAtomicReductionRegion().empty())
        return {};

      return cast<PointerLikeType>(getAtomicReductionLhsArg().getType());
    }
  }];
  let hasRegionVerifier = 1;
}

//===----------------------------------------------------------------------===//
// [Spec 5.2] 10.5 masked Construct
//===----------------------------------------------------------------------===//
def MaskedOp : OpenMP_Op<"masked", clauses = [
    OpenMP_FilterClause
  ], singleRegion = 1> {
  let summary = "masked construct";
  let description = [{
    Masked construct allows to specify a structured block to be executed by a subset of 
    threads of the current team.
  }] # clausesDescription;

  let builders = [
    OpBuilder<(ins CArg<"const MaskedOperands &">:$clauses)>
  ];
}

#endif // OPENMP_OPS


//===- TensorTransformOps.td - Tensor transformation ops ---*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef TENSOR_TRANSFORM_OPS
#define TENSOR_TRANSFORM_OPS

include "mlir/Dialect/Transform/IR/TransformDialect.td"
include "mlir/Dialect/Transform/Interfaces/TransformInterfaces.td"
include "mlir/Dialect/Transform/IR/TransformTypes.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/IR/OpBase.td"

def ApplyDecomposeTensorConcatPatternsOp : Op<Transform_Dialect,
    "apply_patterns.tensor.decompose_concat",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that tensor.concat ops should be decomposed into a chain of
    tensor.insert_slice operations inserting into a materialized destination.
  }];

  let assemblyFormat = "attr-dict";
}


def ApplyDropRedundantInsertSliceRankExpansionPatternsOp : Op<Transform_Dialect,
    "apply_patterns.tensor.drop_redundant_insert_slice_rank_expansion",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that redundant tensor.insert_slice rank reductions should be
    dropped. E.g., cases where a tensor.extract_slice rank reduction immediately
    follows an inverse tensor.insert_slice rank expansion.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyFoldTensorEmptyPatternsOp : Op<Transform_Dialect,
    "apply_patterns.tensor.fold_tensor_empty",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that tensor.extract_slice and reassociative reshapes should be
    folded into tensor.empty.

    If `fold_single_use_only` is set to "true", only tensor.empty that have a
    single use are folded.
  }];

  let arguments = (ins DefaultValuedAttr<BoolAttr, "false">:$fold_single_use_only);
  let assemblyFormat = "attr-dict";
}
def ApplyFoldIntoPackAndUnpackPatternsOp : Op<Transform_Dialect,
    "apply_patterns.tensor.fold_into_pack_and_unpack",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that operations like tensor.pad and tensor.extract_slice should
    be folded into tensor.pack and tensor.unpack operations, respectively.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyFoldTensorSubsetOpsPatternsOp : Op<Transform_Dialect,
    "apply_patterns.tensor.fold_tensor_subset_ops",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that tensor.empty should be folded with tensor.extract_slice,
    tensor.expand_shape and tensor.collapse_shape.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyFoldTensorSubsetOpsIntoVectorTransfersPatternsOp : Op<Transform_Dialect,
    "apply_patterns.tensor.fold_tensor_subset_ops_into_vector_transfers",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that tensor.extract_slice -> vector.transfer_read and
    vector.transfer_write -> tensor.insert_slice op chains should be folded into
    vector tranfer read and write ops
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyMergeConsecutiveInsertExtractSlicePatternsOp : Op<Transform_Dialect,
    "apply_patterns.tensor.merge_consecutive_insert_extract_slice",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that consecutive tensor.extract_slice/tensor.insert_slice ops
    should be merged into a single op. These patterns are not canonicalizations
    because the bufferization is sensitive to IR structure.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyReassociativeReshapeFoldingPatternsOp : Op<Transform_Dialect,
    "apply_patterns.tensor.reassociative_reshape_folding",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that reassociative reshapes (tensor.collapse_shape /
    tensor.expand_shape) should be folded with inverse rank expansions / rank
    reductions (via tensor.insert_slice / tensor.extract_slice).
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyRewriteTensorOpsAsConstantPatternsOp : Op<Transform_Dialect,
    "apply_patterns.tensor.rewrite_as_constant",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let arguments = (ins UnitAttr:$aggressive);
  let description = [{
    Indicates that tensor ops (such as tensor.generate) should be replaced with
    constants (arith.constant) when possible.
  }];

  let assemblyFormat =
      "(`aggressive` $aggressive^)? attr-dict";
}

def Transform_TensorPadOp : Transform_ConcreteOpType<"tensor.pad">;

def MakeLoopIndependentOp
    : Op<Transform_Dialect, "tensor.make_loop_independent",
         [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
          TransformOpInterface, TransformEachOpTrait]> {
  let description = [{
    Rewrite the targeted ops such that their index-typed operands no longer
    depend on any loop induction variable of the `num_loop` enclosing `scf.for`
    loops. I.e., compute an upper bound that is independent of any such loop IV
    for every tensor dimension. The transformed op could then be hoisted from
    the `num_loop` enclosing loops. To preserve the original semantics, place a
    `tensor.extract_slice` inside the loop.

    Currently supported operations are:
    - tensor.empty: Replaced with a new tensor.empty with upper bound sizes,
      followed by a tensor.extract_slice.
    - tensor.pad: Replaced by an upper bound padding, followed by a
      tensor.extract_slice.

    #### Return modes

    This operation fails if at least one induction variable could not be
    eliminated. In case the targeted op is already independent of induction
    variables, this transform succeeds and returns the unmodified target op.

    Otherwise, the returned handle points to a subset of the produced ops:
    - tensor.empty: The returned handle points to the tensor.extract_slice op.
    - tensor.pad: The returned handle points to the tensor.extract_slice op.

    This transform op consumes the target handle and produces a result handle.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target, I64Attr:$num_loops);
  let results = (outs TransformHandleTypeInterface:$transformed);
  let assemblyFormat =
      "$target attr-dict `:` functional-type($target, $transformed)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::Operation *target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

def TypeConversionCastShapeDynamicDimsOp : Op<Transform_Dialect,
    "type_conversion.tensor.cast_shape_dynamic_dims",
    [DeclareOpInterfaceMethods<TypeConverterBuilderOpInterface,
                               ["populateTypeMaterializations"]>]> {
  let description = [{
    Populates a type converter with conversion materialization functions that
    cast a tensor value between two cast-compatible tensors. See `tensor.cast`
    for more information on cast compatibility between tensors.

    If `ignore_dynamic_info` is not set, this will set an additional constraint
    that source materializations do not cast dynamic dimensions to static ones.
  }];
  let arguments = (ins UnitAttr:$ignore_dynamic_info);

  let assemblyFormat =
      "(`ignore_dynamic_info` $ignore_dynamic_info^)? attr-dict";
}

#endif // TENSOR_TRANSFORM_OPS


//===- Vector.td - Vector Dialect --------------------------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This file declares the Vector dialect.
//
//===----------------------------------------------------------------------===//

#ifndef MLIR_DIALECT_VECTOR_IR_VECTOR
#define MLIR_DIALECT_VECTOR_IR_VECTOR

include "mlir/IR/OpBase.td"

def Vector_Dialect : Dialect {
  let name = "vector";
  let cppNamespace = "::mlir::vector";

  let useDefaultAttributePrinterParser = 1;
  let hasConstantMaterializer = 1;
  let dependentDialects = ["arith::ArithDialect"];
}

// Base class for Vector dialect ops.
class Vector_Op<string mnemonic, list<Trait> traits = []> :
    Op<Vector_Dialect, mnemonic, traits>;

#endif // MLIR_DIALECT_VECTOR_IR_VECTOR


//===- PDLOps.td - Pattern descriptor operations -----------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This file declares the Pattern Descriptor Language dialect operations.
//
//===----------------------------------------------------------------------===//

#ifndef MLIR_DIALECT_PDL_IR_PDLOPS
#define MLIR_DIALECT_PDL_IR_PDLOPS

include "mlir/Dialect/PDL/IR/PDLTypes.td"
include "mlir/IR/OpAsmInterface.td"
include "mlir/IR/SymbolInterfaces.td"
include "mlir/Interfaces/SideEffectInterfaces.td"

//===----------------------------------------------------------------------===//
// PDL Ops
//===----------------------------------------------------------------------===//

class PDL_Op<string mnemonic, list<Trait> traits = []>
    : Op<PDL_Dialect, mnemonic, traits>;

//===----------------------------------------------------------------------===//
// pdl::ApplyNativeConstraintOp
//===----------------------------------------------------------------------===//

def PDL_ApplyNativeConstraintOp
    : PDL_Op<"apply_native_constraint", [HasParent<"pdl::PatternOp">]> {
  let summary = "Apply a native constraint to a set of provided entities";
  let description = [{
    `pdl.apply_native_constraint` operations apply a native C++ constraint, that
    has been registered externally with the consumer of PDL, to a given set of
    entities and optionally return a number of values.

    Example:

    ```mlir
    // Apply `myConstraint` to the entities defined by `input`, `attr`, and `op`.
    pdl.apply_native_constraint "myConstraint"(%input, %attr, %op : !pdl.value, !pdl.attribute, !pdl.operation)
    // Apply constraint `with_result` to `root`. This constraint returns an attribute.
    %attr = pdl.apply_native_constraint "with_result"(%root : !pdl.operation) : !pdl.attribute
    ```
  }];

  let arguments = (ins StrAttr:$name, 
                       Variadic<PDL_AnyType>:$args, 
                       DefaultValuedAttr<BoolAttr, "false">:$isNegated);
  let results = (outs Variadic<PDL_AnyType>:$results);
  let assemblyFormat = [{
    $name `(` $args `:` type($args) `)` (`:`  type($results)^ )? attr-dict
  }];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// pdl::ApplyNativeRewriteOp
//===----------------------------------------------------------------------===//

def PDL_ApplyNativeRewriteOp
    : PDL_Op<"apply_native_rewrite", [HasParent<"pdl::RewriteOp">]> {
  let summary = "Apply a native rewrite method inside of pdl.rewrite region";
  let description = [{
    `pdl.apply_native_rewrite` operations apply a native C++ function, that has
    been registered externally with the consumer of PDL, to perform a rewrite
    and optionally return a number of values. The native function may accept any
    number of arguments. This operation is used within a pdl.rewrite region to enable
    the interleaving of native rewrite methods with other pdl constructs.

    Example:

    ```mlir
    // Apply a native rewrite method that returns an attribute.
    %ret = pdl.apply_native_rewrite "myNativeFunc"(%arg0, %attr1) : !pdl.attribute
    ```

    ```c++
    // The native rewrite as defined in C++:
    static Attribute myNativeFunc(PatternRewriter &rewriter, Value arg0, Attribute arg1) {
      // Just return the second arg.
      return arg1;
    }

    void registerNativeRewrite(PDLPatternModule &pdlModule) {
      pdlModule.registerRewriteFunction("myNativeFunc", myNativeFunc);
    }
    ```
  }];

  let arguments = (ins StrAttr:$name, Variadic<PDL_AnyType>:$args);
  let results = (outs Variadic<PDL_AnyType>:$results);
  let assemblyFormat = [{
    $name (`(` $args^ `:` type($args) `)`)? (`:` type($results)^)? attr-dict
  }];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// pdl::AttributeOp
//===----------------------------------------------------------------------===//

def PDL_AttributeOp : PDL_Op<"attribute"> {
  let summary = "Define an input attribute in a pattern";
  let description = [{
    `pdl.attribute` operations capture named attribute edges into an operation.
    Instances of this operation define, and partially constrain, attributes of a
    given operation. A `pdl.attribute` may partially constrain the input by
    specifying an expected attribute value type (via a `pdl.type` operation), or
    a constant value for the attribute (via `val`). Only one of these may be set
    for a given input, as the type of the constant value provides the type. When
    defined within a `pdl.rewrite` region, the constant value must be specified.

    Example:

    ```mlir
    // Define an attribute:
    %attr = pdl.attribute

    // Define an attribute with an expected type:
    %type = pdl.type : i32
    %attr = pdl.attribute : %type

    // Define an attribute with a constant value:
    %attr = pdl.attribute = "hello"
    ```
  }];

  let arguments = (ins Optional<PDL_Type>:$valueType,
                       OptionalAttr<AnyAttr>:$value);
  let results = (outs PDL_Attribute:$attr);
  let assemblyFormat = "(`:` $valueType^)? (`=` $value^)? attr-dict-with-keyword";

  let builders = [
    OpBuilder<(ins CArg<"Value", "Value()">:$type), [{
      build($_builder, $_state, $_builder.getType<AttributeType>(), type,
            Attribute());
    }]>,
    OpBuilder<(ins "Attribute":$attr), [{
      build($_builder, $_state, $_builder.getType<AttributeType>(), Value(), attr);
    }]>,
  ];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// pdl::EraseOp
//===----------------------------------------------------------------------===//

def PDL_EraseOp : PDL_Op<"erase", [HasParent<"pdl::RewriteOp">]> {
  let summary = "Mark an input operation as `erased`";
  let description = [{
    `pdl.erase` operations are used within `pdl.rewrite` regions to specify that
    an input operation should be marked as erased. The semantics of this
    operation correspond with the `eraseOp` method on a `PatternRewriter`.

    Example:

    ```mlir
    pdl.erase %root
    ```
  }];
  let arguments = (ins PDL_Operation:$opValue);
  let assemblyFormat = "$opValue attr-dict";
}

//===----------------------------------------------------------------------===//
// pdl::OperandOp
//===----------------------------------------------------------------------===//

def PDL_OperandOp
    : PDL_Op<"operand", [HasParent<"pdl::PatternOp">]> {
  let summary = "Define an external input operand in a pattern";
  let description = [{
    `pdl.operand` operations capture external operand edges into an operation
    node that originate from operations or block arguments not otherwise
    specified within the pattern (i.e. via `pdl.result` or `pdl.results`). These
    operations define individual operands of a given operation. A `pdl.operand`
    may partially constrain an operand by specifying an expected value type
    (via a `pdl.type` operation).

    Example:

    ```mlir
    // Define an external operand:
    %operand = pdl.operand

    // Define an external operand with an expected type:
    %type = pdl.type : i32
    %operand = pdl.operand : %type
    ```
  }];

  let arguments = (ins Optional<PDL_Type>:$valueType);
  let results = (outs PDL_Value:$value);
  let assemblyFormat = "(`:` $valueType^)? attr-dict";

  let builders = [
    OpBuilder<(ins), [{
      build($_builder, $_state, $_builder.getType<ValueType>(), Value());
    }]>,
  ];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// pdl::OperandsOp
//===----------------------------------------------------------------------===//

def PDL_OperandsOp
    : PDL_Op<"operands", [HasParent<"pdl::PatternOp">]> {
  let summary = "Define a range of input operands in a pattern";
  let description = [{
    `pdl.operands` operations capture external operand range edges into an
    operation node that originate from operations or block arguments not
    otherwise specified within the pattern (i.e. via `pdl.result` or
    `pdl.results`). These operations define groups of input operands into a
    given operation. A `pdl.operands` may partially constrain a set of input
    operands by specifying expected value types (via `pdl.types` operations).

    Example:

    ```mlir
    // Define a range of input operands:
    %operands = pdl.operands

    // Define a range of input operands with expected types:
    %types = pdl.types : [i32, i64, i32]
    %typed_operands = pdl.operands : %types
    ```
  }];

  let arguments = (ins Optional<PDL_RangeOf<PDL_Type>>:$valueType);
  let results = (outs PDL_RangeOf<PDL_Value>:$value);
  let assemblyFormat = "(`:` $valueType^)? attr-dict";

  let builders = [
    OpBuilder<(ins), [{
      build($_builder, $_state, RangeType::get($_builder.getType<ValueType>()),
            Value());
    }]>,
  ];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// pdl::OperationOp
//===----------------------------------------------------------------------===//

def PDL_OperationOp : PDL_Op<"operation", [AttrSizedOperandSegments]> {
  let summary = "Define an operation within a pattern";
  let description = [{
    `pdl.operation` operations define operation nodes within a pattern. Within
    a match sequence, i.e. when directly nested within a `pdl.pattern`, these
    operations correspond to input operations, or those that already existing
    within the MLIR module. Inside of a `pdl.rewrite`, these operations
    correspond to operations that should be created as part of the replacement
    sequence.

    `pdl.operation`s are composed of a name, and a set of attribute, operand,
    and result type values, that map to what those that would be on a
    constructed instance of that operation. The results of a `pdl.operation` are
    a handle to the operation itself. Handles to the results of the operation
    can be extracted via `pdl.result`.

    Example:

    ```mlir
    // Define an instance of a `foo.op` operation.
    %op = pdl.operation "foo.op"(%arg0, %arg1 : !pdl.value, !pdl.value)
      {"attrA" = %attr0} -> (%type, %type : !pdl.type, !pdl.type)
    ```

    When used within a matching context, the name of the operation may be
    omitted.

    When used within a rewriting context, i.e. when defined within a
    `pdl.rewrite`, all of the result types must be "inferable". This means that
    the type must be attributable to either a constant type value or the result
    type of another entity, such as an attribute, the result of a
    `apply_native_rewrite`, or the result type of another operation. If the
    result type value does not meet any of these criteria, the operation must
    override the `InferTypeOpInterface` to ensure that the result types can be
    inferred.

    The operands of the operation are interpreted in the following ways:

    1) A single !pdl.range<value>:

    In this case, the single range is treated as all of the operands of the
    operation.

    ```mlir
    // Define an instance with single range of operands.
    %op = pdl.operation "func.return"(%allArgs : !pdl.range<value>)
    ```

    2) A variadic number of either !pdl.value or !pdl.range<value>:

    In this case, the inputs are expected to correspond with the operand groups
    defined on the operation in ODS.

    ```tablgen
    // Given the following operation definition in ODS:
    def MyIndirectCallOp {
      let results = (outs FunctionType:$call, Variadic<AnyType>:$args);
    }
    ```

    ```mlir
    // We can match the operands as so:
    %op = pdl.operation "my.indirect_call"(%call, %args : !pdl.value, !pdl.range<value>)
    ```

    The results of the operation are interpreted in the following ways:

    1) A single !pdl.range<type>:

    In this case, the single range is treated as all of the result types of the
    operation.

    ```mlir
    // Define an instance with single range of types.
    %allResultTypes = pdl.types
    %op = pdl.operation "builtin.unrealized_conversion_cast" -> (%allResultTypes : !pdl.types)
    ```

    2) A variadic number of either !pdl.type or !pdl.range<type>:

    In this case, the inputs are expected to correspond with the result groups
    defined on the operation in ODS.

    ```tablgen
    // Given the following operation definition in ODS:
    def MyOp {
      let results = (outs SomeType:$result, Variadic<SomeType>:$otherResults);
    }
    ```

    ```mlir
    // We can match the results as so:
    %result = pdl.type
    %otherResults = pdl.types
    %op = pdl.operation "foo.op" -> (%result, %otherResults : !pdl.type, !pdl.range<type>)
    ```
  }];

  let arguments = (ins OptionalAttr<StrAttr>:$opName,
                       Variadic<PDL_InstOrRangeOf<PDL_Value>>:$operandValues,
                       Variadic<PDL_Attribute>:$attributeValues,
                       StrArrayAttr:$attributeValueNames,
                       Variadic<PDL_InstOrRangeOf<PDL_Type>>:$typeValues);
  let results = (outs PDL_Operation:$op);
  let assemblyFormat = [{
    ($opName^)? (`(` $operandValues^ `:` type($operandValues) `)`)?
    custom<OperationOpAttributes>($attributeValues, $attributeValueNames)
    (`->` `(` $typeValues^ `:` type($typeValues) `)`)? attr-dict
  }];

  let builders = [
    OpBuilder<(ins CArg<"std::optional<StringRef>", "std::nullopt">:$name,
      CArg<"ValueRange", "std::nullopt">:$operandValues,
      CArg<"ArrayRef<StringRef>", "std::nullopt">:$attrNames,
      CArg<"ValueRange", "std::nullopt">:$attrValues,
      CArg<"ValueRange", "std::nullopt">:$resultTypes), [{
      auto nameAttr = name ? $_builder.getStringAttr(*name) : StringAttr();
      build($_builder, $_state, $_builder.getType<OperationType>(), nameAttr,
            operandValues, attrValues, $_builder.getStrArrayAttr(attrNames),
            resultTypes);
    }]>,
  ];
  let extraClassDeclaration = [{
    /// Returns true if the operation type referenced supports result type
    /// inference.
    bool hasTypeInference();

    /// Returns true if the operation type referenced might support result type
    /// inference, i.e. it supports type reference or is currently not
    /// registered in the context. Returns false if the root operation name
    /// has not been set.
    bool mightHaveTypeInference();
  }];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// pdl::PatternOp
//===----------------------------------------------------------------------===//

def PDL_PatternOp : PDL_Op<"pattern", [
    IsolatedFromAbove, SingleBlock, Symbol,
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getDefaultDialect"]>
  ]> {
  let summary = "Define a rewrite pattern";
  let description = [{
    `pdl.pattern` operations provide a transformable representation for a
    `RewritePattern`. The attributes on this operation correspond to the various
    metadata on a `RewritePattern`, such as the benefit. The match section of
    the pattern is specified within the region body, with the rewrite provided
    by a terminating `pdl.rewrite`.

    Example:

    ```mlir
    // Provide a pattern matching "foo.op" that replaces the root with its
    // operand.
    pdl.pattern : benefit(1) {
      %resultType = pdl.type
      %inputOperand = pdl.operand
      %root = pdl.operation "foo.op"(%inputOperand) -> (%resultType)
      pdl.rewrite %root {
        pdl.replace %root with (%inputOperand)
      }
    }
    ```
  }];

  let arguments = (ins ConfinedAttr<I16Attr, [IntNonNegative]>:$benefit,
                       OptionalAttr<SymbolNameAttr>:$sym_name);
  let regions = (region SizedRegion<1>:$bodyRegion);
  let assemblyFormat = [{
    ($sym_name^)? `:` `benefit` `(` $benefit `)` attr-dict-with-keyword $bodyRegion
  }];

  let builders = [
    OpBuilder<(ins CArg<"std::optional<uint16_t>", "1">:$benefit,
                   CArg<"std::optional<StringRef>", "std::nullopt">:$name)>,
  ];
  let extraClassDeclaration = [{
    //===------------------------------------------------------------------===//
    // SymbolOpInterface Methods
    //===------------------------------------------------------------------===//

    /// A PatternOp may optionally define a symbol.
    bool isOptionalSymbol() { return true; }

    /// Returns the rewrite operation of this pattern.
    RewriteOp getRewriter();
  }];
  let hasRegionVerifier = 1;
}

//===----------------------------------------------------------------------===//
// pdl::RangeOp
//===----------------------------------------------------------------------===//

def PDL_RangeOp : PDL_Op<"range", [Pure, HasParent<"pdl::RewriteOp">]> {
  let summary = "Construct a range of pdl entities";
  let description = [{
    `pdl.range` operations construct a range from a given set of PDL entities,
    which all share the same underlying element type. For example, a
    `!pdl.range<value>` may be constructed from a list of `!pdl.value`
    or `!pdl.range<value>` entities.

    Example:

    ```mlir
    // Construct a range of values.
    %valueRange = pdl.range %inputValue, %inputRange : !pdl.value, !pdl.range<value>

    // Construct a range of types.
    %typeRange = pdl.range %inputType, %inputRange : !pdl.type, !pdl.range<type>

    // Construct an empty range of types.
    %valueRange = pdl.range : !pdl.range<type>
    ```

    TODO: Range construction is currently limited to rewrites, but it could
    be extended to constraints under certain circustances; i.e., if we can
    determine how to extract the underlying elements. If we can't, e.g. if
    there are multiple sub ranges used for construction, we won't be able
    to determine their sizes during constraint time.
  }];

  let arguments = (ins Variadic<PDL_AnyType>:$arguments);
  let results = (outs PDL_RangeOf<AnyTypeOf<[PDL_Type, PDL_Value]>>:$result);
  let assemblyFormat = [{
    ($arguments^ `:` type($arguments))?
    custom<RangeType>(ref(type($arguments)), type($result))
    attr-dict
  }];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// pdl::ReplaceOp
//===----------------------------------------------------------------------===//

def PDL_ReplaceOp : PDL_Op<"replace", [
    AttrSizedOperandSegments, HasParent<"pdl::RewriteOp">
  ]> {
  let summary = "Mark an input operation as `replaced`";
  let description = [{
    `pdl.replace` operations are used within `pdl.rewrite` regions to specify
    that an input operation should be marked as replaced. The semantics of this
    operation correspond with the `replaceOp` method on a `PatternRewriter`. The
    set of replacement values can be either:
    * a single `Operation` (`replOperation` should be populated)
      - The operation will be replaced with the results of this operation.
    * a set of `Value`s (`replValues` should be populated)
      - The operation will be replaced with these values.

    Example:

    ```mlir
    // Replace root node with 2 values:
    pdl.replace %root with (%val0, %val1 : !pdl.value, !pdl.value)

    // Replace root node with a range of values:
    pdl.replace %root with (%vals : !pdl.range<value>)

    // Replace root with another operation:
    pdl.replace %root with %otherOp
    ```
  }];
  let arguments = (ins PDL_Operation:$opValue,
                       Optional<PDL_Operation>:$replOperation,
                       Variadic<PDL_InstOrRangeOf<PDL_Value>>:$replValues);
  let assemblyFormat = [{
    $opValue `with` (`(` $replValues^ `:` type($replValues) `)`)?
    ($replOperation^)? attr-dict
  }];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// pdl::ResultOp
//===----------------------------------------------------------------------===//

def PDL_ResultOp : PDL_Op<"result", [Pure]> {
  let summary = "Extract a result from an operation";
  let description = [{
    `pdl.result` operations extract result edges from an operation node within
    a pattern or rewrite region. The provided index is zero-based, and
    represents the concrete result to extract, i.e. this is not the result index
    as defined by the ODS definition of the operation.

    Example:

    ```mlir
    // Extract a result:
    %operation = pdl.operation ...
    %pdl_result = pdl.result 1 of %operation

    // Imagine the following IR being matched:
    %result_0, %result_1 = foo.op ...

    // If the example pattern snippet above were matching against `foo.op` in
    // the IR snippet, `%pdl_result` would correspond to `%result_1`.
    ```
  }];

  let arguments = (ins PDL_Operation:$parent, I32Attr:$index);
  let results = (outs PDL_Value:$val);
  let assemblyFormat = "$index `of` $parent attr-dict";
}

//===----------------------------------------------------------------------===//
// pdl::ResultsOp
//===----------------------------------------------------------------------===//

def PDL_ResultsOp : PDL_Op<"results", [Pure]> {
  let summary = "Extract a result group from an operation";
  let description = [{
    `pdl.results` operations extract a result group from an operation within a
    pattern or rewrite region. If an index is provided, this operation extracts
    a result group as defined by the ODS definition of the operation. In this
    case the result of this operation may be either a single `pdl.value` or
    a `pdl.range<value>`, depending on the constraint of the result in ODS. If
    no index is provided, this operation extracts the full result range of the
    operation.

    Example:

    ```mlir
    // Extract all of the results of an operation:
    %operation = pdl.operation ...
    %results = pdl.results of %operation

    // Extract the results in the first result group of an operation, which is
    // variadic:
    %operation = pdl.operation ...
    %results = pdl.results 0 of %operation -> !pdl.range<value>

    // Extract the results in the second result group of an operation, which is
    // not variadic:
    %operation = pdl.operation ...
    %results = pdl.results 1 of %operation -> !pdl.value
    ```
  }];

  let arguments = (ins PDL_Operation:$parent, OptionalAttr<I32Attr>:$index);
  let results = (outs PDL_InstOrRangeOf<PDL_Value>:$val);
  let assemblyFormat = [{
    ($index^)? `of` $parent custom<ResultsValueType>(ref($index), type($val))
    attr-dict
  }];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// pdl::RewriteOp
//===----------------------------------------------------------------------===//

def PDL_RewriteOp : PDL_Op<"rewrite", [
     Terminator, HasParent<"pdl::PatternOp">, NoTerminator, NoRegionArguments,
     SingleBlock, AttrSizedOperandSegments,
     DeclareOpInterfaceMethods<OpAsmOpInterface, ["getDefaultDialect"]>
  ]> {
  let summary = "Specify the rewrite of a matched pattern";
  let description = [{
    `pdl.rewrite` operations terminate the region of a `pdl.pattern` and specify
    the main rewrite of a `pdl.pattern`, on the optional root operation. The
    rewrite is specified either via a string name (`name`) to a native
    rewrite function, or via the region body. The rewrite region, if specified,
    must contain a single block. If the rewrite is external it functions
    similarly to `pdl.apply_native_rewrite`, and takes a set of additional
    positional values defined within the matcher as arguments. If the rewrite is
    external, the root operation is passed to the native function as the leading
    arguments. The root operation, if provided, specifies the starting point in
    the pattern for the subgraph isomorphism search. Pattern matching will proceed
    from this node downward (towards the defining operation) or upward
    (towards the users) until all the operations in the pattern have been matched.
    If the root is omitted, the pdl_interp lowering will automatically select
    the best root of the pdl.rewrite among all the operations in the pattern.

    Example:

    ```mlir
    // Specify an external rewrite function:
    pdl.rewrite %root with "myExternalRewriter"(%value : !pdl.value)

    // Specify a rewrite inline using PDL with the given root:
    pdl.rewrite %root {
      %op = pdl.operation "foo.op"(%arg0, %arg1)
      pdl.replace %root with %op
    }

    // Specify a rewrite inline using PDL, automatically selecting root:
    pdl.rewrite {
      %op1 = pdl.operation "foo.op"(%arg0, %arg1)
      %op2 = pdl.operation "bar.op"(%arg0, %arg1)
      pdl.replace %root1 with %op1
      pdl.replace %root2 with %op2
    }
    ```
  }];

  let arguments = (ins Optional<PDL_Operation>:$root,
                       OptionalAttr<StrAttr>:$name,
                       Variadic<PDL_AnyType>:$externalArgs);
  let regions = (region AnyRegion:$bodyRegion);
  let assemblyFormat = [{
    ($root^)? (`with` $name^ (`(` $externalArgs^ `:` type($externalArgs) `)`)?)?
              ($bodyRegion^)?
    attr-dict-with-keyword
  }];
  let hasRegionVerifier = 1;
}

//===----------------------------------------------------------------------===//
// pdl::TypeOp
//===----------------------------------------------------------------------===//

def PDL_TypeOp : PDL_Op<"type"> {
  let summary = "Define a type handle within a pattern";
  let description = [{
    `pdl.type` operations capture result type constraints of `Attributes`,
    `Values`, and `Operations`. Instances of this operation define, and
    partially constrain, results types of a given entity. A `pdl.type` may
    partially constrain the result by specifying a constant `Type`.

    Example:

    ```mlir
    // Define a type:
    %type = pdl.type

    // Define a type with a constant value:
    %type = pdl.type : i32
    ```
  }];

  let arguments = (ins OptionalAttr<TypeAttr>:$constantType);
  let results = (outs PDL_Type:$result);
  let assemblyFormat = "attr-dict (`:` $constantType^)?";
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// pdl::TypesOp
//===----------------------------------------------------------------------===//

def PDL_TypesOp : PDL_Op<"types"> {
  let summary = "Define a range of type handles within a pattern";
  let description = [{
    `pdl.types` operations capture result type constraints of `Value`s, and
    `Operation`s. Instances of this operation define results types of a given
    entity. A `pdl.types` may partially constrain the results by specifying
    an array of `Type`s.

    Example:

    ```mlir
    // Define a range of types:
    %types = pdl.types

    // Define a range of types with a range of constant values:
    %types = pdl.types : [i32, i64, i32]
    ```
  }];

  let arguments = (ins OptionalAttr<TypeArrayAttr>:$constantTypes);
  let results = (outs PDL_RangeOf<PDL_Type>:$result);
  let assemblyFormat = "attr-dict (`:` $constantTypes^)?";
  let hasVerifier = 1;
}

#endif // MLIR_DIALECT_PDL_IR_PDLOPS


//===- BufferizationTransformOps.td - Buff. transf. ops ----*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef BUFFERIZATION_TRANSFORM_OPS
#define BUFFERIZATION_TRANSFORM_OPS

include "mlir/Dialect/Bufferization/IR/BufferizationEnums.td"
include "mlir/Dialect/Transform/IR/TransformDialect.td"
include "mlir/Dialect/Transform/Interfaces/TransformInterfaces.td"
include "mlir/Dialect/Transform/IR/TransformTypes.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/IR/OpBase.td"

def Transform_EmptyOp : Transform_ConcreteOpType<"tensor.empty">;
def Transform_AllocTensorOp : Transform_ConcreteOpType<"bufferization.alloc_tensor">;

//===----------------------------------------------------------------------===//
// BufferLoopHoistingOp
//===----------------------------------------------------------------------===//

def BufferLoopHoistingOp
    : Op<Transform_Dialect, "bufferization.buffer_loop_hoisting",
        [DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
         TransformEachOpTrait, TransformOpInterface]> {
  let description = [{
    Hoist buffer allocations ("memref.alloc" and "memref.alloca") from loops
    within the targeted op. This transform assumes that there are no buffer
    deallocation ops in the IR.

    This transform reads the `target` handle and modifies the payload.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs);
  let assemblyFormat = "$target attr-dict `:` type($target)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::Operation *target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

//===----------------------------------------------------------------------===//
// OneShotBufferizeOp
//===----------------------------------------------------------------------===//

def OneShotBufferizeOp
    : Op<Transform_Dialect, "bufferization.one_shot_bufferize",
        [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
         DeclareOpInterfaceMethods<TransformOpInterface>]> {
  let description = [{
    Indicates that the given `target` op should be bufferized with One-Shot
    Bufferize. The bufferization can be configured with various attributes that
    corresponding to options in `BufferizationOptions` and the
    `one-shot-bufferize` pass. More information can be found in the pass
    documentation.

    The targeted ops must be modules or functions. This is because there is
    always a single, bufferized replacement op for such targets.

    Note: Only ops that implement `BufferizableOpInterface` are bufferized. All
    other ops are ignored if `allow_unknown_ops`. If `allow_unknown_ops` is
    unset, this transform fails when an unknown/non-bufferizable op is found.
    Many ops implement `BufferizableOpInterface` via an external model. These
    external models must be registered when applying this transform op;
    otherwise, said ops would be considered non-bufferizable.

    #### Return modes

    This operation consumes the `target` handle and produces the `transformed`
    handle.
  }];

  let arguments = (
      ins TransformHandleTypeInterface:$target,
      OptionalAttr<LayoutMapOption>:$function_boundary_type_conversion,
      DefaultValuedAttr<BoolAttr, "false">:$allow_return_allocs_from_loops,
      DefaultValuedAttr<BoolAttr, "false">:$allow_unknown_ops,
      DefaultValuedAttr<BoolAttr, "false">:$bufferize_function_boundaries,
      DefaultValuedAttr<BoolAttr, "false">:$dump_alias_sets,
      DefaultValuedAttr<BoolAttr, "false">:$test_analysis_only,
      DefaultValuedAttr<BoolAttr, "false">:$print_conflicts,
      DefaultValuedAttr<BoolAttr, "true">:$check_parallel_regions,
      DefaultValuedAttr<StrAttr, "\"memref.copy\"">:$memcpy_op);

  let results = (outs TransformHandleTypeInterface:$transformed);

  let hasVerifier = 1;
  let assemblyFormat = [{
    (`layout` `{` $function_boundary_type_conversion^ `}`)?
    $target attr-dict `:` functional-type($target, results)
  }];
}

//===----------------------------------------------------------------------===//
// EliminateEmptyTensorsOp
//===----------------------------------------------------------------------===//

def EliminateEmptyTensorsOp
    : Op<Transform_Dialect, "bufferization.eliminate_empty_tensors",
        [DeclareOpInterfaceMethods<TransformOpInterface>,
         DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
  let description = [{
    Try to eliminate all `tensor.empty` ops within the targeted op by replacing
    them with another destination tensor.

    "tensor.empty" ops cannot be bufferized. They can either be converted to
    "bufferization.alloc_tensor" or replaced with another tensor (via this
    transform). "tensor.empty" does not specify the contents of the returned
    tensor so their results can be replaced with arbitrary tensor values as long
    as the dimensions match.

    This transformation looks for subset ops that insert a tensor that
    originates from a "tensor.empty" (as per the reverse use-def chain). Such
    "tensor.empty" ops are replaced with the destination subset.

    Example:

    ```
    %0 = tensor.empty() : tensor<5xf32>
    %1 = linalg.fill ... outs(%0)
    %2 = tensor.insert_slice %1 into %t[1][5][1]
    ```

    Is rewritten with:
    ```
    %0 = tensor.extract_slice %t[1][5][1]
    %1 = linalg.fill ... outs(%0)
    %2 = tensor.insert_slice %1 into %t[1][5][1]
    ```

    In the above example, the subset op is "tensor.insert_slice". When tracing
    back the reverse use-def chain of a the source, we end up at a
    "tensor.empty" op.

    The above example can bufferize without an allocation (in the absence of
    other conflicts) because there is no longer a `tensor.empty` op.

    See `-eliminate-empty-tensors` for more details.

    #### Return modes

    This transform reads the target handle and modifies the payload. It does
    not produce any handle.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);

  let results = (outs);

  let assemblyFormat = "$target attr-dict `:` type($target)";
}

//===----------------------------------------------------------------------===//
// EmptyTensorToAllocTensorOp
//===----------------------------------------------------------------------===//

def EmptyTensorToAllocTensorOp
    : Op<Transform_Dialect, "bufferization.empty_tensor_to_alloc_tensor",
        [FunctionalStyleTransformOpTrait,
         MemoryEffectsOpInterface,
         TransformOpInterface,
         TransformEachOpTrait]> {
  let description = [{
    Replace a tensor.empty with a bufferization.tensor_alloc.

    #### Return modes

    This operation consumes the `target` handle and produces the `transformed`
    handle. `target` is expected to be a `tensor.empty` operation. The transform
    always succeeds.
  }];

  let arguments = (ins Transform_EmptyOp:$target);
  let results = (outs Transform_AllocTensorOp:$transformed);

  let assemblyFormat = "$target attr-dict `:` functional-type(operands, results)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::tensor::EmptyOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

#endif // BUFFERIZATION_TRANSFORM_OPS


//===- SCFTransformOps.td - SCF (loop) transformation ops --*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef SCF_TRANSFORM_OPS
#define SCF_TRANSFORM_OPS

include "mlir/Dialect/Transform/IR/TransformDialect.td"
include "mlir/Dialect/Transform/Interfaces/TransformInterfaces.td"
include "mlir/Dialect/Transform/IR/TransformTypes.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/IR/OpBase.td"

def ApplyForLoopCanonicalizationPatternsOp : Op<Transform_Dialect,
    "apply_patterns.scf.for_loop_canonicalization",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Collects patterns for canonicalizing operations inside SCF loop bodies.
    At the moment, only affine.min/max computations with iteration variables,
    loop bounds and loop steps are canonicalized.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplySCFStructuralConversionPatternsOp : Op<Transform_Dialect,
    "apply_conversion_patterns.scf.structural_conversions",
    [DeclareOpInterfaceMethods<ConversionPatternDescriptorOpInterface,
      ["populateConversionTargetRules"]>]> {
  let description = [{
    Collects patterns for performing structural conversions of SCF operations.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplySCFToControlFlowPatternsOp : Op<Transform_Dialect,
    "apply_conversion_patterns.scf.scf_to_control_flow",
    [DeclareOpInterfaceMethods<ConversionPatternDescriptorOpInterface>]> {
  let description = [{
    Collects patterns that lower structured control flow ops to unstructured
    control flow.
  }];

  let assemblyFormat = "attr-dict";
}

def Transform_ScfForOp : Transform_ConcreteOpType<"scf.for">;

def ForallToForOp : Op<Transform_Dialect, "loop.forall_to_for",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     DeclareOpInterfaceMethods<TransformOpInterface>]> {
  let summary = "Converts scf.forall into a nest of scf.for operations";
  let description = [{
    Converts the `scf.forall` operation pointed to by the given handle into a
    set of nested `scf.for` operations. Each new operation corresponds to one
    induction variable of the original "multifor" loop.

    The operand handle must be associated with exactly one payload operation.

    Loops with shared outputs are currently not supported.

    #### Return Modes

    Consumes the operand handle. Produces a silenceable failure if the operand
    is not associated with a single `scf.forall` payload operation.
    Returns as many handles as the given `forall` op has induction variables
    that are associated with the generated `scf.for` loops.
    Produces a silenceable failure if another number of resulting handles is
    requested.
  }];
  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs Variadic<TransformHandleTypeInterface>:$transformed);

  let assemblyFormat = "$target attr-dict `:` functional-type(operands, results)";
}

def ForallToParallelOp : Op<Transform_Dialect, "loop.forall_to_parallel",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     DeclareOpInterfaceMethods<TransformOpInterface>]> {
  let summary = "Converts scf.forall into a nest of scf.for operations";
  let description = [{
    Converts the `scf.forall` operation pointed to by the given handle into an
    `scf.parallel` operation.

    The operand handle must be associated with exactly one payload operation.

    Loops with outputs are not supported.

    #### Return Modes

    Consumes the operand handle. Produces a silenceable failure if the operand
    is not associated with a single `scf.forall` payload operation.
    Returns a handle to the new `scf.parallel` operation.
    Produces a silenceable failure if another number of resulting handles is
    requested.
  }];
  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs Variadic<TransformHandleTypeInterface>:$transformed);

  let assemblyFormat = "$target attr-dict `:` functional-type(operands, results)";
}

def LoopOutlineOp : Op<Transform_Dialect, "loop.outline",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     DeclareOpInterfaceMethods<TransformOpInterface>]> {
  let summary = "Outlines a loop into a named function";
  let description = [{
    Moves the loop into a separate function with the specified name and replaces
    the loop in the Payload IR with a call to that function. Takes care of
    forwarding values that are used in the loop as function arguments. If the
    operand is associated with more than one loop, each loop will be outlined
    into a separate function. The provided name is used as a _base_ for forming
    actual function names following `SymbolTable` auto-renaming scheme to avoid
    duplicate symbols. Expects that all ops in the Payload IR have a
    `SymbolTable` ancestor (typically true because of the top-level module).

    #### Return Modes

    Returns a handle to the list of outlined functions and a handle to the
    corresponding function call operations in the same order as the operand
    handle.

    Produces a definite failure if outlining failed for any of the targets.
  }];

  // Note that despite the name of the transform operation and related utility
  // functions, the actual implementation does not require the operation to be
  // a loop.
  let arguments = (ins TransformHandleTypeInterface:$target,
                   StrAttr:$func_name);
  let results = (outs TransformHandleTypeInterface:$function,
                      TransformHandleTypeInterface:$call);

  let assemblyFormat =
    "$target attr-dict `:` functional-type(operands, results)";
}

def LoopPeelOp : Op<Transform_Dialect, "loop.peel",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     TransformOpInterface, TransformEachOpTrait]> {
  let summary = "Peels the first or last iteration of the loop";
  let description = [{
     Rewrite the given loop with a main loop and a partial (first or last) loop.
     When the `peelFront` option is set to true, the first iteration is peeled off.
     Otherwise, updates the given loop so that its step evenly divides its range and puts
     the remaining iteration into a separate loop or a conditional.

     In the absence of sufficient static information, this op may peel a loop,
     even if the step always divides the range evenly at runtime.

     #### Return modes

     This operation ignores non-scf::ForOp ops and drops them in the return.
     The op returns two loops, the peeled loop which has trip count divisible
     by the step, and the remainder loop.

     When `peelFront` is true, the first result (remainder loop) executes all
     but the first iteration of the target loop. The second result (peeled
     loop) corresponds to the first iteration of the loop which can be
     canonicalized away in the following optimizations.

     When `peelFront` is false, the first result (peeled loop) is the portion
     of the target loop with the highest upper bound that is divisible by the
     step. The second result (remainder loop) contains the remaining iterations. 
     
     Note that even though the Payload IR modification may be performed
     in-place, this operation consumes the operand handle and produces a new one.

     #### Return Modes

     Produces a definite failure if peeling fails.
  }];

  let arguments =
      (ins Transform_ScfForOp:$target,
           DefaultValuedAttr<BoolAttr, "false">:$peel_front,
           DefaultValuedAttr<BoolAttr, "false">:$fail_if_already_divisible);
  let results = (outs TransformHandleTypeInterface:$peeled_loop,
                      TransformHandleTypeInterface:$remainder_loop);

  let assemblyFormat =
    "$target attr-dict `:` functional-type(operands, results)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::scf::ForOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

def LoopPipelineOp : Op<Transform_Dialect, "loop.pipeline",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     TransformOpInterface, TransformEachOpTrait]> {
  let summary = "Applies software pipelining to the loop";
  let description = [{
    Transforms the given loops one by one to achieve software pipelining for
    each of them. That is, performs some amount of reads from memory before the
    loop rather than inside the loop, the same amount of writes into memory
    after the loop, and updates each iteration to read the data for a following
    iteration rather than the current one.

    The amount is specified by the attributes.

    The values read and about to be stored are transferred as loop iteration
    arguments. Currently supports memref and vector transfer operations as
    memory reads/writes.

    #### Return modes

    This operation ignores non-scf::For ops and drops them in the return.
    If all the operations referred to by the `target` PDLOperation pipeline
    properly, the transform succeeds. Otherwise the transform produces a
    silenceable failure.  The return handle points to only the subset of
    successfully produced pipelined loops, which can be empty.
  }];

  let arguments = (ins Transform_ScfForOp:$target,
                   DefaultValuedAttr<I64Attr, "1">:$iteration_interval,
                   DefaultValuedAttr<I64Attr, "10">:$read_latency);
  let results = (outs TransformHandleTypeInterface:$transformed);

  let assemblyFormat =
    "$target attr-dict `:` functional-type(operands, results)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::scf::ForOp target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

def LoopPromoteIfOneIterationOp : Op<Transform_Dialect,
    "loop.promote_if_one_iteration", [
        DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
        TransformOpInterface, TransformEachOpTrait]> {
  let summary = "Promote loop if it has one iteration";
  let description = [{
    Promotes the given target loop op if it has a single iteration. I.e., the
    loop op is removed and only the body remains.

    #### Return modes

    This transform fails if the target is mapped to ops that are loops. Ops are
    considered loops if they implement the `LoopLikeOpInterface`. Otherwise,
    this transform always succeeds. The transform consumes the target handle and
    modifies the payload.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs);
  let assemblyFormat = "$target attr-dict `:` type($target)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::LoopLikeOpInterface target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

def LoopUnrollOp : Op<Transform_Dialect, "loop.unroll",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     TransformOpInterface, TransformEachOpTrait]> {
  let summary = "Unrolls the given loop with the given unroll factor";
  let description = [{
    Unrolls each loop associated with the given handle to have up to the given
    number of loop body copies per iteration. If the unroll factor is larger
    than the loop trip count, the latter is used as the unroll factor instead.

    #### Return modes

    This operation ignores non-`scf.for`, non-`affine.for` ops and drops them
    in the return. If all the operations referred to by the `target` operand
    unroll properly, the transform succeeds. Otherwise the transform produces a
    silenceable failure.

    Does not return handles as the operation may result in the loop being
    removed after a full unrolling.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       ConfinedAttr<I64Attr, [IntPositive]>:$factor);

  let assemblyFormat = "$target attr-dict `:` type($target)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::Operation *target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

def LoopUnrollAndJamOp : Op<Transform_Dialect, "loop.unroll_and_jam",
    [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
     TransformOpInterface, TransformEachOpTrait]> {
  let summary = "Unrolls and jam the given loop with the given unroll factor";
  let description = [{
    Unrolls & jams each loop associated with the given handle to have up to the given
    number of loop body copies per iteration. If the unroll factor is larger
    than the loop trip count, the latter is used as the unroll factor instead.

    #### Return modes

    This operation ignores non-`scf.for`, non-`affine.for` ops and drops them
    in the return. If all the operations referred to by the `target` operand
    unroll properly, the transform succeeds. Otherwise the transform produces a
    silenceable failure.

    Does not return handles as the operation may result in the loop being
    removed after a full unrolling.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       ConfinedAttr<I64Attr, [IntPositive]>:$factor);

  let assemblyFormat = "$target attr-dict `:` type($target)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::Operation *target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

def LoopCoalesceOp : Op<Transform_Dialect, "loop.coalesce", [
  FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
  TransformOpInterface, TransformEachOpTrait]> {
  let summary = "Coalesces the perfect loop nest enclosed by a given loop";
  let description = [{
    Given a perfect loop nest identified by the outermost loop,
    perform loop coalescing in a bottom-up one-by-one manner.

    #### Return modes

    The return handle points to the coalesced loop if coalescing happens, or
    the given input loop if coalescing does not happen.
  }];
  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs TransformHandleTypeInterface:$transformed);

  let assemblyFormat =
      "$target attr-dict `:` functional-type($target, $transformed)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::Operation *target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

def TakeAssumedBranchOp : Op<Transform_Dialect, "scf.take_assumed_branch", [
  DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
  TransformOpInterface, TransformEachOpTrait]> {
  let description = [{
    Given an scf.if conditional, inject user-defined information that it is
    always safe to execute only the if or else branch.

    This is achieved by just replacing the scf.if by the content of one of its
    branches.

    This is particularly useful for user-controlled rewriting of conditionals
    that exist solely to guard against out-of-bounds behavior.

    At the moment, no assume or assert operation is emitted as it is not always
    desirable. In the future, this may be controlled by a dedicated attribute.

    #### Return modes

    The transform only consumes its operand and does not produce any result.
    The transform definitely fails if `take_else_branch` is specified and the
    `else` region is empty.
  }];
  let arguments = (ins TransformHandleTypeInterface:$target,
                       OptionalAttr<UnitAttr>:$take_else_branch);
  let results = (outs);

  let assemblyFormat = [{
      $target
      (`take_else_branch` $take_else_branch^)?
      attr-dict
       `:` functional-type(operands, results)
  }];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::scf::IfOp ifOp,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

def LoopFuseSiblingOp : Op<Transform_Dialect, "loop.fuse_sibling",
  [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
   DeclareOpInterfaceMethods<TransformOpInterface>]> {
  let summary = "Fuse a loop into another loop, assuming the fusion is legal.";

  let description = [{
    Fuses the `target` loop into the `source` loop assuming they are
    independent of each other. In the fused loop, the arguments, body and
    results of `target` are placed _before_ those of `source`.

    For fusion of two `scf.for` loops, the bounds and step size must match. For
    fusion of two `scf.forall` loops, the bounds and the mapping must match.
    Otherwise a silencable failure is produced.

    The `target` and `source` handles must refer to exactly one operation,
    otherwise a definite failure is produced. It is the responsibility of the
    user to ensure that the `target` and `source` loops are independent of each
    other -- this op will only perform rudimentary legality checks.

    #### Return modes

    This operation consumes the `target` and `source` handles and produces the
    `fused_loop` handle, which points to the fused loop.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       TransformHandleTypeInterface:$source);
  let results = (outs TransformHandleTypeInterface:$fused_loop);
  let assemblyFormat = "$target `into` $source attr-dict "
                       " `:` functional-type(operands, results)";
}

#endif // SCF_TRANSFORM_OPS


//===- GPUTransformOps.td - GPU transform ops --------------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef GPU_TRANSFORM_OPS
#define GPU_TRANSFORM_OPS

include "mlir/Dialect/Transform/IR/TransformDialect.td"
include "mlir/Dialect/Transform/Interfaces/TransformInterfaces.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/IR/OpBase.td"

//===----------------------------------------------------------------------===//
// Apply...ConversionPatternsOp
//===----------------------------------------------------------------------===//

def ApplyGPUToNVVMConversionPatternsOp : Op<Transform_Dialect,
    "apply_conversion_patterns.gpu.gpu_to_nvvm",
    [DeclareOpInterfaceMethods<ConversionPatternDescriptorOpInterface,
                               ["verifyTypeConverter"]>]> {
  let description = [{
    Collects patterns that convert GPU dialect ops to NVVM dialect ops. These
    patterns require an "LLVMTypeConverter".
  }];
  let assemblyFormat = "attr-dict";
}

def ApplyGPUWwmaToNVVMConversionPatternsOp : Op<Transform_Dialect,
    "apply_conversion_patterns.gpu.gpu_wmma_to_nvvm",
    [DeclareOpInterfaceMethods<ConversionPatternDescriptorOpInterface,
                               ["verifyTypeConverter"]>]> {
  let description = [{
    Collects patterns that convert GPU dialect ops related to wmma ops
    to NVVM dialect ops.
    These patterns require an "LLVMTypeConverter".
  }];
  let assemblyFormat = "attr-dict";
}

def ApplyGPUSubgroupReduceToNVVMConversionPatternsOp : Op<Transform_Dialect,
    "apply_conversion_patterns.gpu.gpu_subgroup_reduce_to_nvvm",
    [DeclareOpInterfaceMethods<ConversionPatternDescriptorOpInterface,
                               ["verifyTypeConverter"]>]> {
  let description = [{
    Collects patterns that convert GPU dialect ops related to wmma ops
    to NVVM dialect ops.
    These patterns require an "LLVMTypeConverter".
  }];
  let assemblyFormat = "attr-dict";
}

//===----------------------------------------------------------------------===//
// Apply...PatternsOp
//===----------------------------------------------------------------------===//

def ApplyGPURewritePatternsOp : Op<Transform_Dialect,
    "apply_patterns.gpu.gpu_rewrite_patterns",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Collects GPU rewrite patterns comprising:
      1. GpuAllReduceRewrite patterns
      2. GpuGlobalIdRewriter patterns
      3. GpuShuffleRewriter patterns
  }];
  let assemblyFormat = "attr-dict";
}

def ApplyUnrollVectorsSubgroupMmaOp : Op<Transform_Dialect,
    "apply_patterns.gpu.unroll_vectors_subgroup_mma",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Unrolls contractions to the target `m`, `n`, and `k` native vector size,
    along with other vector operations based on expected usage. `transfer_read`
    ops unroll based on the extract slice shape introduced by unrolling the
    contractions, while elementwise and `transfer_write` ops unroll to the shape of
    the C matrix (`m x n`).

    This operation applies to pure vector operations and should be applied before
    lowering to subgroup_mma ops.
  }];

  let arguments = (ins I64Attr:$m,
                       I64Attr:$n,
                       I64Attr:$k);

  let assemblyFormat = [{
    `[` $m `,` $n `,` $k `]` attr-dict
  }];
}

def EliminateBarriersOp :
  Op<Transform_Dialect, "apply_patterns.gpu.eliminate_barriers",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Removes unnecessary GPU barriers from the function. If a barrier does not
    enforce any conflicting pair of memory effects, including a pair that is
    enforced by another barrier, it is unnecessary and can be removed.

    The approach is based on "High-Performance GPU-to-CPU Transpilation and
    Optimization via High-Level Parallel Constructs" by  Moses, Ivanov,
    Domke, Endo, Doerfert, and Zinenko in PPoPP 2023. Specifically, it
    analyzes the memory effects of the operations before and after the given
    barrier and checks if the barrier enforces any of the memory
    effect-induced dependencies that aren't already enforced by another
    barrier.

    For example, in the following code

    ```mlir
      store %A
      barrier  // enforces load-after-store
      load %A
      barrier  // load-after-store already enforced by the previous barrier
      load %A
    ```

    the second barrier can be removed.
  }];

  let assemblyFormat = [{ attr-dict }];
}

def MapNestedForallToThreads :
  Op<Transform_Dialect, "gpu.map_nested_forall_to_threads",
    [FunctionalStyleTransformOpTrait,
     MemoryEffectsOpInterface,
     TransformEachOpTrait,
     TransformOpInterface]> {
  let description = [{
      Target the `gpu.launch op` and rewrite all `scf.forall` nested in it to 
      distributed `gpu.thread_id` attribute.

      The operation searches for `scf.forall` ops nested under `target` and maps
      each such op to GPU threads. 
      
      `scf.forall` induction variables are rewritten to `gpu.thread_id` according
      to the `mapping` attribute.

      Different types of mappings attributes are supported:
        - the block_dims is a list of integers that specifies the number of
          threads in each dimension. This is a mandatory attribute that is used
          to constrain the number of threads in each dimension. If an 
          `scf.forall` op is mapped to fewer threads, predication occurs.
        - the warp_dims is a list of integers that specifies the number of
          warps in each dimension. This is an optional attribute that is used
          to constrain the number of warps in each dimension. When present, this
          attribute must be specified in a way that is compatible with the 
          block_dims attribute. If an `scf.forall` op is mapped to fewer warps,
          predication occurs.

      Dynamic `scf.forall` trip counts are currently not supported.
      Dynamic block dim sizes are currently not supported.

      Only **bufferized** `scf.forall` are currently supported.
      Only `scf.forall` distributed to **at most 3 dimensions** are
      currently supported.

      The `sync_after_distribute`attribute controls whether a `gpu.barrier` is
      inserted after each scf.forall op. At this time, this is an all or nothing
      choice. This will need to be tightened in the future.

      The operation alters the block size of the given gpu_launch using the 
      mandatory block_dims argument.

      #### Return modes:

      This operation ignores non-gpu_launch ops and drops them in the return.

      If any scf.forall with tensors is found, the transform definitely
      fails.

      If all the scf.forall operations with gpu.thread mapping contained
      within the LaunchOp referred to by the `target` PDLOperation lower to GPU
      properly, the transform succeeds. Otherwise the transform definitely
      fails.

      scf.forall operations with mappings other than gpu.thread are
      ignored.

      The returned handle points to the same LaunchOp operand, consuming it and
      producing a new SSA value to satisfy chaining and linearity of the IR
      properties.

      #### Example:

      ```
      gpu.launch blocks(%bx, %by, %bz) in (%x = %0, %y = %1, %z = %2)
                 threads(%tx, %ty, %tz) in (%tx = %3, %ty = %4, %tz = %5) {
        scf.forall (%i, %j) in (7, 9) {
          ... // body 1
        } {mapping = [#gpu.thread<x>, #gpu.thread<y>, #gpu.thread<z>]}
        scf.forall (%i) in (12) {
          ... // body 2
        } {mapping = [#gpu.thread<x>]}
        gpu.terminator
      }
      ```

      is translated to:

      ```
      %bdimX = arith.constant 12 : index
      %bdimY = arith.constant 9 : index
      gpu.launch blocks(%bx, %by, %bz) in (%x = %0, %y = %1, %z = %2)
             threads(%tx, %ty, %tz) in (%tx = %bdimX, %ty = %bdimY, %tz = %5) {
        if (threadIdx.x < 9 && threadIdx.y < 7) {
          ... // body 1
        }
        gpu.barrier
        if (threadIdx.y < 1) {
          ... // body 2
        }
        gpu.barrier
        gpu.terminator
      }
      ```
    }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                   DefaultValuedAttr<DenseI64ArrayAttr, "{}">:$block_dims,
                   DefaultValuedAttr<BoolAttr, "true">:$sync_after_distribute,
                   DefaultValuedAttr<I64Attr, "32">:$warp_size);
  let results = (outs TransformHandleTypeInterface:$result);

  let assemblyFormat = [{
    $target
    `block_dims` `=` $block_dims
    (`sync_after_distribute` `=` $sync_after_distribute^)?
    (`warp_size` `=` $warp_size^)?
    attr-dict
    `:` functional-type($target, $result)
  }];
  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::Operation *target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

def MapForallToBlocks :
  Op<Transform_Dialect, "gpu.map_forall_to_blocks",
    [FunctionalStyleTransformOpTrait,
     MemoryEffectsOpInterface,
     TransformOpInterface,
     TransformEachOpTrait]> {
  let description = [{
    Target the gpu_launch op and rewrite the top level `scf.forall`
    to distributed gpu.block_id attribute. If `generate_gpu_launch` attribute
    is set, then first generates `gpu_launch` and moves the top level
    `scf.forall` inside.

    The operation searches top level `scf.forall` ops under
    `gpu_launch` and maps each such op to GPU blocks. Mapping is
    one-to-one and the induction variables of `scf.forall` are
    rewritten to gpu.block_id according to the `thread_dim_mapping` attribute.

    Dynamic, `scf.forall` trip counts are currently not supported.
    Dynamic block dim sizes are currently not supported.

    Only **bufferized** scf.forall are currently supported.
    Only scf.forall distributed to **at most 3 dimensions** are
    currently supported.

    The operation alters the block size of the given gpu_launch using the 
    grid_dims argument.

    #### Return modes:

    This operation ignores non-gpu_launch ops and drops them in the return.

    If any scf.forall with tensors is found, the transform definitely
    fails.

    If all the scf.forall operations contained within the LaunchOp
    referred to by the `target` PDLOperation lower to GPU properly, the
    transform succeeds. Otherwise the transform definitely fails.

    The returned handle points to the same LaunchOp operand, consuming it and
    producing a new SSA value to satisfy chaining and linearity of the IR
    properties.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                   DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:$grid_dims,
                   UnitAttr:$generate_gpu_launch);
  let results = (outs TransformHandleTypeInterface:$result);

  let assemblyFormat = [{
    $target
    (`generate_gpu_launch` $generate_gpu_launch^)?
    (`grid_dims` `=` $grid_dims^)?
    attr-dict
    `:` functional-type($target, $result)
  }];
  let hasVerifier = 1;
  
  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
        ::mlir::transform::TransformRewriter &rewriter,
        ::mlir::Operation *target,
        ::mlir::transform::ApplyToEachResultList &results,
        ::mlir::transform::TransformState &state);
  }];
}

#endif // GPU_TRANSFORM_OPS


//===- VectorTransformBase.td - Vector transform ops --------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef VECTOR_TRANSFORMS_BASE
#define VECTOR_TRANSFORMS_BASE

include "mlir/IR/EnumAttr.td"

// Lower transpose into element-wise extract and inserts.
def VectorTransposeLowering_Elementwise:
  I32EnumAttrCase<"EltWise",  0, "eltwise">;
// Lower 2-D transpose to `vector.flat_transpose`, maps 1-1 to LLVM matrix
// intrinsics.
def VectorTransposeLowering_FlatTranspose:
  I32EnumAttrCase<"Flat",  1, "flat_transpose">;
// Lower 2-D transpose to `vector.shuffle` on 1-D vector.
def VectorTransposeLowering_Shuffle1D:
  I32EnumAttrCase<"Shuffle1D",  2, "shuffle_1d">;
// Lower 2-D transpose to `vector.shuffle` on 16x16 vector.
def VectorTransposeLowering_Shuffle16x16:
  I32EnumAttrCase<"Shuffle16x16",  3, "shuffle_16x16">;
def VectorTransposeLoweringAttr : I32EnumAttr<
    "VectorTransposeLowering",
    "control the lowering of `vector.transpose` operations.",
    [VectorTransposeLowering_Elementwise, VectorTransposeLowering_FlatTranspose,
     VectorTransposeLowering_Shuffle1D, VectorTransposeLowering_Shuffle16x16]> {
  let cppNamespace = "::mlir::vector";
}

// Lower multi_reduction into outer-reduction and inner-parallel ops.
def VectorMultiReductionLowering_InnerParallel:
  I32EnumAttrCase<"InnerParallel", 0, "innerparallel">;
// Lower multi_reduction into outer-parallel and inner-reduction ops.
def VectorMultiReductionLowering_InnerReduction:
  I32EnumAttrCase<"InnerReduction", 1, "innerreduction">;
def VectorMultiReductionLoweringAttr: I32EnumAttr<
    "VectorMultiReductionLowering",
    "control the lowering of `vector.multi_reduction`.",
  [VectorMultiReductionLowering_InnerParallel,
   VectorMultiReductionLowering_InnerReduction]> {
  let cppNamespace = "::mlir::vector";
}

// Progressively lower to finer grained `vector.contract` and dot-products.
def VectorContractLowering_Dot: I32EnumAttrCase<"Dot", 0, "dot">;
// Lower to `vector.matrix_multiply`, maps 1-1 to LLVM matrix intrinsics.
def VectorContractLowering_Matmul:
  I32EnumAttrCase<"Matmul", 1, "matmulintrinsics">;
// Lower to `vector.outerproduct`.
def VectorContractLowering_OuterProduct:
  I32EnumAttrCase<"OuterProduct", 2, "outerproduct">;
// Lower contract with all reduction dimensions unrolled to 1 to a vector
// elementwise operations.
def VectorContractLowering_ParallelArith:
  I32EnumAttrCase<"ParallelArith", 3, "parallelarith">;
def VectorContractLoweringAttr: I32EnumAttr<
    "VectorContractLowering",
    "control the lowering of `vector.contract` operations.",
  [VectorContractLowering_Dot, VectorContractLowering_Matmul,
   VectorContractLowering_OuterProduct, VectorContractLowering_ParallelArith]> {
  let cppNamespace = "::mlir::vector";
}

// Do not split vector transfer operations.
def VectorTransferSplit_None: I32EnumAttrCase<"None", 0, "none">;
// Split using in-bounds + out-of-bounds vector.transfer operations.
def VectorTransferSplit_VectorTransfer:
  I32EnumAttrCase<"VectorTransfer", 1, "vector-transfer">;
// Split using an in-bounds vector.transfer + linalg.fill + linalg.copy
// operations.
def VectorTransferSplit_LinalgCopy:
  I32EnumAttrCase<"LinalgCopy", 2, "linalg-copy">;
// Do not split vector transfer operation but instead mark it as "in-bounds".
def VectorTransferSplit_ForceInBounds:
  I32EnumAttrCase<"ForceInBounds", 3, "force-in-bounds">;
def VectorTransferSplitAttr: I32EnumAttr<
    "VectorTransferSplit",
    "control the splitting of `vector.transfer` operations into in-bounds"
    " and out-of-bounds variants.",
  [VectorTransferSplit_None, VectorTransferSplit_VectorTransfer,
   VectorTransferSplit_LinalgCopy, VectorTransferSplit_ForceInBounds]> {
  let cppNamespace = "::mlir::vector";
}
#endif


//===- TensorOps.td - Tensor op definitions ----------------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef TENSOR_OPS
#define TENSOR_OPS

include "mlir/Dialect/Tensor/IR/TensorBase.td"
include "mlir/Interfaces/CastInterfaces.td"
include "mlir/Interfaces/ControlFlowInterfaces.td"
include "mlir/Interfaces/DestinationStyleOpInterface.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/ParallelCombiningOpInterface.td"
include "mlir/Interfaces/ShapedOpInterfaces.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/Interfaces/TilingInterface.td"
include "mlir/Interfaces/ViewLikeInterface.td"
include "mlir/IR/OpAsmInterface.td"

class Tensor_Op<string mnemonic, list<Trait> traits = []>
    : Op<Tensor_Dialect, mnemonic, traits>;

// Base class for ops with static/dynamic offset, sizes and strides
// attributes/arguments.
class Tensor_OpWithOffsetSizesAndStrides<string mnemonic,
                                         list<Trait> traits = []>
    : Tensor_Op<mnemonic, traits> {
  code extraBaseClassDeclaration = [{
    /// Return the type of the base tensor operand.
    ::mlir::RankedTensorType getSourceType() {
      return ::llvm::cast<RankedTensorType>(getSource().getType());
    }

    /// Return the type of the result tensor.
    ::mlir::RankedTensorType getResultType() {
      return ::llvm::cast<RankedTensorType>(getResult().getType());
    }

    /// Return the dynamic sizes for this subview operation if specified.
    ::mlir::Operation::operand_range getDynamicSizes() { return getSizes(); }

    /// Return the list of Range (i.e. offset, size, stride). Each
    /// Range entry contains either the dynamic value or a ConstantIndexOp
    /// constructed with `b` at location `loc`.
    ::mlir::SmallVector<::mlir::Range, 8> getOrCreateRanges(
        ::mlir::OpBuilder &b, ::mlir::Location loc) {
      return ::mlir::getOrCreateRanges(*this, b, loc);
    }
  }];
}

//===----------------------------------------------------------------------===//
// BitcastOp
//===----------------------------------------------------------------------===//

def Tensor_BitcastOp : Tensor_Op<"bitcast", [
    DeclareOpInterfaceMethods<CastOpInterface>,
    Pure
  ]> {
  let summary = "tensor bitcast operation";
  let description = [{
    Bitcast a tensor from one type to another type of equivalent element width.
    If both are ranked, then the rank should be the same and static dimensions
    should match.

    Example:

    ```mlir
    // Bitcast from unsigned to signed or signless integer.
    %2 = tensor.bitcast %1 : tensor<4xui32> to tensor<4xi32>
    ```
  }];

  let arguments = (ins AnyTensor:$source);
  let results = (outs AnyTensor:$dest);
  let assemblyFormat = "$source attr-dict `:` type($source) `to` type($dest)";

  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// CastOp
//===----------------------------------------------------------------------===//

def Tensor_CastOp : Tensor_Op<"cast", [
    DeclareOpInterfaceMethods<CastOpInterface>,
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    Pure
  ]> {
  let summary = "tensor cast operation";
  let description = [{
    Convert a tensor from one type to an equivalent type without changing any
    data elements. The source and destination types must both be tensor types
    with the same element type. If both are ranked, then the rank should be the
    same and static dimensions should match. The operation is invalid if
    converting to a mismatching constant dimension.

    Example:

    ```mlir
    // Convert from unknown rank to rank 2 with unknown dimension sizes.
    %2 = tensor.cast %1 : tensor<*xf32> to tensor<?x?xf32>

    // Convert to a type with more known dimensions.
    %3 = tensor.cast %2 : tensor<?x?xf32> to tensor<4x?xf32>

    // Discard static dimension and rank information.
    %4 = tensor.cast %3 : tensor<4x?xf32> to tensor<?x?xf32>
    %5 = tensor.cast %4 : tensor<?x?xf32> to tensor<*xf32>
    ```
  }];

  let arguments = (ins AnyTensor:$source);
  let results = (outs AnyTensor:$dest);
  let assemblyFormat = "$source attr-dict `:` type($source) `to` type($dest)";

  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// ConcatOp
//===----------------------------------------------------------------------===//

def Tensor_ConcatOp : Tensor_Op<"concat",
    [Pure,
     DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
     DeclareOpInterfaceMethods<ReifyRankedShapedTypeOpInterface>]> {
  let summary = "tensor concatenation operation";
  let description = [{
    The "concat" operation constructs a tensor out of a variadic list of input
    tensors, concatenated along a static dimension number. All inputs and the
    result type must share the same rank.

    `dim` specifies the dimension along which to concatenate. The size of the
    concatenated dimension in the result must be equal to the sum of the sizes
    of the inputs along that dimension. All other dimensions in both the inputs
    and result must be the same size.

    Example:

    ```mlir
    %0 = tensor.concat dim(0) %0, %1, %2 :
        (tensor<3x6xf32>, tensor<3x6xf32>, tensor<1x6xf32) -> tensor<7x6xf32>

    // Dynamic + dynamic -> static
    %0 = tensor.concat dim(1) %0, %1, %2 :
        (tensor<3x?xf32>, tensor<3x2xf32>, tensor<3x?xf32) -> tensor<3x10xf32>
    ```
  }];
  let arguments = (ins I64Attr:$dim,
                       Variadic<AnyRankedTensor>:$inputs);
  let results = (outs AnyRankedTensor:$result);
  let assemblyFormat = [{
    `dim` `(` $dim `)` $inputs attr-dict
    `:` functional-type(operands, results)
  }];

  let builders = [
    // Builder with an inferred result type.
    OpBuilder<(ins "int64_t":$dim, "ValueRange":$inputs)>,
  ];

  let extraClassDeclaration = [{
    // Helper to infer the concatenated result type for the given list of input
    // types, being concatenated along `dim`. Because concatenation can specify
    // more static information than can automatically be inferred,
    // InferTypeOpInterface is not used.
    static RankedTensorType inferResultType(int64_t dim, TypeRange inputTypes);

    RankedTensorType getResultType() {
      return ::llvm::cast<RankedTensorType>(getResult().getType());
    }

    int64_t getRank() {
      return ::llvm::cast<RankedTensorType>(getResult().getType()).getRank();
    }

    // Method to decompose the operation into a sequence of insert_slices.
    FailureOr<SmallVector<Value>> decomposeOperation(OpBuilder &builder);
  }];

  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// DimOp
//===----------------------------------------------------------------------===//

def Tensor_DimOp : Tensor_Op<"dim", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    ConditionallySpeculatable, NoMemoryEffect,
    ShapedDimOpInterface]> {
  let summary = "dimension index operation";
  let description = [{
    The `tensor.dim` operation takes a tensor and a dimension operand of type
    `index`. It returns the size of the requested dimension of the given
    tensor. If the dimension index is out of bounds, the behavior is undefined.

    The specified tensor type is that of the first operand.

    Example:

    ```mlir
    // Always returns 4, can be constant folded:
    %c0 = arith.constant 0 : index
    %x = tensor.dim %A, %c0 : tensor<4x?xf32>

    // Return the dynamic dimension of %A.
    %c1 = arith.constant 1 : index
    %y = tensor.dim %A, %c1 : tensor<4x?xf32>

    // Equivalent generic form:
    %x = "tensor.dim"(%A, %c0) : (tensor<4x?xf32>, index) -> index
    %y = "tensor.dim"(%A, %c1) : (tensor<4x?xf32>, index) -> index
    ```
  }];

  let arguments = (ins AnyNon0RankedOrUnrankedTensor:$source,
                       Index:$index);
  let results = (outs Index:$result);

  let assemblyFormat = [{
    attr-dict $source `,` $index `:` type($source)
  }];

  let builders = [
    OpBuilder<(ins "Value":$source, "int64_t":$index)>
  ];

  let extraClassDeclaration = [{
    /// Helper function to get the index as a simple integer if it is constant.
    std::optional<int64_t> getConstantIndex();

    /// Interface method of ShapedDimOpInterface: Return the source tensor.
    Value getShapedValue() { return getSource(); }

    /// Interface method of ShapedDimOpInterface: Return the dimension.
    OpFoldResult getDimension() { return getIndex(); }

    /// Interface method for ConditionallySpeculatable.
    Speculation::Speculatability getSpeculatability();
  }];

  let hasCanonicalizer = 1;
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// EmptyOp
//===----------------------------------------------------------------------===//

def Tensor_EmptyOp : Tensor_Op<"empty",
    [Pure,
     DeclareOpInterfaceMethods<ReifyRankedShapedTypeOpInterface>]> {
  let summary = "empty tensor operation";

  let description = [{
    `tensor.empty` is an operation that defines a tensor of a particular shape.
    The shape could be dynamic or static. The contents of the tensor are
    unspecified and the only purpose of the op result is to materialize the
    specified shape in IR and make it available to other transformations.

    `tensor.empty` is useful in transformations that expect destination style
    ops. I.e., ops that implement `DestinationStyleOpInterface`. Ops that are
    not in destination style can be made compatible with such transformations
    with a `tensor.empty` destination.

    Note: This op can be lowered to a `bufferization.alloc_tensor`, at which
    point it turns into an explicit buffer allocation.
  }];

  let arguments = (ins Variadic<Index>:$dynamicSizes);

  let results = (outs AnyRankedTensor:$result);

  let assemblyFormat = "`(`$dynamicSizes`)` attr-dict `:` type($result)";

  let extraClassDeclaration = [{
    RankedTensorType getType() {
      return ::llvm::cast<RankedTensorType>(getResult().getType());
    }

    // Return both static and dynamic sizes as a list of `OpFoldResult`.
    SmallVector<OpFoldResult> getMixedSizes();

    // Return the Value of the dynamic size of the tensor at dimension `idx`.
    // Asserts that the shape is dynamic at that `idx`.
    Value getDynamicSize(unsigned idx);
  }];

  let builders = [
    // Build with fully static sizes.
    OpBuilder<(ins "ArrayRef<int64_t>":$staticShape, "Type":$elementType,
                   CArg<"Attribute", "{}">:$encoding)>,

    // Build with mixed static/dynamic sizes.
    OpBuilder<(ins "ArrayRef<int64_t>":$staticShape, "Type":$elementType,
                   "ValueRange":$dynamicSizes,
                   CArg<"Attribute", "{}">:$encoding)>,

    // Build with mixed static/dynamic sizes.
    OpBuilder<(ins "ArrayRef<OpFoldResult>":$sizes, "Type":$elementType,
                   CArg<"Attribute", "{}">:$encoding)>
  ];

  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// ExtractOp
//===----------------------------------------------------------------------===//

def Tensor_ExtractOp : Tensor_Op<"extract", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    Pure,
    TypesMatchWith<"result type matches element type of tensor",
                   "tensor", "result",
                   "::llvm::cast<TensorType>($_self).getElementType()">]> {
  let summary = "element extraction operation";
  let description = [{
    The `tensor.extract` op reads a ranked tensor and returns one element as
    specified by the given indices. The result of the op is a value with the
    same type as the elements of the tensor. The arity of indices must match
    the rank of the accessed value. All indices should all be of `index` type.

    Example:

    ```mlir
    %4 = tensor.extract %t[%1, %2] : tensor<4x4xi32>
    %5 = tensor.extract %rt[%1, %2] : tensor<?x?xi32>
    ```
  }];

  let arguments = (ins AnyRankedTensor:$tensor, Variadic<Index>:$indices);
  let results = (outs AnyType:$result);
  let assemblyFormat = "$tensor `[` $indices `]` attr-dict `:` type($tensor)";

  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}


//===----------------------------------------------------------------------===//
// ExtractSliceOp
//===----------------------------------------------------------------------===//

def Tensor_ExtractSliceOp : Tensor_OpWithOffsetSizesAndStrides<"extract_slice", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    DeclareOpInterfaceMethods<ReifyRankedShapedTypeOpInterface>,
    AttrSizedOperandSegments,
    Pure,
    OffsetSizeAndStrideOpInterface
  ]> {
  let summary = "extract slice operation";
  let description = [{
    The "extract_slice" operation extract a tensor from another tensor as
    specified by the operation's offsets, sizes and strides arguments.

    The extract_slice operation supports the following arguments:

    * source: the "base" tensor from which to extract a slice.
    * offsets: tensor-rank number of offsets into the "base" tensor from which
               to extract the slice.
    * sizes: tensor-rank number of sizes which specify the sizes of the result
             tensor type.
    * strides: tensor-rank number of strides specifying subsampling in each
               dimension.

    The representation based on offsets, sizes and strides support a
    partially-static specification via attributes specified through the
    `static_offsets`, `static_sizes` and `static_strides` arguments. A special
    sentinel value ShapedType::kDynamic encodes that the corresponding entry has
    a dynamic value.

    After buffer allocation, the "extract_slice" op is expected to lower into a
    memref.subview op.

    An extract_slice operation may additionally reduce the rank of the resulting
    tensor by removing dimensions that are statically known to be of size 1.
    This rank-reduction behavior is not required by the op semantics: this
    flexibility allows to progressively drop unit dimensions while lowering
    between different flavors of ops on that operate on tensors.

    #### Verification vs Inference in the rank-reduced case

    Note that there may be multiple ways to infer a resulting rank-reduced type.
      e.g. 1x6x1 could potentially rank-reduce to either 1x6 or 6x1 2-D shapes.

    To disambiguate, the inference helpers `inferCanonicalRankReducedResultType`
    only drop the first unit dimensions, in order:
      e.g. 1x6x1 rank-reduced to 2-D will infer the 6x1 2-D shape, but not 1x6.

    Verification however has access to result type and does not need to infer.
    The verifier calls `isRankReducedType(getSource(), getResult())` to
    determine whether the result type is rank-reduced from the source type.
    This computes a so-called rank-reduction mask, consisting of dropped unit
    dims, to map the rank-reduced type to the source type by dropping ones:
      e.g. 1x6 is a rank-reduced version of 1x6x1 by mask {2}
           6x1 is a rank-reduced version of 1x6x1 by mask {0}
           1x2x1x4 is a rank-reduced version of 1x1x2x1x1x4x1 by mask {1, 4, 6}
             (remaining common 1 dimensions are matched eagerly)

    Example:

    ```mlir
    // Rank-reducing extract_slice.
    %1 = tensor.extract_slice %0[0, 0, 0][1, 16, 4][1, 1, 1] :
      tensor<8x16x4xf32> to tensor<16x4xf32>
    %3 = tensor.extract_slice %2[%o0, 4, %o2][1, %sz1, 1][1, %st1, 1] :
      tensor<8x16x4xf32> to tensor<1x?xf32>
    ```
  }];

  let arguments = (ins
    AnyRankedTensor:$source,
    Variadic<Index>:$offsets,
    Variadic<Index>:$sizes,
    Variadic<Index>:$strides,
    DenseI64ArrayAttr:$static_offsets,
    DenseI64ArrayAttr:$static_sizes,
    DenseI64ArrayAttr:$static_strides
  );
  let results = (outs AnyRankedTensor:$result);

  let assemblyFormat = [{
    $source ``
    custom<DynamicIndexList>($offsets, $static_offsets)
    custom<DynamicIndexList>($sizes, $static_sizes)
    custom<DynamicIndexList>($strides, $static_strides)
    attr-dict `:` type($source) `to` type($result)
  }];

  let builders = [
    // Build an ExtractSliceOp with mixed static and dynamic entries and
    // inferred result type.
    OpBuilder<(ins "Value":$source, "ArrayRef<OpFoldResult>":$offsets,
      "ArrayRef<OpFoldResult>":$sizes, "ArrayRef<OpFoldResult>":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build an ExtractSliceOp with mixed static and dynamic entries and custom
    // result type. If the type passed is nullptr, it is inferred.
    OpBuilder<(ins "RankedTensorType":$resultType, "Value":$source,
      "ArrayRef<OpFoldResult>":$offsets, "ArrayRef<OpFoldResult>":$sizes,
      "ArrayRef<OpFoldResult>":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build an ExtractSliceOp with dynamic entries and custom result type. If
    // the type passed is nullptr, it is inferred.
    OpBuilder<(ins "Value":$source, "ValueRange":$offsets,
      "ValueRange":$sizes, "ValueRange":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build an ExtractSliceOp with dynamic entries and inferred result type.
    OpBuilder<(ins "RankedTensorType":$resultType, "Value":$source,
      "ValueRange":$offsets, "ValueRange":$sizes, "ValueRange":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build an ExtractSliceOp with mixed static and dynamic entries packed in
    // a Range vector.
    OpBuilder<(ins "Value":$source, "ArrayRef<Range>":$ranges,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
  ];

  let extraClassDeclaration = extraBaseClassDeclaration # [{
    /// The result of an extract_slice is always a tensor.
    // TODO: deprecate
    RankedTensorType getType() {
      return getResultType();
    }

    /// Compute the rank-reduction mask that can be applied to map the source
    /// tensor type to the result tensor type by dropping unit dims.
    std::optional<llvm::SmallDenseSet<unsigned>>
    computeRankReductionMask() {
      return ::mlir::computeRankReductionMask(getSourceType().getShape(),
                                              getType().getShape());
    };

    /// An extract_slice result type can be inferred, when it is not
    /// rank-reduced, from the source type and the static representation of
    /// offsets, sizes and strides. Special sentinels encode the dynamic case.
    static RankedTensorType inferResultType(
      RankedTensorType sourceTensorType,
      ArrayRef<int64_t> staticOffsets,
      ArrayRef<int64_t> staticSizes,
      ArrayRef<int64_t> staticStrides);
    static RankedTensorType inferResultType(
      RankedTensorType sourceTensorType,
      ArrayRef<OpFoldResult> staticOffsets,
      ArrayRef<OpFoldResult> staticSizes,
      ArrayRef<OpFoldResult> staticStrides);

    /// If the rank is reduced (i.e. the desiredResultRank is smaller than the
    /// number of sizes), drop as many size 1 as needed to produce an inferred type
    /// with the desired rank.
    ///
    /// Note that there may be multiple ways to compute this rank-reduced type:
    ///   e.g. 1x6x1 can rank-reduce to either 1x6 or 6x1 2-D tensors.
    ///
    /// To disambiguate, this function always drops the first 1 sizes occurrences.
    static RankedTensorType inferCanonicalRankReducedResultType(
      unsigned resultRank,
      RankedTensorType sourceRankedTensorType,
      ArrayRef<int64_t> staticOffsets,
      ArrayRef<int64_t> staticSizes,
      ArrayRef<int64_t> staticStrides);
    static RankedTensorType inferCanonicalRankReducedResultType(
      unsigned resultRank,
      RankedTensorType sourceRankedTensorType,
      ArrayRef<OpFoldResult> staticOffsets,
      ArrayRef<OpFoldResult> staticSizes,
      ArrayRef<OpFoldResult> staticStrides);

    /// Return the expected rank of each of the`static_offsets`, `static_sizes`
    /// and `static_strides` attributes.
    std::array<unsigned, 3> getArrayAttrMaxRanks() {
      unsigned rank = getSourceType().getRank();
      return {rank, rank, rank};
    }

    /// Return the number of leading operands before the `offsets`, `sizes` and
    /// and `strides` operands.
    static unsigned getOffsetSizeAndStrideStartOperandIndex() { return 1; }

    /// Return the dimensions of the source that are dropped in the
    /// result when the result is rank-reduced.
    llvm::SmallBitVector getDroppedDims();

    /// Given a `value`, asserted to be of RankedTensorType, build an
    /// ExtractSliceOp that results in a rank-reducing extract to the desired
    /// tensor shape and return the new value created.
    /// If the shape of `value` is already the `desiredShape`, just return
    /// `value`.
    /// If the shape of `value` cannot be rank-reduced to `desiredShape`, fail.
    static FailureOr<Value> rankReduceIfNeeded(
      OpBuilder &b, Location loc, Value value, ArrayRef<int64_t> desiredShape);
  }];

  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// FromElementsOp
//===----------------------------------------------------------------------===//

def Tensor_FromElementsOp : Tensor_Op<"from_elements", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    Pure,
    TypesMatchWith<"operand types match result element type",
                   "result", "elements", "SmallVector<Type, 2>("
                   "::llvm::cast<RankedTensorType>($_self).getNumElements(), "
                   "::llvm::cast<RankedTensorType>($_self).getElementType())">
  ]> {
  let summary = "tensor from elements operation.";
  let description = [{
    Create a N-D tensor from a range of same-type arguments. The number of
    provided `elements` should equal to the number of the elements in the
    result type. The `elements` correspond to a flattened tensor.

    Example:

    ```mlir
    tensor.from_elements %a, %b, %c, %d, %e, %f :  tensor<2x3xindex>
    ```

    will result in a tensor

    [[%a, %b, %c]
     [%d, %e, %f]]
  }];

  let arguments = (ins Variadic<AnyType>:$elements);
  let results = (outs AnyStaticShapeTensor:$result);

  let assemblyFormat = "$elements attr-dict `:` type($result)";

  let builders = [
    // Special case builder for when `elements` has size >=1.
    OpBuilder<(ins "ValueRange":$elements)>
  ];

  let hasCanonicalizer = 1;
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// GatherOp
//===----------------------------------------------------------------------===//

def Tensor_GatherOp : Tensor_Op<"gather", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    Pure
  ]> {
  let summary = "gather a subset of a tensor at specified indices";
  let description = [{
    The `gather` operation extracts a subset of the elements from a `source`
    tensor at the given indices.

    In its most general form, the tensor of indices specifies all the coordinates
    of every element to extract (i.e. COO format, without the payload).
    The indices are expected to be confined to coordinate values that fit the
    range of the `source` tensor, otherwise the behavior is undefined.

    The leading dimensions of the index tensor give the result tensor its leading
    dimensions. The trailing dimensions of the result tensor are obtained from
    the source tensor by omitting the dimensions specified in `gather_dims`
    (rank-reducing semantics) or setting them to `1` (rank-preserving semantics)
    (see examples).
    The trailing dimension of the index tensor contains the coordinates and is
    expected to have its size equal to the number of dimensions being gathered.
    This convention allows an idiomatic specification and lowering of "gathering
    multiple N-D slices from the source tensor".

    Note: in the examples below, we separate out the indexing part of the tensor
    type by a whitespace for readability purposes.

    Example:

    ```mlir
        // For each 1x2 triple of coordinates in %indices, extract the
        // element (i.e. 0-D subset) at the coordinates triple in %source.
        //
        %out = tensor.gather %source[%indices] gather_dims([0, 1, 2]) :
          (tensor<4x4x4xf32>, tensor<1x2x 3xindex>) -> tensor<1x2x 1x1x1xf32>

        // Note: result type may be further rank-reduced to tensor<1x2x f32>.
    ```

    A slice variant is provided to allow specifying whole slices of the source
    tensor.

    Example:

    ```mlir
        // For each 5x6 singleton of coordinates in %indices, extract the 2-D
        // slice %source[*, %indices[...]:%indices[...] + 1, *] with the indices
        // corresponding to the `gather_dims` attribute specified by %indices.
        //
        %out = tensor.gather %source[%indices] gather_dims([1]) :
          (tensor<3x4x5xf32>, tensor<6x7x 1xindex>) -> tensor<6x7x 3x1x5xf32>

        // Note: result type may be further rank-reduced to tensor<6x7x 3x5xf32>.
    ```

    The dimensions specified in the gather_dims attribute are ones for which the
    result tensor has size `1`.
    I.e. if the source type is `axbxcxd` and the coordinates are [1, 3], then
    the shape suffix is `ax1xcx1`.
    Gather also allows rank-reducing semantics where the shape `ax1xcx1` can be
    further simplified to `axc`.

    The elemental type of the indices tensor can be any integer type.
    In the absence of target-specific or problem specific information the default
    type one should use is `index`.

    This operation does not support unranked tensors.

    An optional `unique` unit attribute may be specified to indicate that the
    coordinates in `indices` are statically guaranteed to be unique at runtime.
    Incorrectly setting the `unique` attribute when the coordinates are not truly
    unique is undefined behavior.

    Only full slices are meant to be supported by this op, if one desires
    partial slices (e.g. strided windows) one should compose this op with other
    tensor ops (e.g. tensor.extract_slice). This is to avoid a slippery slope of
    complexity that would make the op unusable in practice.

    At the tensor-level, the index tensor is specified in an AoS form (i.e.
    coordinate tuple is the most minor). It is the responsibility of further
    lowerings and bufferization to implement various concrete layouts.

    Note: As currently specified, the operation must lower to an abstraction that
    performs copies to the output tensor. This is because the buffer type system
    is currently not rich enough to allow multiple non-contiguous views in the
    same type. This is visible more clearly in a notional buffer version of the
    op:

    ```mlir
        // memref<?x4x1xf32> is a contiguous buffer of ?x4x1 elements.
        // gather from random source slices must copy to the contiguous output.
        %out = memref.gather %source[%indices] gather_dims([1]) :
          (memref<4x4xf32>, memref<?x 1xindex>) -> memref<?x 4x1xf32>

        // Nested buffer support would allow gather to directly index into the
        // source buffer (i.e. represent a jagged view into the source).
        %out = memref.gather %source[%indices] gather_dims([1]) :
          (memref<4x4xf32>, memref<?x 1xindex>) -> memref<? x memref<4x1xf32>>
    ```
  }];

  let arguments = (ins AnyRankedTensor:$source,
                       RankedTensorOf<[AnySignlessIntegerOrIndex]>:$indices,
                       DenseI64ArrayAttr:$gather_dims,
                       UnitAttr:$unique);
  let results = (outs AnyRankedTensor:$result);

  let assemblyFormat = [{
    $source `[` $indices `]`
      `gather_dims` `(` $gather_dims `)`
      (`unique` $unique^)?
      attr-dict
    `:` functional-type(operands, results)
  }];

  let extraClassDeclaration = [{
    // TODO: InferTypeOpInterface once enough confidence is built with
    // tensor<tensor> and its lowering to memref<memref>.
    static RankedTensorType inferResultType(RankedTensorType sourceType,
                                            RankedTensorType indicesType,
                                            ArrayRef<int64_t> gatherDims,
                                            bool rankReduced);
    RankedTensorType getIndicesType() {
      return ::llvm::cast<RankedTensorType>(getIndices().getType());
    }
    RankedTensorType getSourceType() {
      return ::llvm::cast<RankedTensorType>(getSource().getType());
    }
    RankedTensorType getResultType() {
      return ::llvm::cast<RankedTensorType>(getResult().getType());
    }
  }];
  let hasVerifier = 1;
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// GenerateOp
//===----------------------------------------------------------------------===//

def Tensor_GenerateOp : Tensor_Op<"generate", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    RecursiveMemoryEffects,
    DeclareOpInterfaceMethods<ReifyRankedShapedTypeOpInterface>,
    SingleBlockImplicitTerminator<"mlir::tensor::YieldOp">]> {
  let summary = "Creates a dynamically sized tensor from elements";
  let description = [{
    This operation creates a dynamically sized tensor with elements of any type.
    It expects one index operand per dynamic extent of the result tensor.

    The body region defines the tensor's elements. It takes index operands as
    its region arguments that span the index space. The element at the given
    position is yielded with the `yield` operation (see `YieldOp`). There is
    no defined ordering to the invocations of the body. It is conceptually
    a "parallel map" operation.

    Example:

    ```mlir
      %tnsr = tensor.generate %m, %n {
      ^bb0(%i : index, %j : index, %k : index):
        ...
        yield %elem : f32
      } : tensor<?x3x?f32>
    ```
  }];

  let arguments = (ins Variadic<Index>:$dynamicExtents);
  let results = (outs AnyRankedTensor:$result);
  let regions = (region SizedRegion<1>:$body);
  let assemblyFormat = "$dynamicExtents $body attr-dict `:` type($result)";

  let builders = [
    // Build op and populate its body per callback function.
    OpBuilder<(ins "Type":$resultTy, "ValueRange":$dynamicExtents,
      "function_ref<void(OpBuilder &, Location, ValueRange)>")>,
  ];

  let hasCanonicalizer = 1;
  let hasVerifier = 1;
  let hasRegionVerifier = 1;
}

//===----------------------------------------------------------------------===//
// InsertOp
//===----------------------------------------------------------------------===//

def Tensor_InsertOp : Tensor_Op<"insert", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    DestinationStyleOpInterface,
    Pure,
    TypesMatchWith<"result type matches type of dest",
                   "dest", "result",
                   "$_self">,
    TypesMatchWith<"scalar type matches element type of dest",
                   "dest", "scalar",
                   "::llvm::cast<TensorType>($_self).getElementType()">]> {
  let summary = "element insertion operation";
  let description = [{
    The `tensor.insert` op inserts a scalar into a ranked tensor `dest` as
    specified by the operation's indices.

    It returns a copy of `dest` with the indexed position updated to the value
    of `scalar`.

    The arity of `indices `must match the rank of the tensor `dest`. All
    indices should be of `index` type.

    Example:

    ```mlir
    %4 = tensor.insert %t into %dest[%1, %2] : tensor<4x4xi32>
    %5 = tensor.insert %rt into %dest[%1, %2] : tensor<?x?xi32>
    ```
  }];

  let arguments = (ins AnyType:$scalar,
                       AnyRankedTensor:$dest,
                       Variadic<Index>:$indices);
  let results = (outs AnyRankedTensor:$result);
  let assemblyFormat = [{
    $scalar `into` $dest `[` $indices `]` attr-dict `:` type($dest)
  }];

  let extraClassDeclaration = [{
    MutableOperandRange getDpsInitsMutable() { return getDestMutable(); }
  }];

  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// InsertSliceOp
//===----------------------------------------------------------------------===//

def Tensor_InsertSliceOp : Tensor_OpWithOffsetSizesAndStrides<"insert_slice", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    DeclareOpInterfaceMethods<ReifyRankedShapedTypeOpInterface>,
    AttrSizedOperandSegments,
    DestinationStyleOpInterface,
    Pure,
    OffsetSizeAndStrideOpInterface,
    TypesMatchWith<"expected result type to match dest type",
                   "dest", "result", "$_self">
  ]> {
  let summary = "insert_slice operation";
  let description = [{
    The "insert_slice" operation insert a tensor `source` into another
    tensor `dest` as specified by the operation's offsets, sizes and strides
    arguments.

    It returns a copy of `dest` with the proper slice updated with the value
    of `source`.

    The insert_slice operation supports the following arguments:

    * source: the tensor that is inserted.
    * dest: the tensor into which the source tensor is inserted.
    * offsets: tensor-rank number of offsets into the `dest` tensor into which
               the slice is inserted.
    * sizes: tensor-rank number of sizes which specify the sizes of the source
             tensor type.
    * strides: tensor-rank number of strides that specify subsampling in each
               dimension.

    The representation based on offsets, sizes and strides support a
    partially-static specification via attributes specified through the
    `static_offsets`, `static_sizes` and `static_strides` arguments. A special
    sentinel value ShapedType::kDynamic encodes that the corresponding entry has
    a dynamic value.

    After buffer allocation, the "insert_slice" op is expected to lower into a
    memref.subview op.

    An insert_slice operation may additionally specify insertion into a tensor
    of higher rank than the source tensor, along dimensions that are statically
    known to be of size 1.
    This rank-altering behavior is not required by the op semantics: this
    flexibility allows to progressively drop unit dimensions while lowering
    between different flavors of ops on that operate on tensors.
    The rank-altering behavior of tensor.insert_slice matches the rank-reducing
    behavior of tensor.extract_slice.

    #### Verification in the rank-reduced case

    The same verification discussion and mechanisms apply as for ExtractSliceOp.
    Unlike ExtractSliceOp however, there is no need for a specific inference.

    Example:

    ```mlir
    // Rank-altering insert_slice.
    %1 = tensor.insert_slice %t into %0[0, 0, 0][1, 16, 4][1, 1, 1] :
      tensor<16x4xf32> into tensor<8x16x4xf32>
    %3 = tensor.insert_slice %tt into %2[%o0, 4, %o2][1, %sz1, 1][1, %st1, 1] :
      tensor<1x?xf32> into tensor<8x16x4xf32>
    ```
  }];

  let arguments = (ins
    AnyRankedTensor:$source,
    AnyRankedTensor:$dest,
    Variadic<Index>:$offsets,
    Variadic<Index>:$sizes,
    Variadic<Index>:$strides,
    DenseI64ArrayAttr:$static_offsets,
    DenseI64ArrayAttr:$static_sizes,
    DenseI64ArrayAttr:$static_strides
  );
  let results = (outs AnyRankedTensor:$result);

  let assemblyFormat = [{
    $source `into` $dest ``
    custom<DynamicIndexList>($offsets, $static_offsets)
    custom<DynamicIndexList>($sizes, $static_sizes)
    custom<DynamicIndexList>($strides, $static_strides)
    attr-dict `:` type($source) `into` type($dest)
  }];

  let builders = [
    // Build a InsertSliceOp with mixed static and dynamic entries and inferred
    // result type.
    OpBuilder<(ins "Value":$source, "Value":$dest,
      "ArrayRef<OpFoldResult>":$offsets, "ArrayRef<OpFoldResult>":$sizes,
      "ArrayRef<OpFoldResult>":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build a InsertSliceOp with dynamic entries and inferred result type.
    OpBuilder<(ins "Value":$source, "Value":$dest,
      "ValueRange":$offsets, "ValueRange":$sizes, "ValueRange":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build an InsertSliceOp with mixed static and dynamic entries packed in
    // a Range vector and inferred result type.
    OpBuilder<(ins "Value":$source, "Value":$dest,
      "ArrayRef<Range>":$ranges,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>
  ];

  let extraClassDeclaration = extraBaseClassDeclaration # [{
    /// The result of a insert_slice is always a tensor.
    // TODO: Deprecate this method.
    RankedTensorType getType() {
      return getResultType();
    }

    /// The `dest` type is the same as the result type.
    RankedTensorType getDestType() {
      return getResultType();
    }

    /// Return the expected rank of each of the`static_offsets`, `static_sizes`
    /// and `static_strides` attributes.
    std::array<unsigned, 3> getArrayAttrMaxRanks() {
      unsigned rank = getResultType().getRank();
      return {rank, rank, rank};
    }

    /// Return the dimensions of the dest that are omitted to insert a source
    /// when the result is rank-extended.
    llvm::SmallBitVector getDroppedDims();

    /// Return the number of leading operands before the `offsets`, `sizes` and
    /// and `strides` operands.
    static unsigned getOffsetSizeAndStrideStartOperandIndex() { return 2; }

    MutableOperandRange getDpsInitsMutable() { return getDestMutable(); }
  }];

  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// RankOp
//===----------------------------------------------------------------------===//

def Tensor_RankOp : Tensor_Op<"rank", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    Pure]> {
  let summary = "rank operation";
  let description = [{
    The `tensor.rank` operation takes a tensor operand and returns its rank.

    Example:

    ```mlir
    %0 = tensor.rank %arg0 : tensor<*xf32>
    %1 = tensor.rank %arg1 : tensor<?x?xf32>
    ```
  }];

  let arguments = (ins AnyTensor:$tensor);
  let results = (outs Index);

  let hasFolder = 1;
  let assemblyFormat = "$tensor attr-dict `:` type($tensor)";
}

//===----------------------------------------------------------------------===//
// ReshapeOp
//===----------------------------------------------------------------------===//

def Tensor_ReshapeOp: Tensor_Op<"reshape", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    Pure]>  {
  let summary = "tensor reshape operation";
  let description = [{
    The `reshape` operation converts a tensor from one type to an equivalent
    type with a provided shape. The source and destination types are compatible
    if both have the same element type, same number of elements. The following
    combinations are possible:

    a. Source type is ranked or unranked. Shape argument has static size.
    Result type is ranked.

    ```mlir
    // Reshape statically-shaped tensor.
    %dst = tensor.reshape %src(%shape)
             : (tensor<4x1xf32>, tensor<1xi32>) -> tensor<4xf32>
    %dst0 = tensor.reshape %src(%shape0)
             : (tensor<4x1xf32>, tensor<2xi32>) -> tensor<2x2xf32>
    // Flatten unranked tensor.
    %dst = tensor.reshape %src(%shape)
             : (tensor<*xf32>, tensor<1xi32>) -> tensor<?xf32>
    ```

    b. Source type is ranked or unranked. Shape argument has dynamic size.
    Result type is unranked.

    ```mlir
    // Reshape dynamically-shaped 1D tensor.
    %dst = tensor.reshape %src(%shape)
             : (tensor<?xf32>, tensor<?xi32>) -> tensor<*xf32>
    // Reshape unranked tensor.
    %dst = tensor.reshape %src(%shape)
             : (tensor<*xf32>, tensor<?xi32>) -> tensor<*xf32>
    ```
  }];

  let arguments = (ins
    AnyTensor:$source,
    TensorRankOf<[AnySignlessInteger, Index], [1]>:$shape
  );
  let results = (outs AnyTensor:$result);

  let builders = [OpBuilder<
     (ins "TensorType":$resultType, "Value":$operand, "Value":$shape), [{
       $_state.addOperands(operand);
       $_state.addOperands(shape);
       $_state.addTypes(resultType);
     }]>];

  let extraClassDeclaration = [{
    TensorType getResultType() { return ::llvm::cast<TensorType>(getResult().getType()); }
  }];

  let assemblyFormat = [{
    $source `(` $shape `)` attr-dict `:` functional-type(operands, results)
  }];
  let hasVerifier = 1;
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// ExpandShapeOp / CollapseShapeOp
//===----------------------------------------------------------------------===//

class Tensor_ReassociativeReshapeOp<string mnemonic, list<Trait> traits = []> :
    Tensor_Op<mnemonic, !listconcat(traits, [
      DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
      Pure])>,
    Results<(outs AnyTensor:$result)> {

  code commonExtraClassDeclaration = [{
    static StringRef getReassociationAttrStrName() { return "reassociation"; }
    SmallVector<AffineMap, 4> getReassociationMaps();
    SmallVector<ReassociationExprs, 4> getReassociationExprs();
    SmallVector<ReassociationIndices, 4> getReassociationIndices() {
      SmallVector<ReassociationIndices, 4> reassociationIndices;
      for (auto attr : getReassociation())
        reassociationIndices.push_back(llvm::to_vector<2>(
            llvm::map_range(::llvm::cast<ArrayAttr>(attr), [&](Attribute indexAttr) {
              return ::llvm::cast<IntegerAttr>(indexAttr).getInt();
            })));
      return reassociationIndices;
    }
    RankedTensorType getSrcType() {
      return ::llvm::cast<RankedTensorType>(getSrc().getType());
    }
    RankedTensorType getResultType() {
      return ::llvm::cast<RankedTensorType>(getResult().getType());
    }
  }];

  let hasFolder = 1;
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

def Tensor_ExpandShapeOp : Tensor_ReassociativeReshapeOp<"expand_shape"> {
  let summary = "operation to produce a tensor with a higher rank";
  let description = [{
    The `tensor.expand_shape` op produces a tensor of higher (or equal)
    rank than the operand `src` whose dimension sizes are a reassociation of
    `src`.

    A reassociation is defined as a continuous grouping of dimensions and is
    represented with an array of DenseI64ArrayAttr attribute.  The reassociation
    maps applied to the result tensor with the higher rank must result in the
    operand tensor with the smaller rank.

    The representation for the output shape supports a partially-static
    specification via attributes specified through the `static_output_shape`
    argument.  A special sentinel value `ShapedType::kDynamic` encodes that the
    corresponding entry has a dynamic value.  There must be exactly as many SSA
    inputs in `output_shape` as there are `ShapedType::kDynamic` entries in
    `static_output_shape`.

    Example:

    ```mlir
    // Dimension expansion i -> (i', j') and (k) -> (k')
    %b = tensor.expand_shape %a [[0, 1], [2]] output_shape [%sz0, %sz1, 32]
        : tensor<?x32xf32> into tensor<?x?x32xf32>
    ```
  }];

  let arguments = (ins AnyTensor:$src, IndexListArrayAttr:$reassociation,
                       Variadic<Index>:$output_shape,
                       DenseI64ArrayAttr:$static_output_shape);

  let assemblyFormat = [{
    $src $reassociation `output_shape`
    custom<DynamicIndexList>($output_shape, $static_output_shape) attr-dict `:`
    type($src) `into` type($result)
  }];

  let builders = [
    // Builders using ReassociationIndices.
    OpBuilder<(ins "Type":$resultType, "Value":$src,
      "ArrayRef<ReassociationIndices>":$reassociation,
      "ArrayRef<OpFoldResult>":$outputShape)>,

    // It will infer output shape using inferOutputShape() method.
    OpBuilder<(ins "Type":$resultType, "Value":$src,
      "ArrayRef<ReassociationIndices>":$reassociation)>,

    // Builder using ReassociationExprs.
    OpBuilder<(ins "Type":$resultType, "Value":$src,
      "ArrayRef<ReassociationExprs>":$reassociation),
    [{
      auto reassociationIndices =
          convertReassociationMapsToIndices(reassociation);
      build($_builder, $_state, resultType, src, reassociationIndices);
    }]>,
    OpBuilder<(ins "Type":$resultType, "Value":$src,
      "ArrayRef<ReassociationExprs>":$reassociation,
      "ArrayRef<OpFoldResult>":$outputShape),
    [{
      auto reassociationIndices =
          convertReassociationMapsToIndices(reassociation);
      build($_builder, $_state, resultType, src, reassociationIndices,
            outputShape);
    }]>
  ];

  let extraClassDeclaration = commonExtraClassDeclaration # [{
    int64_t getCorrespondingSourceDim(int64_t resultDim);

    // Infer the output shape for a tensor.expand_shape when it is possible
    // to do so.
    static FailureOr<SmallVector<OpFoldResult>> inferOutputShape(
        OpBuilder &b, Location loc, RankedTensorType expandedType,
        ArrayRef<ReassociationIndices> reassociation,
        ArrayRef<OpFoldResult> inputShape);
  }];

  let hasVerifier = 1;
}

def Tensor_CollapseShapeOp : Tensor_ReassociativeReshapeOp<"collapse_shape"> {
  let summary = "operation to produce a tensor with a smaller rank";
  let arguments = (ins AnyTensor:$src, IndexListArrayAttr:$reassociation);
  let description = [{
    The `tensor.collapse_shape` op produces a new tensor of lower (or equal)
    rank whose dimension sizes are a reassociation of the original `src` dimensions.

    A reassociation is defined as a continuous grouping of dimensions and is
    represented by an array of DenseI64ArrayAttr attribute. The reassociation
    maps are applied to the operand shape to obtain the result shape.


    Example:

    ```mlir
    // Dimension collapse (i, j) -> i' and k -> k'
    %b = tensor.collapse_shape %a [[0, 1], [2]]
        : tensor<?x?x?xf32> into tensor<?x?xf32>
    ```
  }];

  let assemblyFormat = [{
    $src $reassociation attr-dict `:` type($src) `into` type($result)
  }];

  let builders = [
    // Builders for a contracting reshape whose result type is computed from
    // `src` and `reassociation`.
    OpBuilder<(ins "Value":$src,
      "ArrayRef<ReassociationIndices>":$reassociation,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    OpBuilder<(ins "Value":$src,
      "ArrayRef<ReassociationExprs>":$reassociation,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs),
    [{
      auto reassociationMaps =
          convertReassociationMapsToIndices(reassociation);
      build($_builder, $_state, src, reassociationMaps, attrs);
    }]>,

    // Builders for a reshape whose result type is passed explicitly.
    OpBuilder<(ins "Type":$resultType, "Value":$src,
      "ArrayRef<ReassociationIndices>":$reassociation,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs),
    [{
      $_state.addAttribute("reassociation",
          getReassociationIndicesAttribute($_builder, reassociation));
      build($_builder, $_state, resultType, src, attrs);
    }]>,
    OpBuilder<(ins "Type":$resultType, "Value":$src,
      "ArrayRef<ReassociationExprs>":$reassociation,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs),
    [{
      auto reassociationMaps =
          convertReassociationMapsToIndices(reassociation);
      build($_builder, $_state, resultType, src, reassociationMaps, attrs);
    }]>
  ];

  let extraClassDeclaration = commonExtraClassDeclaration # [{
    static RankedTensorType
    inferCollapsedType(RankedTensorType type, ArrayRef<AffineMap> reassociation);
    static RankedTensorType
    inferCollapsedType(RankedTensorType type,
                       SmallVector<ReassociationIndices> reassociation);
  }];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// PadOp
//===----------------------------------------------------------------------===//

def Tensor_PadOp : Tensor_Op<"pad", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    AttrSizedOperandSegments,
    Pure,
    SingleBlockImplicitTerminator<"mlir::tensor::YieldOp">]> {
  let summary = "tensor pad operation";
  let description = [{
    `tensor.pad` is an operation that pads the `source` tensor
    with given `low` and `high` padding config.

    The PadOp operation supports the following arguments:

    * source: the "base" tensor on which to pad.
    * low: A list contains the padding along the start of each
           dimension, i.e., how many padded values are prepended
           to the beginning of the tensor in each dimension.
    * high: A list contains the padding along the end of each
            dimension, i.e., how many padded values are appended
            to the end of the tensor in each dimension.
    * nofold: indicates that the operation should not be folded when source and
              result types are equal.

    The result tensor dimensions are `low[i]` + `dim[i]` + `high[i]` for each
    dimension `i`. The number of elements of `low` and `high` must match the
    rank of the input tensor. They can be either a constant or a dynamic value.

    The region of the `tensor.pad` operation returns the value to use
    for the padding. The arguments of the region represent the index
    of the source being accessed. There should be as many arguments as
    the rank of the `source` tensor. The value `yield`-ed by the
    region is used as the value of the view at the given position.

    If `nofold` is set, the padding operation will not be folded away even
    if the source type and the padded type have the same static shape. This can
    be used, e.g., for packing or promotion to faster memory.

    Example 1: add 3 zeros to the beginning and 5 zeros to the end of a 1D
    tensor.

    ```mlir
      %arg0 = ... : tensor<10xi32>
      %c0_i32 = arith.constant 0 : i32
      %padded = tensor.pad %arg0 low[3] high[5] {
      ^bb0(%arg1: index):
        tensor.yield %c0_i32 : i32
      } : tensor<10xi32> to tensor<18xi32>
    ```

    Example 2: add 1 value to the beginning of dimension 0, 2 values to the end
    of dimension 0, 2 values to the start of dimension 1, and 3 values to the
    end of dimension 1.

    ```mlir
      %pad_value = ... : f32
      %0 = tensor.pad %0 low[1, 2] high[2, 3] {
      ^bb0(%arg0 : index, %arg1 : index):
        tensor.yield %pad_value : f32
      } : tensor<?x?xf32> to tensor<?x?xf32>
    ```

    Example 3:

    ```mlir
      %pad_value = ... : f32
      %0 = tensor.pad %arg0 low[2, %arg1, 3, 3] high[3, 3, %arg1, 2] {
      ^bb0(%arg2: index, %arg3: index, %arg4: index, %arg5: index):
          tensor.yield %pad_value : f32
      } : tensor<1x2x2x?xf32> to tensor<6x?x?x?xf32>
    ```

    Example 4:

    ```mlir
      %pad_value = ... : f32
      %0 = tensor.pad %arg0 low[0, 0] high[%ub0, %ub1] {
      ^bb0(%arg1: index, %arg2: index):
        tensor.yield %pad_value : f32
      } : tensor<2x3xf32> to tensor<?x?xf32>
    ```

    Example 5: Force a padded value to be always exist with `nofold`, even
    though the padding config specifies that no new elements will be added to
    the tensor.

    ```mlir
      %pad_value = ... : f32
      %0 = tensor.pad %arg0 nofold low[0, 0] high[0, 0] {
      ^bb0(%arg1: index, %arg2: index):
        tensor.yield %pad_value : f32
      } : tensor<2x3xf32> to tensor<2x3xf32>
    ```
  }];

  let arguments = (ins
    AnyRankedTensor:$source,
    Variadic<Index>:$low,
    Variadic<Index>:$high,
    DenseI64ArrayAttr:$static_low,
    DenseI64ArrayAttr:$static_high,
    UnitAttr:$nofold);

  let regions = (region SizedRegion<1>:$region);

  let results = (outs AnyRankedTensor:$result);

  // TODO: Remove custom<InferType> when AllTypesMatch supports opt. operands.
  let assemblyFormat = [{
    $source
    (`nofold` $nofold^)?
    `low` `` custom<DynamicIndexList>($low, $static_low)
    `high` `` custom<DynamicIndexList>($high, $static_high)
    $region attr-dict `:` type($source) `to` type($result)
  }];

  let extraClassDeclaration = [{
    static StringRef getStaticLowAttrStrName() {
      return "static_low";
    }

    static StringRef getStaticHighAttrStrName() {
      return "static_high";
    }

    RankedTensorType getSourceType() {
      return ::llvm::cast<RankedTensorType>(getSource().getType());
    }
    RankedTensorType getResultType() {
      return ::llvm::cast<RankedTensorType>(getResult().getType());
    }

    // Infer the shape of the result tensor given the type of the source tensor
    // and paddings. Known result dimensions that cannot necessarily be inferred
    // from low/high padding sizes can be optionally specified. Those will be
    // considered when computing the result type.
    static RankedTensorType inferResultType(
                                RankedTensorType sourceType,
                                ArrayRef<int64_t> staticLow,
                                ArrayRef<int64_t> staticHigh,
                                ArrayRef<int64_t> resultShape = {});

    // Return the pad value if it is a constant. Return null value otherwise.
    Value getConstantPaddingValue();

    // Return a vector of all the static or dynamic values (low/high padding) of
    // the op.
    inline SmallVector<OpFoldResult> getMixedPadImpl(ArrayRef<int64_t> staticAttrs,
                                                     ValueRange values) {
      Builder builder(*this);
      SmallVector<OpFoldResult> res;
      unsigned numDynamic = 0;
      unsigned count = staticAttrs.size();
      for (unsigned idx = 0; idx < count; ++idx) {
        if (ShapedType::isDynamic(staticAttrs[idx]))
          res.push_back(getAsOpFoldResult(values[numDynamic++]));
        else
          res.push_back(builder.getI64IntegerAttr(staticAttrs[idx]));
      }
      return res;
    }
    SmallVector<OpFoldResult> getMixedLowPad() {
      return getMixedPadImpl(getStaticLow(), getLow());
    }
    SmallVector<OpFoldResult> getMixedHighPad() {
      return getMixedPadImpl(getStaticHigh(), getHigh());
    }
    // Return true if low padding is guaranteed to be 0.
    bool hasZeroLowPad() {
      return llvm::all_of(getMixedLowPad(), [](OpFoldResult ofr) {
        return getConstantIntValue(ofr) == static_cast<int64_t>(0);
      });
    }
    // Return true if high padding is guaranteed to be 0.
    bool hasZeroHighPad() {
      return llvm::all_of(getMixedHighPad(), [](OpFoldResult ofr) {
        return getConstantIntValue(ofr) == static_cast<int64_t>(0);
      });
    }
    /// Return the dimensions with a non-zero low or high padding.
    llvm::SmallBitVector getPaddedDims();
  }];

  let builders = [
    // Build a PadOp with mixed static and dynamic entries.
    OpBuilder<(ins "Type":$resultType, "Value":$source,
      "ArrayRef<int64_t>":$staticLow, "ArrayRef<int64_t>":$staticHigh,
      "ValueRange":$low, "ValueRange":$high, CArg<"bool", "false">:$nofold,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build a PadOp with all dynamic entries.
    OpBuilder<(ins "Type":$resultType, "Value":$source, "ValueRange":$low,
      "ValueRange":$high, CArg<"bool", "false">:$nofold,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build a PadOp with mixed static and dynamic entries and custom
    // result type. If the type passed is nullptr, it is inferred.
    OpBuilder<(ins "Type":$resultType, "Value":$source,
      "ArrayRef<OpFoldResult>":$low, "ArrayRef<OpFoldResult>":$high,
      CArg<"bool", "false">:$nofold,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build a PadOp with constant padding,  mixed static and dynamic entries
    // and custom result type. If the type passed is nullptr, it is inferred.
    OpBuilder<(ins "Type":$resultType, "Value":$source,
      "ArrayRef<OpFoldResult>":$low, "ArrayRef<OpFoldResult>":$high,
      "Value":$constantPadValue, CArg<"bool", "false">:$nofold,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>
  ];

  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
  let hasRegionVerifier = 1;
}

//===----------------------------------------------------------------------===//
// ParallelInsertSliceOp
//===----------------------------------------------------------------------===//

// TODO: Implement InParallelOpInterface.
def Tensor_ParallelInsertSliceOp : Tensor_Op<"parallel_insert_slice", [
       AttrSizedOperandSegments,
       OffsetSizeAndStrideOpInterface,
       // TODO: Cannot use an interface here atm, verify this manually for now.
       // HasParent<"ParallelCombiningOpInterface">
  ]> {
  let summary = [{
    Specify the tensor slice update of a single thread of a parent
    ParallelCombiningOpInterface op.
  }];
  let description = [{
    The `parallel_insert_slice` yields a subset tensor value to its parent
    ParallelCombiningOpInterface. These subset tensor values are aggregated to
    in some unspecified order into a full tensor value returned by the parent
    parallel iterating op.
    The `parallel_insert_slice` is one such op allowed in the
    ParallelCombiningOpInterface op.

    Conflicting writes result in undefined semantics, in that the indices written
    to by multiple parallel updates might contain data from any of the updates,
    or even a malformed bit pattern.

    If an index is updated exactly once, the value contained at that index
    in the resulting tensor will be equal to the value at a corresponding index
    of a slice that was used for the updated. If an index is not updated at all,
    its value will be equal to the one in the original tensor.

    This op does not create a new value, which allows maintaining a clean
    separation between the subset and full tensor.

    Note that we cannot mark this operation as pure (Pures), even
    though it has no side effects, because it will get DCEd during
    canonicalization.

    The parallel_insert_slice operation supports the following arguments:

    * source: the tensor that is inserted.
    * dest: the tensor into which the source tensor is inserted.
    * offsets: tensor-rank number of offsets into the `dest` tensor into which
               the slice is inserted.
    * sizes: tensor-rank number of sizes which specify the sizes of the source
             tensor type.
    * strides: tensor-rank number of strides that specify subsampling in each
               dimension.

    The representation based on offsets, sizes and strides support a
    partially-static specification via attributes specified through the
    `static_offsets`, `static_sizes` and `static_strides` arguments. A special
    sentinel value ShapedType::kDynamic encodes that the corresponding entry has
    a dynamic value.

    After buffer allocation, the "parallel_insert_slice" op is expected to lower
    into a memref.subview op.

    A parallel_insert_slice operation may additionally specify insertion into a
    tensor of higher rank than the source tensor, along dimensions that are
    statically known to be of size 1.
    This rank-altering behavior is not required by the op semantics: this
    flexibility allows to progressively drop unit dimensions while lowering
    between different flavors of ops on that operate on tensors.
    The rank-altering behavior of tensor.parallel_insert_slice matches the
    rank-reducing behavior of tensor.insert_slice and tensor.extract_slice.

    #### Verification in the rank-reduced case

    The same verification discussion and mechanisms apply as for ExtractSliceOp.
    Unlike ExtractSliceOp however, there is no need for a specific inference.
  }];

  let arguments = (ins
    AnyRankedTensor:$source,
    AnyRankedTensor:$dest,
    Variadic<Index>:$offsets,
    Variadic<Index>:$sizes,
    Variadic<Index>:$strides,
    DenseI64ArrayAttr:$static_offsets,
    DenseI64ArrayAttr:$static_sizes,
    DenseI64ArrayAttr:$static_strides
  );
  let assemblyFormat = [{
    $source `into` $dest ``
    custom<DynamicIndexList>($offsets, $static_offsets)
    custom<DynamicIndexList>($sizes, $static_sizes)
    custom<DynamicIndexList>($strides, $static_strides)
    attr-dict `:` type($source) `into` type($dest)
  }];

  let extraClassDeclaration = [{
    Type yieldedType() { return getDest().getType(); }

    RankedTensorType getSourceType() {
      return ::llvm::cast<RankedTensorType>(getSource().getType());
    }

    RankedTensorType getDestType() {
      return ::llvm::cast<RankedTensorType>(getDest().getType());
    }

    ParallelCombiningOpInterface getParallelCombiningParent() {
      return dyn_cast<ParallelCombiningOpInterface>(
        getOperation()->getParentOp());
    }

    /// Return the expected rank of each of the `static_offsets`, `static_sizes`
    /// and `static_strides` attributes.
    std::array<unsigned, 3> getArrayAttrMaxRanks() {
      unsigned rank = getDestType().getRank();
      return {rank, rank, rank};
    }

    /// Return the number of leading operands before `offsets`, `sizes` and
    /// `strides` operands.
    static unsigned getOffsetSizeAndStrideStartOperandIndex() { return 1; }

    /// Return the OpResult of the enclosing ForallOp that is
    /// corresponding to this ParallelInsertSliceOp.
    OpResult getTiedOpResult();

    /// Return the dimensions of the dest that are omitted to insert a source
    /// when the result is rank-extended.
    llvm::SmallBitVector getDroppedDims();
  }];

  let builders = [
    // Build a ParallelInsertSliceOp with mixed static and dynamic entries.
    OpBuilder<(ins "Value":$source, "Value":$dest,
      "ArrayRef<OpFoldResult>":$offsets, "ArrayRef<OpFoldResult>":$sizes,
      "ArrayRef<OpFoldResult>":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build a ParallelInsertSliceOp with mixed static and dynamic entries
    // packed into a Range vector.
    OpBuilder<(ins "Value":$source, "Value":$dest,
      "ArrayRef<Range>":$ranges,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build a ParallelInsertSliceOp with dynamic entries.
    OpBuilder<(ins "Value":$source, "Value":$dest,
      "ValueRange":$offsets, "ValueRange":$sizes, "ValueRange":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>
  ];

  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}


//===----------------------------------------------------------------------===//
// ScatterOp
//===----------------------------------------------------------------------===//

def Tensor_ScatterOp : Tensor_Op<"scatter", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    Pure
  ]> {
  let summary =
    "scatter a tensor into a destination tensor at specified indices";
  let description = [{
    The `scatter` operation inserts a `source` tensor into a `dest` tensor at
    the given indices.

    In its most general form, the tensor of indices specifies all the coordinates
    of every element to insert (i.e. COO format, without the payload).
    The indices are expected to be confined to coordinate values that fit the
    range of the `dest` tensor, otherwise the behavior is undefined.

    The leading dimensions of the index tensor must match that of the dest
    tensor. The trailing dimensions of the dest tensor must match those of the
    source tensor by omitting the dimensions specified in scatter_dims
    (rank-reducing semantics) or setting them to `1` (rank-preserving semantics)
    (see examples).
    This convention allows an idiomatic specification and lowering of
    "scattering multiple N-D slices into the dest tensor".
    The result type must match the type of the dest tensor.

    Note: in the examples below, we separate out the indexing part of the tensor
    type by a whitespace for readability purposes.

    Example:

    ```mlir
        // For each 1x2 triple of coordinates in %indices, insert the
        // element (i.e. 0-D subset) at the coordinates triple in %dest.
        //
        %out = tensor.scatter %source into %dest[%indices]
            scatter_dims([0, 1, 2]) unique :
          (tensor<1x2x 1x1x1xf32>, tensor<4x4x4xf32>, tensor<1x2x 3xindex>)
            -> tensor<4x4x4xf32>

        // Note: source type may be further rank-reduced to tensor<1x2x f32>.
    ```

    A slice variant is provided to allow specifying insertion of whole tensor
    slices into the `dest` tensor.

    Example:

    ```mlir
        // For each 3 singleton of coordinates in %indices, insert the 2-D
        // slice into %dest[*, %indices[...]:%indices[...] + 1, *] with the
        // indices corresponding to the scatter_dims attribute specified by
        // %indices.
        //
        %out = tensor.scatter %source into %dest[%indices] scatter_dims([1]) unique :
          (tensor<3x 4x1x6xf32>, tensor<4x5x6xf32>, tensor<3x 1xindex>)
            -> tensor<4x5x6xf32>
    ```

    The dimensions specified in the scatter_dims attribute are ones for which the
    source tensor has size `1`.
    I.e. if the dest type is `axbxcxd` and the coordinates are [1, 3], then
    the source type suffix is `ax1xcx1`.
    Scatter also allows rank-reducing semantics where the shape `ax1xcx1` can be
    further simplified to `axc`.

    The elemental type of the indices tensor can be any integer type.
    In the absence of target-specific or problem specific information the default
    type one should use is `index`.

    This operation does not support unranked tensors.

    A `unique` unit attribute must be be specified to indicate that the
    coordinates are statically guaranteed to be unique at runtime. If coordinates
    are not truly unique at runtime, the behavior is undefined.

    Only full slices are meant to be supported by this op, if one desires
    partial slices (e.g. strided windows) one should compose this op with other
    tensor ops (e.g. tensor.insert_slice). This is to avoid a slippery slope of
    complexity that would make the op unusable in practice.

    At the tensor-level, the index tensor is specified in an AoS form (i.e.
    coordinate tuple is the most minor). It is the responsibility of further
    lowerings and bufferization to implement various concrete layouts.

    Note: As currently specified, the operation must lower to an abstraction that
    performs copies to the output tensor. This is because the buffer type system
    is currently not rich enough to allow multiple non-contiguous views in the
    same type. This is visible more clearly in a notional buffer version of the
    op:

    ```mlir
        // memref<?x 4xf32> is a contiguous buffer of ?x4 elements, scatter into
        // random dest slices must copy to the contiguous dest.
        //
        some_side_effecting_op_writing_into %source, ...: memref<3x 4xf32>
        memref.scatter %source into %dest[%indices] scatter_dims([1]) unique :
          (memref<3x 4xf32>, memref<?x 4xf32>, memref<?x 1xindex>)

        // Nested buffer support in the producing op would allow writing directly
        // into the dest buffer.
        %v = some_nested_buffer_view_op %dest[%indices] scatter_dims([1]) unique :
          memref<? x memref<4xf32>>
        some_side_effecting_op_writing_into %v, ...: memref<? x memref<4xf32>>
    ```
  }];

  let arguments = (ins AnyRankedTensor:$source,
                       AnyRankedTensor:$dest,
                       RankedTensorOf<[AnySignlessIntegerOrIndex]>:$indices,
                       DenseI64ArrayAttr:$scatter_dims,
                       UnitAttr:$unique);
  let results = (outs AnyRankedTensor:$result);

  let assemblyFormat = [{
    $source `into` $dest `[` $indices `]`
      `scatter_dims` `(` $scatter_dims `)`
      (`unique` $unique^)?
      attr-dict
    `:` functional-type(operands, results)
  }];

  let extraClassDeclaration = [{
    RankedTensorType getDestType() {
      return ::llvm::cast<RankedTensorType>(getDest().getType());
    }
    RankedTensorType getIndicesType() {
      return ::llvm::cast<RankedTensorType>(getIndices().getType());
    }
    RankedTensorType getSourceType() {
      return ::llvm::cast<RankedTensorType>(getSource().getType());
    }
    RankedTensorType getResultType() {
      return ::llvm::cast<RankedTensorType>(getResult().getType());
    }
  }];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// SplatOp
//===----------------------------------------------------------------------===//

def Tensor_SplatOp : Tensor_Op<"splat", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    DeclareOpInterfaceMethods<ReifyRankedShapedTypeOpInterface>,
    Pure,
    TypesMatchWith<"operand type matches element type of result",
                   "aggregate", "input",
                   "::llvm::cast<TensorType>($_self).getElementType()">
  ]> {
  let summary = "tensor splat or broadcast operation";
  let description = [{
    Broadcast the operand to all elements of the result tensor. The operand is
    required to be of integer/index/float type.

    An additional argument of type `index` must be provided for each dynamic
    dimension present in the result type.

    Example for a statically shaped tensor:

    ```mlir
    %s = arith.constant 1.0 : f32
    %t = tensor.splat %s : tensor<8x16xf32>
    ```

    Example for a tensor containing dynamic dimensions:

    ```mlir
    // Broadcasts %s to a 3D dynamically shaped tensor, with %m and %n binding
    // to dimensions 0 and 2 of the resulting tensor, respectively.
    %m = arith.constant 10 : index
    %n = arith.constant 30 : index
    %t = tensor.splat %s[%m, %n] : tensor<?x20x?xf32>
    ```
  }];

  let arguments = (ins AnyTypeOf<[AnySignlessInteger, Index, AnyFloat],
                                 "integer/index/float type">:$input,
                       Variadic<Index>:$dynamicSizes);
  let results = (outs AnyRankedTensor:$aggregate);

  let builders = [
    // Build with an explicit result type and a list of values corresponding
    // to the dynamic sizes present in the result type.
    OpBuilder<(ins "Value":$element,
                   "Type":$aggregateType,
                   CArg<"ValueRange", "{}">:$dynamicSizes)>,

    // Build with a result tensor shape and a list of values corresponding to
    // the elements in the result tensor shape set to ShapedType::kDynamic.
    OpBuilder<(ins "Value":$element,
                   "ArrayRef<int64_t>":$staticShape,
                   CArg<"ValueRange", "{}">:$dynamicSizes)>,

    // Build with mixed static/dynamic sizes, where an attribute represents
    // a static dimension and a value represents a dynamic dimension.
    OpBuilder<(ins "Value":$element, "ArrayRef<OpFoldResult>":$sizes)>
  ];

  let assemblyFormat = "$input (`[` $dynamicSizes^ `]`)? attr-dict `:` type($aggregate)";

  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// RelayoutOp
//===----------------------------------------------------------------------===//

class Tensor_RelayoutOp<string mnemonic, list<Trait> traits = []> :
      Tensor_Op<mnemonic, !listconcat(traits, [
        DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
        DestinationStyleOpInterface,
        ConditionallySpeculatable, NoMemoryEffect,
        DeclareOpInterfaceMethods<ReifyRankedShapedTypeOpInterface>,
        TypesMatchWith<"result type matches type of dest",
                   "dest", "result",
                   "$_self">])> {

  code commonExtraClassDeclaration = [{
    size_t getSourceRank() { return getSourceType().getRank(); };
    size_t getDestRank() { return getDestType().getRank(); };
    RankedTensorType getSourceType() {
      return ::llvm::cast<RankedTensorType>(getSource().getType()); };
    RankedTensorType getDestType() {
      return ::llvm::cast<RankedTensorType>(getDest().getType()); };

    MutableOperandRange getDpsInitsMutable() { return getDestMutable(); }

    /// Interface method for ConditionallySpeculatable.
    Speculation::Speculatability getSpeculatability();

    /// Return a mapping from positions `inner_dims_pos` to their
    /// tile factors.
    DenseMap<int64_t, OpFoldResult> getDimAndTileMapping();

    /// Return the tile sizes as OpFoldResult.
    SmallVector<OpFoldResult> getMixedTiles();

    /// Return the tile sizes as `int64_t`. If a tile size is dynamic
    /// a sentinel `kDynamic` is introduced at that position in
    /// the returned vector.
    SmallVector<int64_t> getStaticTiles();

    /// Retrieve all outer dims for this Pack/UnPack Op, i.e. all the leading
    /// dims excluding the trailing dims corresponding to `innerTiles`. Note
    /// that this will include both tiled and non-tiled dimensions. The order
    /// of the output dimensions is consistent with the shape of the packed
    /// tensor.
    ArrayRef<int64_t> getAllOuterDims();

    /// Similar to `getAllOuterDims`, but only retrieve the outer dims that
    /// have been tiled. Also, the order of the output dimensions is consistent
    /// with `inner_dims_pos` rather than the packed tensor.
    SmallVector<int64_t> getTiledOuterDims();
  }];

  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// PackOp
//===----------------------------------------------------------------------===//

def Tensor_PackOp : Tensor_RelayoutOp<"pack", [
    AttrSizedOperandSegments]> {
  let summary = "tensor pack operation";
  let description = [{
    The "pack" operation converts a source tensor of rank `n` into a result
    tensor of rank `n + k` with a tiled and packed layout (maybe with padding)
    and optionally transposes the tiled source tensor dimensions.

    `inner_dims_pos` (mandatory) specifies `k` source tensor dimensions that are
    being tiled, where `0 < k <= n`. The order of the dimensions matters:
     - The tiled dimensions (of size `inner_tiles`) are added to the end of the result
    tensor in the order in which they appear in `inner_dims_pos`.
     - `inner_dims_pos[i]` specifies the source tensor dimension tiled by
    `inner_tiles[i]`.

    `inner_tiles` (mandatory) specifies `k` tile sizes. These tile sizes
    correspond to the least significant ("inner") result tensor dimension sizes,
    in the same order. Tile sizes can be static or dynamic.

    Example: If `inner_tiles = [16, 32]`, the result tensor has a shape of
    `...x16x32`. If `inner_dims_pos = [0, 1]`, the 0th source dimension is tiled
    by 16 and the 1st source dimension is tiled by 32. Other source dimensions
    (if any) are not tiled. If `inner_dims_pos = [1, 0]`, the 1st dimension is
    tiled by 16 and the 0th dimension is tiled by 32.

    Example:
    ```mlir
    // NC to NCnc
    %0 = tensor.pack %source inner_dims_pos = [0, 1] inner_tiles = [8, 32]
        into %dest : tensor<128x256xf32> -> tensor<16x8 x 8x32 xf32>
    //                                             \  /   \  /
    //                                       outer dims  inner dims
    ```

    `outer_dims_perm` (optional) specifies a permutation for the outer
    dimensions. If specified, it must have `n` elements.

    Example:
    ```mlir
    // CK to KCck
    %0 = tensor.pack %source outer_dims_perm = [1, 0] inner_dims_pos = [0, 1]
        inner_tiles = [8, 32] into %dest
        : tensor<128x256xf32> -> tensor<8x16 x 8x32 xf32>
    //                                  \  /
    //            compare with "NC to NCnc": outer dims are transposed
    ```

    `padding_value` specifies a padding value at the boundary on non-perfectly
    divisible dimensions. Padding is optional:
    - If absent, it is UB if the tile does not perfectly divide the dimension.
    - If present, it will pad along high dimensions (high-padding) to make the
      tile complete.

    Example:
    ```mlir
    %0 = tensor.pack %arg0 padding_value(%pad : f32) outer_dims_perm = [2, 1, 0]
        inner_dims_pos = [1] inner_tiles = [2] into %arg1
        : tensor<200x127x256xf32> -> tensor<256x64x200x2xf32>
    //                 \
    //                padded and tiled dim
    //
    // Source dimension 1 is tiled. 64 does not divide 127 evenly, so 1 padded
    // element is added at the end.
    //
    // Note: Only tiled dimensions can be padded.
    ```
  }];
  let arguments = (ins AnyRankedTensor:$source,
                       AnyRankedTensor:$dest,
                       Optional<AnyType>:$padding_value,
                       DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:$outer_dims_perm,
                       DenseI64ArrayAttr:$inner_dims_pos,
                       Variadic<Index>:$inner_tiles,
                       DenseI64ArrayAttr:$static_inner_tiles);
  let results = (outs AnyRankedTensor:$result);
  let assemblyFormat = [{
    $source
    (`padding_value` `(` $padding_value^ `:` type($padding_value) `)`)?
    (`outer_dims_perm` `=` $outer_dims_perm^)?
    `inner_dims_pos` `=` $inner_dims_pos
    `inner_tiles` `=`
    custom<DynamicIndexList>($inner_tiles, $static_inner_tiles)
    `into` $dest attr-dict `:` type($source) `->` type($dest)
  }];

  let builders = [
    OpBuilder<(ins "Value":$source, "Value":$dest,
      "ArrayRef<int64_t>":$innerDimsPos,
      "ArrayRef<OpFoldResult>":$innerTiles,
      CArg<"std::optional<Value>", "std::nullopt">:$paddingValue,
      CArg<"ArrayRef<int64_t>", "{}">:$outerDimsPerm)>
  ];

  let extraClassDeclaration = commonExtraClassDeclaration # [{
    // Method to get the shape of the result as `SmallVector<OpFoldResult>`.
    // This is a static method to allow getting the shape of the destination
    // expected while creating a `pack` op.
    static SmallVector<OpFoldResult> getResultShape(OpBuilder &builder,
        Location loc, ArrayRef<OpFoldResult> sourceDims,
        ArrayRef<OpFoldResult> innerTileDims, ArrayRef<int64_t> innerDimsPos,
        ArrayRef<int64_t> outerDimsPerm = {});

    // Method to get the `RankedTensorType` of the result based on the inner
    // tiles, position of the inner tiles (innerDimsPos)  and interchange vector
    // of outer loops (outerDimsPerm).
    static RankedTensorType inferPackedType(RankedTensorType sourceType,
        ArrayRef<int64_t> innerTileSizes, ArrayRef<int64_t> innerDimsPos,
        ArrayRef<int64_t> outerDimsPerm = {});

    // Returns true if we have enough static information to catch undefined
    // behavior when the tile size does not divide perfectly the dimension of
    // the input tensor. Detecting UB requires that the input size and either
    // corresponding tile or output size are static.
    static bool requirePaddingValue(ArrayRef<int64_t> inputShape,
                                    ArrayRef<int64_t> innerDimsPos,
                                    ArrayRef<int64_t> outputShape,
                                    ArrayRef<int64_t> outerDimsPerm,
                                    ArrayRef<OpFoldResult> innerTiles);

    static Value createDestinationTensor(OpBuilder &b, Location loc,
        Value source, ArrayRef<OpFoldResult> innerTileSizes,
        ArrayRef<int64_t> innerDimsPos, ArrayRef<int64_t> outerDimsPerm);

    /// Build and return a new PackOp that is a clone of the current PackOp with
    /// (innerDimsPos, innerTiles) (resp. outerDimsPerm) are permuted by
    /// innerPermutation (resp. outerPermutation).
    /// A new `tensor.empty` of the proper shape is built in the process.
    /// Asserts that:
    ///   - At least one of innerPermutation or outerPermutation is non-empty.
    ///   - If not empty, innerPermutation is a valid permutation of size
    ///     matching innerDimPos.
    ///   - If not empty, outerPermutation is a valid permutation of size
    ///     matching outerDimsPerm.
    PackOp createTransposedClone(OpBuilder &b,
                                 Location loc,
                                 ArrayRef<int64_t> innerPermutation,
                                 ArrayRef<int64_t> outerPermutation);

    /// Check if this PackOp is like a simple pad operation.
    /// In other words, this operation:
    /// 1. adds useless dimensions (dimension of size 1),
    /// 2. pads the other ones, and
    /// 3. doesn't shuffle the dimensions
    bool isLikePad();
  }];

  let hasCanonicalizeMethod = 1;

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// UnPackOp
//===----------------------------------------------------------------------===//

def Tensor_UnPackOp : Tensor_RelayoutOp<"unpack"> {
  let summary = "tensor unpack operation";
  let description = [{
    The "unpack" operation converts a source tensor of rank `n` with a tiled and
    packed layout to a result tensor of rank `n - k`.

    `inner_dims_pos` (mandatory) specifies `k` source tensor dimensions with
    which the last `k` source tensor dimensions are combined, where
    `0 < k <= n/2`. Each `inner_dims_pos` element must be `>= 0` and `< n - k`.
    The order of the dimensions in `inner_dims_pos` matters: dimension
    `inner_dims_pos[i]` is combined with dimension `n - k + i` (assuming that
    `outer_dims_perm` is not specified).

    `inner_tiles` (mandatory) specifies `k` tile sizes. These tile sizes
    correspond to the least significant ("inner") source tensor dimension sizes.
    The behavior of this op is undefined if:
    - `inner_tiles` do not exactly match with the corresponding source tensor
      dimension sizes.
    - Or, `inner_tiles[i]` does not divide the size of dimension
      `inner_dims_pos[i]` (assuming that `outer_dims_perm` is not specified)
      evenly.

    `outer_dims_perm` (optional) specifies a permutation for the outer
    dimensions. If specified, it must have `n - k` elements. If specified, this
    permutation is applied before combining any dimensions.

    Example:

    ```mlir
    // NCnc to NC:
    %0 = tensor.unpack %source inner_dims_pos = [0, 1] inner_tiles = [8, 32]
        into %dest : tensor<16x8x8x32xf32> -> tensor<128x256xf32>

    // CK to KCck:
    %0 = tensor.unpack %source outer_dims_perm = [1, 0] inner_dims_pos = [0, 1]
        inner_tiles = [8, 32] into %dest
        : tensor<8x16x8x32xf32> -> tensor<128x256xf32>
    ```
  }];
  let arguments = (ins AnyRankedTensor:$source,
                       AnyRankedTensor:$dest,
                       DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:$outer_dims_perm,
                       DenseI64ArrayAttr:$inner_dims_pos,
                       Variadic<Index>:$inner_tiles,
                       DenseI64ArrayAttr:$static_inner_tiles);
  let results = (outs AnyRankedTensor:$result);
  let assemblyFormat = [{
    $source
    (`outer_dims_perm` `=` $outer_dims_perm^)?
    `inner_dims_pos` `=` $inner_dims_pos
    `inner_tiles` `=`
    custom<DynamicIndexList>($inner_tiles, $static_inner_tiles)
    `into` $dest attr-dict `:` type($source) `->` type($dest)
  }];

  let builders = [
    OpBuilder<(ins "Value":$source, "Value":$dest,
    "ArrayRef<int64_t>":$innerDimsPos,
    "ArrayRef<OpFoldResult>":$innerTiles,
    CArg<"ArrayRef<int64_t>", "{}">:$outerDimsPerm)>
  ];

  let extraClassDeclaration = commonExtraClassDeclaration # [{
    static Value createDestinationTensor(OpBuilder &b, Location loc,
        Value source, ArrayRef<OpFoldResult> innerTileSizes,
        ArrayRef<int64_t> innerDimsPos, ArrayRef<int64_t> outerDimsPerm);

    /// Build and return a new UnPackOp that is a clone of the current UnPackOp
    /// with (innerDimsPos, innerTiles) (resp. outerDimsPerm) are permuted by
    /// innerPermutation (resp. outerPermutation).
    /// Asserts that:
    ///   - At least one of innerPermutation or outerPermutation is non-empty.
    ///   - If not empty, innerPermutation is a valid permutation of size
    ///     matching innerDimPos.
    ///   - If not empty, outerPermutation is a valid permutation of size
    ///     matching outerDimsPerm.
    UnPackOp createTransposedClone(OpBuilder &b,
                                   Location loc,
                                   Value transposedSource,
                                   ArrayRef<int64_t> innerPermutation,
                                   ArrayRef<int64_t> outerPermutation);

    /// Check if this UnPackOp is like a simple unpad operation.
    /// In other words, this operation:
    /// 1. drops useless dimensions (dimension of size 1), and
    /// 2. reduces dimensions in place (i.e., no transpose.)
    bool isLikeUnPad();
  }];

  let hasCanonicalizeMethod = 1;

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// YieldOp
//===----------------------------------------------------------------------===//

def Tensor_YieldOp : Tensor_Op<"yield",
    [Pure, ReturnLike, Terminator,
     HasParent<"::mlir::tensor::GenerateOp, ::mlir::tensor::PadOp">]> {
  let summary = "Yield a value from a region";
  let description = [{
     This operation is used to yield a single value from a within a region. It
     is used to create dynamically sized tensors
     (see `tensor.generate` and `tensor.pad` ops).
  }];

  let arguments = (ins AnyType:$value);
  let assemblyFormat = "$value attr-dict `:` type($value)";

  // Dummy builder to appease code in templated ensureTerminator that
  // GenerateOp's auto-generated parser calls.
  let builders = [OpBuilder<(ins), [{ /* nothing to do */ }]>];
}

#endif // TENSOR_OPS


//===- FuncOps.td - Func operation definitions -------------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef MLIR_DIALECT_FUNC_IR_FUNCOPS_TD
#define MLIR_DIALECT_FUNC_IR_FUNCOPS_TD

include "mlir/IR/EnumAttr.td"
include "mlir/IR/OpAsmInterface.td"
include "mlir/IR/SymbolInterfaces.td"
include "mlir/Interfaces/CallInterfaces.td"
include "mlir/Interfaces/ControlFlowInterfaces.td"
include "mlir/Interfaces/FunctionInterfaces.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"

def Func_Dialect : Dialect {
  let name = "func";
  let cppNamespace = "::mlir::func";
  let hasConstantMaterializer = 1;
}

// Base class for Func dialect ops.
class Func_Op<string mnemonic, list<Trait> traits = []> :
    Op<Func_Dialect, mnemonic, traits>;

//===----------------------------------------------------------------------===//
// CallOp
//===----------------------------------------------------------------------===//

def CallOp : Func_Op<"call",
    [CallOpInterface, MemRefsNormalizable,
     DeclareOpInterfaceMethods<SymbolUserOpInterface>]> {
  let summary = "call operation";
  let description = [{
    The `func.call` operation represents a direct call to a function that is
    within the same symbol scope as the call. The operands and result types of
    the call must match the specified function type. The callee is encoded as a
    symbol reference attribute named "callee".

    Example:

    ```mlir
    %2 = func.call @my_add(%0, %1) : (f32, f32) -> f32
    ```
  }];

  let arguments = (ins FlatSymbolRefAttr:$callee, Variadic<AnyType>:$operands);
  let results = (outs Variadic<AnyType>);

  let builders = [
    OpBuilder<(ins "FuncOp":$callee, CArg<"ValueRange", "{}">:$operands), [{
      $_state.addOperands(operands);
      $_state.addAttribute("callee", SymbolRefAttr::get(callee));
      $_state.addTypes(callee.getFunctionType().getResults());
    }]>,
    OpBuilder<(ins "SymbolRefAttr":$callee, "TypeRange":$results,
      CArg<"ValueRange", "{}">:$operands), [{
      $_state.addOperands(operands);
      $_state.addAttribute("callee", callee);
      $_state.addTypes(results);
    }]>,
    OpBuilder<(ins "StringAttr":$callee, "TypeRange":$results,
      CArg<"ValueRange", "{}">:$operands), [{
      build($_builder, $_state, SymbolRefAttr::get(callee), results, operands);
    }]>,
    OpBuilder<(ins "StringRef":$callee, "TypeRange":$results,
      CArg<"ValueRange", "{}">:$operands), [{
      build($_builder, $_state, StringAttr::get($_builder.getContext(), callee),
            results, operands);
    }]>];

  let extraClassDeclaration = [{
    FunctionType getCalleeType();

    /// Get the argument operands to the called function.
    operand_range getArgOperands() {
      return {arg_operand_begin(), arg_operand_end()};
    }

    MutableOperandRange getArgOperandsMutable() {
      return getOperandsMutable();
    }

    operand_iterator arg_operand_begin() { return operand_begin(); }
    operand_iterator arg_operand_end() { return operand_end(); }

    /// Return the callee of this operation.
    CallInterfaceCallable getCallableForCallee() {
      return (*this)->getAttrOfType<SymbolRefAttr>("callee");
    }

    /// Set the callee for this operation.
    void setCalleeFromCallable(CallInterfaceCallable callee) {
      (*this)->setAttr("callee", callee.get<SymbolRefAttr>());
    }
  }];

  let assemblyFormat = [{
    $callee `(` $operands `)` attr-dict `:` functional-type($operands, results)
  }];
}

//===----------------------------------------------------------------------===//
// CallIndirectOp
//===----------------------------------------------------------------------===//

def CallIndirectOp : Func_Op<"call_indirect", [
      CallOpInterface,
      TypesMatchWith<"callee input types match argument types",
                     "callee", "callee_operands",
                     "::llvm::cast<FunctionType>($_self).getInputs()">,
      TypesMatchWith<"callee result types match result types",
                     "callee", "results",
                     "::llvm::cast<FunctionType>($_self).getResults()">
    ]> {
  let summary = "indirect call operation";
  let description = [{
    The `func.call_indirect` operation represents an indirect call to a value
    of function type. The operands and result types of the call must match the
    specified function type.

    Function values can be created with the
    [`func.constant` operation](#funcconstant-constantop).

    Example:

    ```mlir
    %func = func.constant @my_func : (tensor<16xf32>, tensor<16xf32>) -> tensor<16xf32>
    %result = func.call_indirect %func(%0, %1) : (tensor<16xf32>, tensor<16xf32>) -> tensor<16xf32>
    ```
  }];

  let arguments = (ins FunctionType:$callee,
                       Variadic<AnyType>:$callee_operands);
  let results = (outs Variadic<AnyType>:$results);

  let builders = [
    OpBuilder<(ins "Value":$callee, CArg<"ValueRange", "{}">:$operands), [{
      $_state.operands.push_back(callee);
      $_state.addOperands(operands);
      $_state.addTypes(::llvm::cast<FunctionType>(callee.getType()).getResults());
    }]>];

  let extraClassDeclaration = [{
    // TODO: Remove once migrated callers.
    ValueRange operands() { return getCalleeOperands(); }

    /// Get the argument operands to the called function.
    operand_range getArgOperands() {
      return {arg_operand_begin(), arg_operand_end()};
    }

    MutableOperandRange getArgOperandsMutable() {
      return getCalleeOperandsMutable();
    }

    operand_iterator arg_operand_begin() { return ++operand_begin(); }
    operand_iterator arg_operand_end() { return operand_end(); }

    /// Return the callee of this operation.
    CallInterfaceCallable getCallableForCallee() { return getCallee(); }

    /// Set the callee for this operation.
    void setCalleeFromCallable(CallInterfaceCallable callee) {
      setOperand(0, callee.get<Value>());
    }
  }];

  let hasCanonicalizeMethod = 1;
  let assemblyFormat = [{
    $callee `(` $callee_operands `)` attr-dict `:` type($callee)
  }];
}

//===----------------------------------------------------------------------===//
// ConstantOp
//===----------------------------------------------------------------------===//

def ConstantOp : Func_Op<"constant",
    [ConstantLike, Pure,
     DeclareOpInterfaceMethods<SymbolUserOpInterface>,
     DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>]> {
  let summary = "constant";
  let description = [{
    The `func.constant` operation produces an SSA value from a symbol reference
    to a `func.func` operation

    Example:

    ```mlir
    // Reference to function @myfn.
    %2 = func.constant @myfn : (tensor<16xf32>, f32) -> tensor<16xf32>

    // Equivalent generic forms
    %2 = "func.constant"() { value = @myfn } : () -> ((tensor<16xf32>, f32) -> tensor<16xf32>)
    ```

    MLIR does not allow direct references to functions in SSA operands because
    the compiler is multithreaded, and disallowing SSA values to directly
    reference a function simplifies this
    ([rationale](../Rationale/Rationale.md#multithreading-the-compiler)).
  }];

  let arguments = (ins FlatSymbolRefAttr:$value);
  let results = (outs AnyType);
  let assemblyFormat = "attr-dict $value `:` type(results)";

  let extraClassDeclaration = [{
    /// Returns true if a constant operation can be built with the given value
    /// and result type.
    static bool isBuildableWith(Attribute value, Type type);
  }];

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// FuncOp
//===----------------------------------------------------------------------===//

def FuncOp : Func_Op<"func", [
  AffineScope, AutomaticAllocationScope,
  FunctionOpInterface, IsolatedFromAbove, OpAsmOpInterface
]> {
  let summary = "An operation with a name containing a single `SSACFG` region";
  let description = [{
    Operations within the function cannot implicitly capture values defined
    outside of the function, i.e. Functions are `IsolatedFromAbove`. All
    external references must use function arguments or attributes that establish
    a symbolic connection (e.g. symbols referenced by name via a string
    attribute like SymbolRefAttr). An external function declaration (used when
    referring to a function declared in some other module) has no body. While
    the MLIR textual form provides a nice inline syntax for function arguments,
    they are internally represented as “block arguments” to the first block in
    the region.

    Only dialect attribute names may be specified in the attribute dictionaries
    for function arguments, results, or the function itself.

    Example:

    ```mlir
    // External function definitions.
    func.func private @abort()
    func.func private @scribble(i32, i64, memref<? x 128 x f32, #layout_map0>) -> f64

    // A function that returns its argument twice:
    func.func @count(%x: i64) -> (i64, i64)
      attributes {fruit = "banana"} {
      return %x, %x: i64, i64
    }

    // A function with an argument attribute
    func.func private @example_fn_arg(%x: i32 {swift.self = unit})

    // A function with a result attribute
    func.func private @example_fn_result() -> (f64 {dialectName.attrName = 0 : i64})

    // A function with an attribute
    func.func private @example_fn_attr() attributes {dialectName.attrName = false}
    ```
  }];

  let arguments = (ins SymbolNameAttr:$sym_name,
                       TypeAttrOf<FunctionType>:$function_type,
                       OptionalAttr<StrAttr>:$sym_visibility,
                       OptionalAttr<DictArrayAttr>:$arg_attrs,
                       OptionalAttr<DictArrayAttr>:$res_attrs);
  let regions = (region AnyRegion:$body);

  let builders = [OpBuilder<(ins
    "StringRef":$name, "FunctionType":$type,
    CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs,
    CArg<"ArrayRef<DictionaryAttr>", "{}">:$argAttrs)
  >];
  let extraClassDeclaration = [{
    static FuncOp create(Location location, StringRef name, FunctionType type,
                         ArrayRef<NamedAttribute> attrs = {});
    static FuncOp create(Location location, StringRef name, FunctionType type,
                         Operation::dialect_attr_range attrs);
    static FuncOp create(Location location, StringRef name, FunctionType type,
                         ArrayRef<NamedAttribute> attrs,
                         ArrayRef<DictionaryAttr> argAttrs);

    /// Create a deep copy of this function and all of its blocks, remapping any
    /// operands that use values outside of the function using the map that is
    /// provided (leaving them alone if no entry is present). If the mapper
    /// contains entries for function arguments, these arguments are not
    /// included in the new function. Replaces references to cloned sub-values
    /// with the corresponding value that is copied, and adds those mappings to
    /// the mapper.
    FuncOp clone(IRMapping &mapper);
    FuncOp clone();

    /// Clone the internal blocks and attributes from this function into dest.
    /// Any cloned blocks are appended to the back of dest. This function
    /// asserts that the attributes of the current function and dest are
    /// compatible.
    void cloneInto(FuncOp dest, IRMapping &mapper);

    //===------------------------------------------------------------------===//
    // FunctionOpInterface Methods
    //===------------------------------------------------------------------===//

    /// Returns the region on the current operation that is callable. This may
    /// return null in the case of an external callable object, e.g. an external
    /// function.
    ::mlir::Region *getCallableRegion() { return isExternal() ? nullptr : &getBody(); }

    /// Returns the argument types of this function.
    ArrayRef<Type> getArgumentTypes() { return getFunctionType().getInputs(); }

    /// Returns the result types of this function.
    ArrayRef<Type> getResultTypes() { return getFunctionType().getResults(); }

    //===------------------------------------------------------------------===//
    // OpAsmOpInterface Methods
    //===------------------------------------------------------------------===//

    /// Allow the dialect prefix to be omitted.
    static StringRef getDefaultDialect() { return "func"; }

    //===------------------------------------------------------------------===//
    // SymbolOpInterface Methods
    //===------------------------------------------------------------------===//

    bool isDeclaration() { return isExternal(); }
  }];
  let hasCustomAssemblyFormat = 1;
}

//===----------------------------------------------------------------------===//
// ReturnOp
//===----------------------------------------------------------------------===//

def ReturnOp : Func_Op<"return", [Pure, HasParent<"FuncOp">,
                                MemRefsNormalizable, ReturnLike, Terminator]> {
  let summary = "Function return operation";
  let description = [{
    The `func.return` operation represents a return operation within a function.
    The operation takes variable number of operands and produces no results.
    The operand number and types must match the signature of the function
    that contains the operation.

    Example:

    ```mlir
    func.func @foo() : (i32, f8) {
      ...
      return %0, %1 : i32, f8
    }
    ```
  }];

  let arguments = (ins Variadic<AnyType>:$operands);

  let builders = [OpBuilder<(ins), [{
    build($_builder, $_state, std::nullopt);
  }]>];

  let assemblyFormat = "attr-dict ($operands^ `:` type($operands))?";
  let hasVerifier = 1;
}

#endif // MLIR_DIALECT_FUNC_IR_FUNCOPS_TD


//===- PDLExtensionOps.td - Transform dialect operations ---*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef MLIR_DIALECT_TRANSFORM_PDLEXTENSION_PDLEXTENSIONOPS
#define MLIR_DIALECT_TRANSFORM_PDLEXTENSION_PDLEXTENSIONOPS

include "mlir/Dialect/Transform/IR/TransformDialect.td"
include "mlir/Dialect/Transform/Interfaces/TransformInterfaces.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/IR/OpAsmInterface.td"
include "mlir/IR/SymbolInterfaces.td"

def PDLMatchOp : TransformDialectOp<"pdl_match",
    [DeclareOpInterfaceMethods<TransformOpInterface>,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
  let summary = "Finds ops that match the named PDL pattern";
  let description = [{
    Find Payload IR ops nested within the Payload IR op associated with the
    operand that match the PDL pattern identified by its name. The pattern is
    expected to be defined in the closest surrounding `WithPDLPatternsOp`.

    Produces a Transform IR value associated with the list of Payload IR ops
    that matched the pattern. The order of results in the list is that of the
    Operation::walk, clients are advised not to rely on a specific order though.
    If the operand is associated with multiple Payload IR ops, finds matching
    ops nested within each of those and produces a single list containing all
    of the matched ops.

    The transformation is considered successful regardless of whether some
    Payload IR ops actually matched the pattern and only fails if the pattern
    could not be looked up or compiled.
  }];

  let arguments = (ins
    Arg<TransformHandleTypeInterface, "Payload IR scope to match within">:$root,
    SymbolRefAttr:$pattern_name);
  let results = (outs
    Res<TransformHandleTypeInterface, "Handle to the matched Payload IR ops">:$matched);

  let assemblyFormat = "$pattern_name `in` $root attr-dict `:` "
                       "functional-type(operands, results)";
}

def WithPDLPatternsOp : TransformDialectOp<"with_pdl_patterns",
    [DeclareOpInterfaceMethods<TransformOpInterface>, NoTerminator,
     OpAsmOpInterface, PossibleTopLevelTransformOpTrait,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     SymbolTable]> {
  let summary = "Contains PDL patterns available for use in transforms";
  let description = [{
    This op contains a set of named PDL patterns that are available for the
    Transform dialect operations to be used for pattern matching. For example,
    PDLMatchOp can be used to produce a Transform IR value associated with all
    Payload IR operations that match the pattern as follows:

    ```mlir
    transform.with_pdl_patterns {
    ^bb0(%arg0: !transform.any_op):
      pdl.pattern @my_pattern : benefit(1) {
        %0 = pdl.operation //...
        // Regular PDL goes here.
        pdl.rewrite %0 with "transform.dialect"
      }

      sequence %arg0 failures(propagate) {
      ^bb0(%arg1: !transform.any_op):
        %1 = pdl_match @my_pattern in %arg1
        // Use %1 as handle
      }
    }
    ```

    Note that the pattern is expected to finish with a `pdl.rewrite` terminator
    that points to the custom rewriter named "transform.dialect". The rewriter
    actually does nothing, but the transform application will keep track of the
    operations that matched the pattern.

    This op is expected to contain `pdl.pattern` operations and exactly one
    another Transform dialect operation that gets executed with all patterns
    available. This op is a possible top-level Transform IR op, the argument of
    its entry block corresponds to either the root op of the payload IR or the
    ops associated with its operand when provided.
  }];

  let arguments = (ins
    Arg<Optional<TransformHandleTypeInterface>, "Root operation of the Payload IR"
        >:$root);
  let regions = (region SizedRegion<1>:$body);
  let assemblyFormat = "($root^ `:` type($root))? attr-dict-with-keyword regions";

  let hasVerifier = 1;

  let extraClassDeclaration = [{
    /// Allow the dialect prefix to be omitted.
    static StringRef getDefaultDialect() { return "transform"; }
  }];
}

#endif // MLIR_DIALECT_TRANSFORM_PDLEXTENSION_PDLEXTENSIONOPS


//===- MLProgramOps.td - Structural ML Program Ops ---------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef MLPROGRAM_OPS
#define MLPROGRAM_OPS

include "mlir/Dialect/MLProgram/IR/MLProgramBase.td"
include "mlir/Dialect/MLProgram/IR/MLProgramAttributes.td"
include "mlir/Dialect/MLProgram/IR/MLProgramTypes.td"
include "mlir/Interfaces/CallInterfaces.td"
include "mlir/Interfaces/ControlFlowInterfaces.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/Interfaces/FunctionInterfaces.td"
include "mlir/IR/OpAsmInterface.td"
include "mlir/IR/RegionKindInterface.td"
include "mlir/IR/SymbolInterfaces.td"

class MLProgram_Op<string mnemonic, list<Trait> traits = []> :
    Op<MLProgram_Dialect, mnemonic, traits>;

//===----------------------------------------------------------------------===//
// FuncOp
//===----------------------------------------------------------------------===//

def MLProgram_FuncOp : MLProgram_Op<"func", [
    FunctionOpInterface, IsolatedFromAbove,
    RegionKindInterface, Symbol
  ]> {
  let summary = "Function containing a single `SSACFG` region";
  let description = [{
    This simple function container represents callables in an ML program where
    the body is an `SSACFG` region. It must be terminated by a `return` op which
    yields values with the same arity and types as the `FunctionType` results
    of the containing `func`.

    This op is a `Symbol` but does not introduce a new `SymbolTable`. As such,
    it cannot represent nested symbols.

    Example:

    ```mlir
    ml_program.func private @some_extern(i32) -> i32
    ml_program.func @compute(%arg0 : i32) -> i32 {
      ml_program.return %arg0 : i32
    }
    ```
  }];

  let arguments = (ins SymbolNameAttr:$sym_name,
                       TypeAttrOf<FunctionType>:$function_type,
                       OptionalAttr<DictArrayAttr>:$arg_attrs,
                       OptionalAttr<DictArrayAttr>:$res_attrs,
                       OptionalAttr<StrAttr>:$sym_visibility);
  let regions = (region AnyRegion:$body);

  let extraClassDeclaration = [{
    //===------------------------------------------------------------------===//
    // FunctionOpInterface Methods
    //===------------------------------------------------------------------===//

    /// Returns the region on the current operation that is callable. This may
    /// return null in the case of an external callable object, e.g. an external
    /// function.
    ::mlir::Region *getCallableRegion() {
      return isExternal() ? nullptr : &getBody();
    }

    /// Returns the argument types of this function.
    ArrayRef<Type> getArgumentTypes() { return getFunctionType().getInputs(); }

    /// Returns the result types of this function.
    ArrayRef<Type> getResultTypes() { return getFunctionType().getResults(); }

    //===------------------------------------------------------------------===//
    // RegionKindInterface Methods
    //===------------------------------------------------------------------===//
    static ::mlir::RegionKind getRegionKind(unsigned index) {
      return ::mlir::RegionKind::SSACFG;
    }

    //===------------------------------------------------------------------===//
    // SymbolOpInterface Methods
    //===------------------------------------------------------------------===//

    bool isDeclaration() { return isExternal(); }
  }];

  let hasCustomAssemblyFormat = 1;
}

//===----------------------------------------------------------------------===//
// GlobalOp
//===----------------------------------------------------------------------===//

def MLProgram_GlobalOp : MLProgram_Op<"global", [
    Symbol
  ]> {
  let summary = "Module level declaration of a global variable";
  let description = [{
    Declares a named global variable (or constant).

    A global contains a value of a specified type which can be accessed at
    runtime via appropriate load/store operations. It can be mutable or
    constant, optionally taking an initial value or declared as
    extern (in which case, the initial value is found in external storage
    by symbol name).

    Generally, the type of the global and the type of the initial value
    will be the same. However, for type hierarchies which can have a more
    generalized bounding type that can be assigned from a narrow type, this
    is allowed (but not verified).

    Examples:

    ```mlir
    // Constant global.
    ml_program.global @foobar(dense<4> : tensor<4xi32>) : tensor<?xi32>

    // Constant with external linkage.
    ml_program.global mutable @foobar(#ml_program.extern<tensor<4xi32>>)
      : tensor<?xi32>

    // Mutable global with an undefined initial value.
    ml_program.global mutable @foobar : tensor<?xi32>
    ```
  }];

  let arguments = (ins
    SymbolNameAttr:$sym_name,
    TypeAttr:$type,
    UnitAttr:$is_mutable,
    OptionalAttr<AnyAttr>:$value,
    OptionalAttr<StrAttr>:$sym_visibility
  );

  let assemblyFormat = [{
    custom<SymbolVisibility>($sym_visibility)
    (`mutable` $is_mutable^)?
    $sym_name ``
    custom<TypedInitialValue>($type, $value)
    attr-dict
  }];

  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// GlobalLoadOp
//===----------------------------------------------------------------------===//

def MLProgram_GlobalLoadOp : MLProgram_Op<"global_load", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    DeclareOpInterfaceMethods<SymbolUserOpInterface>
  ]> {
  let summary = "Direct load of a mutable value from a global";
  let description = [{
    Performs a non-atomic, non-volatile, non-synchronized load from a global
    that may be mutable.

    It is fully expected that these constraints are not suitable for
    all situations, and alternative ops should be defined and used for more
    advanced cases.

    This op is side effecting and may not be valid to use in graph regions
    without additional consideration to evaluation order constraints. See
    `global_load_graph` for op which allows for explicit ordering constraints.

    Example:

    ```mlir
    %0 = ml_program.global_load @foobar : tensor<?xi32>
    ```
  }];

  let arguments = (ins
    Arg<SymbolRefAttr, "", [MemRead]>:$global
  );
  let results = (outs
    AnyType:$result
  );

  let assemblyFormat = [{
    $global `:` type($result) attr-dict
  }];

  let extraClassDeclaration = [{
    /// Gets the corresponding GlobalOp (or nullptr).
    GlobalOp getGlobalOp(SymbolTableCollection &symbolTable);
  }];

  let extraClassDefinition = [{
    void $cppClass::getAsmResultNames(
        function_ref<void(::mlir::Value, ::llvm::StringRef)> setNameFn) {
      setNameFn(getResult(), getGlobal().getLeafReference());
    }
  }];
}

//===----------------------------------------------------------------------===//
// GlobalLoadConstOp
//===----------------------------------------------------------------------===//

def MLProgram_GlobalLoadConstOp : MLProgram_Op<"global_load_const", [
    Pure,
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    DeclareOpInterfaceMethods<SymbolUserOpInterface>
  ]> {
  let summary = "Direct load a constant value from a global";
  let description = [{
    Loads a constant (immutable) value from a global directly by symbol.

    This op is only legal for globals that are not mutable and exists because
    such a load can be considered to have no side effects.

    Example:

    ```mlir
    %0 = ml_program.global_load_const @foobar : tensor<?xi32>
    ```
  }];

  let arguments = (ins
    SymbolRefAttr:$global
  );
  let results = (outs
    AnyType:$result
  );

  let assemblyFormat = [{
    $global `:` type($result) attr-dict
  }];

  let extraClassDeclaration = [{
    /// Gets the corresponding GlobalOp (or nullptr).
    GlobalOp getGlobalOp(SymbolTableCollection &symbolTable);
  }];

  let extraClassDefinition = [{
    void $cppClass::getAsmResultNames(
      function_ref<void(::mlir::Value, ::llvm::StringRef)> setNameFn) {
        setNameFn(getResult(), getGlobal().getLeafReference());
    }
  }];
}

//===----------------------------------------------------------------------===//
// GlobalLoadGraphOp
//===----------------------------------------------------------------------===//

def MLProgram_GlobalLoadGraphOp : MLProgram_Op<"global_load_graph", [
    DeclareOpInterfaceMethods<SymbolUserOpInterface>
  ]> {
  let summary = "Direct load of a mutable value from a global in Graph region";
  let description = [{
    Performs a non-atomic, non-volatile, non-synchronized load from a global
    that may be mutable.

    It is fully expected that these constraints are not suitable for all
    situations, and alternative ops should be defined and used for more advanced
    cases.

    This op is side effecting and may not be valid to use in graph regions
    without additional consideration to evaluation order constraints.

    Example:

    ```mlir
    %0, %cstr = ml_program.global_load_graph @foobar
      ordering (%token -> !ml_program.token) : tensor<?xi32>
    ```
  }];

  let arguments = (ins
    Arg<SymbolRefAttr, "", [MemRead]>:$global,
    Variadic<MLProgram_TokenType>:$consumeTokens
  );
  let results = (outs
    AnyType:$result,
    MLProgram_TokenType:$produceToken
  );

  let assemblyFormat = [{
    $global `` custom<TokenOrdering>($consumeTokens, type($produceToken)) `:` type($result) attr-dict
  }];

  let extraClassDeclaration = [{
    /// Gets the corresponding GlobalOp (or nullptr).
    GlobalOp getGlobalOp(SymbolTableCollection &symbolTable);
  }];
}

//===----------------------------------------------------------------------===//
// GlobalStoreOp
//===----------------------------------------------------------------------===//

def MLProgram_GlobalStoreOp : MLProgram_Op<"global_store", [
    DeclareOpInterfaceMethods<SymbolUserOpInterface>
  ]> {
  let summary = "Direct store of a value into a mutable global";
  let description = [{
    Performs a non-atomic, non-volatile, non-synchronized store to a mutable
    global.

    It is fully expected that these constraints are not suitable for
    all situations, and alternative ops should be defined and used for more
    advanced cases.

    This op is side effecting and may not be valid to use in graph regions
    without additional consideration to evaluation order constraints. See
    `global_store_graph` for op which allows for explicit ordering constraints.

    Example:

    ```mlir
    ml_program.global_store @foobar = %0 : tensor<?xi32>
    ```
  }];

  let arguments = (ins
    Arg<SymbolRefAttr, "", [MemWrite]>:$global,
    AnyType:$value
  );

  let assemblyFormat = [{
    $global `=` $value `:` type($value) attr-dict
  }];

  let extraClassDeclaration = [{
    /// Gets the corresponding GlobalOp (or nullptr).
    GlobalOp getGlobalOp(SymbolTableCollection &symbolTable);
  }];
}

//===----------------------------------------------------------------------===//
// GlobalStoreGraphOp
//===----------------------------------------------------------------------===//

def MLProgram_GlobalStoreGraphOp : MLProgram_Op<"global_store_graph", [
    DeclareOpInterfaceMethods<SymbolUserOpInterface>
  ]> {
  let summary = "Direct store of a value into a mutable global";
  let description = [{
    Performs a non-atomic, non-volatile, non-synchronized store to a mutable
    global.

    It is fully expected that these constraints are not suitable for
    all situations, and alternative ops should be defined and used for more
    advanced cases.

    This op is side effecting and may not be valid to use in graph regions
    without additional consideration to evaluation order constraints.

    Example:

    ```mlir
    %token = ml_program.global_store @foobar = %0 : tensor<?xi32>
      ordering (%in_token -> !ml_program.token) : tensor<?xi32>
    ```
  }];

  let arguments = (ins
    Arg<SymbolRefAttr, "", [MemRead]>:$global,
    AnyType:$value,
    Variadic<MLProgram_TokenType>:$consumeTokens
  );
  let results = (outs
    MLProgram_TokenType:$produceToken
  );

  let assemblyFormat = [{
    $global `=` $value `` custom<TokenOrdering>($consumeTokens, type($produceToken)) `:` type($value) attr-dict
  }];

  let extraClassDeclaration = [{
    /// Gets the corresponding GlobalOp (or nullptr).
    GlobalOp getGlobalOp(SymbolTableCollection &symbolTable);
  }];
}

//===----------------------------------------------------------------------===//
// SubgraphOp
//===----------------------------------------------------------------------===//

def MLProgram_SubgraphOp : MLProgram_Op<"subgraph", [
    FunctionOpInterface, HasOnlyGraphRegion,
    IsolatedFromAbove, RegionKindInterface, SingleBlock, Symbol
  ]> {
  let summary = "An function containing a single `Graph` region";
  let description = [{
    This simple function container represents callables in an ML program where
    the body is a `Graph` region containing a single block. It must be
    terminated by an `output` op which yields values with the same arity and
    types as the `FunctionType` results of the containing `subgraph`.

    This op is a `Symbol` but does not introduce a new `SymbolTable`. As such,
    it cannot represented nested symbols.

    Example:

    ```mlir
    ml_program.subgraph private @some_extern(i32) -> i32
    ml_program.subgraph @compute(%arg0 : i32) -> i32 {
      ml_program.output %arg0 : i32
    }
    ```
  }];

  let arguments = (ins SymbolNameAttr:$sym_name,
                       TypeAttrOf<FunctionType>:$function_type,
                       OptionalAttr<DictArrayAttr>:$arg_attrs,
                       OptionalAttr<DictArrayAttr>:$res_attrs,
                       OptionalAttr<StrAttr>:$sym_visibility);
  let regions = (region AnyRegion:$body);

  let extraClassDeclaration = [{
    //===------------------------------------------------------------------===//
    // FunctionOpInterface Methods
    //===------------------------------------------------------------------===//

    /// Returns the region on the current operation that is callable. This may
    /// return null in the case of an external callable object, e.g. an external
    /// function.
    ::mlir::Region *getCallableRegion() { return isExternal() ? nullptr : &getBody(); }

    /// Returns the argument types of this function.
    ArrayRef<Type> getArgumentTypes() { return getFunctionType().getInputs(); }

    /// Returns the result types of this function.
    ArrayRef<Type> getResultTypes() { return getFunctionType().getResults(); }

    //===------------------------------------------------------------------===//
    // SymbolOpInterface Methods
    //===------------------------------------------------------------------===//

    bool isDeclaration() { return isExternal(); }
  }];

  let hasCustomAssemblyFormat = 1;
}

//===----------------------------------------------------------------------===//
// OutputOp
//===----------------------------------------------------------------------===//

def MLProgram_OutputOp : MLProgram_Op<"output", [
    Pure, HasParent<"SubgraphOp">, ReturnLike, Terminator
  ]> {
  let summary = "Outputs values from a subgraph function";
  let description = [{
    The `output` operation terminates a subgraph by yielding values
    to the caller.
    The operation takes variable number of operands and produces no results.
    The operand number and types must match the signature of the function
    that contains the operation.
  }];

  let arguments = (ins Variadic<AnyType>:$operands);

  let builders = [OpBuilder<(ins), [{
    build($_builder, $_state, std::nullopt);
  }]>];

  let assemblyFormat = "attr-dict ($operands^ `:` type($operands))?";
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// ReturnOp
//===----------------------------------------------------------------------===//

def MLProgram_ReturnOp : MLProgram_Op<"return", [
    Pure, HasParent<"FuncOp">, ReturnLike, Terminator
  ]> {
  let summary = "Returns values from a `func` function";
  let description = [{
    The `return` operation terminates a `func` function by yielding values
    to the caller.
    The operation takes variable number of operands and produces no results.
    The operand number and types must match the signature of the function
    that contains the operation.
  }];

  let arguments = (ins Variadic<AnyType>:$operands);

  let builders = [OpBuilder<(ins), [{
    build($_builder, $_state, std::nullopt);
  }]>];

  let assemblyFormat = "attr-dict ($operands^ `:` type($operands))?";
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// TokenOp
//===----------------------------------------------------------------------===//

def MLProgram_TokenOp : MLProgram_Op<"token", [
    Pure
  ]> {
  let summary = "Produces a new token value";
  let description = [{
    Token values are used to chain side effecting ops in a graph so as to
    establish an execution order. This op produces a token.
  }];

  let results = (outs
    MLProgram_TokenType:$token
  );

  let assemblyFormat = "attr-dict";
}

#endif // MLPROGRAM_OPS


//===-- AMDGPU.td - AMDGPU dialect definitions *- tablegen -*------===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef AMDGPU
#define AMDGPU

include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/IR/EnumAttr.td"
include "mlir/IR/OpBase.td"

def AMDGPU_Dialect : Dialect {
  let name = "amdgpu";
  let cppNamespace = "::mlir::amdgpu";
  let description = [{
    The `AMDGPU` dialect provides wrappers around AMD-specific functionality
    and LLVM intrinsics. These wrappers should be used in conjunction with
    more generic dialects, such as `gpu` and `vector`, when generating LLVM IR
    that will eventually be executed on AMD hardware.
  }];


  let dependentDialects = [
    "ROCDL::ROCDLDialect",
    "arith::ArithDialect",
    "gpu::GPUDialect"
  ];
  let useDefaultAttributePrinterParser = 1;
}

//===----------------------------------------------------------------------===//
// AMDGPU Op definitions
//===----------------------------------------------------------------------===//

class AMDGPU_Op<string mnemonic, list<Trait> traits = []> :
  Op<AMDGPU_Dialect, mnemonic, traits> {}

def AMDGPU_ExtPackedFp8Op :
    AMDGPU_Op<"ext_packed_fp8", [Pure]>,
    Arguments<(ins AnyTypeOf<[F8E5M2FNUZ, F8E4M3FNUZ,
        VectorOfLengthAndType<[1, 2, 3, 4], [F8E5M2FNUZ, F8E4M3FNUZ]>]>:$source,
      ConfinedAttr<I32Attr, [IntNonNegative, IntMaxValue<3>]>:$index)>,
    Results<(outs F32:$res)> {
  let summary = "Extend one of a vector of packed fp8 values to a float";
  let description = [{
    Extend the value `source[index]` to a 32-bit float and return it.

    This rather unusual signature arises from the fact that AMD GPUs cannot
    easily work with sub 32-bit quantities, so the compiler intrinsics for
    extending 8-bit floats (which are, currently, the only way to work with
    this operation) take packed vectors of 4 such floats.

    If the passed-in vector has fewer than four elements, or the input is scalar,
    the remaining values in the <4 x i8> will be filled with with
    undefined values as needed.
  }];
  let assemblyFormat = [{
    attr-dict $source `[` $index `]` `:` type($source) `to` type($res)
  }];
}

def AMDGPU_PackedTrunc2xFp8Op :
    AMDGPU_Op<"packed_trunc_2xfp8", [Pure, AttrSizedOperandSegments]>,
    Arguments<(ins F32:$sourceA,
      Optional<F32>:$sourceB,
      ConfinedAttr<I32Attr, [IntNonNegative, IntMaxValue<1>]>:$wordIndex,
      Optional<FixedVectorOfLengthAndType<[4], [F8E4M3FNUZ, F8E5M2FNUZ]>>:$existing)>,
    Results<(outs FixedVectorOfLengthAndType<[4], [F8E4M3FNUZ, F8E5M2FNUZ]>:$res)> {
  let summary = "Round two floats into a packed vector of 8-bit floats";
  let description = [{
    Round the inputs `sourceA` and `sourceB` (which is undefined if not
    specified) into the low or high word (bottom two or top two) elements
    of the returned vector, keeping the other two elements of `existing`
    unchanged if present (or undefined if it was not passed in).

    The reason for this odd signature is that AMD GPUs cannot easily work with
    sub-registers, and so the conversion intrinsics (which are currently the
    only way to work with 8-bit float types) take packed vectors of 4 8-bit
    values.
  }];
  let assemblyFormat = [{
    attr-dict $sourceA `,` ($sourceB^):(`undef`)?
    `into` ($existing^):(`undef`)? `[` `word` $wordIndex `]`
    `:` type($sourceA) `to` type($res) (`into` type($existing)^)?
  }];
  let hasVerifier = 1;
}

def AMDGPU_PackedStochRoundFp8Op :
    AMDGPU_Op<"packed_stoch_round_fp8", [Pure]>,
    Arguments<(ins F32:$source,
      I32:$stochiasticParam,
      ConfinedAttr<I32Attr, [IntNonNegative, IntMaxValue<3>]>:$storeIndex,
      Optional<FixedVectorOfLengthAndType<[4], [F8E4M3FNUZ, F8E5M2FNUZ]>>:$existing)>,
    Results<(outs FixedVectorOfLengthAndType<[4], [F8E4M3FNUZ, F8E5M2FNUZ]>:$res)> {
  let summary = "Round float stochiastically into a packed vector of 8-bit floats";
  let description = [{
    Round the input `source`, adding in `stochiasticParam`, and place it into
    the `storeIndex`th element of `res`.

    If `existing` is passed in, elements of `res` other than the one at `storeIndex`
    are copied from `existing`.

    The reason for this odd signature is that AMD GPUs cannot easily work with
    sub-registers, and so the conversion intrinsics (which are currently the
    only way to work with 8-bit float types) take packed vectors of 4 8-bit
    values.
  }];
  let assemblyFormat = [{
    attr-dict $source `+` $stochiasticParam
    `into` ($existing^):(`undef`)? `[` $storeIndex `]`
    `:` type($source) `to` type($res) (`into` type($existing)^)?
  }];
  let hasVerifier = 1;
}

/// Raw buffer load
def AMDGPU_RawBufferLoadOp :
    AMDGPU_Op<"raw_buffer_load", [AllElementTypesMatch<["value", "memref"]>,
      AttrSizedOperandSegments]>,
    Arguments<(ins Arg<AnyMemRef, "buffer to load from", [MemRead]>:$memref,
                   Variadic<I32>:$indices,
                   DefaultValuedAttr<BoolAttr, "true">:$boundsCheck,
                   OptionalAttr<I32Attr>:$indexOffset,
                   Optional<I32>:$sgprOffset)>,
    Results<(outs AnyType:$value)> {

  let summary = "Raw Buffer load, exposing GCN features";
  let description = [{
    The `amdgpu.raw_buffer_load` op is a wrapper around the buffer load intrinsics
    available on AMD GPUs, including extensions in newer GPUs.

    The index into the buffer is computed as for `memref.load` with the additon
    of `indexOffset` and `sgprOffset` (which **may or may not** be considered
    in bounds checks and includes any offset present on the memref type if it's
    non-zero).

    All indices and offsets are in units of the memref's data type and are
    converted to bytes during lowering.

    When a load is out of bounds, the instruction returns zero.
    Partially-out of bounds have chipset-dependent behavior: whether reading
    2 elements starting at index 7 of a `memref<8xf32>` returns the last element
    in the first vector component depends on the architecture.

    The memref struct is converted into a buffer resource (a V#) and the arguments
    are translated to intrinsic arguments as follows:
    - The base address of the buffer is the base address of the memref
    - The stride is 0 to enable raw mode
    - The number of records is the size of the memref, in bytes
      In the case of dynamically-shaped memrefs, this is computed at runtime
      as max_d (size(d) * stride(d)) * sizeof(elementType(memref))
    - The offset enable bit is 1, the index enable bit is 0.
    - The thread ID addition bit is off
    - If `boundsCheck` is false and the target chipset is RDNA, OOB_SELECT is set
      to 2 to disable bounds checks, otherwise it is 3
    - The cache coherency bits are off
  }];
  let assemblyFormat = [{
    attr-dict $memref `[` $indices `]`
      (`sgprOffset` $sgprOffset^)? `:`
      type($memref) (`,` type($indices)^)? `->` type($value)
  }];
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

/// Raw buffer store
def AMDGPU_RawBufferStoreOp :
    AMDGPU_Op<"raw_buffer_store", [AllElementTypesMatch<["value", "memref"]>,
      AttrSizedOperandSegments]>,
    Arguments<(ins AnyType:$value,
                   Arg<AnyMemRef, "buffer to store to", [MemWrite]>:$memref,
                   Variadic<I32>:$indices,
                   DefaultValuedAttr<BoolAttr, "true">:$boundsCheck,
                   OptionalAttr<I32Attr>:$indexOffset,
                   Optional<I32>:$sgprOffset)> {

  let summary = "Raw Buffer Store, exposing GCN features";
  let description = [{
    The `amdgpu.raw_buffer_store` op is a wrapper around the buffer store
    intrinsics available on AMD GPUs, including extensions in newer GPUs.

    The store index is computed as in `memref.store` with the addition of
    `indexOffset` (which is included for uniformity with atomics and may be useful
    when writing vectorized code) and `sgprOffset` (which is added after bounds
    checks and implicitly includes the offset of the memref type if non-zero).
    All index components are in terms of the elements of the memref, not bytes,
    and are scaled up appropriately.

    Out of bounds stores are ignored in hardware.
    Wthether a vector write that includes some in-bounds and soeme out-of-bounds
    components is partically completed is chipset-dependent.

    See `amdgpu.raw_buffer_load` for a description of how the underlying
    instruction is constructed.
  }];
  let assemblyFormat = [{
    attr-dict $value `->` $memref `[` $indices `]`
      (`sgprOffset` $sgprOffset^)? `:`
      type($value) `->` type($memref) (`,` type($indices)^)?
  }];
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

// Raw buffer atomic compare-and-swap
def AMDGPU_RawBufferAtomicCmpswapOp :
    AMDGPU_Op<"raw_buffer_atomic_cmpswap", [
      AttrSizedOperandSegments,
      AllTypesMatch<["src", "cmp", "value"]>,
      AllElementTypesMatch<["value", "memref"]>]>,
    Arguments<(ins AnyType:$src,
                   AnyType:$cmp,
                   Arg<AnyMemRef, "buffer to operate on", [MemRead, MemWrite]>:$memref,
                   Variadic<I32>:$indices,
                   DefaultValuedAttr<BoolAttr, "true">:$boundsCheck,
                   OptionalAttr<I32Attr>:$indexOffset,
                   Optional<I32>:$sgprOffset)>,
    Results<(outs AnyType:$value)> {

  let summary = "Raw Buffer Atomic compare-and-swap";
  let description = [{
    The `amdgpu.raw_buffer_atomic_cmpswap` op is a wrapper around the
    buffer-based atomic compare-and-swap min available on AMD GPUs.

    The index into the buffer is computed as for `memref.store` with the addition
    of `indexOffset` (which is used to aid in emitting vectorized code) and,
    if present `sgprOffset` (which is added after bounds checks and includes
    any non-zero offset on the memref type).

    All indexing components are given in terms of the memref's element size, not
    the byte lengths required by the intrinsic.

    Out of bounds atomic operations are ignored in hardware.

    See `amdgpu.raw_buffer_load` for a description of how the underlying
    instruction is constructed.
  }];
  let assemblyFormat = [{
    attr-dict $src `,` $cmp `->` $memref `[` $indices `]`
      (`sgprOffset` $sgprOffset^)? `:`
      type($value) `->` type($memref) `,` type($indices)
  }];
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

// Raw buffer atomic floating point add
def AMDGPU_RawBufferAtomicFaddOp :
    AMDGPU_Op<"raw_buffer_atomic_fadd", [AllElementTypesMatch<["value", "memref"]>,
      AttrSizedOperandSegments]>,
    Arguments<(ins AnyTypeOf<[F32, VectorOfLengthAndType<[2], [F16, BF16]>]>:$value,
                   Arg<AnyMemRef, "buffer to operate on", [MemRead, MemWrite]>:$memref,
                   Variadic<I32>:$indices,
                   DefaultValuedAttr<BoolAttr, "true">:$boundsCheck,
                   OptionalAttr<I32Attr>:$indexOffset,
                   Optional<I32>:$sgprOffset)> {

  let summary = "Raw Buffer Floating-point Atomic Add (MI-* only)";
  let description = [{
    The `amdgpu.raw_buffer_atomic_fadd` op is a wrapper around the
    buffer-based atomic floating point addition available on the MI-* series
    of AMD GPUs.

    The index into the buffer is computed as for `memref.store` with the addition
    of `indexOffset` (which is used to aid in emitting vectorized code) and,
    if present `sgprOffset` (which is added after bounds checks and includes
    any non-zero offset on the memref type).

    All indexing components are given in terms of the memref's element size, not
    the byte lengths required by the intrinsic.

    Out of bounds atomic operations are ignored in hardware.

    See `amdgpu.raw_buffer_load` for a description of how the underlying
    instruction is constructed.
  }];
  let assemblyFormat = [{
    attr-dict $value `->` $memref `[` $indices `]`
      (`sgprOffset` $sgprOffset^)? `:`
      type($value) `->` type($memref) `,` type($indices)
  }];
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

// Raw buffer atomic floating point max
def AMDGPU_RawBufferAtomicFmaxOp :
    AMDGPU_Op<"raw_buffer_atomic_fmax", [AllElementTypesMatch<["value", "memref"]>,
      AttrSizedOperandSegments]>,
    Arguments<(ins AnyTypeOf<[F32, F64]>:$value,
                   Arg<AnyMemRef, "buffer to operate on", [MemRead, MemWrite]>:$memref,
                   Variadic<I32>:$indices,
                   DefaultValuedAttr<BoolAttr, "true">:$boundsCheck,
                   OptionalAttr<I32Attr>:$indexOffset,
                   Optional<I32>:$sgprOffset)> {

  let summary = "Raw Buffer Floating-point Atomic Max (non-GFX9)";
  let description = [{
    The `amdgpu.raw_buffer_atomic_fmax` op is a wrapper around the
    buffer-based atomic floating point max available on AMD GPUs (except GFX9).

    The index into the buffer is computed as for `memref.store` with the addition
    of `indexOffset` (which is used to aid in emitting vectorized code) and,
    if present `sgprOffset` (which is added after bounds checks and includes
    any non-zero offset on the memref type).

    All indexing components are given in terms of the memref's element size, not
    the byte lengths required by the intrinsic.

    Out of bounds atomic operations are ignored in hardware.

    See `amdgpu.raw_buffer_load` for a description of how the underlying
    instruction is constructed.
  }];
  let assemblyFormat = [{
    attr-dict $value `->` $memref `[` $indices `]`
      (`sgprOffset` $sgprOffset^)? `:`
      type($value) `->` type($memref) `,` type($indices)
  }];
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

// Raw buffer atomic signed integer max
def AMDGPU_RawBufferAtomicSmaxOp :
    AMDGPU_Op<"raw_buffer_atomic_smax", [
      AttrSizedOperandSegments]>,
    Arguments<(ins I32:$value,
                   Arg<AnyMemRef, "buffer to operate on", [MemRead, MemWrite]>:$memref,
                   Variadic<I32>:$indices,
                   DefaultValuedAttr<BoolAttr, "true">:$boundsCheck,
                   OptionalAttr<I32Attr>:$indexOffset,
                   Optional<I32>:$sgprOffset)> {

  let summary = "Raw Buffer Signed Integer Atomic Max";
  let description = [{
    The `amdgpu.raw_buffer_atomic_smax` op is a wrapper around the
    buffer-based atomic signed integer max available on AMD GPUs.

    The index into the buffer is computed as for `memref.store` with the addition
    of `indexOffset` (which is used to aid in emitting vectorized code) and,
    if present `sgprOffset` (which is added after bounds checks and includes
    any non-zero offset on the memref type).

    All indexing components are given in terms of the memref's element size, not
    the byte lengths required by the intrinsic.

    Out of bounds atomic operations are ignored in hardware.

    See `amdgpu.raw_buffer_load` for a description of how the underlying
    instruction is constructed.
  }];
  let assemblyFormat = [{
    attr-dict $value `->` $memref `[` $indices `]`
      (`sgprOffset` $sgprOffset^)? `:`
      type($value) `->` type($memref) `,` type($indices)
  }];
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

// Raw buffer atomic unsigned integer min
def AMDGPU_RawBufferAtomicUminOp :
    AMDGPU_Op<"raw_buffer_atomic_umin", [
      AttrSizedOperandSegments]>,
    Arguments<(ins I32:$value,
                   Arg<AnyMemRef, "buffer to operate on", [MemRead, MemWrite]>:$memref,
                   Variadic<I32>:$indices,
                   DefaultValuedAttr<BoolAttr, "true">:$boundsCheck,
                   OptionalAttr<I32Attr>:$indexOffset,
                   Optional<I32>:$sgprOffset)> {

  let summary = "Raw Buffer Unsigned Integer Atomic Min";
  let description = [{
    The `amdgpu.raw_buffer_atomic_umin` op is a wrapper around the
    buffer-based atomic signed integer min available on AMD GPUs.

    The index into the buffer is computed as for `memref.store` with the addition
    of `indexOffset` (which is used to aid in emitting vectorized code) and,
    if present `sgprOffset` (which is added after bounds checks and includes
    any non-zero offset on the memref type).

    All indexing components are given in terms of the memref's element size, not
    the byte lengths required by the intrinsic.

    Out of bounds atomic operations are ignored in hardware.

    See `amdgpu.raw_buffer_load` for a description of how the underlying
    instruction is constructed.
  }];
  let assemblyFormat = [{
    attr-dict $value `->` $memref `[` $indices `]`
      (`sgprOffset` $sgprOffset^)? `:`
      type($value) `->` type($memref) `,` type($indices)
  }];
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

def AMDGPU_DPPPerm : I32EnumAttr<"DPPPerm",
    "The possible permutations for a DPP operation",
    [
      I32EnumAttrCase<"quad_perm",  0>,
      I32EnumAttrCase<"row_shl",    1>,
      I32EnumAttrCase<"row_shr",    2>,
      I32EnumAttrCase<"row_ror",    3>,
      I32EnumAttrCase<"wave_shl",   4>,
      I32EnumAttrCase<"wave_shr",   5>,
      I32EnumAttrCase<"wave_ror",   6>,
      I32EnumAttrCase<"wave_rol",   7>,
      I32EnumAttrCase<"row_mirror", 8>,
      I32EnumAttrCase<"row_half_mirror", 9>,
      I32EnumAttrCase<"row_bcast_15", 10>,
      I32EnumAttrCase<"row_bcast_31", 11>
    ]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::amdgpu";
}

def AMDGPU_DPPPermAttr : EnumAttr<AMDGPU_Dialect, AMDGPU_DPPPerm,
  "dpp_perm">;

def AMDGPU_DPPOp : AMDGPU_Op<"dpp", [SameTypeOperands, AllTypesMatch<["result", "old", "src"]>]>,
  Arguments<(ins AnyType:$old,
                  AnyType:$src,
                  AMDGPU_DPPPermAttr:$kind,
                  OptionalAttr<AnyAttrOf<[I32Attr, ArrayAttr, UnitAttr]>>:$permArgument,
                  DefaultValuedAttr<I32Attr, "0xf">:$row_mask,
                  DefaultValuedAttr<I32Attr, "0xf">:$bank_mask,
                  DefaultValuedAttr<BoolAttr, "false">:$bound_ctrl)> {
  let summary = "AMDGPU DPP operation";
  let description = [{
    This operation represents DPP functionality in a GPU program.
     DPP provides the following operations:
    - Full crossbar in a group of four (`quad_perm`)
    - Wavefront shift left by one lane (`wave_shl`)
    - Wavefront shift right by one lane (`wave_shr`)
    - Wavefront rotate right by one lane (`wave_ror`)
    - Wavefront rotate left by one lane (`wave_rol`)
    - Row shift left by 1–15 lanes (`row_shl`)
    - Row shift right by 1–15 lanes (`row_shr`)
    - Row rotate right by 1–15 lanes (`row_ror`)
    - Reverse within a row (`row_mirror`)
    - Reverse within a half-row (`row_half_mirror`)
    - Broadcast the 15th lane of each row to the next row (`row_bcast`)
    - Broadcast lane 31 to rows 2 and 3 (`row_bcast`)
  }];
  let results = (outs AnyType:$result);
  let assemblyFormat = [{
    $old $src $kind (`(` $permArgument^ `)`)? attr-dict `:` type($result)
  }];
  let hasVerifier = 1;
}

def AMDGPU_LDSBarrierOp : AMDGPU_Op<"lds_barrier"> {
  let summary = "Barrier that includes a wait for LDS memory operations.";
  let description = [{
    `amdgpu.lds_barrier` is both a barrier (all workitems in a workgroup must reach
    the barrier before any of them may proceed past it) and a wait for all
    operations that affect the Local Data Store (LDS) issued from that wrokgroup
    to complete before the workgroup may continue. Since the LDS is per-workgroup
    memory, this barrier may be used, for example, to ensure all workitems have
    written data to LDS before any workitem attempts to read from it.

    Note that `lds_barrier` does **not** force reads to or from global memory
    to complete before execution continues. Therefore, it should be used when
    operations on global memory can be issued far in advance of when their results
    are used (for example, by writing them to LDS).

    WARNING: On architectures that do not support the BackOffBarrier feature,
    (those which will implement this barrier by emitting inline assembly),
    use of this operation will impede the usabiliity of memory watches (including
    breakpoints set on variables) when debugging.
  }];
  let assemblyFormat = "attr-dict";
}

def AMDGPU_SchedBarrierOpOpt : I32BitEnumAttr<"sched_barrier_opt_enum",
    "The possible options for scheduling barriers",
    [
      I32BitEnumAttrCaseNone<"none">,
      I32BitEnumAttrCaseBit<"non_mem_non_sideffect", 0>,
      I32BitEnumAttrCaseBit<"valu", 1>,
      I32BitEnumAttrCaseBit<"salu", 2>,
      I32BitEnumAttrCaseBit<"mfma_wmma",  3>,
      I32BitEnumAttrCaseBit<"all_vmem",  4>,
      I32BitEnumAttrCaseBit<"vmem_read",  5>,
      I32BitEnumAttrCaseBit<"vmem_write", 6>,
      I32BitEnumAttrCaseBit<"all_ds", 7>,
      I32BitEnumAttrCaseBit<"ds_read", 8>,
      I32BitEnumAttrCaseBit<"ds_write", 9>,
      I32BitEnumAttrCaseBit<"transcendental", 10>
    ]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::amdgpu";
}

def AMDGPU_SchedBarrierOpOptAttr : EnumAttr<AMDGPU_Dialect, AMDGPU_SchedBarrierOpOpt,
  "sched_barrier_opt">{
   let assemblyFormat = "`<` $value `>`";
}

def AMDGPU_SchedBarrierOp :
  AMDGPU_Op<"sched_barrier">,
  Arguments<(ins  AMDGPU_SchedBarrierOpOptAttr:$opts)>
  {
  let summary = "Barrier that limits the backend scheduler of instruction movement";
  let description = [{
    `amdgpu.sched_barrier` serves as a barrier that could be
    configured to restrict movements of instructions through it as
    defined by sched_barrier_opts.
  }];
  let assemblyFormat = [{
    `allow` `=` $opts attr-dict
  }];
}

def AMDGPU_MFMAPermB : I32EnumAttr<"MFMAPermB",
    "The possible permutations of the lanes storing B available in an MFMA",
    [
      I32EnumAttrCase<"none",            0>,
      I32EnumAttrCase<"bcast_first_32",  1>,
      I32EnumAttrCase<"bcast_second_32", 2>,
      I32EnumAttrCase<"rotate_16_right", 3>,
      I32EnumAttrCase<"bcast_first_16",  4>,
      I32EnumAttrCase<"bcast_second_16", 5>,
      I32EnumAttrCase<"bcast_third_16",  6>,
      I32EnumAttrCase<"bcast_fourth_16", 7>
    ]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::amdgpu";
}

def AMDGPU_MFMAPermBAttr : EnumAttr<AMDGPU_Dialect, AMDGPU_MFMAPermB,
  "mfma_perm_b">;

// mfma
def MFMAInTypes : AnyTypeOf<[F32, F64, I32, I64,
                             VectorOfLengthAndType<[2], [F32]>,
                             VectorOfLengthAndType<[4], [F16]>,
                             VectorOfLengthAndType<[2, 4], [BF16]>,
                             VectorOfLengthAndType<[4, 8], [I8]>,
                             VectorOfLengthAndType<[8], [F8E5M2FNUZ, F8E4M3FNUZ]>]>;
def MFMAOutTypes : AnyTypeOf<[F64,
                              VectorOfLengthAndType<[4, 16, 32], [F32]>,
                              VectorOfLengthAndType<[4, 16, 32], [I32]>,
                              VectorOfLengthAndType<[4], [F64]>]>;
// wmma
def WMMAInTypes : AnyTypeOf<[VectorOfLengthAndType<[8, 16], [F16, BF16, I8, SI8, UI8, F8E4M3FN, F8E5M2]>]>;
def WMMAOutTypes : AnyTypeOf<[VectorOfLengthAndType<[4, 8], [F32, I32]>,
                              VectorOfLengthAndType<[8, 16], [F16, BF16]>]>;

def AMDGPU_MFMAOp :
    AMDGPU_Op<"mfma", [AllTypesMatch<["destC", "destD"]>,
                        Pure]>,
    Arguments<(ins
                   I32Attr:$m,
                   I32Attr:$n,
                   I32Attr:$k,
                   I32Attr:$blocks,
                   MFMAInTypes:$sourceA,
                   MFMAInTypes:$sourceB,
                   MFMAOutTypes:$destC,
                   DefaultValuedAttr<I32Attr, "0">:$cbsz,
                   DefaultValuedAttr<I32Attr, "0">:$abid,
                   DefaultValuedAttr<AMDGPU_MFMAPermBAttr,
                    "::mlir::amdgpu::MFMAPermB::none">:$blgp,
                   UnitAttr:$reducePrecision,
                   UnitAttr:$negateA,
                   UnitAttr:$negateB,
                   UnitAttr:$negateC)>,
    Results<(outs MFMAOutTypes: $destD)> {
  let summary = "MLIR wrapper for CDNA mfma instructions";
  let description = [{
    The `amdgpu.mfma` op is an MLIR wrapper around intrinsics
    for various `mfma` instructions in the CDNA architecture, which perform
    multiple outer products in order to allow fast matrix multiplication.

    The wrapper will select an appropriate `mfma` instruction, if one is available,
    based on the provided `m`, `k`, `n`, and `nBlks` attributes, along with the
    types of the source and destination arguments.

    For information on the layouts of the input and output matrces (which are stored
    in `sourceA`, `sourceB`, `destC`, and `destD`), see the CDNA ISA documentation.

    The `cbsz`, `abid`, and `blgp` parameters control how the lanes of the wave
    are permuted when matrix data is being loaded: `blgp` can be any number of
    fixed permutations, `cbsz` specifies the log_2 of the number of chunks the lanes
    holding sourceA are split into, and `abid` selects one of those chunks.

    Note, this wrapper allows specifying `vector<4Kxi8>` arguments to MFMA
    intrinsics that take an integer type of width `4K`. For example,
    one can provide a vector<4xi8> as an argument to an MFMA instruction that
    logically takes 4 i8s but whose intrinsics are specified to take an i32.
    In these cases, the bytes in the vector will be concatenated in little-endian
    order (that is, v[0] will go to arg[7:0], v[1] to arg[15:8] and so on).

    The negateA, negateB, and negateC flags are only supported for double-precision
    operations on gfx940+.
  }];
  let assemblyFormat = [{
    $sourceA `*` $sourceB `+` $destC
    attr-dict
    `blgp` `=` $blgp
    `:` type($sourceA) `,` type($sourceB) `,` type($destC)
  }];
  let hasVerifier = 1;
}

def AMDGPU_WMMAOp :
    AMDGPU_Op<"wmma", [AllTypesMatch<["destC", "destD"]>,
                       AllTypesMatch<["sourceA", "sourceB"]>,
                        Pure]>,
    Arguments<(ins
                   WMMAInTypes:$sourceA,
                   WMMAInTypes:$sourceB,
                   WMMAOutTypes:$destC,
                   DefaultValuedAttr<ConfinedAttr<I32Attr, [IntMinValue<0>, IntMaxValue<1>]>, "0">:$subwordOffset,
                   UnitAttr:$unsignedA,
                   UnitAttr:$unsignedB,
                   UnitAttr:$clamp)>,
    Results<(outs WMMAOutTypes: $destD)> {
  let summary = "MLIR wrapper for RDNA3 wmma instructions";
  let description = [{
    The `amdgpu.wmma` op is an MLIR wrapper around intrinsics
    for various `wmma` instructions in the RDNA3 architecture, which perform
    a 16x16 matrix multiplication for different data types.

    When emitting f16->f16 (or bf16->bf16) wmma the output is a 16xf16 (or 16xbf16) vector
    containing only 8 valid values:
      - If `subwordOffset` is 0, then the output is stored at indices 0, 2, 4, ..., 14.
      - If `subwordOffset` is 1, then the output is stored at indices 1, 3, 5, ..., 15.

    `unsignedA` and `unsignedB` flag that the `int8` LLVM inputs are unsigned.

    The `clamp` flag is used to saturate the output of type T to numeric_limits<T>::max()
    in case of overflow.
  }];
  let assemblyFormat = [{
    $sourceA `*` $sourceB `+` $destC
    attr-dict
    `:` type($sourceA) `,` type($sourceB) `,` type($destC)
  }];
  let hasVerifier = 1;
}

#endif // AMDGPU


//===-- NVVMOps.td - NVVM IR dialect op definition file ----*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This is the NVVM IR operation definition file.
//
//===----------------------------------------------------------------------===//

#ifndef NVVMIR_OPS
#define NVVMIR_OPS

include "mlir/IR/EnumAttr.td"
include "mlir/Dialect/GPU/IR/CompilationAttrInterfaces.td"
include "mlir/Dialect/LLVMIR/LLVMOpBase.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/Dialect/LLVMIR/BasicPtxBuilderInterface.td"

def LLVM_PointerGeneric : LLVM_PointerInAddressSpace<0>;
def LLVM_PointerGlobal : LLVM_PointerInAddressSpace<1>;
def LLVM_PointerShared : LLVM_PointerInAddressSpace<3>;

//===----------------------------------------------------------------------===//
// NVVM dialect definitions
//===----------------------------------------------------------------------===//

def NVVM_Dialect : Dialect {
  let name = "nvvm";
  let cppNamespace = "::mlir::NVVM";
  let dependentDialects = ["LLVM::LLVMDialect"];
  let hasOperationAttrVerify = 1;

  let extraClassDeclaration = [{
    /// Get the name of the attribute used to annotate external kernel
    /// functions.
    static StringRef getKernelFuncAttrName() { return "nvvm.kernel"; }
    /// Get the name of the attribute used to annotate max threads required
    /// per CTA for kernel functions.
    static StringRef getMaxntidAttrName() { return "nvvm.maxntid"; }
    /// Get the name of the metadata names for each dimension
    static StringRef getMaxntidXName() { return "maxntidx"; }
    static StringRef getMaxntidYName() { return "maxntidy"; }
    static StringRef getMaxntidZName() { return "maxntidz"; }

    /// Get the name of the attribute used to annotate exact threads required
    /// per CTA for kernel functions.
    static StringRef getReqntidAttrName() { return "nvvm.reqntid"; }
    /// Get the name of the metadata names for each dimension
    static StringRef getReqntidXName() { return "reqntidx"; }
    static StringRef getReqntidYName() { return "reqntidy"; }
    static StringRef getReqntidZName() { return "reqntidz"; }

    /// Get the name of the attribute used to annotate exact CTAs required
    /// per cluster for kernel functions.
    static StringRef getClusterDimAttrName() { return "nvvm.cluster_dim"; }
    /// Get the name of the metadata names for each dimension
    static StringRef getClusterDimXName() { return "cluster_dim_x"; }
    static StringRef getClusterDimYName() { return "cluster_dim_y"; }
    static StringRef getClusterDimZName() { return "cluster_dim_z"; }

    /// Get the name of the attribute used to annotate maximum number of
    /// CTAs per cluster for kernel functions.
    static StringRef getClusterMaxBlocksAttrName() {  return "nvvm.cluster_max_blocks"; }

    /// Get the name of the attribute used to annotate min CTA required
    /// per SM for kernel functions.
    static StringRef getMinctasmAttrName() { return "nvvm.minctasm"; }

    /// Get the name of the attribute used to annotate max number of
    /// registers that can be allocated per thread.
    static StringRef getMaxnregAttrName() { return "nvvm.maxnreg"; }

    /// Get the name of the attribute used to annotate kernel arguments that
    /// are grid constants.
    static StringRef getGridConstantAttrName() { return "nvvm.grid_constant"; }

    /// Verify an attribute from this dialect on the argument at 'argIndex' for
    /// the region at 'regionIndex' on the given operation. Returns failure if
    /// the verification failed, success otherwise. This hook may optionally be
    /// invoked from any operation containing a region.
    LogicalResult verifyRegionArgAttribute(Operation *op,
                                           unsigned regionIndex,
                                           unsigned argIndex,
                                           NamedAttribute argAttr) override;
  }];

  let useDefaultAttributePrinterParser = 1;
}

//===----------------------------------------------------------------------===//
// NVVM op definitions
//===----------------------------------------------------------------------===//

class NVVM_Op<string mnemonic, list<Trait> traits = []> :
  LLVM_OpBase<NVVM_Dialect, mnemonic, traits> {
}

/// Base class that defines BasicPtxBuilderOpInterface. 
class NVVM_PTXBuilder_Op<string mnemonic, 
  list<Trait> traits = [DeclareOpInterfaceMethods<BasicPtxBuilderOpInterface>]> :
  LLVM_OpBase<NVVM_Dialect, mnemonic, traits> {
}

//===----------------------------------------------------------------------===//
// NVVM attribute definitions
//===----------------------------------------------------------------------===//

class NVVM_Attr<string attrName, string attrMnemonic, list<Trait> traits = []>
    : AttrDef<NVVM_Dialect, attrName, traits> {
  let mnemonic = attrMnemonic;
}

//===----------------------------------------------------------------------===//
// NVVM intrinsic operations
//===----------------------------------------------------------------------===//

class NVVM_IntrOp<string mnem, list<Trait> traits,
                  int numResults>
  : LLVM_IntrOpBase<NVVM_Dialect, mnem, "nvvm_" # !subst(".", "_", mnem),
                    /*list<int> overloadedResults=*/[],
                    /*list<int> overloadedOperands=*/[],
                    traits, numResults>;


//===----------------------------------------------------------------------===//
// NVVM special register op definitions
//===----------------------------------------------------------------------===//

class NVVM_SpecialRegisterOp<string mnemonic, list<Trait> traits = []> :
  NVVM_IntrOp<mnemonic, !listconcat(traits, [Pure]), 1> {
  let arguments = (ins);
  let assemblyFormat = "attr-dict `:` type($res)";
}

class NVVM_SpecialRangeableRegisterOp<string mnemonic, list<Trait> traits = []> :
  NVVM_SpecialRegisterOp<mnemonic, traits> {
  let arguments = (ins OptionalAttr<LLVM_ConstantRangeAttr>:$range);
  let assemblyFormat = "(`range` $range^)? attr-dict `:` type($res)";
  let llvmBuilder = baseLlvmBuilder # setRangeRetAttrCode # baseLlvmBuilderCoda;
  let mlirBuilder = baseMlirBuilder # importRangeRetAttrCode # baseMlirBuilderCoda;

  // Backwards-compatibility builder for an unspecified range.
  let builders = [
    OpBuilder<(ins "Type":$resultType), [{
      build($_builder, $_state, resultType, ::mlir::LLVM::ConstantRangeAttr{});
    }]>
  ];
}

//===----------------------------------------------------------------------===//
// Lane, Warp, SM, Grid index and range
def NVVM_LaneIdOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.laneid">;
def NVVM_WarpSizeOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.warpsize">;
def NVVM_WarpIdOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.warpid">;
def NVVM_WarpDimOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.nwarpid">;
def NVVM_SmIdOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.smid">;
def NVVM_SmDimOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.nsmid">;
def NVVM_GridIdOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.gridid">;

//===----------------------------------------------------------------------===//
// Lane Mask Comparison Ops
def NVVM_LaneMaskEqOp : NVVM_SpecialRegisterOp<"read.ptx.sreg.lanemask.eq">;
def NVVM_LaneMaskLeOp : NVVM_SpecialRegisterOp<"read.ptx.sreg.lanemask.le">;
def NVVM_LaneMaskLtOp : NVVM_SpecialRegisterOp<"read.ptx.sreg.lanemask.lt">;
def NVVM_LaneMaskGeOp : NVVM_SpecialRegisterOp<"read.ptx.sreg.lanemask.ge">;
def NVVM_LaneMaskGtOp : NVVM_SpecialRegisterOp<"read.ptx.sreg.lanemask.gt">;

//===----------------------------------------------------------------------===//
// Thread index and range
def NVVM_ThreadIdXOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.tid.x">;
def NVVM_ThreadIdYOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.tid.y">;
def NVVM_ThreadIdZOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.tid.z">;
def NVVM_BlockDimXOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.ntid.x">;
def NVVM_BlockDimYOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.ntid.y">;
def NVVM_BlockDimZOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.ntid.z">;

//===----------------------------------------------------------------------===//
// Block index and range
def NVVM_BlockIdXOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.ctaid.x">;
def NVVM_BlockIdYOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.ctaid.y">;
def NVVM_BlockIdZOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.ctaid.z">;
def NVVM_GridDimXOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.nctaid.x">;
def NVVM_GridDimYOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.nctaid.y">;
def NVVM_GridDimZOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.nctaid.z">;

//===----------------------------------------------------------------------===//
// CTA Cluster index and range
def NVVM_ClusterIdXOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.clusterid.x">;
def NVVM_ClusterIdYOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.clusterid.y">;
def NVVM_ClusterIdZOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.clusterid.z">;
def NVVM_ClusterDimXOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.nclusterid.x">;
def NVVM_ClusterDimYOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.nclusterid.y">;
def NVVM_ClusterDimZOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.nclusterid.z">;


//===----------------------------------------------------------------------===//
// CTA index and range within Cluster
def NVVM_BlockInClusterIdXOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.cluster.ctaid.x">;
def NVVM_BlockInClusterIdYOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.cluster.ctaid.y">;
def NVVM_BlockInClusterIdZOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.cluster.ctaid.z">;
def NVVM_ClusterDimBlocksXOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.cluster.nctaid.x">;
def NVVM_ClusterDimBlocksYOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.cluster.nctaid.y">;
def NVVM_ClusterDimBlocksZOp : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.cluster.nctaid.z">;

//===----------------------------------------------------------------------===//
// CTA index and across Cluster dimensions
def NVVM_ClusterId : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.cluster.ctarank">;
def NVVM_ClusterDim : NVVM_SpecialRangeableRegisterOp<"read.ptx.sreg.cluster.nctarank">;

//===----------------------------------------------------------------------===//
// Clock registers
def NVVM_ClockOp : NVVM_SpecialRegisterOp<"read.ptx.sreg.clock">;
def NVVM_Clock64Op : NVVM_SpecialRegisterOp<"read.ptx.sreg.clock64">;
def NVVM_GlobalTimerOp : NVVM_SpecialRegisterOp<"read.ptx.sreg.globaltimer">;

//===----------------------------------------------------------------------===//
// envreg registers
foreach index = !range(0, 32) in {
  def NVVM_EnvReg # index # Op : NVVM_SpecialRegisterOp<"read.ptx.sreg.envreg" # index>;
}

//===----------------------------------------------------------------------===//
// NVVM approximate op definitions
//===----------------------------------------------------------------------===//

def NVVM_RcpApproxFtzF32Op : NVVM_IntrOp<"rcp.approx.ftz.f", [Pure], 1> {
  let arguments = (ins F32:$arg);
  let results = (outs F32:$res);
  let assemblyFormat = "$arg attr-dict `:` type($res)";
}

//===----------------------------------------------------------------------===//
// NVVM redux op definitions
//===----------------------------------------------------------------------===//

def ReduxKindNone : I32EnumAttrCase<"NONE", 0, "none">;
def ReduxKindAdd  : I32EnumAttrCase<"ADD", 1, "add">;
def ReduxKindAnd  : I32EnumAttrCase<"AND", 2, "and">;
def ReduxKindMax  : I32EnumAttrCase<"MAX", 3, "max">;
def ReduxKindMin  : I32EnumAttrCase<"MIN", 4, "min">;
def ReduxKindOr   : I32EnumAttrCase<"OR", 5, "or">;
def ReduxKindUmax : I32EnumAttrCase<"UMAX", 6, "umax">;
def ReduxKindUmin : I32EnumAttrCase<"UMIN", 7, "umin">;
def ReduxKindXor  : I32EnumAttrCase<"XOR", 8, "xor">; 

/// Enum attribute of the different kinds.
def ReduxKind : I32EnumAttr<"ReduxKind", "NVVM redux kind",
  [ReduxKindAdd, ReduxKindAnd, ReduxKindMax, ReduxKindMin, ReduxKindOr, 
    ReduxKindUmax, ReduxKindUmin, ReduxKindXor]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::NVVM";
}

def ReduxKindAttr : EnumAttr<NVVM_Dialect, ReduxKind, "redux_kind">;

def NVVM_ReduxOp :
  NVVM_Op<"redux.sync">,
  Results<(outs LLVM_Type:$res)>,
  Arguments<(ins LLVM_Type:$val,
                 ReduxKindAttr:$kind,
                 I32:$mask_and_clamp)> {
  string llvmBuilder = [{
      auto intId = getReduxIntrinsicId($_resultType, $kind);
      $res = createIntrinsicCall(builder, intId, {$val, $mask_and_clamp});
  }];
  let assemblyFormat = [{
    $kind $val `,` $mask_and_clamp  attr-dict `:` type($val) `->` type($res)
   }];   
}

//===----------------------------------------------------------------------===//
// NVVM Split arrive/wait barrier
//===----------------------------------------------------------------------===//

/// mbarrier.init instruction with generic pointer type
def NVVM_MBarrierInitOp : NVVM_PTXBuilder_Op<"mbarrier.init">,
  Arguments<(ins LLVM_AnyPointer:$addr, I32:$count, PtxPredicate:$predicate)> {
  string llvmBuilder = [{
      createIntrinsicCall(builder, llvm::Intrinsic::nvvm_mbarrier_init, {$addr, $count});
  }];
  let assemblyFormat = "$addr `,` $count (`,` `predicate` `=` $predicate^)? attr-dict `:` type(operands)";
  let extraClassDeclaration = [{
    bool hasIntrinsic() { if(getPredicate()) return false; return true; }
  }];
  let extraClassDefinition = [{
    std::string $cppClass::getPtx() { return std::string("mbarrier.init.b64 [%0], %1;"); }
  }];
}

/// mbarrier.init instruction with shared pointer type
def NVVM_MBarrierInitSharedOp : NVVM_PTXBuilder_Op<"mbarrier.init.shared">,
  Arguments<(ins LLVM_PointerShared:$addr, I32:$count, PtxPredicate:$predicate)> {
  string llvmBuilder = [{
      createIntrinsicCall(builder, llvm::Intrinsic::nvvm_mbarrier_init_shared, {$addr, $count});
  }];
  let assemblyFormat = "$addr `,` $count (`,` `predicate` `=` $predicate^)? attr-dict `:` type(operands)";
  let extraClassDeclaration = "bool hasIntrinsic() { return !getPredicate(); }";
  let extraClassDefinition = [{
    std::string $cppClass::getPtx() { return std::string("mbarrier.init.shared.b64 [%0], %1;"); }
  }];
}

def NVVM_MBarrierInvalOp : NVVM_Op<"mbarrier.inval">,
  Arguments<(ins LLVM_AnyPointer:$addr)> {
  string llvmBuilder = [{
      createIntrinsicCall(builder, llvm::Intrinsic::nvvm_mbarrier_inval, {$addr});
  }];
  let assemblyFormat = "$addr attr-dict `:` type(operands)";
}

def NVVM_MBarrierInvalSharedOp : NVVM_Op<"mbarrier.inval.shared">,
  Arguments<(ins LLVM_PointerShared:$addr)> {
  string llvmBuilder = [{
      createIntrinsicCall(builder, llvm::Intrinsic::nvvm_mbarrier_inval_shared, {$addr});
  }];
  let assemblyFormat = "$addr attr-dict `:` type(operands)";
}

def NVVM_MBarrierArriveOp : NVVM_Op<"mbarrier.arrive">,
  Results<(outs LLVM_Type:$res)>,
  Arguments<(ins LLVM_AnyPointer:$addr)> {
  string llvmBuilder = [{
      $res = createIntrinsicCall(builder, llvm::Intrinsic::nvvm_mbarrier_arrive, {$addr});
  }];
  let assemblyFormat = "$addr attr-dict `:` type($addr) `->` type($res)";
}

def NVVM_MBarrierArriveSharedOp : NVVM_Op<"mbarrier.arrive.shared">,
  Results<(outs LLVM_Type:$res)>,
  Arguments<(ins LLVM_PointerShared:$addr)> {
  string llvmBuilder = [{
      $res = createIntrinsicCall(builder, llvm::Intrinsic::nvvm_mbarrier_arrive_shared, {$addr});
  }];
  let assemblyFormat = "$addr attr-dict `:` qualified(type($addr)) `->` type($res)";
}

def NVVM_MBarrierArriveNocompleteOp : NVVM_Op<"mbarrier.arrive.nocomplete">,
  Results<(outs LLVM_Type:$res)>,
  Arguments<(ins LLVM_AnyPointer:$addr, I32:$count)> {
  string llvmBuilder = [{
      $res = createIntrinsicCall(builder, llvm::Intrinsic::nvvm_mbarrier_arrive_noComplete, {$addr, $count});
  }];
  let assemblyFormat = "$addr `,` $count attr-dict `:` type(operands) `->` type($res)";
}

def NVVM_MBarrierArriveNocompleteSharedOp : NVVM_Op<"mbarrier.arrive.nocomplete.shared">,
  Results<(outs LLVM_Type:$res)>,
  Arguments<(ins LLVM_PointerShared:$addr, I32:$count)> {
  string llvmBuilder = [{
      $res = createIntrinsicCall(builder, llvm::Intrinsic::nvvm_mbarrier_arrive_noComplete_shared, {$addr, $count});
  }];
  let assemblyFormat = "$addr `,` $count attr-dict `:` type(operands) `->` type($res)";
}

def NVVM_MBarrierArriveExpectTxOp : NVVM_PTXBuilder_Op<"mbarrier.arrive.expect_tx">,  
  Arguments<(ins LLVM_AnyPointer:$addr, I32:$txcount, PtxPredicate:$predicate)> {
  let assemblyFormat = "$addr `,` $txcount (`,` `predicate` `=` $predicate^)? attr-dict `:` type(operands)";
  let extraClassDefinition = [{
    std::string $cppClass::getPtx() { return std::string("mbarrier.arrive.expect_tx.b64 _, [%0], %1;"); }
  }];
}

def NVVM_MBarrierArriveExpectTxSharedOp : NVVM_PTXBuilder_Op<"mbarrier.arrive.expect_tx.shared">,  
  Arguments<(ins LLVM_PointerShared:$addr, I32:$txcount, PtxPredicate:$predicate)> {    
  let assemblyFormat = "$addr `,` $txcount (`,` `predicate` `=` $predicate^)? attr-dict `:` type(operands)";
  let extraClassDefinition = [{
    std::string $cppClass::getPtx() { return std::string("mbarrier.arrive.expect_tx.shared.b64 _, [%0], %1;"); }
  }];
}

def NVVM_MBarrierTryWaitParityOp : NVVM_PTXBuilder_Op<"mbarrier.try_wait.parity">,  
  Arguments<(ins LLVM_AnyPointer:$addr, I32:$phase, I32:$ticks)> {  
  let assemblyFormat = "$addr `,` $phase `,` $ticks attr-dict `:` type(operands)";
  let extraClassDefinition = [{
    std::string $cppClass::getPtx() {
      return std::string(
        "{\n\t"
        ".reg .pred       P1; \n\t"
        "LAB_WAIT: \n\t"
        "mbarrier.try_wait.parity.b64 P1, [%0], %1, %2; \n\t"
        "@P1 bra.uni DONE; \n\t"
        "bra.uni     LAB_WAIT; \n\t"
        "DONE: \n\t"
        "}"
      ); 
    }
  }];
}

def NVVM_MBarrierTryWaitParitySharedOp : NVVM_PTXBuilder_Op<"mbarrier.try_wait.parity.shared">,  
  Arguments<(ins LLVM_PointerShared:$addr, I32:$phase, I32:$ticks)> {  
  let assemblyFormat = "$addr `,` $phase `,` $ticks attr-dict `:` type(operands)";
  let extraClassDefinition = [{
    std::string $cppClass::getPtx() {
      return std::string(
        "{\n\t"
        ".reg .pred       P1; \n\t"
        "LAB_WAIT: \n\t"
        "mbarrier.try_wait.parity.shared.b64 P1, [%0], %1, %2; \n\t"
        "@P1 bra.uni DONE; \n\t"
        "bra.uni     LAB_WAIT; \n\t"
        "DONE: \n\t"
        "}"
      ); 
    }
  }];
}

def NVVM_MBarrierTestWaitOp : NVVM_Op<"mbarrier.test.wait">,
  Results<(outs LLVM_Type:$res)>,
  Arguments<(ins LLVM_AnyPointer:$addr, LLVM_Type:$state)> {
  string llvmBuilder = [{
      $res = createIntrinsicCall(builder, llvm::Intrinsic::nvvm_mbarrier_test_wait, {$addr, $state});
  }];
  let assemblyFormat = "$addr `,` $state attr-dict `:` type(operands) `->` type($res)";
}

def NVVM_MBarrierTestWaitSharedOp : NVVM_Op<"mbarrier.test.wait.shared">,
  Results<(outs LLVM_Type:$res)>,
  Arguments<(ins LLVM_PointerShared:$addr, LLVM_Type:$state)> {
  string llvmBuilder = [{
      $res = createIntrinsicCall(builder, llvm::Intrinsic::nvvm_mbarrier_test_wait_shared, {$addr, $state});
  }];
  let assemblyFormat = "$addr `,` $state attr-dict `:` type(operands) `->` type($res)";
}

//===----------------------------------------------------------------------===//
// NVVM synchronization op definitions
//===----------------------------------------------------------------------===//

def NVVM_Barrier0Op : NVVM_Op<"barrier0"> {
  string llvmBuilder = [{
      createIntrinsicCall(builder, llvm::Intrinsic::nvvm_barrier0);
  }];
  let assemblyFormat = "attr-dict";
}

def NVVM_BarrierOp : NVVM_Op<"barrier", [AttrSizedOperandSegments]> {
  let arguments = (ins     
    Optional<I32>:$barrierId,
    Optional<I32>:$numberOfThreads);
  string llvmBuilder = [{
    if ($numberOfThreads && $barrierId) {
      createIntrinsicCall(builder, llvm::Intrinsic::nvvm_barrier,
                {$barrierId, $numberOfThreads});
    } else if($barrierId) {
      createIntrinsicCall(builder, llvm::Intrinsic::nvvm_barrier_n,
                {$barrierId});   
    } else {
      createIntrinsicCall(builder, llvm::Intrinsic::nvvm_barrier0);
    }
  }];
  let hasVerifier = 1;
  let assemblyFormat = "(`id` `=` $barrierId^)? (`number_of_threads` `=` $numberOfThreads^)? attr-dict";
}

def NVVM_BarrierArriveOp : NVVM_PTXBuilder_Op<"barrier.arrive"> 
{
  let arguments = (ins Optional<I32>:$barrierId, I32:$numberOfThreads);

  let description = [{
    Thread that executes this op announces their arrival at the barrier with 
    given id and continue their execution.

    The default barrier id is 0 that is similar to `nvvm.barrier` Op. When 
    `barrierId` is not present, the default barrier id is used. 

    [For more information, see PTX ISA]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-bar)
  }];
  
  let assemblyFormat = "(`id` `=` $barrierId^)? `number_of_threads` `=` $numberOfThreads attr-dict";

  let extraClassDefinition = [{
    std::string $cppClass::getPtx() {
      std::string ptx = "bar.arrive ";
      if (getBarrierId()) { ptx += "%0, %1;"; } 
      else { ptx += "0, %0;"; }
      return ptx;
    }
  }];
}

def NVVM_ClusterArriveOp : NVVM_Op<"cluster.arrive"> {
  let arguments = (ins OptionalAttr<UnitAttr>:$aligned);

  let summary = "Cluster Barrier Arrive Op";
  let description = [{
    The `cluster.arrive` can be used by the threads within the cluster for synchronization and
    communication. The `cluster.arrive` instruction marks the warps' arrival at the barrier
    without causing the executing thread to wait for other participating threads.

    The `aligned` attribute, when provided, generates the .aligned version of the PTX instruction.

    [For more information, see PTX ISA]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-barrier-cluster)
  }];

  string llvmBuilder = [{
      if ($aligned)
        createIntrinsicCall(builder, llvm::Intrinsic::nvvm_barrier_cluster_arrive_aligned);
      else
        createIntrinsicCall(builder, llvm::Intrinsic::nvvm_barrier_cluster_arrive);
  }];
  let assemblyFormat = "attr-dict";
}

def NVVM_ClusterArriveRelaxedOp : NVVM_Op<"cluster.arrive.relaxed"> {
  let arguments = (ins OptionalAttr<UnitAttr>:$aligned);

  let summary = "Cluster Barrier Relaxed Arrive Op";
  let description = [{
    The `cluster.arrive` can be used by the threads within the cluster for synchronization and
    communication. The `cluster.arrive` instruction marks the warps' arrival at the barrier
    without causing the executing thread to wait for other participating threads.

    The `aligned` attribute, when provided, generates the .aligned version of the PTX instruction.
    The .relaxed qualifier on `cluster.arrive` specifies that there are no memory
    ordering and visibility guarantees provided for the memory accesses performed prior to
    `cluster.arrive`.

    [For more information, see PTX ISA]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-barrier-cluster)
  }];

  string llvmBuilder = [{
      if ($aligned)
        createIntrinsicCall(builder, llvm::Intrinsic::nvvm_barrier_cluster_arrive_relaxed_aligned);
      else
        createIntrinsicCall(builder, llvm::Intrinsic::nvvm_barrier_cluster_arrive_relaxed);
  }];
  let assemblyFormat = "attr-dict";
}

def NVVM_ClusterWaitOp : NVVM_Op<"cluster.wait"> {
  let arguments = (ins OptionalAttr<UnitAttr>:$aligned);

  let summary = "Cluster Barrier Wait Op";
  let description = [{
    The `cluster.wait` causes the executing thread to wait for all non-exited threads
    of the cluster to perform `cluster.arrive`. The `aligned` attribute, when provided,
    generates the .aligned version of the PTX instruction.

    [For more information, see PTX ISA]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-barrier-cluster)
  }];

  string llvmBuilder = [{
      if ($aligned)
        createIntrinsicCall(builder, llvm::Intrinsic::nvvm_barrier_cluster_wait_aligned);
      else
        createIntrinsicCall(builder, llvm::Intrinsic::nvvm_barrier_cluster_wait);
  }];
  let assemblyFormat = "attr-dict";
}

def NVVM_FenceScClusterOp : NVVM_Op<"fence.sc.cluster"> {
  string llvmBuilder = [{
      createIntrinsicCall(builder, llvm::Intrinsic::nvvm_fence_sc_cluster);
  }];
  let assemblyFormat = "attr-dict";
}

def SharedSpaceCTA : I32EnumAttrCase<"shared_cta", 0, "cta">;
def SharedSpaceCluster   : I32EnumAttrCase<"shared_cluster", 1, "cluster">;
def SharedSpace : I32EnumAttr<"SharedSpace", "Shared memory space",
  [SharedSpaceCTA, SharedSpaceCluster]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::NVVM";
}
def SharedSpaceAttr : EnumAttr<NVVM_Dialect, SharedSpace, "shared_space"> {
  let assemblyFormat = "`<` $value `>`";
}

def ProxyAlias : I32EnumAttrCase<"alias", 0, "alias">;
def ProxyAsync   : I32EnumAttrCase<"async", 1, "async">;
def ProxyAsyncGlobal   : I32EnumAttrCase<"async_global", 2, "async.global">;
def ProxyAsyncShared   : I32EnumAttrCase<"async_shared", 3, "async.shared">;
def ProxyTensorMap : I32EnumAttrCase<"TENSORMAP", 4, "tensormap">;
def ProxyGeneric : I32EnumAttrCase<"GENERIC", 5, "generic">;
def ProxyKind : I32EnumAttr<"ProxyKind", "Proxy kind",
  [ProxyAlias, ProxyAsync, ProxyAsyncGlobal, ProxyAsyncShared, ProxyTensorMap, ProxyGeneric]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::NVVM";
}

def ProxyKindAttr : EnumAttr<NVVM_Dialect, ProxyKind, "proxy_kind"> {
  let assemblyFormat = "`<` $value `>`";
}

def NVVM_FenceProxyOp : NVVM_PTXBuilder_Op<"fence.proxy">,
  Arguments<(ins ProxyKindAttr:$kind,
                 OptionalAttr<SharedSpaceAttr>:$space)> {
  let description = [{
    Fence operation with proxy to establish an ordering between memory accesses
    that may happen through different proxies.
    [For more information, see PTX ISA]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-membar)
  }];
  
  let assemblyFormat = "attr-dict";
  let extraClassDefinition = [{
    std::string $cppClass::getPtx() {
      std::string ptx = "fence.proxy.";
      ptx += stringifyProxyKind(getKind());
      if(getKind() == NVVM::ProxyKind::async_shared)
        { ptx += "::"; ptx += stringifySharedSpace(getSpace().value()); }
      ptx += ";";
      return ptx;
    }
  }];
  let hasVerifier = 1;
}

// Attrs describing the scope of the Memory Operation
def MemScopeKindCTA      : I32EnumAttrCase<"CTA", 0, "cta">;
def MemScopeKindCluster  : I32EnumAttrCase<"CLUSTER", 1, "cluster">;
def MemScopeKindGPU      : I32EnumAttrCase<"GPU", 2, "gpu">;
def MemScopeKindSYS      : I32EnumAttrCase<"SYS", 3, "sys">;

def MemScopeKind : I32EnumAttr<"MemScopeKind", "NVVM Memory Scope kind",
  [MemScopeKindCTA, MemScopeKindCluster, MemScopeKindGPU, MemScopeKindSYS]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::NVVM";
}
def MemScopeKindAttr : EnumAttr<NVVM_Dialect, MemScopeKind, "mem_scope"> {
  let assemblyFormat = "`<` $value `>`";
}

def NVVM_FenceProxyAcquireOp : NVVM_Op<"fence.proxy.acquire">,
      Arguments<(ins MemScopeKindAttr:$scope, LLVM_PointerGeneric:$addr, I32:$size,
                     DefaultValuedAttr<ProxyKindAttr,
                                       "ProxyKind::GENERIC">:$fromProxy,
                     DefaultValuedAttr<ProxyKindAttr,
                                       "ProxyKind::TENSORMAP">:$toProxy)> {
  let summary = "Uni-directional proxy fence operation with acquire semantics";
  let description = [{
    `fence.proxy.acquire` is a uni-directional fence used to establish ordering
    between a prior memory access performed via the generic proxy and a
    subsequent memory access performed via the tensormap proxy

    The address operand `addr` and the operand `size` together specify the
    memory range `[addr, addr+size)` on which the ordering guarantees on the
    memory accesses across the proxies is to be provided. The only supported
    value for the `size` operand is 128 and must be an immediate. Generic Addressing
    is used unconditionally, and the address specified by the operand `addr` must
    fall within the `.global` state space. Otherwise, the behavior is undefined
    [For more information, see PTX ISA]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-membar)
  }];

  let assemblyFormat = "$scope $addr `,` $size (`from_proxy` `=` $fromProxy^)? (`to_proxy` `=` $toProxy^)? attr-dict";
  let llvmBuilder = [{
    createIntrinsicCall(
        builder,
        getUnidirectionalFenceProxyID($fromProxy, $toProxy, $scope, false),
        {$addr, $size});
  }];

  let hasVerifier = 1;
}

def NVVM_FenceProxyReleaseOp : NVVM_Op<"fence.proxy.release">,
      Arguments<(ins MemScopeKindAttr:$scope,
                     DefaultValuedAttr<ProxyKindAttr,
                                       "ProxyKind::GENERIC">:$fromProxy,
                     DefaultValuedAttr<ProxyKindAttr,
                                       "ProxyKind::TENSORMAP">:$toProxy)> {
  let summary = "Uni-directional proxy fence operation with release semantics";
  let description = [{
    `fence.proxy.release` is a uni-directional fence used to establish ordering
    between a prior memory access performed via the generic proxy and a
    subsequent memory access performed via the tensormap proxy. `fence.proxy.release`
    operation can form a release sequence that synchronizes with an acquire
    sequence that contains the fence.proxy.acquire proxy fence operation
    [For more information, see PTX ISA]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-membar)
  }];

  let assemblyFormat = "$scope (`from_proxy` `=` $fromProxy^)? (`to_proxy` `=` $toProxy^)? attr-dict";
  let llvmBuilder = [{
    createIntrinsicCall(builder, getUnidirectionalFenceProxyID(
                                     $fromProxy, $toProxy, $scope, true));
  }];

  let hasVerifier = 1;
}

def SetMaxRegisterActionIncrease : I32EnumAttrCase<"increase", 0>;
def SetMaxRegisterActionDecrease   : I32EnumAttrCase<"decrease", 1>;
def SetMaxRegisterAction : I32EnumAttr<"SetMaxRegisterAction", "NVVM set max register action",
  [SetMaxRegisterActionDecrease, SetMaxRegisterActionIncrease]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::NVVM";
}
def SetMaxRegisterActionAttr : EnumAttr<NVVM_Dialect, SetMaxRegisterAction, "action">;

def NVVM_SetMaxRegisterOp : NVVM_Op<"setmaxregister"> {
  let arguments = (ins I32Attr:$regCount, SetMaxRegisterActionAttr:$action);
  let assemblyFormat = "$action $regCount attr-dict";
  let hasVerifier = 1;
  string llvmBuilder = [{
    auto intId = (op.getAction() == NVVM::SetMaxRegisterAction::increase) ?
      llvm::Intrinsic::nvvm_setmaxnreg_inc_sync_aligned_u32 :
      llvm::Intrinsic::nvvm_setmaxnreg_dec_sync_aligned_u32;

    createIntrinsicCall(builder, intId, builder.getInt32($regCount));
  }];
}

def NVVM_FenceMbarrierInitOp : NVVM_PTXBuilder_Op<"fence.mbarrier.init"> {
  let arguments = (ins );
    let description = [{
    Fence operation that applies on the prior nvvm.mbarrier.init
    [For more information, see PTX ISA]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-membar)
  }];
  
  let assemblyFormat = "attr-dict";
  let extraClassDefinition = [{        
    std::string $cppClass::getPtx() {
      return std::string("fence.mbarrier_init.release.cluster;");
    }
  }];
}

def ShflKindBfly : I32EnumAttrCase<"bfly", 0>;
def ShflKindUp   : I32EnumAttrCase<"up", 1>;
def ShflKindDown : I32EnumAttrCase<"down", 2>;
def ShflKindIdx  : I32EnumAttrCase<"idx", 3>;

/// Enum attribute of the different shuffle kinds.
def ShflKind : I32EnumAttr<"ShflKind", "NVVM shuffle kind",
  [ShflKindBfly, ShflKindUp, ShflKindDown, ShflKindIdx]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::NVVM";
}
def ShflKindAttr : EnumAttr<NVVM_Dialect, ShflKind, "shfl_kind">;

def NVVM_ShflOp :
  NVVM_Op<"shfl.sync">,
  Results<(outs LLVM_Type:$res)>,
  Arguments<(ins I32:$thread_mask,
                 LLVM_Type:$val,
                 I32:$offset,
                 I32:$mask_and_clamp,
                 ShflKindAttr:$kind,
                 OptionalAttr<UnitAttr>:$return_value_and_is_valid)> {
  let summary = "NVVM Dialect Op for shfl.sync";
  let description = [{
    The `shfl.sync` Op implements data shuffle within threads of a warp.
    The `thread_mask` denotes the threads participating in the Op where
    the bit position corresponds to a particular thread’s laneid.
    The `offset` specifies a source lane or source lane offset
    (depending on `kind`). The `val` is the input value to be copied from
    the source. The `mask_and_clamp` contains two packed values specifying
    a mask for logically splitting warps into sub-segments and an upper bound
    for clamping the source lane index.
    [For more information, refer PTX ISA]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/#data-movement-and-conversion-instructions-shfl-sync)
  }];
  string llvmBuilder = [{
      auto intId = getShflIntrinsicId(
          $_resultType, $kind, static_cast<bool>($return_value_and_is_valid));
      $res = createIntrinsicCall(builder,
          intId, {$thread_mask, $val, $offset, $mask_and_clamp});
  }];
  let assemblyFormat = [{
    $kind $thread_mask `,` $val `,` $offset `,` $mask_and_clamp  attr-dict
     `:` type($val) `->` type($res)
   }];
   let hasVerifier = 1;
}

def NVVM_VoteBallotOp :
  NVVM_Op<"vote.ballot.sync">,
  Results<(outs LLVM_Type:$res)>,
  Arguments<(ins LLVM_Type:$mask, LLVM_Type:$pred)> {
  string llvmBuilder = [{
      $res = createIntrinsicCall(builder,
            llvm::Intrinsic::nvvm_vote_ballot_sync, {$mask, $pred});
  }];
  let hasCustomAssemblyFormat = 1;
}

def NVVM_SyncWarpOp :
  NVVM_Op<"bar.warp.sync">,
  Arguments<(ins LLVM_Type:$mask)> {
  string llvmBuilder = [{
      createIntrinsicCall(builder, llvm::Intrinsic::nvvm_bar_warp_sync, {$mask});
  }];
  let assemblyFormat = "$mask attr-dict `:` type($mask)";
}

def NVVM_ElectSyncOp : NVVM_Op<"elect.sync">
{  
  let summary = "Elect one leader thread";
  let description = [{
    The `elect.sync` instruction elects one predicated active leader
    thread from among a set of threads specified in membermask.
    The membermask is set to `0xFFFFFFFF` for the current version
    of this Op. The predicate result is set to `True` for the
    leader thread, and `False` for all other threads.

    [For more information, see PTX ISA]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-elect-sync)
  }];

  let results = (outs I1:$pred);
  let assemblyFormat = "attr-dict `->` type(results)";  
  string llvmBuilder = [{
    auto *resultTuple = createIntrinsicCall(builder,
        llvm::Intrinsic::nvvm_elect_sync, {builder.getInt32(0xFFFFFFFF)});
    // Extract the second value into $pred
    $pred = builder.CreateExtractValue(resultTuple, 1);
  }];
}

def LoadCacheModifierCA : I32EnumAttrCase<"CA", 0, "ca">;
def LoadCacheModifierCG : I32EnumAttrCase<"CG", 1, "cg">;
def LoadCacheModifierCS : I32EnumAttrCase<"CS", 2, "cs">;
def LoadCacheModifierLU : I32EnumAttrCase<"LU", 3, "lu">;
def LoadCacheModifierCV : I32EnumAttrCase<"CV", 4, "cv">;

/// Enum attribute of the different kinds.
def LoadCacheModifierKind : I32EnumAttr<"LoadCacheModifierKind", 
                                "NVVM load cache modifier kind",
  [LoadCacheModifierCA, LoadCacheModifierCG, LoadCacheModifierCS, 
    LoadCacheModifierLU, LoadCacheModifierCV]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::NVVM";
  let description = [{
    Enum attribute of the different kinds of cache operators for load instructions.

    [For more information, see PTX ISA](https://docs.nvidia.com/cuda/parallel-thread-execution/#id62)    
  }];
}

def LoadCacheModifierAttr : EnumAttr<NVVM_Dialect, LoadCacheModifierKind, "load_cache_modifier">;

def NVVM_CpAsyncOp : NVVM_PTXBuilder_Op<"cp.async.shared.global">,
  Arguments<(ins LLVM_PointerShared:$dst,
                 LLVM_PointerGlobal:$src,
                 I32Attr:$size,
                 LoadCacheModifierAttr:$modifier,
                 Optional<LLVM_Type>:$cpSize)> {
  string llvmBuilder = [{
      llvm::Intrinsic::ID id;
      switch ($size) {
        case 4:
          id = llvm::Intrinsic::nvvm_cp_async_ca_shared_global_4;
          break;
        case 8:
          id = llvm::Intrinsic::nvvm_cp_async_ca_shared_global_8;
          break;
        case 16:
          if($modifier == NVVM::LoadCacheModifierKind::CG)
            id = llvm::Intrinsic::nvvm_cp_async_cg_shared_global_16;
          else if($modifier == NVVM::LoadCacheModifierKind::CA)
            id = llvm::Intrinsic::nvvm_cp_async_ca_shared_global_16;
          else 
            llvm_unreachable("unsupported cache modifier");
          break;
        default:
          llvm_unreachable("unsupported async copy size");
      }
      createIntrinsicCall(builder, id, {$dst, $src});
  }];
  let assemblyFormat = "$dst `,` $src `,` $size `,` `cache` `=` $modifier (`,` $cpSize^)? attr-dict `:` type(operands)";
  let hasVerifier = 1;
  let extraClassDeclaration = [{
    bool hasIntrinsic() { if(getCpSize()) return false; return true; }

    void getAsmValues(RewriterBase &rewriter, 
        llvm::SmallVectorImpl<std::pair<mlir::Value, mlir::NVVM::PTXRegisterMod>> &asmValues) {
      asmValues.push_back({getDst(), PTXRegisterMod::Read});
      asmValues.push_back({getSrc(), PTXRegisterMod::Read});
      asmValues.push_back({makeConstantI32(rewriter, getSize()), PTXRegisterMod::Read});
      asmValues.push_back({getCpSize(), PTXRegisterMod::Read});
    }        
  }];
  let extraClassDefinition = [{        
    std::string $cppClass::getPtx() { 
      if(getModifier() == NVVM::LoadCacheModifierKind::CG)
        return std::string("cp.async.cg.shared.global [%0], [%1], %2, %3;\n");
      if(getModifier() == NVVM::LoadCacheModifierKind::CA)
        return std::string("cp.async.ca.shared.global [%0], [%1], %2, %3;\n");
      llvm_unreachable("unsupported cache modifier");      
    }
  }];
}

def NVVM_CpAsyncCommitGroupOp : NVVM_Op<"cp.async.commit.group"> {
  string llvmBuilder = [{
      createIntrinsicCall(builder, llvm::Intrinsic::nvvm_cp_async_commit_group);
  }];
  let assemblyFormat = "attr-dict";
}

def NVVM_CpAsyncWaitGroupOp : NVVM_Op<"cp.async.wait.group">,
  Arguments<(ins I32Attr:$n)> {
  string llvmBuilder = [{
      createIntrinsicCall(
        builder,
        llvm::Intrinsic::nvvm_cp_async_wait_group,
        llvm::ConstantInt::get(
          llvm::Type::getInt32Ty(moduleTranslation.getLLVMContext()),
          $n));
  }];
  let assemblyFormat = "$n attr-dict";
}

def NVVM_CpAsyncMBarrierArriveOp : NVVM_Op<"cp.async.mbarrier.arrive"> {
  let summary = "NVVM Dialect Op for cp.async.mbarrier.arrive";
  let description = [{
    The `cp.async.mbarrier.arrive` Op makes the mbarrier object track
    all prior cp.async operations initiated by the executing thread.
    The `addr` operand specifies the address of the mbarrier object
    in generic address space. The `noinc` attr impacts how the
    mbarrier's state is updated.
    [For more information, refer PTX ISA]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-cp-async-mbarrier-arrive)
  }];
  let assemblyFormat = "$addr attr-dict `:` type(operands)";

  let arguments = (ins
    LLVM_AnyPointer:$addr, DefaultValuedAttr<I1Attr, "0">:$noinc);

  string llvmBuilder = [{
    auto intId = $noinc ?
      llvm::Intrinsic::nvvm_cp_async_mbarrier_arrive_noinc :
      llvm::Intrinsic::nvvm_cp_async_mbarrier_arrive;

    createIntrinsicCall(builder, intId, {$addr});
  }];
}

def NVVM_CpAsyncMBarrierArriveSharedOp : NVVM_Op<"cp.async.mbarrier.arrive.shared"> {
  let summary = "NVVM Dialect Op for cp.async.mbarrier.arrive.shared";
  let description = [{
    The `cp.async.mbarrier.arrive.shared` Op makes the mbarrier object
    track all prior cp.async operations initiated by the executing thread.
    The `addr` operand specifies the address of the mbarrier object in
    shared memory. The `noinc` attr impacts how the mbarrier's state
    is updated. [For more information, refer PTX ISA]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-cp-async-mbarrier-arrive)
  }];
  let assemblyFormat = "$addr attr-dict `:` type(operands)";

  let arguments = (ins
    LLVM_PointerShared:$addr, DefaultValuedAttr<I1Attr, "0">:$noinc);

  string llvmBuilder = [{
    auto intId = $noinc ?
      llvm::Intrinsic::nvvm_cp_async_mbarrier_arrive_noinc_shared :
      llvm::Intrinsic::nvvm_cp_async_mbarrier_arrive_shared;

    createIntrinsicCall(builder, intId, {$addr});
  }];
}

/// Helpers to instantiate different version of wmma intrinsics.
/// This matches the hierarchy used in IntrinsicsNVVM.td to define all the
/// combinations of the intrinsics.
class GEOM<int M, int N, int K> {
  int m = M;
  int n = N;
  int k = K;
}

/// Class containing information about valid mma matrix types.
class WMMA_REGS<GEOM Geom, string Frag, string PtxEltType> {
  int m = Geom.m;
  int n = Geom.n;
  int k = Geom.k;
  string geom = "m"#Geom.m#"n"#Geom.n#"k"#Geom.k;
  string frag = Frag;
  string ptx_elt_type = PtxEltType;
  string gft = geom#":"#Frag#":"#ptx_elt_type;
}

//// Generate enum value of the mma.load/mma.store intrinsic.
class WMMA_NAME_LDST<string Op, WMMA_REGS Frag, string Layout, int WithStride> {
  string id =   "llvm::Intrinsic::nvvm_wmma"
                # "_" # Frag.geom
                # "_" # Op
                # "_" # Frag.frag
                # "_" # Frag.ptx_elt_type
                # "_" # Layout
                # !if(WithStride, "_stride", "");
}

/// Generate the signature part of the mma intrinsic name.
class MMA_SIGNATURE<WMMA_REGS A, WMMA_REGS B, WMMA_REGS C, WMMA_REGS D> {
  list<WMMA_REGS> id_frags = !cond(
     // FP16 ops are identified by accumulator & result type.
     !eq(A.ptx_elt_type, "f16") : [D, C],
     // other ops are identified by input types.
     !ne(A.ptx_elt_type, B.ptx_elt_type): [A, B],
     true: [A]
     );
   string ret = !foldl("", id_frags, a, b, !strconcat(a, "_", b.ptx_elt_type));
}

/// Generate enum value of the wmma.mma intrinsic.
class WMMA_NAME<string Op, string ALayout, string BLayout, WMMA_REGS A,
  WMMA_REGS B, WMMA_REGS C, WMMA_REGS D> {
  string signature = MMA_SIGNATURE<A, B, C, D>.ret;
  string id =   "llvm::Intrinsic::nvvm_wmma"
                # "_" # A.geom
                # "_" # Op
                # "_" # ALayout
                # "_" # BLayout
                # signature;
}

// Generates list of 4-tuples of WMMA_REGS representing a valid MMA op.
//   Geom: list of supported geometries.
//   TypeN: PTX type of the corresponding fragment's element.
//   TypeB and TypeD may be empty if it must match that of TypeA or TypeC.
class MMA_OPS<list<GEOM> Geom, list<string> TypeA, list<string> TypeB,
            list<string> TypeC, list<string> TypeD> {
  list<list<WMMA_REGS>> ret =
     !foldl([]<list<WMMA_REGS>>, Geom, t1, geom, !listconcat(t1,
     !foldl([]<list<WMMA_REGS>>, TypeA, t2, type_a, !listconcat(t2,
     !foldl([]<list<WMMA_REGS>>, !if(!size(TypeB), TypeB, [type_a]), t3, type_b, !listconcat(t3,
     !foldl([]<list<WMMA_REGS>>, TypeC, t4, type_c, !listconcat(t4,
     !foldl([]<list<WMMA_REGS>>, !if(!size(TypeD), TypeD, [type_c]), t5, type_d, !listconcat(t5,
            [[WMMA_REGS<geom, "a", type_a>,
              WMMA_REGS<geom, "b", type_b>,
              WMMA_REGS<geom, "c", type_c>,
              WMMA_REGS<geom, "d", type_d>]]))))))))));
   // Debugging aid for readable representation of the list above.
   list<list<string>> ops = !foreach(x, ret, [x[0].gft, x[1].gft, x[2].gft, x[3].gft]);
}

/// Creates a list of combinations of load/store operations supported.
class MMA_LDST_OPS<list<GEOM> Geom, list<string> Frags, list<string> Types> {
  list<WMMA_REGS> ret =
     !foldl([]<WMMA_REGS>, Geom, t1, geom, !listconcat(t1,
     !foldl([]<WMMA_REGS>, Frags, t2, frag, !listconcat(t2,
     !foldl([]<WMMA_REGS>, Types, t3, type, !listconcat(t3,
            [WMMA_REGS<geom, frag, type>]))))));
   // Debugging aid for readable representation of the list above.
   list<string> ops = !foreach(x, ret, x.gft);
}

// Creates list of valid combinations of fragments. This is a subset of what
// llvm supports and can be extended as needed.
class NVVM_MMA_OPS {
  // "wmma" operations
  list<list<WMMA_REGS>> tf32_wmma_ops = MMA_OPS<
            [GEOM<16, 16, 8>],
            ["tf32"], [], ["f32"], []>.ret;
  list<list<WMMA_REGS>> fp_wmma_ops = MMA_OPS<
            [GEOM<16, 16, 16>, GEOM<32, 8, 16>, GEOM<8, 32, 16>],
            ["f16"], [], ["f16", "f32"], []>.ret;
  list<list<WMMA_REGS>> i8_wmma_ops = MMA_OPS<
            [GEOM<16, 16, 16>, GEOM<32, 8, 16>, GEOM<8, 32, 16>],
            ["s8","u8"], [], ["s32"], []>.ret;
  list<list<WMMA_REGS>> all_wmma_ops = !listconcat(
            tf32_wmma_ops,
            fp_wmma_ops,
            i8_wmma_ops);

  list<WMMA_REGS> ldst_ab_ops = MMA_LDST_OPS<
            [GEOM<16, 16, 16>, GEOM<32, 8, 16>, GEOM<8, 32, 16>],
            ["a", "b"], ["f16","s8","u8"]>.ret;
  list<WMMA_REGS> ldst_cd_ops = MMA_LDST_OPS<
            [GEOM<16, 16, 16>, GEOM<32, 8, 16>, GEOM<8, 32, 16>],
            ["c", "d"], ["f16", "f32","s32"]>.ret;
  list<WMMA_REGS> ldst_tf32_ab_ops = MMA_LDST_OPS<
            [GEOM<16, 16, 8>],
            ["a", "b"], ["tf32"]>.ret;
  list<WMMA_REGS> ldst_tf32_cd_ops = MMA_LDST_OPS<
            [GEOM<16, 16, 8>],
            ["c", "d"], ["f32"]>.ret;
  list<WMMA_REGS> all_ldst_ops = !listconcat(ldst_ab_ops, ldst_cd_ops,
                                             ldst_tf32_ab_ops,
                                             ldst_tf32_cd_ops);
  // Separate A/B/C fragments (loads) from D (stores).
  list<WMMA_REGS> all_ld_ops = !filter(op, all_ldst_ops, !ne(op.frag, "d"));
  list<WMMA_REGS> all_st_ops = !filter(op, all_ldst_ops, !eq(op.frag, "d"));

  // "mma_sync" operations
  list<list<WMMA_REGS>> tf32_mma_ops = MMA_OPS<
            [GEOM<16,8,4>, GEOM<16,8,8>],
            ["tf32"], [], ["f32"], []>.ret;
  list<list<WMMA_REGS>> bf16_mma_ops = MMA_OPS<
            [GEOM<16,8,16>, GEOM<16,8,8>],
            ["bf16"], [], ["f32"], []>.ret;
  list<list<WMMA_REGS>> f64_mma_ops = MMA_OPS<
            [GEOM<8,8,4>],
            ["f64"], [], ["f64"], []>.ret;
  list<list<WMMA_REGS>> fp_mma_ops = MMA_OPS<
            [GEOM<8,8,4>, GEOM<16,8,8>, GEOM<16,8,16>],
            ["f16"], [], ["f16", "f32"], ["f16", "f32"]>.ret;
  list<list<WMMA_REGS>> int_mma_ops = MMA_OPS<
            [GEOM<8,8,16>, GEOM<16,8,16>, GEOM<16,8,32>],
            ["s8", "u8"], ["s8", "u8"], ["s32"], []>.ret;
  list<list<WMMA_REGS>> subint_mma_ops = MMA_OPS<
            [GEOM<8,8,32>, GEOM<16,8,32>, GEOM<16,8,64>],
            ["s4", "u4"], ["s4", "u4"], ["s32"], []>.ret;
  list<list<WMMA_REGS>> bit_mma_ops = MMA_OPS<
            [GEOM<8,8,128>, GEOM<16,8,128>, GEOM<16,8,256>],
            ["b1"], [], ["s32"], []>.ret;
  list<list<WMMA_REGS>> all_mma_sync_ops = !listconcat(
            tf32_mma_ops, bf16_mma_ops, f64_mma_ops,
            fp_mma_ops, int_mma_ops, subint_mma_ops, bit_mma_ops);
}

def NVVM_MMA_OPS : NVVM_MMA_OPS;

/// Helper to create the mapping between the configuration and the store
/// intrinsic enum value.
class MMA_ST_INTR<string op> {
  list<list<string>> cond0 = !foreach(frag, NVVM_MMA_OPS.all_st_ops,
                                !foreach(layout, ["row", "col"],
  "if (layout == \"" # layout #  "\" && m == " # frag.m # " &&"
  "    n == " #frag.n # " && k == " # frag.k # " && \"" #
       frag.ptx_elt_type # "\" == eltype)"
  "  return " #WMMA_NAME_LDST<op, frag, layout, 1>.id #";"));
  string id = !foldl("",
                !foldl([""], cond0, acc, el, !listconcat(acc, el)),
                acc1, el1, acc1 # "\n" # el1);
}

/// Helper to map a mxk shape to a supported mxnxk matrix type. This will return
/// the n value of the supported configuration.
class MMA_ST_INFER_N<list<WMMA_REGS> ldst> {
  list<string> cond = !foreach(frag, ldst,
  "if (m == " # frag.m # " && k == " #frag.k # " && \"" #
       frag.ptx_elt_type # "\" == eltype)"
  "  return "# frag.n #";");
  string id = !foldl("", cond, acc, el, acc # "\n" # el);
}

/// Helper to map a kxn shape to a supported mxnxk matrix type. This will return
/// the m value of the supported configuration.
class MMA_ST_INFER_M<list<WMMA_REGS> ldst> {
  list<string> cond = !foreach(frag, ldst,
  "if (n == " # frag.n # " && k == " #frag.k # " && \"" #
       frag.ptx_elt_type # "\" == eltype)"
  "  return "# frag.m #";");
  string id = !foldl("", cond, acc, el, acc # "\n" # el);
}

/// Helper to map a mxn shape to a supported mxnxk matrix type. This will return
/// the k value of the supported configuration.
class MMA_ST_INFER_K<list<WMMA_REGS> ldst> {
  list<string> cond = !foreach(frag, ldst,
  "if (m == " # frag.m # " && n == " #frag.n # " && \"" #
       frag.ptx_elt_type # "\" == eltype)"
  "  return "# frag.k #";");
  string id = !foldl("", cond, acc, el, acc # "\n" # el);
}

/// Helper to create the mapping between the configuration and the load
/// intrinsic enum value.
class MMA_LD_INTR<string op> {
  list<list<string>> cond0 = !foreach(frag, NVVM_MMA_OPS.all_ld_ops,
                                !foreach(layout, ["row", "col"],
  "if (layout == \"" # layout #  "\" && m == " # frag.m # " &&"
  "    n == " #frag.n # " && k == " # frag.k # " && \"" #
       frag.ptx_elt_type # "\" == eltype && frag == \""#frag.frag#"\")"
  "  return "# WMMA_NAME_LDST<op, frag, layout, 1>.id #";"));
  string id = !foldl("",
                !foldl([""], cond0, acc, el, !listconcat(acc, el)),
                acc1, el1, acc1 # "\n" # el1);
}

/// Helper to create the mapping between the configuration and the wmma.mma
/// intrinsic enum value.
class MMA_MMA_INTR<string opName> {
  list<list<list<string>>> cond0 =
    !foreach(op, NVVM_MMA_OPS.all_wmma_ops,
      !foreach(layoutA, ["row", "col"],
        !foreach(layoutB, ["row", "col"],
  "if (layoutA == \"" # layoutA #  "\" && layoutB == \"" # layoutB #  "\" && "
  "    m == " # op[0].m # " && n == " #op[0].n # " && k == " # op[0].k #
  "    && \"" # op[0].ptx_elt_type # "\" == eltypeA && \""
   # op[3].ptx_elt_type # "\" == eltypeB)"
  "  return " #
       WMMA_NAME<opName, layoutA, layoutB, op[0], op[1], op[2], op[3]>.id # ";")));
  list<string> f = !foldl([""],
                     !foldl([[""]], cond0, acc, el, !listconcat(acc, el)),
                          acc1, el1, !listconcat(acc1, el1));
  string id = !foldl("", f, acc, el, acc # "\n" # el);
}

/// Enum attribute for binary (b1) MMA operation type
def MMAB1OpNone : I32EnumAttrCase<"none", 0>;
def MMAB1OpXorPopc : I32EnumAttrCase<"xor_popc", 1>;
def MMAB1OpAndPopc : I32EnumAttrCase<"and_popc", 2>;
def MMAB1Op : I32EnumAttr<"MMAB1Op", "MMA binary operations",
  [MMAB1OpNone, MMAB1OpXorPopc, MMAB1OpAndPopc]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::NVVM";
}
def MMAB1OpAttr : EnumAttr<NVVM_Dialect, MMAB1Op, "mma_b1op"> {
  let assemblyFormat = "`<` $value `>`";
}

/// Enum attribute type for the overflow behavior of MMA integer operations
def MMAIntOverflowWrap : I32EnumAttrCase<"wrapped", 0>;
def MMAIntOverflowSat : I32EnumAttrCase<"satfinite", 1>;
def MMAIntOverflow : I32EnumAttr<"MMAIntOverflow", "MMA overflow options",
  [MMAIntOverflowSat, MMAIntOverflowWrap]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::NVVM";
}
def MMAIntOverflowAttr : EnumAttr<NVVM_Dialect, MMAIntOverflow, "mma_int_overflow"> {
  let assemblyFormat = "`<` $value `>`";
}

/// Attribute to hold the MMA shape
def NVVM_MMAShapeAttr : NVVM_Attr<"MMAShape", "shape"> {
  let summary = "Attribute for MMA operation shape.";
  let parameters = (ins "int":$m, "int":$n, "int":$k);
  let assemblyFormat = "`<` struct(params) `>`";
}

// Returns true if this combination of layout/satf for MMA ops is supported;
// false otherwise.
// E.g.
// if NVVM_MMA_SUPPORTED<...>.ret then
//   def : FOO<>; // The record will only be defined for supported ops.
//
class NVVM_MMA_SUPPORTED<list<WMMA_REGS> frags, string layout_a, string layout_b, int satf> {
  // MMA ops check both layouts.
  string layout = layout_a # ":" # layout_b;
  string a_type = frags[0].ptx_elt_type;
  string b_type = frags[1].ptx_elt_type;
  string c_type = frags[2].ptx_elt_type;
  string d_type = frags[3].ptx_elt_type;
  string geom = frags[0].geom;

  // gcd is a shortcut used to identify instructions that depend on
  // geom+frag_c+frag_d.
  string gcd = geom # ":" # c_type # d_type;
  bit ret = !cond(

    // Limit satf to valid types
    !and(!eq(satf, 1),
         !ne(a_type, "s8"),
         !ne(a_type, "u8"),
         !ne(a_type, "s4"),
         !ne(a_type, "u4")): false,

    // m8n8k4 has no C=f32 D=f16 variant.
    !eq(gcd, "m8n8k4:f32f16"): false,

    // only m8n8k4 for f16 does not require row:col layout
    !and(!ne(layout, "row:col"),
         !or(!ne(geom, "m8n8k4"),
             !ne(a_type, "f16"))) : false,

    // m16n8k8 requires A and B to be the same type and C and D to be the same
    // type.
    !and(!eq(geom, "m16n8k8"),
         !or(!ne(a_type, b_type),
             !ne(c_type, d_type))): false,

    // m16n8k8 requires C and D to be the same type.
    !and(!eq(geom, "m16n8k8"),
         !ne(c_type, d_type)): false,

    // All other are OK.
    true: true
  );
}

// Returns a list of operation suffixes corresponding to possible b1
// multiply-and-accumulate operations for all fragments which have a
// b1 type. For all other fragments, the list returned holds a list
// containing the empty string.
class NVVM_MMA_B1OPS<list<WMMA_REGS> frags> {
  list<string> ret = !cond(
    !eq(frags[0].ptx_elt_type, "b1") : ["xor_popc", "and_popc"],
    true: [""]
  );
}

/// Generate enum value of the mma.sync intrinsic.
class MMA_SYNC_NAME<string ALayout, string BLayout, string b1op, int Satfinite,
               WMMA_REGS A, WMMA_REGS B, WMMA_REGS C, WMMA_REGS D> {
  string signature = MMA_SIGNATURE<A, B, C, D>.ret;
  string id = "llvm::Intrinsic::nvvm_mma"
                # !if(!ne(b1op, ""), "_" # b1op, "")
                # "_" # A.geom
                # "_" # ALayout
                # "_" # BLayout
                # !if(Satfinite, "_satfinite", "")
                # signature;
}

/// Helper to create the mapping between the configuration and the mma.sync
/// intrinsic enum value.
class MMA_SYNC_INTR {
  list<list<list<list<list<string>>>>> cond0 =
    !foreach(op, NVVM_MMA_OPS.all_mma_sync_ops,
      !foreach(layoutA, ["row", "col"],
        !foreach(layoutB, ["row", "col"],
          !foreach (sat, [0, 1],
            !foreach (b1op, NVVM_MMA_B1OPS<op>.ret,
              !if(NVVM_MMA_SUPPORTED<[op[0], op[1], op[2], op[3]],
                                     layoutA, layoutB, sat>.ret,
      "if (layoutA == \"" # layoutA #  "\" && layoutB == \"" # layoutB #  "\" && "
      "    m == " # op[0].m # " && n == " # op[0].n # " && k == " # op[0].k #
      "    && \"" # op[0].ptx_elt_type # "\" == eltypeA && \""
       # op[1].ptx_elt_type # "\" == eltypeB && "
       # " \"" # op[2].ptx_elt_type # "\" == eltypeC && "
       # " \"" # op[3].ptx_elt_type # "\" == eltypeD "
       # " && (sat.has_value()  ? " # sat # " == static_cast<int>(*sat) : true)"
       # !if(!ne(b1op, ""), " && (b1Op.has_value() ? MMAB1Op::" # b1op # " == *b1Op : true)", "") # ")\n"
       # "  return " #
       MMA_SYNC_NAME<layoutA, layoutB, b1op, sat, op[0], op[1], op[2], op[3]>.id # ";",
          "") // if supported
          ) // b1op
        ) // sat
      ) // layoutB
    ) // layoutA
  ); // all_mma_sync_ops
  list<list<list<string>>> f1 = !foldl([[[""]]],
                                  !foldl([[[[""]]]], cond0, acc, el,
                                      !listconcat(acc, el)),
                                    acc1, el1, !listconcat(acc1, el1));
  list<list<string>> f2 = !foldl([[""]], f1, acc1, el1, !listconcat(acc1, el1));
  list<string> f3 = !foldl([""], f2, acc, el, !listconcat(acc, el));
  string id = !foldl("", f3, acc, el, acc # "\n" # el);
}

def MMALayoutRow : I32EnumAttrCase<"row", 0>;
def MMALayoutCol : I32EnumAttrCase<"col", 1>;

/// Enum attribute of the different matrix layout.
def MMALayout : I32EnumAttr<"MMALayout", "NVVM MMA layout",
  [MMALayoutRow, MMALayoutCol]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::NVVM";
}
def MMALayoutAttr : EnumAttr<NVVM_Dialect, MMALayout, "mma_layout"> {
  let assemblyFormat = "`<` $value `>`";
}

/// Enum attribute of the different PTX element types used for MMA operands.
def MMATypeF16  : I32EnumAttrCase<"f16", 0>;
def MMATypeF32  : I32EnumAttrCase<"f32", 1>;
def MMATypeTF32 : I32EnumAttrCase<"tf32", 2>;
def MMATypeU8 : I32EnumAttrCase<"u8", 3>;
def MMATypeS8 : I32EnumAttrCase<"s8", 4>;
def MMATypeS32 : I32EnumAttrCase<"s32", 5>;
def MMATypeB1 : I32EnumAttrCase<"b1", 6>;
def MMATypeU4 : I32EnumAttrCase<"u4", 7>;
def MMATypeS4 : I32EnumAttrCase<"s4", 8>;
def MMATypeBF16 : I32EnumAttrCase<"bf16", 9>;
def MMATypeF64 : I32EnumAttrCase<"f64", 10>;

def MMATypes : I32EnumAttr<"MMATypes", "NVVM MMA types",
  [MMATypeF16, MMATypeF32, MMATypeTF32,
  MMATypeBF16, MMATypeS8, MMATypeU8,
  MMATypeS32, MMATypeS4, MMATypeU4,
  MMATypeB1, MMATypeF64]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::NVVM";
}
def MMATypesAttr : EnumAttr<NVVM_Dialect, MMATypes, "mma_type"> {
  let assemblyFormat = "`<` $value `>`";
}

def MMAFragA : I32EnumAttrCase<"a", 0>;
def MMAFragB : I32EnumAttrCase<"b", 1>;
def MMAFragC : I32EnumAttrCase<"c", 2>;

/// Enum attribute of the different frag types.
def MMAFrag: I32EnumAttr<"MMAFrag", "NVVM MMA frag type",
  [MMAFragA, MMAFragB, MMAFragC]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::NVVM";
}
def MMAFragAttr : EnumAttr<NVVM_Dialect, MMAFrag, "mma_frag"> {
  let assemblyFormat = "`<` $value `>`";
}

def NVVM_WMMALoadOp: NVVM_Op<"wmma.load">,
  Results<(outs LLVM_AnyStruct:$res)>,
  Arguments<(ins LLVM_AnyPointer: $ptr, I32: $stride, I32Attr:$m,
             I32Attr:$n, I32Attr:$k, MMALayoutAttr:$layout,
             MMATypesAttr:$eltype, MMAFragAttr:$frag)> {

  let summary = "Warp synchronous matrix load";

  // Since LLVM intrinsic IDs are enum that cannot be dynamically generated in
  // C++ we instanciate a function in tablegen to map the valide configuration
  // to the corresponsding intrinsic ID.
  // Because we want a single source of truth, this mean the source of truth
  // about valid combinations needs to be in tablgen, therefore we generate
  // extra helpers to query valid configurations based on the shapes of
  // load/store operations.
  let extraClassDeclaration =
    "static llvm::Intrinsic::ID getIntrinsicID("
    "int m, int n, int k, mlir::NVVM::MMALayout layoutEnum,"
    "mlir::NVVM::MMATypes eltypeEnum,mlir::NVVM::MMAFrag fragEnum) {"
    "llvm::StringRef layout = stringifyEnum(layoutEnum);"
    "llvm::StringRef eltype = stringifyEnum(eltypeEnum);"
    "llvm::StringRef frag = stringifyEnum(fragEnum);"
    #MMA_LD_INTR<"load">.id# "\n"
    "return 0;"
    "}\n"
    "/// Helpers to find valid n dimension based on mxk load shape.\n"
    "static int inferNDimension(int m, int k, mlir::NVVM::MMATypes eltypeEnum) {"
    "  llvm::StringRef eltype = stringifyEnum(eltypeEnum);"
    #MMA_ST_INFER_N<!filter(op, NVVM_MMA_OPS.all_ld_ops, !eq(op.frag, "a"))>.id# "\n"
    "return 0;"
    "}\n"
    "/// Helpers to find valid m dimension based on kxn load shape.\n"
    "static int inferMDimension(int k, int n, mlir::NVVM::MMATypes eltypeEnum) {"
    "  llvm::StringRef eltype = stringifyEnum(eltypeEnum);"
    #MMA_ST_INFER_M<!filter(op, NVVM_MMA_OPS.all_ld_ops, !eq(op.frag, "b"))>.id# "\n"
    "return 0;"
    "}\n"
    "/// Helpers to find valid k dimension based on mxn load shape.\n"
    "static int inferKDimension(int m, int n, mlir::NVVM::MMATypes eltypeEnum) {"
    "  llvm::StringRef eltype = stringifyEnum(eltypeEnum);"
    #MMA_ST_INFER_K<!filter(op, NVVM_MMA_OPS.all_ld_ops, !eq(op.frag, "c"))>.id# "\n"
    "return 0;"
    "}\n";


  string llvmBuilder = [{
      auto operands = moduleTranslation.lookupValues(opInst.getOperands());
      auto intId = mlir::NVVM::WMMALoadOp::getIntrinsicID(
        $m, $n, $k, $layout, $eltype, $frag);
      $res = createIntrinsicCall(builder, intId, operands, {operands[0]->getType()});
  }];

  string baseDescription = [{
    The `nvvm.wmma.load` operation loads a matrix collectively using all the
    threads in a warp.

    The operation takes two arguments, the address from where the matrix
    elements are to be loaded from and a stride. The stride argument
    represents the leading dimension of the source matrix. The address and
    the stride are required to be the same across all threads in the warp.
    Each thread in a warp holds a certain number of elements. The Op returns
    a LLVMStruct which holds the elements of the matrix held by this thread.

    This op is meant to be used along with `nvvm.wmma.store` and
    `nvvm.wmma.mma`.

    Example:

    ```mlir
    %2 = nvvm.wmma.load %0, %1
      {eltype = "f16", frag = "a", k = 16 : i32, layout = "row", m = 16 : i32, n = 16 : i32}
      : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
    ```
    }];

  let assemblyFormat = "$ptr `,` $stride attr-dict `:` functional-type($ptr, $res)";
  let hasVerifier = 1;
}

def NVVM_WMMAStoreOp : NVVM_Op<"wmma.store">,
  Arguments<(ins LLVM_AnyPointer: $ptr,
             I32Attr:$m, I32Attr:$n, I32Attr:$k, MMALayoutAttr:$layout,
             MMATypesAttr:$eltype, Variadic<LLVM_Type>:$args, I32: $stride)>{
  let summary = "Warp synchronous matrix store";

  let extraClassDeclaration =
    "static llvm::Intrinsic::ID getIntrinsicID("
    "int m, int n, int k, mlir::NVVM::MMALayout layoutEnum,"
    "mlir::NVVM::MMATypes eltypeEnum) {"
    "  llvm::StringRef layout = stringifyEnum(layoutEnum);"
    "  llvm::StringRef eltype = stringifyEnum(eltypeEnum);"
    #MMA_ST_INTR<"store">.id# "\n"
    "return 0;"
    "}\n"
    "/// Helpers to find valid k dimension based on mxn store shape.\n"
    "static int inferKDimension(int m, int n, mlir::NVVM::MMATypes eltypeEnum) {"
    "  llvm::StringRef eltype = stringifyEnum(eltypeEnum);"
    #MMA_ST_INFER_K<NVVM_MMA_OPS.all_st_ops>.id#  "\n"
    "return 0;"
    "}";

  string llvmBuilder = [{
      auto operands = moduleTranslation.lookupValues(opInst.getOperands());
      auto intId =
        mlir::NVVM::WMMAStoreOp::getIntrinsicID($m, $n, $k, $layout, $eltype);
      createIntrinsicCall(builder, intId, operands, {operands[0]->getType()});
  }];

  string baseDescription = [{
    The `nvvm.wmma.store` operation stores a matrix collectively using
    all the threads in a warp.

    The operation takes as arguments the address to where the matrix elements are
    to be stored, a stride and the elements to store, held by the current thread.
    The stride argument represents the leading dimension of the destination matrix.
    The address and the stride are required to be the same across all threads in the
    warp.

    This op is meant to be used along with `nvvm.wmma.m16n16k16.load` and
    `nvvm.wmma.m16n16k16.mma`.

    Example:

    ```mlir
    nvvm.wmma.store %0, %1, %2, %3, %4, %5
      {eltype = "f16", k = 16 : i32, layout = "row", m = 16 : i32, n = 16 : i32}
      : !llvm.ptr<3>, vector<2 x f16>, vector<2 x f16>, vector<2 x f16>, vector<2 x f16>
    ```
  }];

  let assemblyFormat = [{
    $ptr `,` $stride `,` $args attr-dict `:` qualified(type($ptr)) `,`
    type($args)
  }];
  let hasVerifier = 1;
}

// Base class for all the variants of WMMA mmaOps that may be defined.
def NVVM_WMMAMmaOp : NVVM_Op<"wmma.mma">,
  Results<(outs LLVM_AnyStruct:$res)>,
  Arguments<(ins I32Attr:$m, I32Attr:$n, I32Attr:$k, MMALayoutAttr:$layoutA,
             MMALayoutAttr:$layoutB, MMATypesAttr:$eltypeA,
             MMATypesAttr:$eltypeB, Variadic<LLVM_Type>:$args)>{
  let summary = "Warp synchronous matrix-multiply accumulate using tensor cores.";

  let extraClassDeclaration =
    "static llvm::Intrinsic::ID getIntrinsicID("
    "int m, int n, int k, mlir::NVVM::MMALayout layoutAEnum,"
    "mlir::NVVM::MMALayout layoutBEnum, mlir::NVVM::MMATypes eltypeAEnum,"
    "mlir::NVVM::MMATypes eltypeBEnum) {"
    "llvm::StringRef layoutA = stringifyEnum(layoutAEnum);"
    "llvm::StringRef layoutB = stringifyEnum(layoutBEnum);"
    "llvm::StringRef eltypeA = stringifyEnum(eltypeAEnum);"
    "llvm::StringRef eltypeB = stringifyEnum(eltypeBEnum);"
    #MMA_MMA_INTR<"mma">.id# "\n"
    "return 0;"
    "}";

  string llvmBuilder = [{
      auto operands = moduleTranslation.lookupValues(opInst.getOperands());
      auto intId = mlir::NVVM::WMMAMmaOp::getIntrinsicID(
        $m, $n, $k, $layoutA, $layoutB, $eltypeA, $eltypeB);
      $res = createIntrinsicCall(builder, intId, operands);
  }];

  string baseDescription = [{
    The `nvvm.wmma.mma` operation performs a matrix-multiply accumulate
    (mma) operation using all the threads in a warp.

    The operation performed is represented as `D = A * B + C`. The operation takes
    as arguments the elements of the matrices `A`, `B`, `C` and `D`, held by the
    current thread. The op returns a LLVM struct which holds a part of the result
    held by the current thread.

    This op is meant to be used along with `nvvm.wmma.load` and
    `nvvm.wmma.store`.

    Example:

    ```mlir
    %16 = nvvm.wmma.mma %0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, %15
      {eltypeA = "tf32", eltypeB = "f32", k = 8 : i32, layoutA = "row", layoutB = "row", m = 16 : i32, n = 16 : i32}
      : (i32, i32, i32, i32, i32, i32, i32, i32, f32, f32, f32, f32, f32, f32, f32, f32)
      -> !llvm.struct<(f32, f32, f32, f32, f32, f32, f32, f32)>
    ```
  }];

  let assemblyFormat = "$args attr-dict `:` functional-type($args, $res)";
  let hasVerifier = 1;
}

def NVVM_StMatrixOp: NVVM_PTXBuilder_Op<"stmatrix">, 
  Arguments<(ins LLVM_PointerShared:$ptr, 
                 Variadic<I32>:$sources, 
                 MMALayoutAttr:$layout)> {
  let summary = "cooperative matrix store";
  let description = [{
    Collectively store one or more matrices across all threads in a warp to the
    location indicated by the address operand $ptr in shared memory.
    [For more information, see PTX ISA]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-store-instruction-stmatrix)
  }];
  
  let assemblyFormat = "$ptr `,` $sources attr-dict `:` type(operands)";
  let extraClassDefinition = [{
    std::string $cppClass::getPtx() {
      int d = getSources().size();
      std::string ptx = "stmatrix.sync.aligned";
      ptx += ".x" + std::to_string(d);
      if (getLayout() == NVVM::MMALayout::col)
        ptx += ".trans";
      if(d == 1) ptx += ".m8n8.shared.b16 [%0], {%1};";
      if(d == 2) ptx += ".m8n8.shared.b16 [%0], {%1, %2};";
      if(d == 4) ptx += ".m8n8.shared.b16 [%0], {%1, %2, %3, %4};";
      return ptx;
    }
  }];
  let hasVerifier = 1;
}

def NVVM_LdMatrixOp: NVVM_Op<"ldmatrix">,
  Results<(outs AnyType:$res)>,
  Arguments<(ins LLVM_AnyPointer: $ptr, I32Attr:$num, MMALayoutAttr:$layout)> {

  let summary = "cooperative matrix load";

  string llvmBuilder = [{
      auto operands = moduleTranslation.lookupValues(opInst.getOperands());
      auto intId = getLdMatrixIntrinsicId($layout, $num);
      $res = createIntrinsicCall(builder, intId, operands, {operands[0]->getType()});
  }];

  string baseDescription = [{
    The `nvvm.ldmatrix` operation collectively loads one or more matrices across
    all threads in a warp from the location indicated by the address operand
    `ptr` from shared memory.

    The attribute `num` indicates how many 8x8 16-bit matrices are to be loaded.

    All the threads in the warp must execute the same ldmatrix operations.

    Each row of 8 elements needs to be consecutive in memory. Each lane of the
    warp contains the start address of a row of 8 elements laid out as below:

    ```
    num | lane 0--7    | Threads 8--15  | Threads 16--31
    1   | addr0--addr7 |                |
    2   | addr0--addr7 | addr8--addr15  |
    4   | addr0--addr7 | addr8--addr15  | addr16--addr31
    ```

    Example:
    ```mlir
    %l1 = nvvm.ldmatrix %ptr {num = 1 : i32, layout = #nvvm.mma_layout<row>} :
      (!llvm.ptr<3>) -> i32
    %l2 = nvvm.ldmatrix %ptr {num = 4 : i32, layout = #nvvm.mma_layout<row>} :
      (!llvm.ptr<3>) -> !llvm.struct<(i32, i32, i32, i32)>
    ```
  }];

  let assemblyFormat = "$ptr attr-dict `:` functional-type($ptr, $res)";
  let hasVerifier = 1;
}

def NVVM_MmaOp : NVVM_Op<"mma.sync", [AttrSizedOperandSegments]> {

  let summary = "cooperative matrix-multiply and accumulate";

  let description = [{
    The `nvvm.mma.sync` operation collectively performs the operation
    `D = matmul(A, B) + C` using all threads in a warp.

    All the threads in the warp must execute the same `mma.sync` operation.

    For each possible multiplicand PTX data type, there are one or more possible
    instruction shapes given as "mMnNkK". The below table describes the posssibilities
    as well as the types required for the operands. Note that the data type for
    C (the accumulator) and D (the result) can vary independently when there are
    multiple possibilities in the "C/D Type" column.

    When an optional attribute cannot be immediately inferred from the types of
    the operands and the result during parsing or validation, an error will be
    raised.

    `b1Op` is only relevant when the binary (b1) type is given to
    `multiplicandDataType`. It specifies how the multiply-and-acumulate is
    performed and is either `xor_popc` or `and_poc`. The default is `xor_popc`.

    `intOverflowBehavior` is only relevant when the `multiplicandType` attribute
    is one of `u8, s8, u4, s4`, this attribute describes how overflow is handled
    in the accumulator. When the attribute is `satfinite`, the accumulator values
    are clamped in the int32 range on overflow. This is the default behavior.
    Alternatively, accumulator behavior `wrapped` can also be specified, in
    which case overflow wraps from one end of the range to the other.

    `layoutA` and `layoutB` are required and should generally be set to
    `#nvvm.mma_layout<row>` and `#nvvm.mma_layout<col>` respectively, but other
    combinations are possible for certain layouts according to the table below.

    ```
    | A/B Type | Shape     | ALayout | BLayout | A Type   | B Type   | C/D Type          |
    |----------|-----------|---------|---------|----------|----------|-------------------|
    | f64      | .m8n8k4   | row     | col     | 1x f64   | 1x f64   | 2x f64            |
    | f16      | .m8n8k4   | row/col | row/col | 2x f16x2 | 2x f16x2 | 4x f16x2 or 8xf32 |
    |          | .m16n8k8  | row     | col     | 2x f16x2 | 1x f16x2 | 2x f16x2 or 4 f32 |
    |          | .m16n8k16 | row     | col     | 4x f16x2 | 2x f16x2 | 2x f16x2 or 4 f32 |
    | bf16     | .m16n8k8  | row     | col     | 2x f16x2 | 1x f16x2 | 2x f16x2 or 4 f32 |
    |          | .m16n8k16 | row     | col     | 4x f16x2 | 2x f16x2 | 2x f16x2 or 4 f32 |
    | tf32     | .m16n8k4  | row     | col     | 2x i32   | 1x i32   | 4x f32            |
    |          | .m16n8k8  | row     | col     | 4x i32   | 2x i32   | 2x f16x2 or 4 f32 |
    | u8/s8    | .m8n8k16  | row     | col     | 1x i32   | 1x i32   | 2x i32            |
    |          | .m16n8k16 | row     | col     | 2x i32   | 1x i32   | 4x i32            |
    |          | .m16n8k32 | row     | col     | 4x i32   | 2x i32   | 4x i32            |
    | u4/s4    | .m8n8k32  | row     | col     | 1x i32   | 1x i32   | 2x i32            |
    |          | m16n8k32  | row     | col     | 2x i32   | 1x i32   | 4x i32            |
    |          | m16n8k64  | row     | col     | 4x i32   | 2x i32   | 4x i32            |
    | b1       | m8n8k128  | row     | col     | 1x i32   | 1x i32   | 2x i32            |
    |          | m16n8k128 | row     | col     | 2x i32   | 1x i32   | 4x i32            |
    ```


    Example:
    ```mlir

    %128 = nvvm.mma.sync A[%120, %121, %122, %123]
                         B[%124, %125]
                         C[%126, %127]
                         {layoutA = #nvvm.mma_layout<row>,
                          layoutB = #nvvm.mma_layout<col>,
                          shape = {k = 16 : i32, m = 16 : i32, n = 8 : i32}}
        : (vector<2xf16>, vector<2xf16>, vector<2xf16>)
           -> !llvm.struct<(vector<2xf16>, vector<2xf16>)>
    ```
  }];

  let results = (outs LLVM_AnyStruct:$res);
  let arguments = (ins NVVM_MMAShapeAttr:$shape,
             OptionalAttr<MMAB1OpAttr>:$b1Op,
             OptionalAttr<MMAIntOverflowAttr>:$intOverflowBehavior,
             MMALayoutAttr:$layoutA,
             MMALayoutAttr:$layoutB,
             OptionalAttr<MMATypesAttr>:$multiplicandAPtxType,
             OptionalAttr<MMATypesAttr>:$multiplicandBPtxType,
             Variadic<LLVM_Type>:$operandA,
             Variadic<LLVM_Type>:$operandB,
             Variadic<LLVM_Type>:$operandC);

  let extraClassDeclaration = !strconcat([{
      static llvm::Intrinsic::ID getIntrinsicID(
            int64_t m, int64_t n, uint64_t k,
            std::optional<MMAB1Op> b1Op,
            std::optional<MMAIntOverflow> sat,
            mlir::NVVM::MMALayout layoutAEnum, mlir::NVVM::MMALayout layoutBEnum,
            mlir::NVVM::MMATypes eltypeAEnum, mlir::NVVM::MMATypes eltypeBEnum,
            mlir::NVVM::MMATypes eltypeCEnum, mlir::NVVM::MMATypes eltypeDEnum) {
        llvm::StringRef layoutA = stringifyEnum(layoutAEnum);
        llvm::StringRef layoutB = stringifyEnum(layoutBEnum);
        llvm::StringRef eltypeA = stringifyEnum(eltypeAEnum);
        llvm::StringRef eltypeB = stringifyEnum(eltypeBEnum);
        llvm::StringRef eltypeC = stringifyEnum(eltypeCEnum);
        llvm::StringRef eltypeD = stringifyEnum(eltypeDEnum);
        }],
        MMA_SYNC_INTR<>.id, [{
        return 0;
      }

      static std::optional<mlir::NVVM::MMATypes> inferOperandMMAType(Type operandElType,
        bool isAccumulator);

      MMATypes accumPtxType();
      MMATypes resultPtxType();
    }]);

  let builders = [
      OpBuilder<(ins  "Type":$resultType, "ValueRange":$operandA,
        "ValueRange":$operandB, "ValueRange":$operandC,
        "ArrayRef<int64_t>":$shape, "std::optional<MMAB1Op>":$b1Op,
        "std::optional<MMAIntOverflow>":$intOverflow,
        "std::optional<std::array<MMATypes, 2>>":$multiplicandPtxTypes,
        "std::optional<std::array<MMALayout, 2>>":$multiplicandLayouts)>
    ];

  string llvmBuilder = [{
    auto operands = moduleTranslation.lookupValues(opInst.getOperands());
    auto intId = mlir::NVVM::MmaOp::getIntrinsicID(
        $shape.getM(), $shape.getN(), $shape.getK(),
        $b1Op, $intOverflowBehavior,
        $layoutA, $layoutB,
        *$multiplicandAPtxType,
        *$multiplicandBPtxType,
        op.accumPtxType(),
        op.resultPtxType());

    $res = createIntrinsicCall(
      builder, intId, operands);
  }];

  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// NVVM TMA Ops
//===----------------------------------------------------------------------===//

def NVVM_CpAsyncBulkCommitGroupOp : NVVM_Op<"cp.async.bulk.commit.group">,
  Arguments<(ins )> {
  let assemblyFormat = "attr-dict";
  let description = [{
    This Op commits all prior initiated but uncommitted cp.async.bulk
    instructions into a cp.async.bulk-group.

    [For more information, see PTX ISA]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-commit-group)
  }];

  string llvmBuilder = [{
    createIntrinsicCall(builder, llvm::Intrinsic::nvvm_cp_async_bulk_commit_group);
  }];
}

def NVVM_CpAsyncBulkWaitGroupOp : NVVM_Op<"cp.async.bulk.wait_group">,
  Arguments<(ins 
    ConfinedAttr<I32Attr, [IntMinValue<0>]>:$group, 
    OptionalAttr<UnitAttr>:$read)> {
  let assemblyFormat = "$group attr-dict";
  let description = [{
    Op waits for completion of the most recent bulk async-groups.

    The `$group` operand tells waiting has to be done until for $group or fewer
    of the most recent bulk async-groups. If `$group` is 0, the op wait until 
    all the most recent bulk async-groups have completed.

    The `$read` indicates that the waiting has to be done until all the bulk 
    async operations in the specified bulk async-group have completed reading 
    from their source locations.

    [For more information, see PTX ISA]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-wait-group)
  }];
  
  string llvmBuilder = [{
    auto intId = op.getRead() ?
      llvm::Intrinsic::nvvm_cp_async_bulk_wait_group_read :
      llvm::Intrinsic::nvvm_cp_async_bulk_wait_group;
    createIntrinsicCall(builder, intId, builder.getInt32($group));
  }];
}

def NVVM_CpAsyncBulkTensorGlobalToSharedClusterOp : 
  NVVM_Op<"cp.async.bulk.tensor.shared.cluster.global", 
  [DeclareOpInterfaceMethods<BasicPtxBuilderOpInterface>, 
  AttrSizedOperandSegments]>,
  Arguments<(ins  LLVM_PointerShared:$dstMem,
                  LLVM_AnyPointer:$tmaDescriptor,
                  Variadic<I32>:$coordinates,
                  LLVM_PointerShared:$mbar,                  
                  Variadic<I16>:$im2colOffsets,
                  Optional<I16>:$multicastMask,
                  Optional<I64>:$l2CacheHint,
                  PtxPredicate:$predicate)> {
  let description = [{
    Initiates an asynchronous copy operation on the tensor data from global 
    memory to shared memory. 

    The Op operates has two load modes:
    1) Tiled Mode: It's the default mode. The source multi-dimensional tensor 
    layout is preserved at the destination. 

    2) Im2col Mode: This mode is used when `im2colOffsets` operands are present.
    the elements in the Bounding Box of the source tensor are rearranged into
    columns at the destination. In this mode, the tensor has to be at least 
    3-dimensional. 

    The `multicastMask` operand is optional. When it is present, the Op copies
    data from global memory to shared memory of multiple CTAs in the cluster.
    Operand `multicastMask` specifies the destination CTAs in the cluster such 
    that each bit position in the 16-bit `multicastMask` operand corresponds to
    the `nvvm.read.ptx.sreg.ctaid` of the destination CTA.     

    The `l2CacheHint` operand is optional, and it is used to specify cache 
    eviction policy that may be used during the memory access.
    
    [For more information, see PTX ISA]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor)
  }];

  let assemblyFormat = [{ 
    $dstMem `,` 
    $tmaDescriptor `,` 
    $mbar `,` 
    `box` `[`$coordinates `]` 
    (`im2col` `[` $im2colOffsets^ `]` )?
    (`multicast_mask` `=` $multicastMask^ )?
    (`l2_cache_hint` `=` $l2CacheHint^ )?
    (`predicate` `=` $predicate^)? 
    attr-dict  `:` type($dstMem) `,` type($tmaDescriptor)
  }];

  let extraClassDefinition = [{
    std::string $cppClass::getPtx() {
      int im2colDim = getIm2colOffsets().size();
      int dim = getCoordinates().size();
      std::string ptx = "cp.async.bulk.tensor.";
      ptx += std::to_string(dim) + "d.";
      ptx += "shared::cluster.global.mbarrier::complete_tx::bytes";      
      if(im2colDim) ptx += ".im2col";
      if(getMulticastMask()) ptx += ".multicast::cluster";      
      if(getL2CacheHint()) ptx += ".L2::cache_hint";
      
      auto preg = [](int r) { return "%" + std::to_string(r); };

      // Build Registers
      ptx += " [%0], [%1, {";
      int r = 2;      
      for(int i = 0; i < dim; i++) ptx += preg(r+i) + ",";
      ptx.pop_back(); r += dim;
      ptx += "} ], [%" + std::to_string(r++) + "]";
      if(im2colDim) {
        ptx += ",{";
        for(int i = 0; i < im2colDim; i++) ptx += preg(r+i) + ",";
        ptx.pop_back(); r += im2colDim;
        ptx += "}";
      }
      if(getMulticastMask()) ptx += ", " + preg(r++);
      if(getL2CacheHint()) ptx += ", " + preg(r++);
      ptx += ";";
      return ptx;
    }
  }];
  let hasVerifier = 1;
}

def NVVM_CpAsyncBulkTensorSharedCTAToGlobalOp : 
  NVVM_Op<"cp.async.bulk.tensor.global.shared.cta", 
  [DeclareOpInterfaceMethods<BasicPtxBuilderOpInterface>, 
  AttrSizedOperandSegments]>,
  Arguments<(ins  LLVM_AnyPointer:$tmaDescriptor,
                  LLVM_PointerShared:$srcMem,
                  Variadic<I32>:$coordinates,
                  PtxPredicate:$predicate)> {
  let assemblyFormat = [{ 
    $tmaDescriptor `,` 
    $srcMem `,` 
    `box` `[`$coordinates `]` 
    (`,` `predicate` `=` $predicate^)?  
    attr-dict  `:` type(operands)
  }];
  let extraClassDefinition = [{
    std::string $cppClass::getPtx() {
      int dim = getCoordinates().size();
      std::string ptx = "cp.async.bulk.tensor.";
      ptx += std::to_string(dim) + "d.";
      ptx += "global.shared::cta.bulk_group";
      if(dim == 1) ptx += " [%0, {%2} ], [%1];";
      if(dim == 2) ptx += " [%0, {%2, %3} ], [%1];";
      if(dim == 3) ptx += " [%0, {%2, %3, %4} ], [%1];";
      if(dim == 4) ptx += " [%0, {%2, %3, %4, %5} ], [%1];";
      if(dim == 5) ptx += " [%0, {%2, %3, %4, %5, %6} ], [%1];";
      return ptx;
    }
  }];
  let hasVerifier = 1;
}

def NVVM_PrefetchTensorMapOp : NVVM_Op<"prefetch.tensormap",
                    [DeclareOpInterfaceMethods<BasicPtxBuilderOpInterface>]>,
  Arguments<(ins LLVM_AnyPointer:$tmaDescriptor, PtxPredicate:$predicate)> {
  let assemblyFormat = "$tmaDescriptor (`,` `predicate` `=` $predicate^)? attr-dict `:` type(operands)";
  let extraClassDefinition = [{
    std::string $cppClass::getPtx() { 
      return std::string("prefetch.tensormap [%0];");
    }
  }];
}

def NVVM_CpAsyncBulkTensorPrefetchOp :
  NVVM_Op<"cp.async.bulk.tensor.prefetch", [AttrSizedOperandSegments]> {
  let arguments = (ins
    LLVM_AnyPointer:$tmaDescriptor,
    Variadic<I32>:$coordinates,
    Variadic<I16>:$im2colOffsets,
    Optional<I64>:$l2CacheHint);

  let description = [{
    Initiates an asynchronous prefetch operation on the tensor data from global
    memory to L2 cache.

    The Op has two modes:
    1) Tiled Mode: It's the default mode. The source multi-dimensional tensor
    layout is preserved at the destination.

    2) Im2col Mode: This mode is used when `im2colOffsets` operands are present.
    the elements in the Bounding Box of the source tensor are rearranged into
    columns at the destination. In this mode, the tensor has to be at least
    3-dimensional.

    The `l2CacheHint` operand is optional, and it is used to specify cache
    eviction policy that may be used during the memory access.

    [For more information, see PTX ISA]
    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-prefetch-tensor)
  }];

  let assemblyFormat = [{
    $tmaDescriptor `,`
    `box` `[`$coordinates `]`
    (`im2col` `[` $im2colOffsets^ `]` )?
    (`l2_cache_hint` `=` $l2CacheHint^ )?
    attr-dict  `:` type($tmaDescriptor)
  }];

  let extraClassDeclaration = [{
    static llvm::Intrinsic::ID getIntrinsicID(int tensorDims, bool isIm2Col);
  }];

  let hasVerifier = 1;

  string llvmBuilder = [{
    // Arguments to the intrinsic:
    // tmaDesc, tensorDims, im2colOffsets
    // cache_hint(if applicable) and flag(boolean)
    llvm::SmallVector<llvm::Value *> translatedOperands;
    translatedOperands.push_back($tmaDescriptor);

    for (auto v : op.getCoordinates())
      translatedOperands.push_back(moduleTranslation.lookupValue(v));

    for (auto v : op.getIm2colOffsets())
      translatedOperands.push_back(moduleTranslation.lookupValue(v));

    llvm::LLVMContext &ctx = moduleTranslation.getLLVMContext();
    auto *i64Undef = llvm::UndefValue::get(llvm::IntegerType::get(ctx, 64));

    bool isCacheHint = op.getL2CacheHint() ? true : false;
    translatedOperands.push_back(isCacheHint ? $l2CacheHint : i64Undef);
    translatedOperands.push_back(builder.getInt1(isCacheHint));

    auto intId = NVVM::CpAsyncBulkTensorPrefetchOp::getIntrinsicID(
        op.getCoordinates().size(), op.getIm2colOffsets().size() > 0);
    createIntrinsicCall(builder, intId, translatedOperands);
  }];
}

//===----------------------------------------------------------------------===//
// NVVM Wgmma Ops
//===----------------------------------------------------------------------===//

def NVVM_WgmmaFenceAlignedOp : NVVM_PTXBuilder_Op<"wgmma.fence.aligned"> {
  let arguments = (ins);
  let description = [{
    Enforce an ordering of register accesses between warpgroup level matrix 
    multiplication and other operations. 
    
    [For more information, see PTX ISA](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions-wgmma-fence)
  }];
  let assemblyFormat = "attr-dict";
  let extraClassDefinition = [{
    std::string $cppClass::getPtx() { return std::string("wgmma.fence.sync.aligned;"); }
  }];
}

def NVVM_WgmmaGroupSyncAlignedOp : NVVM_PTXBuilder_Op<"wgmma.commit.group.sync.aligned">,
  Arguments<(ins )> {
  let assemblyFormat = "attr-dict";
  let description = [{
    Commits all prior uncommitted warpgroup level matrix multiplication operations.
    
    [For more information, see PTX ISA](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions-wgmma-commit-group)
  }];
  let extraClassDefinition = [{
    std::string $cppClass::getPtx() { return std::string("wgmma.commit_group.sync.aligned;"); }
  }];
}

def NVVM_WgmmaWaitGroupSyncOp : NVVM_PTXBuilder_Op<"wgmma.wait.group.sync.aligned">{
  let arguments = (ins I32Attr:$group);
  let assemblyFormat = "attr-dict $group";
  let description = [{
    Signal the completion of a preceding warpgroup operation.
    
    [For more information, see PTX ISA](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions-wgmma-wait-group)
  }];
  let extraClassDefinition = [{
    std::string $cppClass::getPtx() { return std::string("wgmma.wait_group.sync.aligned %0;"); }
  }];
}

/// Enum attribute type for the negating of input operands
def WGMMAScaleInNeg : I32EnumAttrCase<"neg", -1>;
def WGMMAScaleInOne : I32EnumAttrCase<"one", 1>;
def WGMMAScaleIn : I32EnumAttr<"WGMMAScaleIn", "WGMMA overflow options",
  [WGMMAScaleInOne, WGMMAScaleInNeg]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::NVVM";
}
def WGMMAScaleInAttr : EnumAttr<NVVM_Dialect, WGMMAScaleIn, "wgmma_scale_in"> {
  let assemblyFormat = "`<` $value `>`";
}

/// Enum attribute type for the output operand
def WGMMAScaleOutZero : I32EnumAttrCase<"zero", 0>;
def WGMMAScaleOutOne : I32EnumAttrCase<"one", 1>;
def WGMMAScaleOut : I32EnumAttr<"WGMMAScaleOut", "WGMMA input predicate",
  [WGMMAScaleOutZero, WGMMAScaleOutOne]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::NVVM";
}
def WGMMAScaleOutAttr : EnumAttr<NVVM_Dialect, WGMMAScaleOut, "wgmma_scale_out"> {
  let assemblyFormat = "`<` $value `>`";
}

/// Enum attribute of the different PTX element types used for WGMMA operands.
def WGMMATypeF16  : I32EnumAttrCase<"f16", 0>;
def WGMMATypeTF32 : I32EnumAttrCase<"tf32", 1>;
def WGMMATypeU8 : I32EnumAttrCase<"u8", 2>;
def WGMMATypeS8 : I32EnumAttrCase<"s8", 3>;
def WGMMATypeB1 : I32EnumAttrCase<"b1", 4>;
def WGMMATypeBF16 : I32EnumAttrCase<"bf16", 5>;
def WGMMATypeF8E4M3 : I32EnumAttrCase<"e4m3", 6>;
def WGMMATypeF8E5M2 : I32EnumAttrCase<"e5m2", 7>;
def WGMMATypeF32 : I32EnumAttrCase<"f32", 8>;
def WGMMATypeS32 : I32EnumAttrCase<"s32", 9>;

def WGMMATypes : I32EnumAttr<"WGMMATypes", "NVVM WGMMA types",
  [WGMMATypeF16, WGMMATypeTF32,
    WGMMATypeU8, WGMMATypeS8,
    WGMMATypeB1, WGMMATypeBF16, WGMMATypeF8E4M3, 
    WGMMATypeF8E5M2, WGMMATypeF32, WGMMATypeS32]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::NVVM";
}
def WGMMATypesAttr : EnumAttr<NVVM_Dialect, WGMMATypes, "wgmma_type"> {
  let assemblyFormat = "`<` $value `>`";
}


def NVVM_WgmmaMmaAsyncOp : NVVM_Op<"wgmma.mma_async", 
              [DeclareOpInterfaceMethods<BasicPtxBuilderOpInterface>,
                PredOpTrait<"input struct and result struct must be the same type",
                  TCresIsSameAsOpBase<0, 0>>,]> 
{
  let results = (outs LLVM_AnyStruct:$results);
  let arguments = (ins 
    LLVM_AnyStruct:$inouts,
    I64:$descriptorA, 
    I64:$descriptorB, 
    NVVM_MMAShapeAttr:$shape,
    WGMMATypesAttr:$typeA,
    WGMMATypesAttr:$typeB,
    WGMMATypesAttr:$typeD,
    WGMMAScaleOutAttr:$scaleD,
    WGMMAScaleInAttr:$scaleA,
    WGMMAScaleInAttr:$scaleB, 
    MMALayoutAttr:$layoutA,
    MMALayoutAttr:$layoutB,
    OptionalAttr<MMAIntOverflowAttr>:$satfinite
  );  
  
   let assemblyFormat = [{ 
      $descriptorA `,` $descriptorB `,` $inouts `,` $shape `,`
      `D` `[` $typeD `,` $scaleD (`,` $satfinite^)? `]` `,`
      `A` `[` $typeA `,` $scaleA `,` $layoutA `]` `,` 
      `B` `[` $typeB `,` $scaleB `,` $layoutB `]`
      attr-dict `:` 
      type($inouts) `->` type($results)
    }];
  
  let description = [{
    The warpgroup (128 threads) level matrix multiply and accumulate operation 
    has either of the following forms, where matrix D is called accumulator:
      D = A * B + D
      D = A * B, where the input from accumulator D is disabled.

    Supported shapes:  
    ```
    |--------------|--------------|------------|--------------|---------------|
    |              |              |            |              |f16+=e4m3*e4m3 |
    |              |              |            |              |f16+=e5m2*e5m2 |
    |f32+=tf32*tf32|f16+=f16 *f16 | s32+=s8*s8 |s32 += b1 * b1|f16+=e5m2*e4m3 |
    |              |f32+=f16 *f16 | s32+=u8*u8 |              |f16+=e4m3*e5m2 |
    |              |f32+=bf16*bf16| s32+=u8*u8 |              |f16+=e4m3*e5m2 |
    |              |f32+=bf16*bf16| s32+=s8*u8 |              |f32+=e4m3*e4m3 |
    |              |              | s32+=u8*s8 |              |f32+=e5m2*e5m2 |
    |              |              |            |              |f32+=e4m3*e5m2 |
    |              |              |            |              |f32+=e4m3*e5m2 |
    |--------------|--------------|------------|--------------|---------------|
    |   .m64n8k8   |  .m64n8k16   | .m64n8k32  | .m64n8k256   | .m64n8k32     |
    |   .m64n16k8  |  .m64n16k16  | .m64n16k32 | .m64n16k256  | .m64n16k32    |
    |   .m64n24k8  |  .m64n24k16  | .m64n24k32 | .m64n24k256  | .m64n24k32    |
    |   .m64n32k8  |  .m64n32k16  | .m64n32k32 | .m64n32k256  | .m64n32k32    |
    |   .m64n40k8  |  .m64n40k16  | .m64n48k32 | .m64n48k256  | .m64n40k32    |
    |   .m64n48k8  |  .m64n48k16  | .m64n64k32 | .m64n64k256  | .m64n48k32    |
    |   .m64n56k8  |  .m64n56k16  | .m64n80k32 | .m64n80k256  | .m64n56k32    |
    |   .m64n64k8  |  .m64n64k16  | .m64n96k32 | .m64n96k256  | .m64n64k32    |
    |   .m64n72k8  |  .m64n72k16  | .m64n112k32| .m64n112k256 | .m64n72k32    |
    |   .m64n80k8  |  .m64n80k16  | .m64n128k32| .m64n128k256 | .m64n80k32    |
    |   .m64n88k8  |  .m64n88k16  | .m64n144k32| .m64n144k256 | .m64n88k32    |
    |   .m64n96k8  |  .m64n96k16  | .m64n160k32| .m64n160k256 | .m64n96k32    |
    |   .m64n104k8 |  .m64n104k16 | .m64n176k32| .m64n176k256 | .m64n104k32   |
    |   .m64n112k8 |  .m64n112k16 | .m64n192k32| .m64n192k256 | .m64n112k32   |
    |   .m64n120k8 |  .m64n120k16 | .m64n208k32| .m64n208k256 | .m64n120k32   |
    |   .m64n128k8 |  .m64n128k16 | .m64n224k32| .m64n224k256 | .m64n128k32   |
    |   .m64n136k8 |  .m64n136k16 | .m64n240k32| .m64n240k256 | .m64n136k32   |
    |   .m64n144k8 |  .m64n144k16 | .m64n256k32| .m64n256k256 | .m64n144k32   |
    |   .m64n152k8 |  .m64n152k16 |            |              | .m64n152k32   |
    |   .m64n160k8 |  .m64n160k16 |            |              | .m64n160k32   |
    |   .m64n168k8 |  .m64n168k16 |            |              | .m64n168k32   |
    |   .m64n176k8 |  .m64n176k16 |            |              | .m64n176k32   |
    |   .m64n184k8 |  .m64n184k16 |            |              | .m64n184k32   |
    |   .m64n192k8 |  .m64n192k16 |            |              | .m64n192k32   |
    |   .m64n200k8 |  .m64n200k16 |            |              | .m64n200k32   |
    |   .m64n208k8 |  .m64n208k16 |            |              | .m64n208k32   |
    |   .m64n216k8 |  .m64n216k16 |            |              | .m64n216k32   |
    |   .m64n224k8 |  .m64n224k16 |            |              | .m64n224k32   |
    |   .m64n232k8 |  .m64n232k16 |            |              | .m64n232k32   |
    |   .m64n240k8 |  .m64n240k16 |            |              | .m64n240k32   |
    |   .m64n248k8 |  .m64n248k16 |            |              | .m64n248k32   |
    |   .m64n256k8 |  .m64n256k16 |            |              | .m64n256k32   |
    |--------------|--------------|------------|--------------|---------------|
    ```

    
    [For more information, see PTX ISA](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions)
  }];
  
  let hasVerifier = 1;

  let extraClassDeclaration = [{
    void getAsmValues(RewriterBase &rewriter, 
        llvm::SmallVectorImpl<std::pair<mlir::Value, mlir::NVVM::PTXRegisterMod>> &asmValues);
  }];
}

//===----------------------------------------------------------------------===//
// NVVM breakpoint Op
//===----------------------------------------------------------------------===//

def NVVM_Breakpoint : NVVM_Op<"breakpoint"> {
  let summary = "Breakpoint Op";
  let description = [{
    Breakpoint suspends execution of the program for debugging.
    [For more information, see PTX ISA](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#miscellaneous-instructions-brkpt)
  }];
  string llvmBuilder = [{
    createIntrinsicCall(builder, llvm::Intrinsic::debugtrap);
  }];

  let assemblyFormat = "attr-dict";
}

//===----------------------------------------------------------------------===//
// NVVM target attribute.
//===----------------------------------------------------------------------===//

def NVVM_TargettAttr : NVVM_Attr<"NVVMTarget", "target"> {
  let description = [{
    GPU target attribute for controlling compilation of NVIDIA targets. All
    parameters decay into default values if not present.

    Examples:

    1. Target with default values.
    ```
      gpu.module @mymodule [#nvvm.target] attributes {...} {
        ...
      }
    ```

    2. Target with `sm_90` chip and fast math.
    ```
      gpu.module @mymodule [#nvvm.target<chip = "sm_90", flags = {fast}>] {
        ...
      }
    ```
  }];
  let parameters = (ins
    DefaultValuedParameter<"int", "2", "Optimization level to apply.">:$O,
    StringRefParameter<"Target triple.", "\"nvptx64-nvidia-cuda\"">:$triple,
    StringRefParameter<"Target chip.", "\"sm_50\"">:$chip,
    StringRefParameter<"Target chip features.", "\"+ptx60\"">:$features,
    OptionalParameter<"DictionaryAttr", "Target specific flags.">:$flags,
    OptionalParameter<"ArrayAttr", "Files to link to the LLVM module.">:$link
  );
  let assemblyFormat = [{
    (`<` struct($O, $triple, $chip, $features, $flags, $link)^ `>`)?
  }];
  let builders = [
    AttrBuilder<(ins CArg<"int", "2">:$optLevel,
                     CArg<"StringRef", "\"nvptx64-nvidia-cuda\"">:$triple,
                     CArg<"StringRef", "\"sm_50\"">:$chip,
                     CArg<"StringRef", "\"+ptx60\"">:$features,
                     CArg<"DictionaryAttr", "nullptr">:$targetFlags,
                     CArg<"ArrayAttr", "nullptr">:$linkFiles), [{
      return Base::get($_ctxt, optLevel, triple, chip, features, targetFlags, linkFiles);
    }]>
  ];
  let skipDefaultBuilders = 1;
  let genVerifyDecl = 1;
  let extraClassDeclaration = [{
    bool hasFlag(StringRef flag) const;
    bool hasFastMath() const;
    bool hasFtz() const;
  }];
  let extraClassDefinition = [{
    bool $cppClass::hasFlag(StringRef flag) const {
      if (DictionaryAttr flags = getFlags())
        return flags.get(flag) != nullptr;
      return false;
    }
    bool $cppClass::hasFastMath() const {
      return hasFlag("fast");
    }
    bool $cppClass::hasFtz() const {
      return hasFlag("ftz");
    }
  }];
}

#endif // NVVMIR_OPS


//===- ShapeOps.td - Shape operations definition -----------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This is the operation definition file for Shape dialect operations.
//
//===----------------------------------------------------------------------===//

#ifndef SHAPE_OPS
#define SHAPE_OPS

include "mlir/Dialect/Shape/IR/ShapeBase.td"
include "mlir/Interfaces/CallInterfaces.td"
include "mlir/Interfaces/CastInterfaces.td"
include "mlir/Interfaces/ControlFlowInterfaces.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/IR/OpAsmInterface.td"
include "mlir/Interfaces/FunctionInterfaces.td"
include "mlir/IR/SymbolInterfaces.td"

//===----------------------------------------------------------------------===//
// Shape op definitions
//===----------------------------------------------------------------------===//

// Base class for the operation in this dialect
class Shape_Op<string mnemonic, list<Trait> traits = []> :
    Op<ShapeDialect, mnemonic, traits>;

def Shape_AddOp : Shape_Op<"add",
    [Commutative, Pure, InferTypeOpAdaptorWithIsCompatible]> {
  let summary = "Addition of sizes and indices";
  let description = [{
    Adds two sizes or indices. If either operand is an error it will be
    propagated to the result. The operands can be of type `size` or `index`. If
    at least one of the operands can hold an error, i.e. if it is of type
    `size`, the result must be of type `size`. If error propagation is not
    possible because both operands are of type `index` then the result may be
    of type `size` or `index`.
  }];

  let arguments = (ins Shape_SizeOrIndexType:$lhs, Shape_SizeOrIndexType:$rhs);
  let results = (outs Shape_SizeOrIndexType:$result);

  let assemblyFormat = [{
    $lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `->` type($result)
  }];

  let hasFolder = 1;
  let hasVerifier = 1;
}

def Shape_BroadcastOp : Shape_Op<"broadcast", [Commutative, Pure]> {
  let summary = "Returns the broadcasted output shape of two or more inputs";
  let description = [{
    Returns the broadcasted shape for input shapes or extent tensors. The rest
    of this description is simplified for the 2 input case but can be extended
    to more inputs. Both operands can be of type `shape.shape` or
    `tensor<?xindex>`. The result is of type `shape.shape` and, if both
    operands are tensors, may be of type `tensor<?xindex>`.

    If the two operand shapes are of different rank the smaller one is padded
    with 1's from the left. The resulting broadcasted shape is then defined as

        result[i] = lhs[i] if lhs[i] == rhs[i]
                  = lhs[i] if rhs[i] == 1
                  = rhs[i] if lhs[i] == 1.

    In case the resulting shape is undefined, i.e. if corresponding extents are
    different from each other but none is 1, the result is an error shape.
    Likewise error values are propagated if any of the operands holds an error
    value. If the result type is an extent tensor (and can therefore not hold
    the error value) the behavior may be undefined. The optional string
    attribute can be used to describe the error case.
  }];

  let arguments = (ins Variadic<Shape_ShapeOrExtentTensorType>:$shapes,
                       OptionalAttr<StrAttr>:$error);
  let results = (outs Shape_ShapeOrExtentTensorType:$result);

  let builders = [OpBuilder<(ins "Value":$shape)>];

  let assemblyFormat = [{
    $shapes attr-dict `:` type($shapes) `->` type($result)
  }];

  let builders = [OpBuilder<(ins "::mlir::Type":$result,
                                "::mlir::Value":$lhs, "::mlir::Value":$rhs,
                                "/*optional*/ ::mlir::StringAttr":$error), [{
      build($_builder, $_state, result, ::llvm::ArrayRef({lhs, rhs}),
        error);
    }]>
  ];

  let hasFolder = 1;
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

def Shape_ConstShapeOp : Shape_Op<"const_shape",
    [ConstantLike, Pure, InferTypeOpAdaptorWithIsCompatible]> {
  let summary = "Creates a constant shape or extent tensor";
  let description = [{
    Creates a constant shape or extent tensor. The individual extents are given
    as the `shape` attribute. The number of these values equals the shape's
    rank.

    ```mlir
    %0 = shape.const_shape [] : !shape.shape
    %1 = shape.const_shape [1, 2, 3] : !shape.shape
    %2 = shape.const_shape [4, 5, 6] : tensor<3xindex>
    ```
  }];
  let arguments = (ins IndexElementsAttr:$shape);
  let results = (outs Shape_ShapeOrExtentTensorType:$result);

  let hasCustomAssemblyFormat = 1;
  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

def Shape_ConstSizeOp : Shape_Op<"const_size", [
    ConstantLike,
    Pure,
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>
  ]> {
  let summary = "Creates a constant of type `shape.size`";
  let description = [{
    Creates a `shape.size` type representing the constant size given by `value`.

    ```mlir
    %x = shape.const_size 10
    ```
  }];

  let arguments = (ins IndexAttr:$value);
  let results = (outs Shape_SizeType:$result);

  let builders = [OpBuilder<(ins "int64_t":$value)>];

  let assemblyFormat = "$value attr-dict";
  let hasFolder = 1;
}

def Shape_DivOp : Shape_Op<"div", [Pure, InferTypeOpAdaptorWithIsCompatible]> {
  let summary = "Division of sizes and indices";
  let description = [{
    Divides two sizes or indices. If either operand is an error it will be
    propagated to the result. The operands can be of type `size` or `index`.
    If at least one of the operands can hold an error, i.e. if it is of type
    `size`, the result must be of type `size`. If error propagation is not
    possible because both operands are of type `index` then the result may be
    of type  `size` or `index`. If both operands and result are of type
    `index`, their runtime values could be negative. The result is rounded
    toward negative infinity, i.e. floor(lhs / rhs), such that

        div(lhs, rhs) * rhs + mod(lhs, rhs) = lhs

    always holds. If any of the values is of type `size`, the behavior for
    negative value is undefined.
  }];

  let arguments = (ins Shape_SizeOrIndexType:$lhs,
                       Shape_SizeOrIndexType:$rhs);
  let results = (outs Shape_SizeOrIndexType:$result);

  let assemblyFormat = [{
    $lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `->` type($result)
  }];

  let hasFolder = 1;
  let hasVerifier = 1;
}

def Shape_ShapeEqOp : Shape_Op<"shape_eq", [Pure, Commutative]> {
  let summary = "Returns whether the input shapes or extent tensors are equal";
  let description = [{
    Takes one or more shape or extent tensor operands and determines whether
    they are equal. When extent tensors are compared to shapes they are
    regarded as their equivalent non-error shapes. Error shapes can be tested
    for equality like any other shape value, meaning that the error value is
    equal to itself.
  }];

  let arguments = (ins Variadic<Shape_ShapeOrExtentTensorType>:$shapes);
  let results = (outs I1:$result);

  // Convenience builder alias for the binary version.
  let builders = [
  OpBuilder<(ins "::mlir::Value":$lhs, "::mlir::Value":$rhs),
    [{ build($_builder, $_state, ::llvm::ArrayRef({lhs, rhs})); }]>,
  ];

  let assemblyFormat = "$shapes attr-dict `:` type($shapes)";
  let hasFolder = 1;
}

def Shape_FromExtentsOp : Shape_Op<"from_extents", [Pure]> {
  let summary = "Creates a shape from extents";
  let description = [{
    Creates a shape from multiple SSA values representing the extents of
    the shape.

    ```mlir
    // Rank 2 shape.
    %s0 = shape.from_extents %a, %b
    // Rank 0 shape.
    %s1 = shape.from_extents
    ```
  }];
  let arguments = (ins Variadic<Shape_SizeOrIndexType>:$extents);
  let results = (outs Shape_ShapeType:$shape);

  let assemblyFormat = "$extents attr-dict `:` type($extents)";

  let hasFolder = 1;
}

def Shape_FromExtentTensorOp : Shape_Op<"from_extent_tensor", [Pure]> {
  let summary = "Creates a shape from a tensor of extents";
  let description = [{
    Creates a shape from a 1D integral tensor of extents. The rank of the
    resulting shape equals the number of elements in the tensor, and the
    extents match the values of the elements.
  }];

  let arguments = (ins 1DTensorOf<[Index]>:$input);
  let results = (outs Shape_ShapeType:$result);

  let assemblyFormat = "$input attr-dict `:` type($input)";
}

def Shape_IsBroadcastableOp : Shape_Op<"is_broadcastable", [Commutative]> {
  let summary = "Determines if 2+ shapes can be successfully broadcasted";
  let description = [{
    Given multiple input shapes or extent tensors, return a predicate
    specifying if they are broadcastable. This broadcastable follows the same
    logic as what shape.broadcast documents.

    Concretely, shape.is_broadcastable returning true implies that
    shape.broadcast will not give an error, and shape.cstr_broadcastable will
    not result in an assertion failure. Similarly, false implies an error or
    assertion failure.

    Example:
    ```mlir
    %true = shape.is_broadcastable [2,2], [3,1,2]
    %false = shape.is_broadcastable [2,2], [3,2]
    ```
  }];

  let arguments = (ins Variadic<Shape_ShapeOrExtentTensorType>:$shapes);
  let results = (outs I1:$result);

  let builders = [
  OpBuilder<(ins "::mlir::Value":$lhs, "::mlir::Value":$rhs),
    [{ build($_builder, $_state, ::llvm::ArrayRef({lhs, rhs})); }]>,
  ];

  let hasFolder = 1;
  let hasCanonicalizer = 1;

  let assemblyFormat = "$shapes attr-dict `:` type($shapes)";
}

def Shape_RankOp : Shape_Op<"rank",
    [Pure, InferTypeOpAdaptorWithIsCompatible]> {
  let summary = "Gets the rank of a shape";
  let description = [{
    Returns the rank of the shape or extent tensor, i.e. the number of extents.
  }];

  let arguments = (ins Shape_ShapeOrExtentTensorType:$shape);
  let results = (outs Shape_SizeOrIndexType:$rank);

  let assemblyFormat = "$shape attr-dict `:` type($shape) `->` type($rank)";

  let hasFolder = 1;
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

def Shape_ToExtentTensorOp : Shape_Op<"to_extent_tensor", [
    DeclareOpInterfaceMethods<CastOpInterface>, Pure
  ]> {
  let summary = "Creates a dimension tensor from a shape";
  let description = [{
    Converts a shape to a 1D integral tensor of extents. The number of elements
    in the tensor equals the rank of the shape, and the elements equal the
    extents of the shape.

    If the shape represents an error, this op's behavior is undefined.
  }];

  let arguments = (ins Shape_ShapeOrExtentTensorType:$input);
  let results = (outs IndexTensor:$result);

  let assemblyFormat = "$input attr-dict `:` type($input) `->` type($result)";

  let hasFolder = 1;
}

def Shape_DimOp : Shape_Op<"dim",
    [Pure, InferTypeOpAdaptorWithIsCompatible]> {
  let summary = "Gets the specified extent from the shape of a shaped input";
  let description = [{
    Gets the extent indexed by `dim` from the shape of the `value` operand. If
    the index is error or out-of-bound then it returns an invalid size if the
    return type carries error information else the behavior is undefined.

    This is a convenience op that performs the equivalent of getting the extent
    of a shape (e.g., `dim(x, i) == get_extent(shape_of(x), i)`).
  }];
  let arguments = (ins AnyShaped:$value,
                       Shape_SizeOrIndexType:$index);
  let results = (outs Shape_SizeOrIndexType:$extent);
  let assemblyFormat = "$value `,` $index attr-dict `:` type($value) `,`"
                       "type($index) `->` type($extent)";

  let builders = [
    // Builder that allows passing a constant dimension as a simple integer.
    OpBuilder<(ins "Value":$value, "int64_t":$index)>
  ];

  let extraClassDeclaration = [{
    /// Get the `index` value as integer if it is constant.
    std::optional<int64_t> getConstantIndex();
  }];

  let hasFolder = 1;
}

def Shape_GetExtentOp : Shape_Op<"get_extent",
    [Pure, InferTypeOpAdaptorWithIsCompatible]> {
  let summary = "Gets the specified extent from a shape or extent tensor";
  let description = [{
    Gets the extent indexed by `dim` from the `shape` operand. If the shape is
    an error then it returns an invalid size.
  }];
  let arguments = (ins Shape_ShapeOrExtentTensorType:$shape,
                       Shape_SizeOrIndexType:$dim);
  let results = (outs Shape_SizeOrIndexType:$extent);
  let assemblyFormat = "$shape `,` $dim attr-dict `:` type($shape) `,` "
                       "type($dim) `->` type($extent)";

  let builders = [
    // Builder that allows passing a constant dimension as a simple integer.
    OpBuilder<(ins "Value":$shape, "int64_t":$dim)>
  ];

  let extraClassDeclaration = [{
    /// Get the `dim` value as integer if it is constant.
    std::optional<int64_t> getConstantDim();
  }];

  let hasFolder = 1;
  let hasVerifier = 1;
}

def Shape_IndexToSizeOp : Shape_Op<"index_to_size", [Pure]> {
  let summary = "Converts a standard index to a shape size";
  let description = [{
    Converts a standard index to a `shape.size`. This operation and its
    inverse, `size_to_index`, facilitate index conversion between the standard
    and the shape dialect.

    The behavior is undefined for negative indices.
  }];

  let arguments = (ins Index:$arg);
  let results = (outs Shape_SizeType:$result);

  let assemblyFormat = "$arg attr-dict";

  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

def Shape_MaxOp : Shape_Op<"max",
    [Commutative, Pure, InferTypeOpAdaptorWithIsCompatible]> {
  let summary = "Elementwise maximum";
  let description = [{
    Computes the elementwise maximum of two sizes or shapes with equal ranks.
    If either operand is an error, then an error will be propagated to the
    result. If the input types mismatch or the ranks do not match, then the
    result is an error.
  }];

  let arguments = (ins Shape_ShapeOrSizeType:$lhs, Shape_ShapeOrSizeType:$rhs);
  let results = (outs Shape_ShapeOrSizeType:$result);

  let assemblyFormat = [{
    $lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `->` type($result)
  }];

  let hasFolder = 1;
}

def Shape_MeetOp : Shape_Op<"meet",
    [Commutative, InferTypeOpAdaptorWithIsCompatible]> {
  let summary = "Returns the least general shape or size of its operands";
  let description = [{
    An operation that computes the least general shape or dim of input operands.
    This effectively asserts that corresponding static dimensions are equal.
    The behavior is to match each element of the shape/size and propagate the
    most restrictive information, returning an invalid shape if there are
    contradictory requirements. E.g., using pseudo code

    ```
    shape.meet([*], [*]) -> [*]
    shape.meet([*], [1, ?]) -> [1, ?]
    shape.meet([1, 2], [1, ?]) -> [1, 2]
    shape.meet([*], [1, 2]) -> [1, 2]
    shape.meet([], []) -> []
    shape.meet([], [*]) -> []
    shape.meet([], [?, ?]) -> [invalid]
    shape.meet([1, ?], [2, ?, ?]) -> [invalid]
    ```

    `shape.meet` also allows specifying an optional error string, that may be
    used to return an error to the user upon mismatch of dimensions.

    ```mlir
    %c = shape.meet %a, %b, error="<reason>" : !shape.shape, !shape.shape -> !shape.shape
    ```
  }];

  let arguments = (ins
    Shape_AnyShapeOrSizeType:$arg0,
    Shape_AnyShapeOrSizeType:$arg1,
    OptionalAttr<StrAttr>:$error);
  let results = (outs Shape_AnyShapeOrSizeType:$result);

  let assemblyFormat = [{
    $arg0 `,` $arg1 (`,` `error` `=` $error^)? attr-dict `:`
      type($arg0) `,` type($arg1) `->` type($result)
  }];
}

def Shape_MinOp : Shape_Op<"min",
    [Commutative, Pure, InferTypeOpAdaptorWithIsCompatible]> {
  let summary = "Elementwise minimum";
  let description = [{
    Computes the elementwise minimum of two sizes or shapes with equal ranks.
    If either operand is an error, then an error will be propagated to the
    result. If the input types mismatch or the ranks do not match, then the
    result is an error.
  }];

  let arguments = (ins Shape_ShapeOrSizeType:$lhs, Shape_ShapeOrSizeType:$rhs);
  let results = (outs Shape_ShapeOrSizeType:$result);

  let assemblyFormat = [{
    $lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `->` type($result)
  }];

  let hasFolder = 1;
}

def Shape_MulOp : Shape_Op<"mul",
    [Commutative, Pure, InferTypeOpAdaptorWithIsCompatible]> {
  let summary = "Multiplication of sizes and indices";
  let description = [{
    Multiplies two sizes or indices. If either operand is an error it will be
    propagated to the result. The operands can be of type `size` or `index`. If
    at least one of the operands can hold an error, i.e. if it is of type
    `size`, the result must be of type `size`. If error propagation is not
    possible because both operands are of type `index` then the result may be
    of type `size` or `index`.
  }];

  let arguments = (ins Shape_SizeOrIndexType:$lhs, Shape_SizeOrIndexType:$rhs);
  let results = (outs Shape_SizeOrIndexType:$result);

  let assemblyFormat = [{
    $lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `->` type($result)
  }];

  let hasFolder = 1;
  let hasVerifier = 1;
}

def Shape_NumElementsOp : Shape_Op<"num_elements",
    [Pure, InferTypeOpAdaptorWithIsCompatible]> {
  let summary = "Returns the number of elements for a given shape";
  let description = [{
    Returns the number of elements for a given shape which is the product of
    its extents. If the argument is of type `shape` then the result will be of
    type `size` and potential errors will be propagated. Otherwise, if the
    argument is and extent tensor `tensor<?xindex>` then the result will be of
    type `index`.
  }];

  let arguments = (ins Shape_ShapeOrExtentTensorType:$shape);
  let results = (outs Shape_SizeOrIndexType:$result);

  let assemblyFormat = "$shape attr-dict `:` type($shape) `->` type($result)";

  let hasFolder = 1;
  let hasVerifier = 1;
}

def Shape_ReduceOp : Shape_Op<"reduce",
    [SingleBlockImplicitTerminator<"YieldOp">]> {
  let summary = "Returns an expression reduced over a shape or extent tensor";
  let description = [{
    An operation that takes as input a shape or extent tensor, and a number of
    initial values. This operation has a region that is applied repeatedly for
    every extent of the input. Starting with the initial values, the individual
    extents are then aggregated as defined by the associated region.

    Conceptually this op performs the following reduction:

    ```
    res[] = init;
    for (int i = 0, i < shape.rank(); i++) {
      res = reduce(i, shape[i], res[0], ..., res[n]);
    }
    ```

    Where `reduce` represents the region attached and the result of the reduce
    op is the last computed output of the reduce region. As an example, the
    number of elements can be computed as follows:

    ```mlir
    func.func @reduce(%shape : !shape.shape, %init : !shape.size) ->
        !shape.size {
      %num_elements = shape.reduce(%shape, %init) -> !shape.size  {
        ^bb0(%index: index, %dim: !shape.size, %acc: !shape.size):
          %updated_acc = "shape.mul"(%acc, %dim) :
            (!shape.size, !shape.size) -> !shape.size
          shape.yield %updated_acc : !shape.size
      }
      return %num_elements : !shape.size
    }
    ```
  }];

  let arguments = (ins Shape_ShapeOrExtentTensorType:$shape,
                       Variadic<AnyType>:$initVals);
  let results = (outs Variadic<AnyType>:$result);
  let regions = (region SizedRegion<1>:$region);

  let builders = [OpBuilder<(ins "Value":$shape, "ValueRange":$initVals)>];

  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

def Shape_ShapeOfOp : Shape_Op<"shape_of",
    [Pure, InferTypeOpAdaptorWithIsCompatible]> {
  let summary = "Returns shape of a value or shaped type operand";

  let description = [{
    The operation takes a value or a shaped operand as an argument and it
    returns a shape or extent tensor.
  }];

  let arguments = (ins AnyTypeOf<[AnyShaped, Shape_ValueShapeType]>:$arg);
  let results = (outs Shape_ShapeOrExtentTensorType:$result);

  let assemblyFormat = "$arg attr-dict `:` type($arg) `->` type($result)";

  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

def Shape_ValueOfOp : Shape_Op<"value_of", [Pure]> {
  let summary = "Returns value of a !shape.value_shape operand";

   let description = [{
    The operation takes !shape.value_shape, a.k.a. (value, shape) tuple as an
    argument, and returns its value. The behavior is undefined for unknown and
    invalid arguments.
  }];

  let arguments = (ins Shape_ValueShapeType:$arg);
  let results = (outs AnyShaped:$result);

  let assemblyFormat = "$arg attr-dict `:` type($result)";
}

def Shape_SizeToIndexOp : Shape_Op<"size_to_index", [
    DeclareOpInterfaceMethods<CastOpInterface>, Pure
  ]> {
  let summary = "Casts between index types of the shape and standard dialect";
  let description = [{
    Converts a `shape.size` to a standard index. This operation and its
    inverse, `index_to_size`, facilitate index conversion between the standard
    and the shape dialect. The behavior is undefined for unknown and invalid
    arguments.
  }];

  let arguments = (ins Shape_SizeOrIndexType:$arg);
  let results = (outs Index:$result);

  let assemblyFormat = "$arg attr-dict `:` type($arg)";

  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

def Shape_ValueAsShapeOp : Shape_Op<"value_as_shape", [Pure]> {
  let summary = "Returns value as a shape";

  let description = [{
    The operations takes a ValueShape and returns a Shape corresponding to the
    value.  If the input value cannot be shape (e.g., not a 1D tensor of
    integral value representing sizes) then this propagages the error shape.
    E.g.,

    ```mlir
    // The following
    %0 = arith.constant dense<[1,2]> : tensor<2xi32>
    %shape = shape.value_as_shape %0 : tensor<2xi32> -> !shape.shape
    // is equivalent to
    %shape' = shape.const_shape [1, 2] : !shape.shape
    ```

    This operation is the complement of `shape_of` wrt ValueShape values.
  }];

  let arguments = (ins AnyTypeOf<[1DTensorOf<[AnyInteger, Index]>,
                       Shape_ValueShapeType]>:$arg);
  let results = (outs Shape_ShapeOrExtentTensorType:$result);

  let assemblyFormat = "$arg attr-dict `:` type($arg) `->` type($result)";
}

def Shape_WithOp : Shape_Op<"with_shape", [Pure]> {
  let summary = "Returns ValueShape with given shape";
  let description = [{
    Returns ValueShape with the shape updated to match the shape operand. That
    is a new ValueShape tuple is created with value equal to `operand`'s
    value and shape equal to `shape`. If the ValueShape and given `shape` are
    non-conformant, then the returned ValueShape will represent an error of
    this mismatch. Similarly if either inputs are in an error state, then an
    error is propagated.

    Usage:
      %0 = shape.with_shape %1, %2 : tensor<...>, !shape.shape

    This is used, for example, where one combines shape function calculations
    and/or call one shape function from another. E.g.,

    ```mlir
    func.func @shape_foobah(%a: !shape.value_shape,
                       %b: !shape.value_shape,
                       %c: !shape.value_shape) -> !shape.shape {
      %0 = call @shape_foo(%a, %b) :
        (!shape.value_shape, !shape.value_shape) -> !shape.shape
      %1 = shape.with_shape %b, %0 : !shape.value_shape, !shape.shape
      %2 = call @shape_bah(%c, %1) :
        (!shape.value_shape, !shape.value_shape) -> !shape.shape
      return %2 : !shape.shape
    }
    ```

    This op need not be a refinement of the shape. In non-error cases the input
    ValueShape's value and shape are conformant and so too for the output, but
    the result may be less specified than `operand`'s shape as `shape` is
    merely used to construct the new ValueShape. If join behavior is desired
    then a join op should be used.
  }];

  let arguments = (ins AnyTypeOf<[AnyShaped, Shape_ValueShapeType]>:$operand,
                       Shape_ShapeOrExtentTensorType:$shape);
  let results = (outs Shape_ValueShapeType:$result);

  let assemblyFormat = "operands attr-dict `:` type($operand) `,` type($shape)";
}

def Shape_YieldOp : Shape_Op<"yield",
    [HasParent<"ReduceOp, FunctionLibraryOp">,
     Pure,
     ReturnLike,
     Terminator]> {
  let summary = "Returns the value to parent op";

  let arguments = (ins Variadic<AnyType>:$operands);

  let builders = [OpBuilder<(ins),
    [{ build($_builder, $_state, std::nullopt); }]>
  ];

  let assemblyFormat = "attr-dict ($operands^ `:` type($operands))?";
  let hasVerifier = 1;
}

// TODO: Add Ops: if_static, if_ranked

// For testing usage.
def Shape_DebugPrintOp : Shape_Op<"debug_print", []> {
  let summary = "Prints the input shape or size";
  let description = [{
    Prints the input dim or shape and passes through input.

    Note: This is intended for testing and debugging only.
  }];

  let arguments = (ins Shape_ShapeOrSizeType:$input);
  let results =  (outs Shape_ShapeOrSizeType:$output);
}

def Shape_SplitAtOp : Shape_Op<"split_at", [Pure]> {
  let summary = "Splits a shape at a given index";
  let description = [{
    Splits a shape at a given dimension `index`, returning two shapes. If
    `index` is negative, it is treated as indexing from the back of the shape.
    This negative-handling behavior is important when handling unranked shapes,
    where the positive index is not necessarily knowable due to a dynamic
    number of leading dimensions. If the result is in extent tensor form out of
    bounds indices result in undefined behavior.

    Examples:
    - split_at([4,5,6], index=0) -> [], [4,5,6]
    - split_at([4,5,6], index=1) -> [4], [5,6]
    - split_at([4,5,6], index=2) -> [4,5], [6]
    - split_at([4,5,6], index=3) -> [4,5,6], []
    - split_at([4,5,6], index=4) -> error
    - split_at([4,5,6], index=-1) -> [4,5], [6]
    - split_at([4,5,6], index=-2) -> [4], [5,6]
    - split_at([4,5,6], index=-3) -> [], [4,5,6]
    - split_at([4,5,6], index=-4) -> error

    Requires:
    - `index` is in the range [-rank(operand),rank(operand)]
  }];

  let arguments = (ins Shape_ShapeOrExtentTensorType:$operand,
                       Shape_SizeOrIndexType:$index);
  let results = (outs Shape_ShapeOrExtentTensorType:$head,
                      Shape_ShapeOrExtentTensorType:$tail);
  let hasFolder = 1;
}

def Shape_ConcatOp : Shape_Op<"concat", [Pure]> {
  let summary = "Concatenates two shapes";
  let description = [{
    Creates a shape whose dimensions consist of first the dimensions from `lhs`
    followed by the dimensions of `rhs`.

    Example:
    concat([2,3], [4,5]) -> [2,3,4,5]
    concat([], []) -> []
    concat([], [4,5,6]) -> [4,5,6]
  }];

  let arguments = (ins Shape_ShapeOrExtentTensorType:$lhs,
                       Shape_ShapeOrExtentTensorType:$rhs);
  let results = (outs Shape_ShapeOrExtentTensorType:$result);

  let assemblyFormat = [{
    $lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `->` type($result)
  }];

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Shape constraint related ops.
//===----------------------------------------------------------------------===//

// TODO: Move the code below and witnesses to a different file.
def Shape_AnyOp : Shape_Op<"any", [Commutative,
                                   Pure]> {
  let summary = "Return any combination of the input shapes";
  let description = [{
    This operation takes multiple input shapes or extent tensors and returns
    some combination of their dimensions. This can be best seen with examples
    below.

    The result is undefined, but still side-effect free, in cases where the
    inputs have differing ranks or differ in extents of shared dimensions.

    Example:
    ```mlir
    %s0 = shape.any [2,?], [?,3] // [2,3]
    %s1 = shape.any [?,?], [1,2] // [1,2]
    ```
  }];

  let arguments = (ins Variadic<Shape_ShapeOrExtentTensorType>:$inputs);
  let results = (outs Shape_ShapeOrExtentTensorType:$result);

  let assemblyFormat = "$inputs attr-dict `:` type($inputs) `->` type($result)";

  let hasFolder = 1;
}

def Shape_AssumingAllOp : Shape_Op<"assuming_all", [Commutative, Pure]> {
  let summary = "Return a logical AND of all witnesses";
  let description = [{
    Used to simplify constraints as any single failing precondition is enough
    to prevent execution.

    "assuming" operations represent an execution order restriction to the
    compiler, information for dependent code to rely on (by assuming), and
    nothing else. They should not exist after a program is fully lowered and
    ready to execute.

    Example:
    ```mlir
    %w0 = shape.cstr_broadcastable [2,2], [3,1,2] // Passing
    %w1 = shape.cstr_broadcastable [2,2], [3,2] // Failure
    %w2 = shape.cstr_eq [1,2], [1,2], [1,2] // Passing
    %wf = shape.assuming_all %w0, %w1 // Failure
    %wt = shape.assuming_all %w0, %w2 // Passing
    ```
  }];

  let arguments = (ins Variadic<Shape_WitnessType>:$inputs);
  let results = (outs Shape_WitnessType:$result);

  let assemblyFormat = "$inputs attr-dict";

  let hasFolder = 1;
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

def Shape_AssumingOp : Shape_Op<"assuming", [
    SingleBlockImplicitTerminator<"AssumingYieldOp">,
    DeclareOpInterfaceMethods<RegionBranchOpInterface>,
    RecursiveMemoryEffects]> {
  let summary = "Execute the region";
  let description = [{
    Executes the region assuming all witnesses are true.

    "assuming" operations represent an execution order restriction to the
    compiler, information for dependent code to rely on (by assuming), and
    nothing else. They should not exist after a program is fully lowered and
    ready to execute.
  }];
  let arguments = (ins Shape_WitnessType:$witness);
  let regions = (region SizedRegion<1>:$doRegion);
  let results = (outs Variadic<AnyType>:$results);

  let extraClassDeclaration = [{
    // Inline the region into the region containing the AssumingOp and delete
    // the AssumingOp.
    //
    // This does no checks on the inputs to the AssumingOp.
    static void inlineRegionIntoParent(AssumingOp &op,
      PatternRewriter &rewriter);
  }];

  let builders = [
    OpBuilder<(ins "Value":$witness,
        CArg<"function_ref<SmallVector<Value, 2>(OpBuilder &, Location)>">)>
  ];

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
}

def Shape_AssumingYieldOp : Shape_Op<"assuming_yield",
       [Pure, ReturnLike, Terminator, HasParent<"AssumingOp">]> {
  let summary = "Yield operation";
  let description = [{
    This yield operation represents a return operation within the
    `shape.assuming` operation region. The operation takes variable number of
    operands and produces no results. The operand number and types must match
    the number and types of parent `shape.assuming` results.
  }];

  let arguments = (ins Variadic<AnyType>:$operands);

  let builders = [
    OpBuilder<(ins), [{ /* nothing to do */ }]>,
  ];

  let assemblyFormat = "attr-dict ($operands^ `:` type($operands))?";
}

def Shape_CstrBroadcastableOp : Shape_Op<"cstr_broadcastable", [Commutative]> {
  let summary = "Determines if 2+ shapes can be successfully broadcasted";
  let description = [{
    Given input shapes or extent tensors, return a witness specifying if they
    are broadcastable. This broadcastable follows the same logic as what
    shape.broadcast documents.

    "cstr" operations represent runtime assertions.

    Example:
    ```mlir
    %w0 = shape.cstr_broadcastable [2,2], [3,1,2] // Passing
    %w1 = shape.cstr_broadcastable [2,2], [3,2] // Failure
    ```
  }];

  let arguments = (ins Variadic<Shape_ShapeOrExtentTensorType>:$shapes);
  let results = (outs Shape_WitnessType:$result);

  let assemblyFormat = "$shapes attr-dict `:` type($shapes)";

  let builders = [
  OpBuilder<(ins "::mlir::Value":$lhs, "::mlir::Value":$rhs),
    [{ build($_builder, $_state, ::llvm::ArrayRef({lhs, rhs})); }]>,
  ];

  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

def Shape_CstrEqOp : Shape_Op<"cstr_eq", [Commutative]> {
  let summary = "Determines if all input shapes are equal";
  let description = [{
    Given 1 or more input shapes, determine if all shapes are the exact same.

    "cstr" operations represent runtime assertions.

    Example:
    ```mlir
    %w0 = shape.cstr_eq [1,2], [1,2], [1,2] // Passing
    %w1 = shape.cstr_eq [2,2], [1,2] // Failure
    ```
  }];
  let arguments = (ins Variadic<Shape_ShapeOrExtentTensorType>:$shapes);
  let results = (outs Shape_WitnessType:$result);

  let assemblyFormat = "$shapes attr-dict `:` type($shapes)";

  let hasCanonicalizer = 1;
  let hasFolder = 1;
}

def Shape_ConstWitnessOp : Shape_Op<"const_witness", [ConstantLike, Pure]> {
  let summary = "An operation that returns a statically known witness value";
  let description = [{
  This operation represents a statically known witness result. This can be
  often used to canonicalize/fold constraint and assuming code that will always
  pass.

  ```mlir
  %0 = shape.const_shape [1,2,3]
  %1 = shape.const_shape [1,2,3]
  %w0 = shape.cstr_eq(%0, %1) // Can be folded to "const_witness true"
  %w1 = shape.const_witness true
  %w2 = shape.assuming_all(%w0, %w2) // Can be folded to "const_witness true"
  ```
  }];
  let arguments = (ins BoolAttr:$passing);
  let results = (outs Shape_WitnessType:$result);

  let assemblyFormat = "$passing attr-dict";

  let hasFolder = 1;
}

def Shape_CstrRequireOp : Shape_Op<"cstr_require", []> {
  let summary = "Represents a runtime assertion that an i1 is `true`";
  let description = [{
    Represents a runtime assertion that an i1 is true. It returns a
    !shape.witness to order this assertion.

    For simplicity, prefer using other cstr_* ops if they are available for a
    given constraint.

    Example:
    ```mlir
    %bool = ...
    %w0 = shape.cstr_require %bool, "msg" // Passing if `%bool` is true.
    ```

    Since this op can be used to express many different possible assertions
    (depending on whatever computation calculated `pred`), the `msg`
    should clarify the nature of the assertion for users.
  }];
  let arguments = (ins I1:$pred, StrAttr:$msg);
  let results = (outs Shape_WitnessType:$result);

  let assemblyFormat = "$pred `,` $msg attr-dict";

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Shape collection ops.
//===----------------------------------------------------------------------===//

def Shape_FunctionLibraryOp : Shape_Op<"function_library",
    [AffineScope, IsolatedFromAbove, NoRegionArguments, SymbolTable, Symbol,
     NoTerminator, OpAsmOpInterface, SingleBlock]> {
  let summary = "Represents shape functions and corresponding ops";
  let description = [{
    Represents a list of shape functions and the ops whose shape transfer
    functions they represent.

    Example:

    ```mlir
    shape.function_library {
      func @same_result_shape(%arg: !shape.value_shape) -> !shape.shape {
        %0 = shape_of %arg : !shape.value_shape -> !shape.shape
        return %0 : !shape.shape
      }
    } mapping {
      std.atan = @same_result_shape
    }
    ```
  }];

  let arguments = (ins SymbolNameAttr:$sym_name,
                       OptionalAttr<StrAttr>:$sym_visibility,
                       DictionaryAttr:$mapping);
  let regions = (region AnyRegion:$body);

  let extraClassDeclaration = [{
    /// Returns an associated shape function for an operation if defined.
    FuncOp getShapeFunction(Operation *op);

    //===------------------------------------------------------------------===//
    // OpAsmOpInterface
    //===------------------------------------------------------------------===//

    // This will filter the `shape.` prefix in front of operations inside the
    // func body.
    static StringRef getDefaultDialect() { return "shape";}
  }];

  let builders = [OpBuilder<(ins "StringRef":$name)>];
  let skipDefaultBuilders = 1;
  let hasCustomAssemblyFormat = 1;
}

def Shape_FuncOp : Shape_Op<"func",
    [AffineScope, AutomaticAllocationScope,
     FunctionOpInterface, IsolatedFromAbove, OpAsmOpInterface]> {
  let summary = "Shape function";
  let description = [{
    An operation with a name containing a single `SSACFG` region which
    represents a shape transfer function or helper function for shape transfer
    function.
  }];

  let arguments = (ins SymbolNameAttr:$sym_name,
                       TypeAttrOf<FunctionType>:$function_type,
                       OptionalAttr<DictArrayAttr>:$arg_attrs,
                       OptionalAttr<DictArrayAttr>:$res_attrs,
                       OptionalAttr<StrAttr>:$sym_visibility);
  let regions = (region AnyRegion:$body);

  let builders = [OpBuilder<(ins
    "StringRef":$name, "FunctionType":$type,
    CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs,
    CArg<"ArrayRef<DictionaryAttr>", "{}">:$argAttrs)
  >];

  let extraClassDeclaration = [{
    static FuncOp create(Location location, StringRef name, FunctionType type,
                         ArrayRef<NamedAttribute> attrs = {});
    static FuncOp create(Location location, StringRef name, FunctionType type,
                         Operation::dialect_attr_range attrs);
    static FuncOp create(Location location, StringRef name, FunctionType type,
                         ArrayRef<NamedAttribute> attrs,
                         ArrayRef<DictionaryAttr> argAttrs);
    //===------------------------------------------------------------------===//
    // FunctionOpInterface Methods
    //===------------------------------------------------------------------===//

    /// Returns the region on the current operation that is callable. This may
    /// return null in the case of an external callable object, e.g. an external
    /// function.
    ::mlir::Region *getCallableRegion() {
      return isExternal() ? nullptr : &getBody();
    }

    /// Returns the argument types of this function.
    ArrayRef<Type> getArgumentTypes() { return getFunctionType().getInputs(); }

    /// Returns the result types of this function.
    ArrayRef<Type> getResultTypes() { return getFunctionType().getResults(); }

    //===------------------------------------------------------------------===//
    // OpAsmOpInterface
    //===------------------------------------------------------------------===//

    // This will filter the `shape.` prefix in front of operations inside the
    // func body.
    static StringRef getDefaultDialect() { return "shape";}

    //===------------------------------------------------------------------===//
    // SymbolOpInterface Methods
    //===------------------------------------------------------------------===//

    bool isDeclaration() { return isExternal(); }
  }];
  let hasCustomAssemblyFormat = 1;
}

def Shape_ReturnOp : Shape_Op<"return",
    [Pure, HasParent<"FuncOp">, ReturnLike, Terminator]> {
  let summary = "Shape function return operation";
  let description = [{
    The `shape.return` operation represents a return operation within a
    function.  The operation takes variable number of operands and produces no
    results.
  }];

  let arguments = (ins Variadic<AnyType>:$operands);

  let assemblyFormat = "attr-dict ($operands^ `:` type($operands))?";

  // TODO: Tighten verification.
}

#endif // SHAPE_OPS


//===-- TosaOps.td - TOSA dialect operation definitions ----*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This file defines the operation set for the TOSA dialect as defined in
// the TOSA specfication (https://developer.mlplatform.org/w/tosa/).
//
//===----------------------------------------------------------------------===//

#ifndef TOSA_OPS
#define TOSA_OPS

include "mlir/IR/OpBase.td"

include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/LoopLikeInterface.td"
include "mlir/Dialect/Tosa/IR/TosaInterfaces.td"

include "mlir/Dialect/Tosa/IR/TosaTypesBase.td"
include "mlir/Dialect/Tosa/IR/TosaOpBase.td"

//===----------------------------------------------------------------------===//
// TOSA Spec Section 2.2
// Operator Class: Tensor Data Engine Operators.
//===----------------------------------------------------------------------===//

//===----------------------------------------------------------------------===//
// Operator: argmax
//===----------------------------------------------------------------------===//
def Tosa_ArgMaxOp : Tosa_InferShapedTypeOp<"argmax"> {
  let summary = "Perform argmax on the input.";

  let description = [{
    This returns the index with the largest value across the given axis of the
    input tensor.
  }];

  let arguments = (ins
    Tosa_Tensor: $input,
    I32Attr: $axis
  );

  let results = (outs
    Tosa_Tensor: $output
  );

  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Accumulator types.
//===----------------------------------------------------------------------===//

def Tosa_AccType : AnyTypeOf<[I<32>, SI<32>, F16, F32]>;

//===----------------------------------------------------------------------===//
// Operator: avg_pool2d
//===----------------------------------------------------------------------===//
def Tosa_AvgPool2dOp : Tosa_InferShapedTypeOp<"avg_pool2d"> {
  let summary = "Performs average pooling on the input.";

  let description = [{
    This performs an average pooling over the given input tensor. A sliding
    window of size given by <kernel size> is passed over the input tensor, with
    the mean value being placed in the output tensor.
  }];

  let arguments = (ins
    Tosa_Tensor4D:$input,
    Tosa_IntArrayAttr2:$kernel,
    Tosa_IntArrayAttr2:$stride,
    Tosa_IntArrayAttr4:$pad,
    TypeAttrOf<Tosa_AccType>:$acc_type,
    OptionalAttr<Tosa_UnaryOpQuantizationAttr>:$quantization_info
  );

  let results = (outs
    Tosa_Tensor4D:$output
  );

  let builders = [Tosa_AvgPool2dOpQuantInfoBuilder];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Operator: conv2d
//===----------------------------------------------------------------------===//
def Tosa_Conv2DOp : Tosa_InferShapedTypeOp<"conv2d"> {
  let summary = "2D Convolution Operator";

  let description = [{
    Performs a 2D convolution over the given tensor input, using the weight
    tensor.
  }];

  let arguments = (ins
    Tosa_Tensor4D:$input,
    TosaTensorRankOf<[Tosa_Weight], [4]>:$weight,
    Tosa_Tensor1D:$bias,
    Tosa_IntArrayAttr4:$pad,
    Tosa_IntArrayAttr2:$stride,
    Tosa_IntArrayAttr2:$dilation,
    OptionalAttr<Tosa_ConvOpQuantizationAttr>:$quantization_info,
    DefaultValuedOptionalAttr<BoolAttr, "false">:$local_bound
  );

  let results = (outs
    Tosa_Tensor4D:$output
  );

  let builders = [Tosa_ConvOpQuantInfoBuilder];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Operator: conv3d
//===----------------------------------------------------------------------===//
def Tosa_Conv3DOp : Tosa_InferShapedTypeOp<"conv3d"> {
  let summary = "3D Convolution operator";

  let description = [{
    Performs a 3D convolution over the given input tensor.
  }];

  let arguments = (ins
    Tosa_Tensor5D:$input,
    TosaTensorRankOf<[Tosa_Weight], [5]>:$weight,
    Tosa_Tensor1D:$bias,
    Tosa_IntArrayAttr6:$pad,
    Tosa_IntArrayAttr3:$stride,
    Tosa_IntArrayAttr3:$dilation,
    OptionalAttr<Tosa_ConvOpQuantizationAttr>:$quantization_info,
    DefaultValuedOptionalAttr<BoolAttr, "false">:$local_bound
  );

  let results = (outs
    Tosa_Tensor5D:$output
  );

  let builders = [Tosa_ConvOpQuantInfoBuilder];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Operator: depthwise_conv2d
//===----------------------------------------------------------------------===//
def Tosa_DepthwiseConv2DOp : Tosa_InferShapedTypeOp<"depthwise_conv2d"> {
  let summary = "Depthwise 2D Convolution operator";

  let description = [{
    Performs 2D convolutions separately over each channel of the given tensor
    input, using the weight tensor.
  }];

  let arguments = (ins
    Tosa_Tensor4D:$input,
    TosaTensorRankOf<[Tosa_Weight], [4]>:$weight,
    Tosa_Tensor1D:$bias,
    Tosa_IntArrayAttr4:$pad,
    Tosa_IntArrayAttr2:$stride,
    Tosa_IntArrayAttr2:$dilation,
    OptionalAttr<Tosa_ConvOpQuantizationAttr>:$quantization_info,
    DefaultValuedOptionalAttr<BoolAttr, "false">:$local_bound
  );

  let results = (outs
    Tosa_Tensor4D:$output
  );

  let builders = [Tosa_ConvOpQuantInfoBuilder];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Operator: fft2d
//===----------------------------------------------------------------------===//
def Tosa_FFT2dOp : Tosa_InferShapedTypeOp<"fft2d"> {
  let summary = "Performs FFT2D operation on the input.";

  let description = [{
    Performs a batched complex 2D Fast Fourier Transform over the input. The
    complex input values are constructed from the corresponding values in the
    input_real and input_imag tensors. The resulting values in the output are
    split into the output_real and output_imag tensors. No normalization is
    applied on either the forward or inverse versions of the operation.

    Example:

    ```mlir
     %out_real, %out_imag = tosa.fft2d %in_real, %in_imag : (tensor<8x9xf32>, tensor<8x9xf32>) -> (tensor<8x9xf32>, tensor<8x9xf32>)
    ```
  }];

  let arguments = (ins
    Tosa_Tensor3D:$input_real,
    Tosa_Tensor3D:$input_imag,

    BoolAttr:$inverse,
    DefaultValuedOptionalAttr<BoolAttr, "false">:$local_bound
  );

  let results = (outs
    Tosa_Tensor3D:$output_real,
    Tosa_Tensor3D:$output_imag
  );

  let assemblyFormat = [{
    $input_real `,` $input_imag attr-dict `:` `(` type($input_real) `,`
    type($input_imag) `)` `->` `(` type($output_real) `,` type($output_imag) `)`
  }];
}

//===----------------------------------------------------------------------===//
// Operator: fully_connected
//===----------------------------------------------------------------------===//
def Tosa_FullyConnectedOp : Tosa_InferShapedTypeOp<"fully_connected"> {
  let summary = "Fully Connected operator";

  let description = [{
    Performs a fully connected network.
  }];

  let arguments = (ins
    Tosa_Tensor2D:$input,
    TosaTensorRankOf<[Tosa_Weight], [2]>:$weight,
    Tosa_Tensor1D:$bias,
    OptionalAttr<Tosa_ConvOpQuantizationAttr>:$quantization_info
  );

  let results = (outs
    Tosa_Tensor2D:$output
  );

  let builders = [Tosa_FCOpQuantInfoBuilder];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Operator: matmul
//===----------------------------------------------------------------------===//
def Tosa_MatMulOp : Tosa_InferShapedTypeOp<"matmul"> {
  let summary = "Matrix multiplication with bias";

  let description = [{
    Performs a two dimensional matrix multiplication. This allows both inputs to
    be activations, rather than reserving weights as an attribute in the
    FULLY_CONNECTED operator.
  }];

  let arguments = (ins
    Tosa_Tensor3D:$a,
    Tosa_Tensor3D:$b,
    OptionalAttr<Tosa_MatMulOpQuantizationAttr>:$quantization_info
  );

  let results = (outs
    Tosa_Tensor3D:$c
  );

  let builders = [Tosa_MatMulOpQuantInfoBuilder];
}

//===----------------------------------------------------------------------===//
// Operator: max_pool2d
//===----------------------------------------------------------------------===//
def Tosa_MaxPool2dOp : Tosa_InferShapedTypeOp<"max_pool2d"> {
  let summary = "Performs max pooling on the input.";

  let description = [{
    This performs a max pooling over the given input tensor. A sliding window of
    size given by <kernel size> is passed over the input tensor, with the
    maximum value being placed in the
    output tensor.
  }];

  let arguments = (ins
    Tosa_Tensor4D:$input,

    Tosa_IntArrayAttr2:$kernel,
    Tosa_IntArrayAttr2:$stride,
    Tosa_IntArrayAttr4:$pad
  );

  let results = (outs
    Tosa_Tensor4D:$output
  );

  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// Operator: rfft2d
//===----------------------------------------------------------------------===//
def Tosa_RFFT2dOp : Tosa_InferShapedTypeOp<"rfft2d"> {
  let summary = "Performs RFFT2D operation on the input.";

  let description = [{
    Performs a batched 2D real-valued Fast Fourier Transform over the input where
    the input tensor consists of real values producing complex valued output. The
    complex output values will be split into the output_real and output_imag
    tensor arguments. RFFT2D takes advantage of Hermitian symmetry to only
    calculate the first half of the final output axis. Imaginary values with
    locations (0,0), (0,W/2), (H/2,0) and (H/2,W/2) are zero.

    Example:

    ```mlir
     %real, %imag = tosa.rfft2d %in : (tensor<8x16xf32>) -> (tensor<8x9xf32>, tensor<8x9xf32>)
    ```
  }];

  let arguments = (ins
    Tosa_Tensor3D:$input,
    DefaultValuedOptionalAttr<BoolAttr, "false">:$local_bound
  );

  let results = (outs
    Tosa_Tensor3D:$output_real,
    Tosa_Tensor3D:$output_imag
  );

  let assemblyFormat = [{
    $input attr-dict `:` `(` type($input) `)` `->` `(` type($output_real) `,` type($output_imag) `)`
  }];
}

//===----------------------------------------------------------------------===//
// Operator: transpose_conv2d
//===----------------------------------------------------------------------===//
def Tosa_TransposeConv2DOp : Tosa_InferShapedTypeOp<"transpose_conv2d"> {
  let summary = "Transpose 2D Convolution operator.";

  let description = [{
    Performs a 2D transposed convolution over the given tensor input, using the
    weights tensor.
  }];

  let arguments = (ins
    Tosa_Tensor4D:$input,
    TosaTensorRankOf<[Tosa_Weight], [4]>:$filter,
    Tosa_Tensor1D:$bias,
    Tosa_IntArrayAttr4:$out_pad,
    Tosa_IntArrayAttr2:$stride,
    Tosa_IntArrayAttr4:$out_shape,
    OptionalAttr<Tosa_ConvOpQuantizationAttr>:$quantization_info,
    DefaultValuedOptionalAttr<BoolAttr, "false">:$local_bound
  );

  let results = (outs
    Tosa_Tensor4D:$output
  );

  let builders = [Tosa_TransConvOpQuantInfoBuilder];
}

//===----------------------------------------------------------------------===//
// TOSA Spec Section 2.3
// Operator Class: Activation Functions.
//===----------------------------------------------------------------------===//

//===----------------------------------------------------------------------===//
// Operator: clamp
//===----------------------------------------------------------------------===//
def Tosa_ClampOp : Tosa_ElementwiseUnaryOp<"clamp"> {
  let summary = "Computes clamp(features, min, max).";

  let description = [{
    Clamp to an arbitrary minimum and maximum value.
    Maximum and minimum values are specified as values in the range of the
    input type.
    No zero point subtraction is done to the values, thus to clamp to the zero
    point value, the zero point itself should be supplied as the minimum value.
  }];

  let arguments = (ins
    Tosa_Tensor:$input,
    I64Attr:$min_int,
    I64Attr:$max_int,
    Tosa_FloatAttr:$min_fp,
    Tosa_FloatAttr:$max_fp
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Operator: sigmoid
//===----------------------------------------------------------------------===//
def Tosa_SigmoidOp : Tosa_ElementwiseUnaryOp<"sigmoid"> {
  let summary = "Computes elementwise sigmoid of input.";

  let description = [{
    Sigmoid function: output = 1 / (1 + exp(-input))
    For quantized integer data types, the TABLE operator should be used instead
    with the following definition.  The sigmoid table has 513 entries each of
    16-bit precision and covering the input range -16.0 to +16.0
    in steps of 1/16.
  }];

  let arguments = (ins
    Tosa_Tensor:$input
  );

  let results = (outs
    Tosa_Tensor:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: tanh
//===----------------------------------------------------------------------===//
def Tosa_TanhOp : Tosa_ElementwiseUnaryOp<"tanh"> {
  let summary = "Computes elementwise hyperbolic tangent of input";

  let description = [{
    Parameterized hyperbolic tangent.
    For quantized integer data types, the TABLE operator should be used instead
    with the following definition.  The tanh_table has 513 entries each of
    16-bit precision and covering the input range -8.0 to +8.0 in steps of 1/32.
  }];

  let arguments = (ins
    Tosa_Tensor:$input
  );

  let results = (outs
    Tosa_Tensor:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: erf
//===----------------------------------------------------------------------===//
def Tosa_ErfOp : Tosa_ElementwiseUnaryOp<"erf"> {
  let summary = "Computes gauss error function of input";

  let description = [{
    Gauss error function: $ erf(x) = \frac{2}{\sqrt(\pi)} \int_{0}^{x} e^{-t^2} \,dt $
    For quantized integer data types, the TABLE operator should be used instead
    with the following definition.  The erf_table has 513 entries each of
    16-bit/8-bit precision and covering the input range -4.0 to +4.0 in steps of 1/64.
  }];

  let arguments = (ins
    Tosa_Tensor:$input
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let assemblyFormat = "operands attr-dict `:` functional-type(operands, results)";
}

//===----------------------------------------------------------------------===//
// TOSA Spec Section 2.4
// Operator Class: Elementwise unary/binary/ternary operators.
// Operator Subclass: Elementwise binary ops.
//===----------------------------------------------------------------------===//

//===----------------------------------------------------------------------===//
// Operator: add
//===----------------------------------------------------------------------===//
def Tosa_AddOp : Tosa_ElementwiseOp<"add", [
    Commutative,
    SameOperandsAndResultElementType]> {
  let summary = "Elementwise addition operator";

  let description = [{
    Elementwise addition of input1 and input2. Axis of size 1 will be broadcast,
    as necessary.

    Example:

    ```mlir
    // Elementwise addition.
    %out = tosa.add %in1, %in2 : tensor<12x6xf32>, tensor<12x6xf32> -> tensor<12x6xf32>

    // Elementwise addition with broadcasting.
    %out = tosa.add %in1, %in2 : tensor<12x6xsi32>, tensor<1x1xsi32> -> tensor<12x6xsi32>
    ```
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    Tosa_Tensor:$input2
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Operator: arithmetic_right_shift
//===----------------------------------------------------------------------===//
def Tosa_ArithmeticRightShiftOp : Tosa_ElementwiseOp<"arithmetic_right_shift",
    [SameOperandsAndResultElementType]> {
  let summary = "Elementwise Arithmetic Right Shift";

  let description = [{
    Elementwise arithmetic right shift of input1 by the amount specified in
    input2. Axis of size 1 will be broadcast, as necessary.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    Tosa_Tensor:$input2,
    BoolAttr:$round
  );

  let results = (outs
    Tosa_Tensor:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: bitwise_and
//===----------------------------------------------------------------------===//
def Tosa_BitwiseAndOp : Tosa_ElementwiseOp<"bitwise_and", [
    Commutative,
    SameOperandsAndResultElementType]> {
  let summary = "Bitwise AND operator";

  let description = [{
    Elementwise bitwise AND of input1 and input2. Axis of size 1
    will be broadcast as necessary.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    Tosa_Tensor:$input2
  );

  let results = (outs
    Tosa_Tensor:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: bitwise_or
//===----------------------------------------------------------------------===//
def Tosa_BitwiseOrOp : Tosa_ElementwiseOp<"bitwise_or", [
    Commutative,
    SameOperandsAndResultElementType]> {
  let summary = "Bitwise OR operator";

  let description = [{
    Elementwise bitwise OR of input1 and input2. Axis of size 1 will be
    broadcast as necessary.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    Tosa_Tensor:$input2
  );

  let results = (outs
    Tosa_Tensor:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: bitwise_xor
//===----------------------------------------------------------------------===//
def Tosa_BitwiseXorOp : Tosa_ElementwiseOp<"bitwise_xor", [
    Commutative,
    SameOperandsAndResultElementType]> {
  let summary = "Bitwise XOR operator";

  let description = [{
    Elementwise bitwise XOR of input1 and input2. Axis of size 1 will be
    broadcast as necessary.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    Tosa_Tensor:$input2
  );

  let results = (outs
    Tosa_Tensor:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: int_div
//===----------------------------------------------------------------------===//
def Tosa_IntDivOp : Tosa_ElementwiseOp<"int_div", [SameOperandsAndResultElementType]> {
  let summary = "Integer divide operator";

  let description = [{
    Elementwise integer divide operator of input1 by input2. Axis of size 1
    will be broadcast, as necessary.
  }];

  let arguments = (ins
    Tosa_Int32Tensor:$input1,
    Tosa_Int32Tensor:$input2
  );

  let results = (outs
    Tosa_Int32Tensor:$output
  );

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Operator: logical_and
//===----------------------------------------------------------------------===//
def Tosa_LogicalAndOp : Tosa_ElementwiseOp<"logical_and", [
    Commutative,
    SameOperandsAndResultElementType]> {
  let summary = "Returns the truth value of x AND y element-wise.";

  let description = [{
    Elementwise logical AND of input1 and input2. Axis of size 1 will be
    broadcast, as necessary.
  }];

  let arguments = (ins
    Tosa_I1Tensor:$input1,
    Tosa_I1Tensor:$input2
  );

  let results = (outs
    Tosa_I1Tensor:$z
  );
}

//===----------------------------------------------------------------------===//
// Operator: logical_left_shift
//===----------------------------------------------------------------------===//
def Tosa_LogicalLeftShiftOp : Tosa_ElementwiseOp<"logical_left_shift",
    [SameOperandsAndResultElementType]> {
  let summary = "Elementwise Logical Left Shift";

  let description = [{
    Elementwise left shift of input1 and input2. Axis of size 1 will be
    broadcast, as necessary.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    Tosa_Tensor:$input2
  );

  let results = (outs
    Tosa_Tensor:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: logical_right_shift
//===----------------------------------------------------------------------===//
def Tosa_LogicalRightShiftOp : Tosa_ElementwiseOp<"logical_right_shift",
    [SameOperandsAndResultElementType]> {
  let summary = "Elementwise Logical Right Shift";

  let description = [{
    Elementwise logical right shift of input1 by the amount specified in input2.
    Axis of size 1 will be broadcast, as necessary.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    Tosa_Tensor:$input2
  );

  let results = (outs
    Tosa_Tensor:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: logical_or
//===----------------------------------------------------------------------===//
def Tosa_LogicalOrOp : Tosa_ElementwiseOp<"logical_or", [
    Commutative,
    SameOperandsAndResultElementType]> {
  let summary = "Returns the truth value of x OR y element-wise.";

  let description = [{
    Elementwise logical OR of input1 and input2. Axis of size 1 will be
    broadcast as necessary.
  }];

  let arguments = (ins
    Tosa_I1Tensor:$input1,
    Tosa_I1Tensor:$input2
  );

  let results = (outs
    Tosa_I1Tensor:$z
  );
}

//===----------------------------------------------------------------------===//
// Operator: logical_xor
//===----------------------------------------------------------------------===//
def Tosa_LogicalXorOp : Tosa_ElementwiseOp<"logical_xor", [
    Commutative,
    SameOperandsAndResultElementType]> {
  let summary = "Returns the truth value of x XOR y element-wise.";

  let description = [{
    Elementwise logical XOR of input1 and input2.  Axis of size 1 will be
    broadcast as necessary.
  }];

  let arguments = (ins
    Tosa_I1Tensor:$input1,
    Tosa_I1Tensor:$input2
  );

  let results = (outs
    Tosa_I1Tensor:$z
  );
}

//===----------------------------------------------------------------------===//
// Operator: maximum
//===----------------------------------------------------------------------===//
def Tosa_MaximumOp : Tosa_ElementwiseOp<"maximum", [
    Commutative,
    SameOperandsAndResultElementType]> {
  let summary = "Elementwise Maximum";

  let description = [{
    Elementwise max of input1 and input2. Axis of size 1 will be broadcast, as
    necessary.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    Tosa_Tensor:$input2
  );

  let results = (outs
    Tosa_Tensor:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: minimum
//===----------------------------------------------------------------------===//
def Tosa_MinimumOp : Tosa_ElementwiseOp<"minimum", [
    Commutative,
    SameOperandsAndResultElementType]> {
  let summary = "Elementwise Minimum";

  let description = [{
    Elementwise minimum of input1 and input2. Axis of size 1
    will be broadcast, as necessary.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    Tosa_Tensor:$input2
  );

  let results = (outs
    Tosa_Tensor:$output
  );
}

def MulOperandsAndResultElementType :
  NativeOpTrait<"MulOperandsAndResultElementType"> {
  let cppNamespace = "mlir::OpTrait::tosa";
}

//===----------------------------------------------------------------------===//
// Operator: mul
//===----------------------------------------------------------------------===//
def Tosa_MulOp : Tosa_ElementwiseOp<"mul", [
    Commutative,
    MulOperandsAndResultElementType]> {
  let summary = "Multiplication operator";

  let description = [{
    Elementwise multiplication (Hadamard product) of input1 and input2.
    Axis of size 1 will be broadcast, as necessary.
    i8/i16 input type can be promoted to i32 result type.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    Tosa_Tensor:$input2,
    I8Attr:$shift
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Operator: pow
//===----------------------------------------------------------------------===//
def Tosa_PowOp : Tosa_ElementwiseOp<"pow", [SameOperandsAndResultElementType]> {
  let summary = "Computes the power of one value to another.";

  let description = [{
    Elementwise input1 raised to the power of input2.
    Axis of size 1 will be broadcast, as necessary.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    Tosa_Tensor:$input2
  );

  let results = (outs
    Tosa_Tensor:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: sub
//===----------------------------------------------------------------------===//
def Tosa_SubOp : Tosa_ElementwiseOp<"sub", [SameOperandsAndResultElementType]> {
  let summary = "Elementwise subtraction operator";

  let description = [{
    Elementwise subtraction of input1 and input2. Axis of size 1 will be
    broadcast as necessary.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    Tosa_Tensor:$input2
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Operator: table
//===----------------------------------------------------------------------===//
def Tosa_TableOp : Tosa_InferShapedTypeOp<"table"> {
  let summary = "Table lookup op";

  let description = [{
    Interpolated table lookup operation. Input values are scaled to create a
    fixed-point 9.7 value.    The high 9 bits are used to index into the table.
    The fractional bits are used to interpolate based on the looked up value and
    the index+1 value in the table. The TABLE operator then returns a 16.7
    interpolated value. Note that there must be 513 values to handle the full
    range of inputs.

    The TABLE operator is expected to be used as follows:
    * A RESCALE node is expected before the TABLE operator to scale the input
      to a full int16_t range for the table lookup
    * If an int16_t result is required then follow the TABLE operator with a
      RESCALE with a right shift of 7
    * If an int8_t result is required then follow the TABLE operator with a
      RESCALE with a right shift of 15
  }];

  let arguments = (ins
    Tosa_Tensor: $input1,
    Tosa_Tensor1D: $table
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let assemblyFormat = [{
    $input1 `,` $table attr-dict `:` `(` type($input1) `,` type($table) `)` `->` type($output)
  }];

  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// TOSA Spec Section 2.5
// Operator Class: Elementwise unary/binary/ternary operators.
// Operator Subclass: Elementwise unary ops.
//===----------------------------------------------------------------------===//

//===----------------------------------------------------------------------===//
// Operator: abs
//===----------------------------------------------------------------------===//
def Tosa_AbsOp : Tosa_ElementwiseUnaryOp<"abs"> {
  let summary = "Elementwise abs op";

  let description = [{
    Elementwise absolute value operation

    Example:

    ```mlir
    %out = tosa.abs(%in) : (tensor<21x3xf32>) -> tensor<21x3xf32>
    ```
  }];

  let arguments = (ins
    Tosa_Tensor:$input1
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Operator: bitwise_not
//===----------------------------------------------------------------------===//
def Tosa_BitwiseNotOp : Tosa_ElementwiseUnaryOp<"bitwise_not"> {
  let summary = "Bitwise NOT operator";

  let description = [{
    Elementwise bitwise NOT of input tensor.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1
  );

  let results = (outs
    Tosa_Tensor:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: ceil
//===----------------------------------------------------------------------===//
def Tosa_CeilOp : Tosa_ElementwiseUnaryOp<"ceil"> {
  let summary = "Elementwise ceil op";

  let description = [{
    Elementwise ceiling operation
  }];

  let arguments = (ins
    Tosa_Tensor:$input1
  );

  let results = (outs
    Tosa_Tensor:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: clz
//===----------------------------------------------------------------------===//
def Tosa_ClzOp : Tosa_ElementwiseUnaryOp<"clz"> {
  let summary = "Elementwise count leading zero op";

  let description = [{
    Elementwise count leading zeros operation
  }];

  let arguments = (ins
    Tosa_Tensor:$input1
  );

  let results = (outs
    Tosa_Tensor:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: cos
//===----------------------------------------------------------------------===//
def Tosa_CosOp : Tosa_ElementwiseUnaryOp<"cos"> {
  let summary = "Elementwise cos op";

  let description = [{
    Elementwise cosine operation for values given in radians.
  }];

  let arguments = (ins
    Tosa_FloatTensor:$input1
  );

  let results = (outs
    Tosa_FloatTensor:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: exp
//===----------------------------------------------------------------------===//
def Tosa_ExpOp : Tosa_ElementwiseUnaryOp<"exp"> {
  let summary = "Elementwise exp op";

  let description = [{
    Elementwise e to the x operation
  }];

  let arguments = (ins
    Tosa_Tensor:$input1
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Operator: floor
//===----------------------------------------------------------------------===//
def Tosa_FloorOp : Tosa_ElementwiseUnaryOp<"floor"> {
  let summary = "Elementwise floor op";

  let description = [{
    Elementwise floor operation
  }];

  let arguments = (ins
    Tosa_Tensor:$input1
  );

  let results = (outs
    Tosa_Tensor:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: log
//===----------------------------------------------------------------------===//
def Tosa_LogOp : Tosa_ElementwiseUnaryOp<"log"> {
  let summary = "Elementwise log op";

  let description = [{
    Elementwise natural logarithm operation
  }];

  let arguments = (ins
    Tosa_Tensor:$input1
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Operator: logical_not
//===----------------------------------------------------------------------===//
def Tosa_LogicalNotOp : Tosa_ElementwiseUnaryOp<"logical_not"> {
  let summary = "Returns the truth value of NOT x element-wise.";

  let description = [{
    Elementwise logical NOT of input.
  }];

  let arguments = (ins
    Tosa_I1Tensor:$input1
  );

  let results = (outs
    Tosa_I1Tensor:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: negate
//===----------------------------------------------------------------------===//
def Tosa_NegateOp : Tosa_ElementwiseUnaryOp<"negate"> {
  let summary = "Elementwise negate op";

  let description = [{
    Elementwise negation operation
  }];

  let arguments = (ins
      Tosa_Tensor:$input1,
      OptionalAttr<Tosa_UnaryOpQuantizationAttr>:$quantization_info
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let builders = [Tosa_UnaryOpQuantInfoBuilder];

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Operator: reciprocal
//===----------------------------------------------------------------------===//
def Tosa_ReciprocalOp : Tosa_ElementwiseUnaryOp<"reciprocal"> {
  let summary = "Elementwise reciprocal op";

  let description = [{
    Elementwise reciprocal operation. For integer operation, a TABLE should be
    used with the appropriate ranges.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let extraClassDeclaration = [{
    /// Return the reciprocal result on the operand.
    static inline APFloat calcOneElement(const APFloat &operand) {
      APFloat recip = APFloat(operand.getSemantics(), 1);
      recip.divide(operand, APFloat::rmNearestTiesToEven);
      return recip;
    }
  }];

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Operator: rsqrt
//===----------------------------------------------------------------------===//
def Tosa_RsqrtOp : Tosa_ElementwiseUnaryOp<"rsqrt"> {
  let summary = "Elementwise 1/sqrt op";

  let description = [{
    Elementwise reciprocal square root operation. For integer operation, a TABLE
    should be used with the appropriate ranges.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1
  );

  let results = (outs
    Tosa_Tensor:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: sin
//===----------------------------------------------------------------------===//
def Tosa_SinOp : Tosa_ElementwiseUnaryOp<"sin"> {
  let summary = "Elementwise sin op";

  let description = [{
    Elementwise sine operation for values given in radians.
  }];

  let arguments = (ins
    Tosa_FloatTensor:$input1
  );

  let results = (outs
    Tosa_FloatTensor:$output
  );
}

//===----------------------------------------------------------------------===//
// TOSA Spec Section 2.6
// Operator Class: Elementwise unary/binary/ternary operators.
// Operator Subclass: Elementwise ternary ops.
//===----------------------------------------------------------------------===//

//===----------------------------------------------------------------------===//
// Operator: select
//===----------------------------------------------------------------------===//
def Tosa_SelectOp : Tosa_ElementwiseOp<"select"> {
  let summary = "Elementwise select operator";

  let description = [{
    Elementwise select of the output based on a condition.
  }];

  let arguments = (ins
    Tosa_I1Tensor:$pred,
    Tosa_Tensor:$on_true,
    Tosa_Tensor:$on_false
  );

  let results = (outs
    Tosa_Tensor:$output
  );
  let hasCanonicalizeMethod = 1;
  let hasFolder = 1;

  let assemblyFormat = [{
    operands attr-dict `:` `(` type($pred) `,` type($on_true) `,` type($on_false)
    `)` `->` type($output)
  }];
}

//===----------------------------------------------------------------------===//
// TOSA Spec Section 2.7
// Operator Class: Logical Operations.
//===----------------------------------------------------------------------===//

//===----------------------------------------------------------------------===//
// Operator: equal
//===----------------------------------------------------------------------===//
def Tosa_EqualOp : Tosa_ElementwiseOp<"equal", [
    InferTensorType,
    Commutative,
    SameOperandsElementType]> {
  let summary = "Returns the truth value of (x == y) element-wise.";

  let description = [{
     Elementwise comparison operation
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    Tosa_Tensor:$input2
  );

  let results = (outs
    Tosa_I1Tensor:$output
  );

  let extraClassDeclaration = [{
    /// Returns when two result types are compatible for this op; method used by
    /// InferTypeOpInterface.
    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
  }];

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Operator: greater
//===----------------------------------------------------------------------===//
def Tosa_GreaterOp : Tosa_ElementwiseOp<"greater", [SameOperandsElementType]> {
  let summary = "Returns the truth value of (x > y) element-wise.";

  let description = [{
    Elementwise greater than comparison operation
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    Tosa_Tensor:$input2
  );

  let results = (outs
    Tosa_I1Tensor:$output
  );

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Operator: greater_equal
//===----------------------------------------------------------------------===//
def Tosa_GreaterEqualOp : Tosa_ElementwiseOp<"greater_equal",
    [SameOperandsElementType]> {
  let summary = "Returns the truth value of (x >= y) element-wise.";

  let description = [{
    Elementwise comparison operation
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    Tosa_Tensor:$input2
  );

  let results = (outs
    Tosa_I1Tensor:$output
  );

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// TOSA Spec Section 2.8
// Operator Class: Reduction Ops.
//===----------------------------------------------------------------------===//

//===----------------------------------------------------------------------===//
// Operator: reduce_all
//===----------------------------------------------------------------------===//
def Tosa_ReduceAllOp : Tosa_InferTensorTypeOp<"reduce_all"> {
  let summary = "Reduce All operator";

  let description = [{
    Reduce a tensor along the given axis with a logical AND operation
  }];

  let arguments = (ins
    Tosa_Tensor:$input,
    I32Attr:$axis
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let hasFolder = 1;
  let hasVerifier = 1;

  let extraClassDeclaration = [{
    /// Returns true when two result types are compatible for this op;
    /// Method used by InferTypeOpInterface.
    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);

    /// Return the AND result between two integer operands
    static inline APInt calcOneElement(APInt leftOperand, APInt rightOperand) {
      return leftOperand & rightOperand;
    }
  }];
}

//===----------------------------------------------------------------------===//
// Operator: reduce_any
//===----------------------------------------------------------------------===//
def Tosa_ReduceAnyOp : Tosa_InferTensorTypeOp<"reduce_any"> {
  let summary = "Reduce Any operator";

  let description = [{
    Reduce a tensor along the given axis with a logical OR operation
  }];

  let arguments = (ins
    Tosa_Tensor:$input,
    I32Attr:$axis
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let hasFolder = 1;
  let hasVerifier = 1;

  let extraClassDeclaration = [{
    /// Returns true when two result types are compatible for this op;
    /// Method used by InferTypeOpInterface.
    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);

    /// Return the OR result between two integer operands
    static inline APInt calcOneElement(APInt leftOperand, APInt rightOperand) {
      return leftOperand | rightOperand;
    }
  }];
}

//===----------------------------------------------------------------------===//
// Operator: reduce_max
//===----------------------------------------------------------------------===//
def Tosa_ReduceMaxOp : Tosa_InferTensorTypeOp<"reduce_max"> {
  let summary = "Reduce Max operator";

  let description = [{
    Reduce a tensor along the given axis with a maximum operation
  }];

  let arguments = (ins
    Tosa_Tensor:$input,
    I32Attr:$axis
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let hasFolder = 1;
  let hasVerifier = 1;

  let extraClassDeclaration = [{
    /// Returns true when two result types are compatible for this op;
    /// Method used by InferTypeOpInterface.
    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);

    /// Return the max of the two integer operands
    static inline APInt calcOneElement(APInt leftOperand, APInt rightOperand) {
      const llvm::APInt subtractRes = leftOperand - rightOperand;
      return (!subtractRes.isNegative()) ? leftOperand : rightOperand;
    }
  }];
}

//===----------------------------------------------------------------------===//
// Operator: reduce_min
//===----------------------------------------------------------------------===//
def Tosa_ReduceMinOp : Tosa_InferTensorTypeOp<"reduce_min"> {
  let summary = "Reduce Min operator";

  let description = [{
    Reduce a tensor along the given axis with a minimum operation
  }];

  let arguments = (ins
    Tosa_Tensor:$input,
    I32Attr:$axis
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let hasFolder = 1;
  let hasVerifier = 1;

  let extraClassDeclaration = [{
    /// Returns true when two result types are compatible for this op;
    /// Method used by InferTypeOpInterface.
    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);

    /// Return the min of the two integer operands
    static inline APInt calcOneElement(APInt leftOperand, APInt rightOperand) {
      const llvm::APInt subtractRes = leftOperand - rightOperand;
      return (!subtractRes.isNegative()) ? rightOperand : leftOperand;
    }
  }];
}

//===----------------------------------------------------------------------===//
// Operator: reduce_prod
//===----------------------------------------------------------------------===//
def Tosa_ReduceProdOp : Tosa_InferTensorTypeOp<"reduce_prod"> {
  let summary = "Reduce Prod operator";

  let description = [{
    Reduce a tensor along the given axis by computing the product of the axis.
  }];

  let arguments = (ins
    Tosa_Tensor:$input,
    I32Attr:$axis
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let hasFolder = 1;
  let hasVerifier = 1;

  let extraClassDeclaration = [{
    /// Returns true when two result types are compatible for this op;
    /// Method used by InferTypeOpInterface.
    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);

    /// Return the prod of the two integer operands
    static inline APInt calcOneElement(APInt leftOperand, APInt rightOperand) {
      return leftOperand * rightOperand;
    }
  }];
}

//===----------------------------------------------------------------------===//
// Operator: reduce_sum
//===----------------------------------------------------------------------===//
def Tosa_ReduceSumOp : Tosa_InferTensorTypeOp<"reduce_sum"> {
  let summary = "Reduce Sum operator";

  let description = [{
    Reduce a tensor along the given axis by computing the sum of the axis.
  }];

  let arguments = (ins
    Tosa_Tensor:$input,
    I32Attr:$axis
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let hasFolder = 1;
  let hasVerifier = 1;

  let extraClassDeclaration = [{
    /// Returns true when two result types are compatible for this op;
    /// Method used by InferTypeOpInterface.
    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);

    /// Return the sum of the two integer operands
    static inline APInt calcOneElement(APInt leftOperand, APInt rightOperand) {
      return leftOperand + rightOperand;
    }
  }];
}

//===----------------------------------------------------------------------===//
// TOSA Spec Section 2.9
// Operator Class: Data Layout / Memory Reinterpretation.
//===----------------------------------------------------------------------===//

//===----------------------------------------------------------------------===//
// Operator: concat
//===----------------------------------------------------------------------===//
def Tosa_ConcatOp : Tosa_InferTensorTypeOp<"concat"> {
  let summary = "Concatenates tensors along one dimension.";

  let description = [{
    Concatenate a variadic amount of tensors along a given axis. No data
    conversion happens during a concat operation.
  }];

  let arguments = (ins
    Variadic<Tosa_Tensor>:$input1,
    I32Attr:$axis
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let hasCanonicalizer = 1;
  let hasFolder = 1;

  let extraClassDeclaration = [{
    /// Returns true when two result types are compatible for this op;
    /// Method used by InferTypeOpInterface.
    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
  }];
}

//===----------------------------------------------------------------------===//
// Operator: pad
//===----------------------------------------------------------------------===//
def Tosa_PadOp : Tosa_InferShapedTypeOp<"pad"> {
  let summary = "Pads a tensor with value specified.";

  let description = [{
    The `tosa.pad` operation pads a tensor along borders of each dimension with
    `pad_const` (defaults to zero), given a padding configuration `padding`
    specifying low and high values along the dimensions.

    Example:

    ```mlir
    %0 = arith.constant dense<[[1, 2], [3, 4]]> : tensor<2x2xi32>
    tosa.pad %arg0, %0 : (tensor<1x2xf32>, tensor<2x2xi32>)  -> (tensor<4x9xf32>)
    ```

    Example 2:

    ```mlir
    %0 = arith.constant dense<[[-1, 2], [3, 4]]> : tensor<2x2xi32>
    tosa.pad %arg0, %0 : (tensor<1x2xf32>, tensor<2x2xi32>)  -> (tensor<?x9xf32>)
    ```
  }];

  let arguments = (ins
    Tosa_RankedTensor:$input1,
    Tosa_Int32Or64Tensor:$padding,
    Optional<Tosa_ScalarTensor>:$pad_const,
    OptionalAttr<Tosa_PadOpQuantizationAttr>:$quantization_info
  );

  let results = (outs
    Tosa_RankedTensor:$output
  );

  let builders = [Tosa_PadOpQuantInfoBuilder,
                  Tosa_ExplicitValuePadOpQuantInfoBuilder];

  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Operator: reshape
//===----------------------------------------------------------------------===//
def Tosa_ReshapeOp : Tosa_InferTensorTypeOp<"reshape"> {
  let summary = "Reshape operator";

  let description = [{
    Returns a tensor with the same type/values as the input, with a new shape
    specified by the shape argument. Reshape may operate on tensors of any rank.
    No data conversion happens during a reshape operation.
  }];

  let hasFolder = 1;
  let hasVerifier = 1;

  let arguments = (ins
    Tosa_Tensor:$input1,
    DenseI64ArrayAttr:$new_shape
  );

  let results = (outs
    Tosa_RankedTensor:$output
  );

  let extraClassDeclaration = [{
    /// Returns true when two result types are compatible for this op;
    /// Method used by InferTypeOpInterface.
    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
  }];

  let assemblyFormat = "operands attr-dict `:` functional-type(operands, results)";
}

//===----------------------------------------------------------------------===//
// Operator: reverse
//===----------------------------------------------------------------------===//
def Tosa_ReverseOp: Tosa_Op<"reverse", [
    DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
                              ["inferReturnTypeComponents"]>, Pure]> {
  let summary = "Reverse operator";

  let description = [{
    Returns a tensor with the same type/values as the input, with the data
    reversed along the given axis. No data conversion happens during a reverse
    operation.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    I32Attr:$axis
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let hasFolder = 1;
  let hasVerifier = 1;

  let assemblyFormat = "operands attr-dict `:` functional-type(operands, results)";
}

//===----------------------------------------------------------------------===//
// Operator: slice
//===----------------------------------------------------------------------===//
def Tosa_SliceOp : Tosa_InferShapedTypeOp<"slice"> {
  let summary = "Slice operator";

  let description = [{
    Extracts a slice of the input1 on the given axis, beginning at the
    start coordinates, and extending for size elements in each direction.  No
    data conversion happens during a slice operation.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    DenseI64ArrayAttr:$start,
    DenseI64ArrayAttr:$size
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Operator: tile
//===----------------------------------------------------------------------===//
def Tosa_TileOp : Tosa_InferShapedTypeOp<"tile"> {
  let summary = "Tile operator";

  let description = [{
    Replicates input 0 multiplies times along each dimension.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    DenseI64ArrayAttr:$multiples);

  let results = (outs
    Tosa_Tensor:$output
  );

  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Operator: transpose
//===----------------------------------------------------------------------===//
def Tosa_TransposeOp : Tosa_InferShapedTypeOp<"transpose",
                [DeclareOpInterfaceMethods<ReifyRankedShapedTypeOpInterface>]> {
  let summary = "Transpose operator";

  let description = [{
    Permutes the dimensions based on perm.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1,
    Tosa_Int32Tensor:$perms
  );

  let results = (
    outs Tosa_Tensor:$output
  );

  let extraClassDeclaration = [{
    LogicalResult getConstantPerms(llvm::SmallVector<int32_t> &perms);
  }];

  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// TOSA Spec Section 2.10
// Operator Class: Scatter/gather Operations.
//===----------------------------------------------------------------------===//

//===----------------------------------------------------------------------===//
// Operator: gather
//===----------------------------------------------------------------------===//
def Tosa_GatherOp : Tosa_InferShapedTypeOp<"gather"> {
  let summary = "Gather operation,";

  let description = [{
    Generate a tensor for which each element in the output is a slice of the
    values tensor based on the value of indices.
  }];

  let arguments = (ins
    Tosa_Tensor3D:$values,
    TosaTensorRankOf<[Tosa_Int32], [2]>:$indices
  );

  let results = (outs
    Tosa_Tensor3D:$output
  );
}

//===----------------------------------------------------------------------===//
// Operator: scatter
//===----------------------------------------------------------------------===//
def Tosa_ScatterOp : Tosa_InferShapedTypeOp<"scatter"> {
  let summary = "Scatter operation,";

  let description = [{
    The values_out tensor is set to the values_in tensor with data modified as follows:
    data from the input tensor is inserted at the positions specified by the indices tensor.
  }];

  let arguments = (ins
    Tosa_Tensor3D:$values_in,
    TosaTensorRankOf<[Tosa_Int32], [2]>:$indices,
    Tosa_Tensor3D:$input
  );

  let results = (outs
    Tosa_Tensor3D:$values_out
  );
}

//===----------------------------------------------------------------------===//
// TOSA Spec Section 2.11
// Operator Class: Image Frontend Functions.
//===----------------------------------------------------------------------===//

//===----------------------------------------------------------------------===//
// Operator: resize
//===----------------------------------------------------------------------===//
def Tosa_ResizeOp : Tosa_InferShapedTypeOp<"resize"> {
  let summary = "Resize operation, supports various resize/upsample modes";

  let description = [{
    Resizes a tensor. Resize is only allowed in the H and W dimensions. In
    expected use, The height dimension is scaled by factor (scale_y_n/scale_y_d).
    And the width dimension is scaled by factor (scale_x_n/scale_x_d). Thus the
    output dimensions can be derived from the input dimensions by inverting the
    scale. And the [order_y, border_x] values adjust the output size to allow
    fractional sampling beyond integer input position (IH-1,IW-1).
  }];

  let arguments = (ins
    Tosa_Tensor4D:$input,
    Tosa_IntArrayAttr4:$scale,
    Tosa_IntArrayAttr2:$offset,
    Tosa_IntArrayAttr2:$border,
    Tosa_ResizeTypeAttr:$mode
  );

  let results = (outs
    Tosa_Tensor4D:$output
  );

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// TOSA Spec Section 2.12
// Operator Class: Type Conversion.
//===----------------------------------------------------------------------===//

//===----------------------------------------------------------------------===//
// Operator: cast
//===----------------------------------------------------------------------===//
def Tosa_CastOp: Tosa_Op<"cast", [Pure,
      DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
                              ["inferReturnTypeComponents"]>]> {

  let summary = "Cast operation";

  let description = [{
    Performs a set of permissible cast operations

    | Mode                     | Input   | Output  |
    |--------------------------|---------|---------|
    | signed 8 to bool         | int8    | Boolean |
    | signed 16 to bool        | int16   | Boolean |
    | signed 32 to bool        | int32   | Boolean |
    | bool to 8                | Boolean | int8    |
    | bool to 16               | Boolean | int16   |
    | bool to 32               | Boolean | int32   |
    | signed 8 to signed 16    | int8    | int16   |
    | signed 8 to signed 32    | int8    | int32   |
    | signed 16 to signed 8    | int16   | int8    |
    | signed 16 to signed 32   | int16   | int32   |
    | signed 32 to signed 8    | int32   | int8    |
    | signed 32 to signed 16   | int32   | int16   |
    | float to signed 8        | float   | int8    |
    | float to signed 16       | float   | int16   |
    | signed 8 to float        | int8    | float   |
    | signed 16 to float       | int16   | float   |
    | float 32 to float 64     | float32 | float64 |
    | float 64 to float 32     | float64 | float32 |
  }];

  let arguments = (ins
    Tosa_Tensor:$input
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let assemblyFormat = "operands attr-dict `:` functional-type(operands, results)";

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Operator: rescale
//===----------------------------------------------------------------------===//
def Tosa_RescaleOp: Tosa_Op<"rescale", [Pure,
      DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
                              ["inferReturnTypeComponents"]>]> {
  let summary = "Tosa rescale operator";

  let description = [{
    Rescale quantized values into a new domain. Supported rescalings are:

    | Mode                   | Input | Output | Unsigned | Unsigned |
    |                        |       |        |  input   |  output  |
    |------------------------|-------|--------|----------|----------|
    | signed 8 to 8          | int8  | int8   |  false   |  false   |
    | signed 8 to 16         | int8  | int16  |  false   |  false   |
    | signed 8 to 32         | int8  | int32  |  false   |  false   |
    | signed 16 to 8         | int16 | int8   |  false   |  false   |
    | signed 16 to 16        | int16 | int16  |  false   |  false   |
    | signed 16 to 32        | int16 | int32  |  false   |  false   |
    | signed 32 to 8         | int32 | int8   |  false   |  false   |
    | signed 32 to 16        | int32 | int16  |  false   |  false   |
    | signed 32 to 32        | int32 | int32  |  false   |  false   |
    | signed 48 to 8         | int48 | int8   |  false   |  false   |
    | signed 48 to 16        | int48 | int16  |  false   |  false   |
    | signed 48 to 32        | int48 | int32  |  false   |  false   |
    | unsigned 8 to signed 8 | uint8 | int8   |  true    |  false   |
    | signed 8 to unsigned 8 | int8  | uint8  |  false   |  true    |
  }];

  let arguments = (ins
    Tosa_Tensor:$input,
    I32Attr:$input_zp,
    I32Attr:$output_zp,
    DenseI32ArrayAttr:$multiplier,
    DenseI8ArrayAttr:$shift,
    BoolAttr:$scale32,
    BoolAttr:$double_round,
    BoolAttr:$per_channel,
    DefaultValuedOptionalAttr<BoolAttr, "false">:$input_unsigned,
    DefaultValuedOptionalAttr<BoolAttr, "false">:$output_unsigned
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let assemblyFormat = "operands attr-dict `:` functional-type(operands, results)";
}

//===----------------------------------------------------------------------===//
// TOSA Spec Section 2.13
// Operator Class: Data Node Ops.
//===----------------------------------------------------------------------===//

//===----------------------------------------------------------------------===//
// Operator: const
//===----------------------------------------------------------------------===//
def Tosa_ConstOp : Tosa_Op<"const", [ConstantLike, Pure,
                                     AllShapesMatch<["value", "output"]>,
                                     FirstAttrDerivedResultType]> {
  let summary = "Constant op.";

  let description = [{
    A node containing constant data for use as the input to an operation. May
    hold data in any of the supported data formats.

    Example:

    ```mlir
    // Generic form
    %out = "tosa.const"() {value = dense<0> : tensor<2x3xi32>} : () -> tensor<2x3xi32>
    ```
  }];

  let arguments = (ins
    ElementsAttr:$value
  );

  let results = (outs
    TosaTensorOf<[AnyTypeOf<[Tosa_AnyNumber]>]>:$output
  );

  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Operator: identity
//===----------------------------------------------------------------------===//
def Tosa_IdentityOp: Tosa_Op<"identity", [Pure,
      DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
                              ["inferReturnTypeComponents"]>]> {
  let summary = "Identity operator";
  let description = [{
    Returns a tensor with the same shape, size, type
    and content as the input.
  }];

  let arguments = (ins
    Tosa_Tensor:$input1
  );

  let results = (outs
    Tosa_Tensor:$output
  );

  let assemblyFormat = "operands attr-dict `:` functional-type(operands, results)";
}

//===----------------------------------------------------------------------===//
// TOSA Spec Section 2.14
// Operator Class: Custom Operators.
//===----------------------------------------------------------------------===//

//===----------------------------------------------------------------------===//
// Operator: custom
//===----------------------------------------------------------------------===//
def Tosa_CustomOp : Tosa_Op<"custom"> {

  let summary = "Custom operator wrapper for Tosa";

  let description = [{
    Hardware implementing TOSA may choose to add additional custom operators
    that are not expressed in the existing TOSA operations. These operators are
    not expected to be portable across TOSA implementations. The input and
    output signatures must be expressed in the corresponding TOSA node.

    `operator_name` is a string that tells the backend which custom operator is
    being called.

    `domain_name` is a string identifier which can help avoid name collisions on
    the identifier field.

    `implementation_attrs` is a string which is a backend and identifier specific
    set of attributes to the custom operator.

    `input_list` is the set of tensor inputs to the custom operator.

    `output_list` is the list of tensors returned by the operator. The number of operators
    is backend specific.

    Example:

    ```mlir
    %out = tosa.custom %in {domain_name = "tosa_mlir_test", operator_name =
           "custom_test", implementation_attrs = ""}: (tensor<10xi32>) ->
           (tensor<10xi32>)
    ```
  }];

  let arguments = (ins
    StrAttr:$operator_name,
    StrAttr:$domain_name,
    StrAttr:$implementation_attrs,
    Variadic<Tosa_Tensor>:$input_list
  );

  let results = (outs
    Variadic<Tosa_Tensor>:$output_list
  );

  let assemblyFormat = "operands attr-dict `:` functional-type(operands, results)";
}

//===----------------------------------------------------------------------===//
// TOSA Spec Section 2.15
// Operator Class: Control Flow Operators.
//===----------------------------------------------------------------------===//

//===----------------------------------------------------------------------===//
// Operator: cond_if
//===----------------------------------------------------------------------===//
//===----------------------------------------------------------------------===//
// Further described in docs/Rationale/RationaleTOSADialect.md .
//===----------------------------------------------------------------------===//
def Tosa_IfOp : Tosa_Op<"cond_if",
       [InferShapedTypeOpAdaptor,
       SingleBlockImplicitTerminator<"YieldOp">,
       RecursiveMemoryEffects]> {
  let summary = "Conditional if operator";

  let description = [{
    Evaluates a Boolean condition and then takes one of two distinct execution
    paths. This implements the semantic If-then-else structure.
  }];

  let arguments = (ins
    Tosa_I1Tensor:$cond,
    Variadic<Tosa_Tensor>:$inputs
  );

  let results = (outs
    Variadic<Tosa_Tensor>:$output
  );

  let regions = (region
    SizedRegion<1>:$then_branch,
    SizedRegion<1>:$else_branch
  );

  let hasCustomAssemblyFormat = 1;
}

//===----------------------------------------------------------------------===//
// Operator: while_loop
//===----------------------------------------------------------------------===//
//===----------------------------------------------------------------------===//
// Further described in docs/Rationale/RationaleTOSADialect.md .
//===----------------------------------------------------------------------===//
def Tosa_WhileOp : Tosa_Op<"while_loop", [
       DeclareOpInterfaceMethods<LoopLikeOpInterface>,
       InferShapedTypeOpAdaptor,
       SingleBlockImplicitTerminator<"YieldOp">,
       RecursiveMemoryEffects]> {
  let summary = "output = input; While (Cond(output)) {output = Body(output)}";

  let description = [{
    Generates and evaluates a Bool condition and either executes a loop body or
    exits to another control point. This action is performed repeatedly after
    updating and re-evaluating the Boolean condition every iteration. This
    implements the semantic foreach or while iterative loop structure.
  }];

  let arguments = (ins
    Variadic<Tosa_Tensor>:$inputs
  );

  let results = (outs
    Variadic<Tosa_Tensor>:$output
  );

  let regions = (region
    SizedRegion<1>:$cond,
    SizedRegion<1>:$body
  );

  let hasCustomAssemblyFormat = 1;
}

include "mlir/Dialect/Tosa/IR/TosaUtilOps.td"

#endif // TOSA_OPS


//===- BufferizationEnums.td - Bufferization enums ---------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This is the definition file for enums used in Bufferization.
//
//===----------------------------------------------------------------------===//

#ifndef BUFFERIZATION_ENUMS
#define BUFFERIZATION_ENUMS

include "mlir/IR/EnumAttr.td"

def LayoutMapOption : I32EnumAttr<"LayoutMapOption",
                                  "option for map layout", [
  I32EnumAttrCase<"InferLayoutMap", 0>,
  I32EnumAttrCase<"IdentityLayoutMap", 1>,
  I32EnumAttrCase<"FullyDynamicLayoutMap", 2>
]> {
  let cppNamespace = "::mlir::bufferization";
}

#endif // BUFFERIZATION_ENUMS


//===- SparseTensorOps.td - Sparse tensor dialect ops ------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef SPARSETENSOR_OPS
#define SPARSETENSOR_OPS

include "mlir/Dialect/SparseTensor/IR/SparseTensorAttrDefs.td"
include "mlir/Dialect/SparseTensor/IR/SparseTensorBase.td"
include "mlir/Dialect/SparseTensor/IR/SparseTensorTypes.td"
include "mlir/Dialect/SparseTensor/IR/SparseTensorInterfaces.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/Interfaces/ControlFlowInterfaces.td"
include "mlir/Interfaces/LoopLikeInterface.td"

//===----------------------------------------------------------------------===//
// Base class.
//===----------------------------------------------------------------------===//

class SparseTensor_Op<string mnemonic, list<Trait> traits = []>
  : Op<SparseTensor_Dialect, mnemonic, traits>;

//===----------------------------------------------------------------------===//
// Sparse Tensor Operations.
//===----------------------------------------------------------------------===//

def SparseTensor_NewOp : SparseTensor_Op<"new", [Pure]> {
  string summary = "Materializes a new sparse tensor from given source";
  string description = [{
    Materializes a sparse tensor with contents taken from an opaque pointer
    provided by `source`. For targets that have access to a file system,
    for example, this pointer may be a filename (or file) of a sparse
    tensor in a particular external storage format. The form of the operation
    is kept deliberately very general to allow for alternative implementations
    in the future, such as pointers to buffers or runnable initialization
    code. The operation is provided as an anchor that materializes a properly
    typed sparse tensor with inital contents into a computation.

    Reading in a symmetric matrix will result in just the lower/upper triangular
    part of the matrix (so that only relevant information is stored). Proper
    symmetry support for operating on symmetric matrices is still TBD.

    Example:

    ```mlir
    sparse_tensor.new %source : !Source to tensor<1024x1024xf64, #CSR>
    ```
  }];

  let arguments = (ins AnyType:$source);
  let results = (outs AnySparseTensor:$result);
  let assemblyFormat = "$source attr-dict `:` type($source) `to` type($result)";
}

def SparseTensor_AssembleOp : SparseTensor_Op<"assemble", [Pure]> {
  let summary = "Returns a sparse tensor assembled from the given levels and values";
  let description = [{
    Assembles the per-level position and coordinate arrays together with
    the values arrays into a sparse tensor. The order and types of the
    provided levels must be consistent with the actual storage layout of
    the returned sparse tensor described below.

    - `levels: [tensor<? x iType>, ...]`
      supplies the sparse tensor position and coordinate arrays
      of the sparse tensor for the corresponding level as specifed by
      `sparse_tensor::StorageLayout`.
    - `values : tensor<? x V>`
      supplies the values array for the stored elements in the sparse tensor.

    This operation can be used to assemble a sparse tensor from an
    external source; e.g., by passing numpy arrays from Python. It
    is the user's responsibility to provide input that can be correctly
    interpreted by the sparsifier, which does not perform any sanity
    test to verify data integrity.

    Example:

    ```mlir
    %pos    = arith.constant dense<[0, 3]>                : tensor<2xindex>
    %index  = arith.constant dense<[[0,0], [1,2], [1,3]]> : tensor<3x2xindex>
    %values = arith.constant dense<[ 1.1,   2.2,   3.3 ]> : tensor<3xf64>
    %s = sparse_tensor.assemble (%pos, %index), %values
       : (tensor<2xindex>, tensor<3x2xindex>), tensor<3xf64> to tensor<3x4xf64, #COO>
    // yields COO format |1.1, 0.0, 0.0, 0.0|
    //     of 3x4 matrix |0.0, 0.0, 2.2, 3.3|
    //                   |0.0, 0.0, 0.0, 0.0|
    ```
  }];

  let arguments = (ins Variadic<RankedTensorOf<[AnySignlessIntegerOrIndex]>>:$levels,
                       RankedTensorOf<[AnyType]>:$values);
  let results = (outs AnySparseTensor: $result);
  let assemblyFormat =
    "` ` `(` $levels       `)` `,` $values attr-dict `:`"
    "    `(` type($levels) `)` `,` type($values) `to` type($result)";

  let hasVerifier = 1;
}

def SparseTensor_DisassembleOp : SparseTensor_Op<"disassemble", [Pure, SameVariadicResultSize]> {
  let summary = "Copies the levels and values of the given sparse tensor";
  let description = [{
    The disassemble operation is the inverse of `sparse_tensor::assemble`.
    It copies the per-level position and coordinate arrays together with
    the values array of the given sparse tensor into the user-supplied buffers
    along with the actual length of the memory used in each returned buffer.

    This operation can be used for returning a disassembled MLIR sparse tensor;
    e.g., copying the sparse tensor contents into pre-allocated numpy arrays
    back to Python. It is the user's responsibility to allocate large enough
    buffers of the appropriate types to hold the sparse tensor contents.
    The sparsifier simply copies all fields of the sparse tensor into the
    user-supplied buffers without any sanity test to verify data integrity.

    Example:

    ```mlir
    // input COO format |1.1, 0.0, 0.0, 0.0|
    //    of 3x4 matrix |0.0, 0.0, 2.2, 3.3|
    //                  |0.0, 0.0, 0.0, 0.0|
    %p, %c, %v, %p_len, %c_len, %v_len =
      sparse_tensor.disassemble %s : tensor<3x4xf64, #COO>
         out_lvls(%op, %oi : tensor<2xindex>, tensor<3x2xindex>)
         out_vals(%od : tensor<3xf64>) ->
           (tensor<2xindex>, tensor<3x2xindex>), tensor<3xf64>, (index, index), index
    // %p = arith.constant dense<[ 0,              3 ]> : tensor<2xindex>
    // %c = arith.constant dense<[[0,0], [1,2], [1,3]]> : tensor<3x2xindex>
    // %v = arith.constant dense<[ 1.1,   2.2,   3.3 ]> : tensor<3xf64>
    // %p_len = 2
    // %c_len = 6 (3x2)
    // %v_len = 3
    ```
  }];

  let arguments = (ins AnySparseTensor:$tensor,
                       Variadic<RankedTensorOf<[AnySignlessIntegerOrIndex]>>:$out_levels,
                       RankedTensorOf<[AnyType]>:$out_values);
  let results = (outs Variadic<RankedTensorOf<[AnySignlessIntegerOrIndex]>>:$ret_levels,
                      RankedTensorOf<[AnyType]>:$ret_values,
                      Variadic<AnyIndexingScalarLike>:$lvl_lens,
                      AnyIndexingScalarLike:$val_len);
  let assemblyFormat =
    "$tensor attr-dict `:` type($tensor)"
    "`out_lvls` `(` $out_levels `:` type($out_levels) `)` "
    "`out_vals` `(` $out_values `:` type($out_values) `)` `->`"
    "`(` type($ret_levels) `)` `,` type($ret_values) `,` "
    "`(` type($lvl_lens)   `)` `,` type($val_len)";

  let hasVerifier = 1;
}

def SparseTensor_ConvertOp : SparseTensor_Op<"convert",
  [Pure, StageWithSortSparseOpInterface]> {
  string summary = "Converts between different tensor types";
  string description = [{
    Converts one sparse or dense tensor type to another tensor type. The rank
    of the source and destination types must match exactly, and the dimension
    sizes must either match exactly or relax from a static to a dynamic size.
    The sparse encoding of the two types can obviously be completely different.
    The name `convert` was preferred over `cast`, since the operation may incur
    a non-trivial cost.

    When converting between two different sparse tensor types, only explicitly
    stored values are moved from one underlying sparse storage format to
    the other. When converting from an unannotated dense tensor type to a
    sparse tensor type, an explicit test for nonzero values is used. When
    converting to an unannotated dense tensor type, implicit zeroes in the
    sparse storage format are made explicit. Note that the conversions can have
    non-trivial costs associated with them, since they may involve elaborate
    data structure transformations. Also, conversions from sparse tensor types
    into dense tensor types may be infeasible in terms of storage requirements.

    Trivial dense-to-dense convert will be removed by canonicalization while
    trivial sparse-to-sparse convert will be removed by the sparse codegen. This
    is because we use trivial sparse-to-sparse convert to tell bufferization
    that the sparse codegen will expand the tensor buffer into sparse tensor
    storage.

    Examples:

    ```mlir
    %0 = sparse_tensor.convert %a : tensor<32x32xf32> to tensor<32x32xf32, #CSR>
    %1 = sparse_tensor.convert %a : tensor<32x32xf32> to tensor<?x?xf32, #CSR>
    %2 = sparse_tensor.convert %b : tensor<8x8xi32, #CSC> to tensor<8x8xi32, #CSR>
    %3 = sparse_tensor.convert %c : tensor<4x8xf64, #CSR> to tensor<4x?xf64, #CSC>

    // The following conversion is not allowed (since it would require a
    // runtime assertion that the source's dimension size is actually 100).
    %4 = sparse_tensor.convert %d : tensor<?xf64> to tensor<100xf64, #SV>
    ```

  }];

  let arguments = (ins AnyRankedTensor:$source);
  let results = (outs AnyRankedTensor:$dest);
  let assemblyFormat = "$source attr-dict `:` type($source) `to` type($dest)";

  let extraClassDeclaration = [{
     // Whether the convert can be done by a single step or it would require
     // an extra sort. Inherited from StageWithSortSparseOpInterface.
     bool needsExtraSort();
  }];

  let hasFolder = 1;
  let hasVerifier = 1;
}

def SparseTensor_ReinterpretMapOp : SparseTensor_Op<"reinterpret_map",
    [NoMemoryEffect]> {
  let summary = "Reinterprets the dimension/level maps of the source tensor";
  let description = [{
    Reinterprets the dimension-to-level and level-to-dimension map specified in
    `source` according to the type of `dest`.
    `reinterpret_map` is a no-op and is introduced merely to resolve type conflicts.
    It does not make any modification to the source tensor and source/dest tensors
    are considered to be aliases.

    `source` and `dest` tensors are "reinterpretable" if and only if they have
    the exactly same storage at a low level.
    That is, both `source` and `dest` has the same number of levels and level types,
    and their shape is consistent before and after `reinterpret_map`.

    Example:
    ```mlir
    #CSC = #sparse_tensor.encoding<{
      map = (d0, d1) -> (d1: dense, d0: compressed)
    }>
    #CSR = #sparse_tensor.encoding<{
      map = (d0, d1) -> (d0: dense, d1: compressed)
    }>
    %t1 = sparse_tensor.reinterpret_map %t0 : tensor<3x4xi32, #CSC> to tensor<4x3xi32, #CSR>

    #BSR = #sparse_tensor.encoding<{
      map = ( i, j ) -> ( i floordiv 2 : dense,
                          j floordiv 3 : compressed,
                          i mod 2      : dense,
                          j mod 3      : dense
      )
    }>
    #DSDD = #sparse_tensor.encoding<{
      map = (i, j, k, l) -> (i: dense, j: compressed, k: dense, l: dense)
    }>
    %t1 = sparse_tensor.reinterpret_map %t0 : tensor<6x12xi32, #BSR> to tensor<3x4x2x3xi32, #DSDD>
    ```
    }];

  let arguments = (ins AnySparseTensor:$source);
  let results = (outs AnySparseTensor:$dest);
  let assemblyFormat = "$source attr-dict `:` type($source) `to` type($dest)";

  let builders = [
    OpBuilder<(ins "SparseTensorEncodingAttr":$dstEnc, "Value":$source)>
  ];

  let hasFolder = 1;
  let hasVerifier = 1;
}

def SparseTensor_ToPositionsOp : SparseTensor_Op<"positions",
      [Pure, DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "Extracts the `level`-th positions array of the `tensor`";
  let description = [{
    Returns the positions array of the tensor's storage at the given
    level.  This is similar to the `bufferization.to_memref` operation
    in the sense that it provides a bridge between a tensor world view
    and a bufferized world view.  Unlike the `bufferization.to_memref`
    operation, however, this sparse operation actually lowers into code
    that extracts the positions array from the sparse storage itself
    (either by calling a support library or through direct code).

    Writing into the result of this operation is undefined behavior.

    Example:

    ```mlir
    %1 = sparse_tensor.positions %0 { level = 1 : index }
       : tensor<64x64xf64, #CSR> to memref<?xindex>
    ```
  }];

  let arguments = (ins AnySparseTensor:$tensor, LevelAttr:$level);
  let results = (outs AnyNon0RankedMemRef:$result);
  let assemblyFormat = "$tensor attr-dict `:` type($tensor) `to` type($result)";

  let hasVerifier = 1;
}

def SparseTensor_ToCoordinatesOp : SparseTensor_Op<"coordinates",
      [Pure, DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "Extracts the `level`-th coordinates array of the `tensor`";
  let description = [{
    Returns the coordinates array of the tensor's storage at the given
    level.  This is similar to the `bufferization.to_memref` operation
    in the sense that it provides a bridge between a tensor world view
    and a bufferized world view.  Unlike the `bufferization.to_memref`
    operation, however, this sparse operation actually lowers into code
    that extracts the coordinates array from the sparse storage itself
    (either by calling a support library or through direct code).

    Writing into the result of this operation is undefined behavior.

    Example:

    ```mlir
    %1 = sparse_tensor.coordinates %0 { level = 1 : index }
       : tensor<64x64xf64, #CSR> to memref<?xindex>
    ```
  }];

  let arguments = (ins AnySparseTensor:$tensor, LevelAttr:$level);
  let results = (outs AnyNon0RankedMemRef:$result);
  let assemblyFormat = "$tensor attr-dict `:` type($tensor) `to` type($result)";

  let hasVerifier = 1;
}

def SparseTensor_ToCoordinatesBufferOp : SparseTensor_Op<"coordinates_buffer",
      [Pure, DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "Extracts the linear coordinates array from a tensor";
  let description = [{
    Returns the linear coordinates array for a sparse tensor with
    a trailing COO region with at least two levels.  It is an error
    if the tensor doesn't contain such a COO region.  This is similar
    to the `bufferization.to_memref` operation in the sense that it
    provides a bridge between a tensor world view and a bufferized
    world view.  Unlike the `bufferization.to_memref` operation,
    however, this operation actually lowers into code that extracts
    the linear coordinates array from the sparse storage scheme that
    stores the coordinates for the COO region as an array of structures.
    For example, a 2D COO sparse tensor with two non-zero elements at
    coordinates (1, 3) and (4, 6) are stored in a linear buffer as
    (1, 4, 3, 6) instead of two buffer as (1, 4) and (3, 6).

    Writing into the result of this operation is undefined behavior.

    Example:

    ```mlir
    %1 = sparse_tensor.coordinates_buffer %0
       : tensor<64x64xf64, #COO> to memref<?xindex>
    ```
  }];

  let arguments = (ins AnySparseTensor:$tensor);
  let results = (outs AnyNon0RankedMemRef:$result);
  let assemblyFormat = "$tensor attr-dict `:` type($tensor) `to` type($result)";

  let hasVerifier = 1;
}

def SparseTensor_ToValuesOp : SparseTensor_Op<"values",
      [Pure, DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "Extracts numerical values array from a tensor";
  let description = [{
    Returns the values array of the sparse storage format for the given
    sparse tensor, independent of the actual dimension. This is similar to
    the `bufferization.to_memref` operation in the sense that it provides a bridge
    between a tensor world view and a bufferized world view. Unlike the
    `bufferization.to_memref` operation, however, this sparse operation actually
    lowers into code that extracts the values array from the sparse storage
    scheme (either by calling a support library or through direct code).

    Writing into the result of this operation is undefined behavior.

    Example:

    ```mlir
    %1 = sparse_tensor.values %0 : tensor<64x64xf64, #CSR> to memref<?xf64>
    ```
  }];

  let arguments = (ins AnySparseTensor:$tensor);
  let results = (outs AnyNon0RankedMemRef:$result);
  let assemblyFormat = "$tensor attr-dict `:` type($tensor) `to` type($result)";

  let hasVerifier = 1;
}

def SparseTensor_NumberOfEntriesOp : SparseTensor_Op<"number_of_entries", [Pure]> {
  let summary = "Returns the number of entries that are stored in the tensor.";
  let description = [{
    Returns the number of entries that are stored in the given sparse tensor.
    Note that this is typically the number of nonzero elements in the tensor,
    but since explicit zeros may appear in the storage formats, the more
    accurate nomenclature is used.

    Example:

    ```mlir
    %noe = sparse_tensor.number_of_entries %tensor : tensor<64x64xf64, #CSR>
    ```
  }];

  let arguments = (ins AnySparseTensor:$tensor);
  let results = (outs Index:$result);
  let assemblyFormat = "$tensor attr-dict `:` type($tensor)";
}

def SparseTensor_ConcatenateOp : SparseTensor_Op<"concatenate",
      [Pure, StageWithSortSparseOpInterface]> {
  let summary = "Concatenates a list of tensors into a single tensor.";
  let description = [{
     Concatenates a list input tensors and the output tensor with the same
     dimension-rank.  The concatenation happens on the specified `dimension`
     (0 <= dimension < dimRank).  The resulting `dimension` size is the
     sum of all the input sizes for that dimension, while all the other
     dimensions should have the same size in the input and output tensors.

     Only statically-sized input tensors are accepted, while the output tensor
     can be dynamically-sized.

     Example:

     ```mlir
     %0 = sparse_tensor.concatenate %1, %2 { dimension = 0 : index }
       : tensor<64x64xf64, #CSR>, tensor<64x64xf64, #CSR> to tensor<128x64xf64, #CSR>
     ```
   }];

  let extraClassDeclaration = [{
     // Whether the concatenate can be done by a single step or it would require
     // an extra sort. Inherited from StageWithSortSparseOpInterface.
     bool needsExtraSort();
  }];

  let arguments = (ins Variadic<AnyRankedTensor>:$inputs, DimensionAttr:$dimension);
  let results = (outs AnyRankedTensor:$result);
  let assemblyFormat = "$inputs attr-dict `:` type($inputs) `to` type($result)";

  let hasVerifier = 1;
}

def SparseTensor_ToSliceOffsetOp : SparseTensor_Op<"slice.offset", [Pure]> {
  let summary = "Extracts the offset of the sparse tensor slice at the given dimension";
  let description = [{
    Extracts the offset of the sparse tensor slice at the given dimension.

    Currently, sparse tensor slices are still a work in progress, and only
    works when runtime library is disabled (i.e., running the sparsifier
    with `enable-runtime-library=false`).

    Example:

    ```mlir
    %0 = tensor.extract_slice %s[%v1, %v2][64, 64][1, 1] : tensor<128x128xf64, #DCSR>
                                                        to tensor<64x64xf64, #Slice>

    %1 = sparse_tensor.slice.offset %0 at 0 : tensor<64x64xf64, #Slice>
    %2 = sparse_tensor.slice.offset %0 at 1 : tensor<64x64xf64, #Slice>
    // %1 = %v1
    // %2 = %v2
    ```
  }];

  let arguments = (ins AnySparseTensorSlice:$slice, IndexAttr:$dim);
  let results = (outs Index:$offset);
  let assemblyFormat = "$slice `at` $dim attr-dict `:` type($slice)";

  let hasVerifier = 1;
}

def SparseTensor_ToSliceStrideOp : SparseTensor_Op<"slice.stride", [Pure]> {
  let summary = "Extracts the stride of the sparse tensor slice at the given dimension";
  let description = [{
    Extracts the stride of the sparse tensor slice at the given dimension.

    Currently, sparse tensor slices are still a work in progress, and only
    works when runtime library is disabled (i.e., running the sparsifier
    with `enable-runtime-library=false`).

    Example:

    ```mlir
    %0 = tensor.extract_slice %s[%v1, %v2][64, 64][%s1, %s2] : tensor<128x128xf64, #DCSR>
                                                            to tensor<64x64xf64, #Slice>

    %1 = sparse_tensor.slice.stride %0 at 0 : tensor<64x64xf64, #Slice>
    %2 = sparse_tensor.slice.stride %0 at 1 : tensor<64x64xf64, #Slice>
    // %1 = %s1
    // %2 = %s2

    ```
  }];

  let arguments = (ins AnySparseTensorSlice:$slice, IndexAttr:$dim);
  let results = (outs Index:$stride);
  let assemblyFormat = "$slice `at` $dim attr-dict `:` type($slice)";

  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Sparse Tensor Storage Specifier Operations.
//===----------------------------------------------------------------------===//

def SparseTensor_StorageSpecifierInitOp : SparseTensor_Op<"storage_specifier.init",
      [Pure]> {
  let summary = "";
  let description = [{
    Returns an initial storage specifier value.  A storage specifier
    value holds the level-sizes, position arrays, coordinate arrays,
    and the value array.
    If this is a specifier for slices, it also holds the extra strides/offsets
    for each tensor dimension.

    TODO: The sparse tensor slice support is currently in a unstable state, and
    is subject to change in the future.

    Example:

    ```mlir
    #CSR = #sparse_tensor.encoding<{
      map = (i, j) -> (i : dense, j : compressed)
    }>
    #CSR_SLICE = #sparse_tensor.encoding<{
      map = (d0 : #sparse_tensor<slice(1, 4, 1)>,
             d1 : #sparse_tensor<slice(1, 4, 2)>) ->
            (d0 : dense, d1 : compressed)
    }>

    %0 = sparse_tensor.storage_specifier.init :  !sparse_tensor.storage_specifier<#CSR>
    %1 = sparse_tensor.storage_specifier.init with %src
         : !sparse_tensor.storage_specifier<#CSR> to
           !sparse_tensor.storage_specifier<#CSR_SLICE>
    ```
  }];

  let arguments = (ins Optional<SparseTensorStorageSpecifier>:$source);
  let results = (outs SparseTensorStorageSpecifier:$result);
  let assemblyFormat = "attr-dict (`with` $source^)? `:` (`from` qualified(type($source))^ `to`)?"
                                                        " qualified(type($result))";
  let builders = [
    OpBuilder<(ins "Type":$result),
    [{
      build($_builder, $_state, result, Value());
    }]>
  ];


}

def SparseTensor_GetStorageSpecifierOp : SparseTensor_Op<"storage_specifier.get", [Pure]> {
  let summary = "";
  let description = [{
    Returns the requested field of the given storage_specifier.

    Example of querying the size of the coordinates array for level 0:

    ```mlir
    %0 = sparse_tensor.storage_specifier.get %arg0 crd_mem_sz at 0
         : !sparse_tensor.storage_specifier<#COO>
    ```
  }];

  let arguments = (ins SparseTensorStorageSpecifier:$specifier,
                   SparseTensorStorageSpecifierKindAttr:$specifierKind,
                   OptionalAttr<LevelAttr>:$level);
  let results = (outs Index:$result);
  let assemblyFormat = "$specifier $specifierKind (`at` $level^)? attr-dict"
                       "`:` qualified(type($specifier))";

  let hasVerifier = 1;
  let hasFolder = 1;
}

def SparseTensor_SetStorageSpecifierOp : SparseTensor_Op<"storage_specifier.set",
    [Pure, AllTypesMatch<["result", "specifier"]>]> {
  let summary = "";
  let description = [{
    Set the field of the storage specifier to the given input value. Returns
    the updated storage_specifier as a new SSA value.

    Example of updating the sizes of the coordinates array for level 0:

    ```mlir
    %0 = sparse_tensor.storage_specifier.set %arg0 crd_mem_sz at 0 with %new_sz
       : !sparse_tensor.storage_specifier<#COO>
    ```
  }];

  let arguments = (ins SparseTensorStorageSpecifier:$specifier,
                   SparseTensorStorageSpecifierKindAttr:$specifierKind,
                   OptionalAttr<LevelAttr>:$level,
                   Index:$value);
  let results = (outs SparseTensorStorageSpecifier:$result);
  let assemblyFormat = "$specifier $specifierKind (`at` $level^)? `with` $value"
                       " attr-dict `:` qualified(type($result))";

  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Sparse Tensor Coordinate Operations.
//===----------------------------------------------------------------------===//

def SparseTensor_LvlOp : SparseTensor_Op<"lvl", [ConditionallySpeculatable, NoMemoryEffect]> {
  let summary = "level index operation";
  let description = [{
    The `sparse_tensor.lvl` behaves similar to `tensor.dim` operation.
    It takes a sparse tensor and a level operand of type `index` and returns
    the size of the requested level of the given sparse tensor.
    If the sparse tensor has an identity dimension to level mapping, it returns
    the same result as `tensor.dim`.
    If the level index is out of bounds, the behavior is undefined.

    Example:

    ```mlir
    #BSR = #sparse_tensor.encoding<{
      map = ( i, j ) ->
        ( i floordiv 2 : dense,
          j floordiv 3 : compressed,
          i mod 2      : dense,
          j mod 3      : dense
        )
    }>

    // Always returns 2 (4 floordiv 2), can be constant folded:
    %c0 = arith.constant 0 : index
    %x = sparse_tensor.lvl %A, %c0 : tensor<4x?xf32, #BSR>

    // Return the dynamic dimension of %A computed by %j mod 3.
    %c1 = arith.constant 1 : index
    %y = sparse_tensor.lvl %A, %c1 : tensor<4x?xf32, #BSR>

    // Always return 3 (since j mod 3 < 3), can be constant fold
    %c3 = arith.constant 3 : index
    %y = sparse_tensor.lvl %A, %c3 : tensor<4x?xf32, #BSR>
    ```
  }];

  let arguments = (ins AnySparseTensor:$source, Index:$index);
  let results = (outs Index:$result);
  let assemblyFormat = "attr-dict $source `,` $index `:` type($source) ";

  let builders = [
    OpBuilder<(ins "Value":$source, "int64_t":$index)>
  ];

  let extraClassDeclaration = [{
    /// Helper function to get the index as a simple integer if it is constant.
    std::optional<uint64_t> getConstantLvlIndex();

    /// Interface method for ConditionallySpeculatable.
    Speculation::Speculatability getSpeculatability();
  }];

  let hasVerifier = 1;
  let hasFolder = 1;
}

def SparseTensor_CrdTranslateOp : SparseTensor_Op<"crd_translate", [Pure]> {
  string summary = "Performs coordinate translation between level and dimension coordinate space.";
  string description = [{
    Performs coordinate translation between level and dimension coordinate space according
    to the affine maps defined by $encoder.

    Example:

    ```mlir
    %l0, %l1, %l2, %l3 = sparse_tensor.crd_translate dim_to_lvl [%d0, %d1] as #BSR
                       : index, index, index, index
    ```
  }];

  let arguments = (ins Variadic<Index>:$in_crds,
                   SparseTensorCrdTransDirectionAttr:$direction,
                   SparseTensorEncodingAttr:$encoder);
  let results = (outs Variadic<Index>:$out_crds);
  let assemblyFormat = "$direction `[` $in_crds `]` `as` $encoder attr-dict `:` type($out_crds)";

  let hasVerifier = 1;
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Sparse Tensor Management Operations. These operations are "impure" in the
// sense that some behavior is defined by side-effects. These operations provide
// a bridge between "sparsification" on one hand and a support library or actual
// code generation on the other hand. The semantics of these operations may be
// refined over time as our sparse abstractions evolve.
//===----------------------------------------------------------------------===//

def SparseTensor_PushBackOp : SparseTensor_Op<"push_back",
    [TypesMatchWith<"value type matches element type of inBuffer",
                    "inBuffer", "value",
                    "::llvm::cast<ShapedType>($_self).getElementType()">,
     AllTypesMatch<["inBuffer", "outBuffer"]>]> {
  string summary = "Pushes a value to the back of a given buffer";
  string description = [{
    Pushes `value` to the end of the given sparse tensor storage buffer
    `inBuffer` as indicated by the value of `curSize` and returns the
    new size of the buffer in `newSize` (`newSize = curSize + n`).
    The capacity of the buffer is recorded in the memref type of `inBuffer`.
    If the current buffer is full, then `inBuffer.realloc` is called before
    pushing the data to the buffer. This is similar to std::vector push_back.

    The optional input `n` specifies the number of times to repeately push
    the value to the back of the tensor. When `n` is a compile-time constant,
    its value can't be less than 1. If `n` is a runtime value that is less
    than 1, the behavior is undefined. Although using input `n` is semantically
    equivalent to calling push_back n times, it gives compiler more chances to
    to optimize the memory reallocation and the filling of the memory with the
    same value.

    The `inbounds` attribute tells the compiler that the insertion won't go
    beyond the current storage buffer. This allows the compiler to not generate
    the code for capacity check and reallocation. The typical usage will be for
    "dynamic" sparse tensors for which a capacity can be set beforehand.

    Note that this operation is "impure" in the sense that even though
    the result is modeled through an SSA value, referencing the memref
    through the old SSA value after this operation is undefined behavior.

    Example:

    ```mlir
    %buf, %newSize = sparse_tensor.push_back %curSize, %buffer, %val
       : index, memref<?xf64>, f64
    ```

    ```mlir
    %buf, %newSize = sparse_tensor.push_back inbounds %curSize, %buffer, %val
       : xindex, memref<?xf64>, f64
    ```

    ```mlir
    %buf, %newSize = sparse_tensor.push_back inbounds %curSize, %buffer, %val, %n
       : xindex, memref<?xf64>, f64
    ```
  }];

  let arguments = (ins Index:$curSize,
                       StridedMemRefRankOf<[AnyType], [1]>:$inBuffer,
                       AnyType:$value, Optional<Index>:$n,
                       UnitAttr:$inbounds);
  let results = (outs StridedMemRefRankOf<[AnyType], [1]>:$outBuffer,
                      Index:$newSize);
  let assemblyFormat = "(`inbounds` $inbounds^)? $curSize `,` $inBuffer"
                       " `,` $value (`,` $n^ )?  attr-dict `:`"
                       " type($curSize) `,` type($inBuffer) `,`"
                       " type($value) (`,` type($n)^ )?";

  let builders = [
    // Build an op (reusing type from curSize and inBuffer) without input `n`
    OpBuilder<(ins "Value":$curSize, "Value":$inBuffer, "Value":$value)>
  ];

  let hasVerifier = 1;
}

def SparseTensor_ExpandOp : SparseTensor_Op<"expand", []> {
  string summary = "Expands an access pattern for insertion";
  string description = [{
    Performs an access pattern expansion for the innermost levels of the
    given tensor. This operation is useful to implement kernels in which a
    sparse tensor appears as output. This technique is known under several
    different names and using several alternative implementations,
    for example, phase counter [Gustavson72], expanded or switch array
    [Pissanetzky84], in phase scan [Duff90], access pattern expansion [Bik96],
    and workspaces [Kjolstad19].

    The `values` and `filled` arrays must have lengths equal to the
    level-size of the innermost level (i.e., as if the innermost level
    were *dense*).  The `added` array and `count` are used to store new
    level-coordinates when a false value is encountered in the `filled`
    array.  All arrays should be allocated before the loop (possibly even
    shared between loops in a future optimization) so that their *dense*
    initialization can be amortized over many iterations.  Setting and
    resetting the dense arrays in the loop nest itself is kept *sparse*
    by only iterating over set elements through an indirection using
    the added array, so that the operations are kept proportional to
    the number of nonzeros.

    Note that this operation is "impure" in the sense that even though the
    results are modeled through SSA values, the operation relies on a proper
    side-effecting context that sets and resets the expanded arrays.

    Example:

    ```mlir
    %values, %filled, %added, %count = sparse_tensor.expand %tensor
      : tensor<4x4xf64, #CSR> to memref<?xf64>, memref<?xi1>, memref<?xindex>
    ```
  }];


  let arguments = (ins AnySparseTensor:$tensor);
  let results = (outs AnyStridedMemRefOfRank<1>:$values,
                      StridedMemRefRankOf<[I1],[1]>:$filled,
                      StridedMemRefRankOf<[Index],[1]>:$added,
                      Index:$count);
  let assemblyFormat = "$tensor attr-dict `:` type($tensor) `to` type($values)"
                       " `,` type($filled) `,` type($added)";
}

def SparseTensor_CompressOp : SparseTensor_Op<"compress",
    [AllTypesMatch<["tensor", "result"]>]> {
  string summary = "Compressed an access pattern for insertion";
  string description = [{
    Finishes a single access pattern expansion by moving inserted elements
    into the sparse storage scheme of the given tensor with the given
    level-coordinates.  The arity of `lvlCoords` is one less than the
    level-rank of the tensor, with the coordinate of the innermost
    level defined through the `added` array.  The `values` and `filled`
    arrays are reset in a *sparse* fashion by only iterating over set
    elements through an indirection using the `added` array, so that
    the operations are kept proportional to the number of nonzeros.
    See the `sparse_tensor.expand` operation for more details.

    Note that this operation is "impure" in the sense that even though
    the result is modeled through an SSA value, the insertion is eventually
    done "in place", and referencing the old SSA value is undefined behavior.

    Example:

    ```mlir
    %result = sparse_tensor.compress %values, %filled, %added, %count into %tensor[%i]
      : memref<?xf64>, memref<?xi1>, memref<?xindex>, tensor<4x4xf64, #CSR>
    ```
  }];

  let arguments = (ins AnyStridedMemRefOfRank<1>:$values,
                   StridedMemRefRankOf<[I1],[1]>:$filled,
                   StridedMemRefRankOf<[Index],[1]>:$added,
                   Index:$count,
                   AnySparseTensor:$tensor,
                   Variadic<Index>:$lvlCoords);
  let results = (outs AnySparseTensor:$result);
  let assemblyFormat = "$values `,` $filled `,` $added `,` $count"
                       " `into` $tensor `[` $lvlCoords `]` attr-dict"
                       " `:` type($values) `,` type($filled) `,` type($added)"
                       " `,` type($tensor)";
  let hasVerifier = 1;
}

def SparseTensor_LoadOp : SparseTensor_Op<"load", [SameOperandsAndResultType]> {
  let summary =
    "Rematerializes tensor from underlying sparse storage format";
  let description = [{
    Rematerializes a tensor from the underlying sparse storage format of the
    given tensor. This is similar to the `bufferization.to_tensor` operation
    in the sense that it provides a bridge between a bufferized world view
    and a tensor world view. Unlike the `bufferization.to_tensor` operation,
    however, this sparse operation is used only temporarily to maintain a
    correctly typed intermediate representation during progressive
    bufferization.

    The `hasInserts` attribute denote whether insertions to the underlying
    sparse storage format may have occurred, in which case the underlying
    sparse storage format needs to be finalized. Otherwise, the operation
    simply folds away.

    Note that this operation is "impure" in the sense that even though
    the result is modeled through an SSA value, the operation relies on
    a proper context of materializing and inserting the tensor value.

    Examples:

    ```mlir
    %result = sparse_tensor.load %tensor : tensor<8xf64, #SV>

    %1 = sparse_tensor.load %0 hasInserts : tensor<16x32xf32, #CSR>
    ```
  }];

  let arguments = (ins AnySparseTensor:$tensor, UnitAttr:$hasInserts);
  let results = (outs AnyTensor:$result);
  let assemblyFormat = "$tensor (`hasInserts` $hasInserts^)? attr-dict `:` type($tensor)";
}

def SparseTensor_OutOp : SparseTensor_Op<"out", []> {
  string summary = "Outputs a sparse tensor to the given destination";
  string description = [{
    Outputs the contents of a sparse tensor to the destination defined by an
    opaque pointer provided by `dest`. For targets that have access to a file
    system, for example, this pointer may specify a filename (or file) for output.
    The form of the operation is kept deliberately very general to allow for
    alternative implementations in the future, such as sending the contents to
    a buffer defined by a pointer.

    Note that this operation is "impure" in the sense that its behavior
    is solely defined by side-effects and not SSA values.

    Example:

    ```mlir
    sparse_tensor.out %t, %dest : tensor<1024x1024xf64, #CSR>, !Dest
    ```
  }];

  let arguments = (ins AnySparseTensor:$tensor, AnyType:$dest);
  let assemblyFormat = "$tensor `,` $dest attr-dict `:` type($tensor) `,` type($dest)";
}

//===----------------------------------------------------------------------===//
// Sparse Tensor Sorting/Ordering Operations.
//===----------------------------------------------------------------------===//

def SparseTensor_SortOp : SparseTensor_Op<"sort"> {
  let summary = "Sorts the arrays in xs and ys lexicographically on the "
                "integral values found in the xs list";
  let description = [{
    Sorts the `xs` values along with some `ys` values that are put in a single linear
    buffer `xy`.  The affine map attribute `perm_map` specifies the permutation to be
    applied on the `xs` before comparison, the rank of the permutation map
    also specifies the number of `xs` values in `xy`.
    The optional index attribute `ny` provides the number of `ys` values in `xy`.
    When `ny` is not explicitly specified, its value is 0.
    This instruction supports a more efficient way to store the COO definition
    in sparse tensor type.

    The buffer xy should have a dimension not less than n * (rank(perm_map) + ny) while the
    buffers in `ys` should have a dimension not less than `n`. The behavior of
    the operator is undefined if this condition is not met.

    Example:

    ```mlir
    sparse_tensor.sort insertion_sort_stable %n, %x { perm_map = affine_map<(i,j) -> (j,i)> }
      : memref<?xindex>
    ```
  }];

  let arguments = (ins Index:$n,
                       StridedMemRefRankOf<[AnyInteger, Index], [1]>:$xy,
                       Variadic<StridedMemRefRankOf<[AnyType], [1]>>:$ys,
                       AffineMapAttr:$perm_map, OptionalAttr<IndexAttr>:$ny,
                       SparseTensorSortKindAttr:$algorithm);
  let assemblyFormat = "$algorithm $n"
                       "`,`$xy (`jointly` $ys^)? attr-dict"
                       "`:` type($xy) (`jointly` type($ys)^)?";
  let hasVerifier = 1;
}

def SparseTensor_ReorderCOOOp : SparseTensor_Op<"reorder_coo", [Pure]> {
  let summary = "Reorder the input COO such that it has the the same order as "
                "the output COO";
  let description = [{
    Reorders the input COO to the same order as specified by the output format.
    E.g., reorder an unordered COO into an ordered one.

    The input and result COO tensor must have the same element type, position type and
    coordinate type. At the moment, the operation also only supports ordering
    input and result COO with the same dim2lvl map.

    Example:

    ```mlir
    %res = sparse_tensor.reorder_coo quick_sort %coo : tensor<?x?xf64 : #Unordered_COO> to
                                                       tensor<?x?xf64 : #Ordered_COO>

    ```
  }];

  let arguments = (ins AnySparseTensor: $input_coo,
                       SparseTensorSortKindAttr:$algorithm);
  let results = (outs AnySparseTensor: $result_coo);
  let assemblyFormat = "$algorithm $input_coo attr-dict"
                       "`:` type($input_coo) `to` type($result_coo)";

  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Sparse Tensor Syntax Operations.
//===----------------------------------------------------------------------===//

def SparseTensor_BinaryOp : SparseTensor_Op<"binary", [Pure]> {
  let summary = "Binary set operation utilized within linalg.generic";
  let description = [{
      Defines a computation within a `linalg.generic` operation that takes two
      operands and executes one of the regions depending on whether both operands
      or either operand is nonzero (i.e. stored explicitly in the sparse storage
      format).

      Three regions are defined for the operation and must appear in this order:
      - overlap (elements present in both sparse tensors)
      - left (elements only present in the left sparse tensor)
      - right (element only present in the right sparse tensor)

      Each region contains a single block describing the computation and result.
      Every non-empty block must end with a sparse_tensor.yield and the return
      type must match the type of `output`. The primary region's block has two
      arguments, while the left and right region's block has only one argument.

      A region may also be declared empty (i.e. `left={}`), indicating that the
      region does not contribute to the output. For example, setting both
      `left={}` and `right={}` is equivalent to the intersection of the two
      inputs as only the overlap region will contribute values to the output.

      As a convenience, there is also a special token `identity` which can be
      used in place of the left or right region. This token indicates that
      the return value is the input value (i.e. func(%x) => return %x).
      As a practical example, setting `left=identity` and `right=identity`
      would be equivalent to a union operation where non-overlapping values
      in the inputs are copied to the output unchanged.

      Due to the possibility of empty regions, i.e. lack of a value for certain
      cases, the result of this operation may only feed directly into the output
      of the `linalg.generic` operation or into into a custom reduction
      `sparse_tensor.reduce` operation that follows in the same region.

      Example of isEqual applied to intersecting elements only:

      ```mlir
      %C = tensor.empty(...)
      %0 = linalg.generic #trait
        ins(%A: tensor<?xf64, #SparseVector>,
            %B: tensor<?xf64, #SparseVector>)
        outs(%C: tensor<?xi8, #SparseVector>) {
        ^bb0(%a: f64, %b: f64, %c: i8) :
          %result = sparse_tensor.binary %a, %b : f64, f64 to i8
            overlap={
              ^bb0(%arg0: f64, %arg1: f64):
                %cmp = arith.cmpf "oeq", %arg0, %arg1 : f64
                %ret_i8 = arith.extui %cmp : i1 to i8
                sparse_tensor.yield %ret_i8 : i8
            }
            left={}
            right={}
          linalg.yield %result : i8
      } -> tensor<?xi8, #SparseVector>
      ```

      Example of A+B in upper triangle, A-B in lower triangle:

      ```mlir
      %C = tensor.empty(...)
      %1 = linalg.generic #trait
        ins(%A: tensor<?x?xf64, #CSR>, %B: tensor<?x?xf64, #CSR>
        outs(%C: tensor<?x?xf64, #CSR> {
        ^bb0(%a: f64, %b: f64, %c: f64) :
          %row = linalg.index 0 : index
          %col = linalg.index 1 : index
          %result = sparse_tensor.binary %a, %b : f64, f64 to f64
            overlap={
              ^bb0(%x: f64, %y: f64):
                %cmp = arith.cmpi "uge", %col, %row : index
                %upperTriangleResult = arith.addf %x, %y : f64
                %lowerTriangleResult = arith.subf %x, %y : f64
                %ret = arith.select %cmp, %upperTriangleResult, %lowerTriangleResult : f64
                sparse_tensor.yield %ret : f64
            }
            left=identity
            right={
              ^bb0(%y: f64):
                %cmp = arith.cmpi "uge", %col, %row : index
                %lowerTriangleResult = arith.negf %y : f64
                %ret = arith.select %cmp, %y, %lowerTriangleResult : f64
                sparse_tensor.yield %ret : f64
            }
          linalg.yield %result : f64
      } -> tensor<?x?xf64, #CSR>
      ```

      Example of set difference. Returns a copy of A where its sparse structure
      is *not* overlapped by B. The element type of B can be different than A
      because we never use its values, only its sparse structure:

      ```mlir
      %C = tensor.empty(...)
      %2 = linalg.generic #trait
        ins(%A: tensor<?x?xf64, #CSR>, %B: tensor<?x?xi32, #CSR>
        outs(%C: tensor<?x?xf64, #CSR> {
        ^bb0(%a: f64, %b: i32, %c: f64) :
          %result = sparse_tensor.binary %a, %b : f64, i32 to f64
            overlap={}
            left=identity
            right={}
          linalg.yield %result : f64
      } -> tensor<?x?xf64, #CSR>
      ```
  }];

  let regions = (region AnyRegion:$overlapRegion, AnyRegion:$leftRegion, AnyRegion:$rightRegion);
  let arguments = (ins AnyType:$x, AnyType:$y, UnitAttr:$left_identity, UnitAttr:$right_identity);
  let results = (outs AnyType:$output);
  let assemblyFormat = [{
        $x `,` $y `:` attr-dict type($x) `,` type($y) `to` type($output) `\n`
        `overlap` `=` $overlapRegion `\n`
        `left` `=` (`identity` $left_identity^):($leftRegion)? `\n`
        `right` `=` (`identity` $right_identity^):($rightRegion)?
  }];

  let hasVerifier = 1;
}

def SparseTensor_UnaryOp : SparseTensor_Op<"unary", [Pure]> {

  let arguments = (ins AnyType:$x);

  let results = (outs AnyType:$output);

  let summary = "Unary set operation utilized within linalg.generic";
  let description = [{
      Defines a computation with a `linalg.generic` operation that takes a single
      operand and executes one of two regions depending on whether the operand is
      nonzero (i.e. stored explicitly in the sparse storage format).

      Two regions are defined for the operation must appear in this order:
      - present (elements present in the sparse tensor)
      - absent (elements not present in the sparse tensor)

      Each region contains a single block describing the computation and result.
      A non-empty block must end with a sparse_tensor.yield and the return type
      must match the type of `output`. The primary region's block has one
      argument, while the missing region's block has zero arguments. The
      absent region may only generate constants or values already computed
      on entry of the `linalg.generic` operation.

      A region may also be declared empty (i.e. `absent={}`), indicating that the
      region does not contribute to the output.

      Due to the possibility of empty regions, i.e. lack of a value for certain
      cases, the result of this operation may only feed directly into the output
      of the `linalg.generic` operation or into into a custom reduction
      `sparse_tensor.reduce` operation that follows in the same region.

      Example of A+1, restricted to existing elements:

      ```mlir
      %C = tensor.empty(...) : tensor<?xf64, #SparseVector>
      %0 = linalg.generic #trait
         ins(%A: tensor<?xf64, #SparseVector>)
        outs(%C: tensor<?xf64, #SparseVector>) {
        ^bb0(%a: f64, %c: f64) :
          %result = sparse_tensor.unary %a : f64 to f64
            present={
            ^bb0(%arg0: f64):
              %cf1 = arith.constant 1.0 : f64
              %ret = arith.addf %arg0, %cf1 : f64
              sparse_tensor.yield %ret : f64
            }
            absent={}
          linalg.yield %result : f64
      } -> tensor<?xf64, #SparseVector>
      ```

      Example returning +1 for existing values and -1 for missing values:

      ```mlir
      %p1 = arith.constant  1 : i32
      %m1 = arith.constant -1 : i32
      %C = tensor.empty(...) : tensor<?xi32, #SparseVector>
      %1 = linalg.generic #trait
         ins(%A: tensor<?xf64, #SparseVector>)
        outs(%C: tensor<?xi32, #SparseVector>) {
        ^bb0(%a: f64, %c: i32) :
          %result = sparse_tensor.unary %a : f64 to i32
            present={
            ^bb0(%x: f64):
              sparse_tensor.yield %p1 : i32
            }
            absent={
              sparse_tensor.yield %m1 : i32
            }
          linalg.yield %result : i32
      } -> tensor<?xi32, #SparseVector>
      ```

      Example showing a structural inversion (existing values become missing in
      the output, while missing values are filled with 1):

      ```mlir
      %c1 = arith.constant 1 : i64
      %C = tensor.empty(...) : tensor<?xi64, #SparseVector>
      %2 = linalg.generic #trait
         ins(%A: tensor<?xf64, #SparseVector>)
        outs(%C: tensor<?xi64, #SparseVector>) {
        ^bb0(%a: f64, %c: i64) :
          %result = sparse_tensor.unary %a : f64 to i64
            present={}
            absent={
              sparse_tensor.yield %c1 : i64
            }
          linalg.yield %result : i64
      } -> tensor<?xi64, #SparseVector>
      ```
  }];

  let regions = (region AnyRegion:$presentRegion, AnyRegion:$absentRegion);
  let assemblyFormat = [{
        $x attr-dict `:` type($x) `to` type($output) `\n`
        `present` `=` $presentRegion `\n`
        `absent` `=` $absentRegion
  }];
  let hasVerifier = 1;
}

def SparseTensor_ReduceOp : SparseTensor_Op<"reduce", [Pure, SameOperandsAndResultType]> {
  let summary = "Custom reduction operation utilized within linalg.generic";
  let description = [{
      Defines a computation with a `linalg.generic` operation that takes two
      operands and an identity value and reduces all stored values down to a
      single result based on the computation in the region.

      The region must contain exactly one block taking two arguments. The block
      must end with a sparse_tensor.yield and the output must match the input
      argument types.

      Note that this operation is only required for custom reductions beyond
      the standard reduction operations (add, sub, or, xor) that can be
      sparsified by merely reducing the stored values. More elaborate reduction
      operations (mul, and, min, max, etc.) would need to account for implicit
      zeros as well. They can still be handled using this custom reduction
      operation. The `linalg.generic` `iterator_types` defines which indices
      are being reduced. When the associated operands are used in an operation,
      a reduction will occur. The use of this explicit `reduce` operation
      is not required in most cases.

      Example of Matrix->Vector reduction using max(product(x_i), 100):

      ```mlir
      %cf1 = arith.constant 1.0 : f64
      %cf100 = arith.constant 100.0 : f64
      %C = tensor.empty(...)
      %0 = linalg.generic #trait
         ins(%A: tensor<?x?xf64, #SparseMatrix>)
        outs(%C: tensor<?xf64, #SparseVector>) {
        ^bb0(%a: f64, %c: f64) :
          %result = sparse_tensor.reduce %c, %a, %cf1 : f64 {
              ^bb0(%arg0: f64, %arg1: f64):
                %0 = arith.mulf %arg0, %arg1 : f64
                %cmp = arith.cmpf "ogt", %0, %cf100 : f64
                %ret = arith.select %cmp, %cf100, %0 : f64
                sparse_tensor.yield %ret : f64
            }
          linalg.yield %result : f64
      } -> tensor<?xf64, #SparseVector>
      ```
  }];

  let regions = (region SizedRegion<1>:$region);
  let arguments = (ins AnyType:$x, AnyType:$y, AnyType:$identity);
  let results = (outs AnyType:$output);
  let assemblyFormat = "$x `,` $y `,` $identity attr-dict `:` type($output) $region";

  let hasVerifier = 1;
}

def SparseTensor_SelectOp : SparseTensor_Op<"select", [Pure, SameOperandsAndResultType]> {
  let summary = "Select operation utilized within linalg.generic";
  let description = [{
      Defines an evaluation within a `linalg.generic` operation that takes a single
      operand and decides whether or not to keep that operand in the output.

      A single region must contain exactly one block taking one argument. The block
      must end with a sparse_tensor.yield and the output type must be boolean.

      Value threshold is an obvious usage of the select operation. However, by using
      `linalg.index`, other useful selection can be achieved, such as selecting the
      upper triangle of a matrix.

      Example of selecting A >= 4.0:

      ```mlir
      %C = tensor.empty(...)
      %0 = linalg.generic #trait
         ins(%A: tensor<?xf64, #SparseVector>)
        outs(%C: tensor<?xf64, #SparseVector>) {
        ^bb0(%a: f64, %c: f64) :
          %result = sparse_tensor.select %a : f64 {
              ^bb0(%arg0: f64):
                %cf4 = arith.constant 4.0 : f64
                %keep = arith.cmpf "uge", %arg0, %cf4 : f64
                sparse_tensor.yield %keep : i1
            }
          linalg.yield %result : f64
      } -> tensor<?xf64, #SparseVector>
      ```

      Example of selecting lower triangle of a matrix:

      ```mlir
      %C = tensor.empty(...)
      %1 = linalg.generic #trait
         ins(%A: tensor<?x?xf64, #CSR>)
        outs(%C: tensor<?x?xf64, #CSR>) {
        ^bb0(%a: f64, %c: f64) :
          %row = linalg.index 0 : index
          %col = linalg.index 1 : index
          %result = sparse_tensor.select %a : f64 {
              ^bb0(%arg0: f64):
                %keep = arith.cmpf "olt", %col, %row : f64
                sparse_tensor.yield %keep : i1
            }
          linalg.yield %result : f64
      } -> tensor<?x?xf64, #CSR>
      ```
  }];

  let regions = (region SizedRegion<1>:$region);
  let arguments = (ins AnyType:$x);
  let results = (outs AnyType:$output);
  let assemblyFormat = "$x attr-dict `:` type($x) $region";

  let hasVerifier = 1;
}

def SparseTensor_YieldOp : SparseTensor_Op<"yield", [Pure, Terminator,
    ParentOneOf<["BinaryOp", "UnaryOp", "ReduceOp", "SelectOp",
                 "ForeachOp", "IterateOp", "CoIterateOp"]>]> {
  let summary = "Yield from sparse_tensor set-like operations";
  let description = [{
      Yields a value from within a `binary`, `unary`, `reduce`,
      `select` or `foreach` block.

      Example:

      ```mlir
      %0 = sparse_tensor.unary %a : i64 to i64 {
        present={
          ^bb0(%arg0: i64):
            %cst = arith.constant 1 : i64
            %ret = arith.addi %arg0, %cst : i64
            sparse_tensor.yield %ret : i64
        }
      }
      ```
  }];

  let builders = [
    OpBuilder<(ins),
    [{
      build($_builder, $_state, ValueRange());
    }]>,
    OpBuilder<(ins "Value":$yieldVal),
    [{
      build($_builder, $_state, ValueRange(yieldVal));
    }]>
  ];

  let extraClassDeclaration = [{
     Value getSingleResult() {
        assert(hasSingleResult());
        return getResults().front();
     }
     bool hasSingleResult() {
        return getResults().size() == 1;
     }
  }];

  let arguments = (ins Variadic<AnyType>:$results);
  let assemblyFormat = "$results attr-dict `:` type($results)";
}

def SparseTensor_ForeachOp : SparseTensor_Op<"foreach",
    [SingleBlockImplicitTerminator<"YieldOp">]> {
  let summary = "Iterates over elements in a tensor";
  let description = [{
     Iterates over stored elements in a tensor (which are typically, but not always,
     non-zero for sparse tensors) and executes the block.

     `tensor`: the input tensor to iterate over.
     `initArgs`: the initial loop argument to carry and update during each iteration.
     `order`: an optional permutation affine map that specifies the order in which
     the dimensions are visited (e.g., row first or column first). This is only
     applicable when the input tensor is a non-annotated dense tensor.

     For an input tensor with dim-rank `n`, the block must take `n + 1`
     arguments (plus additional loop-carried variables as described below).
     The first `n` arguments provide the dimension-coordinates of the element
     being visited, and must all have `index` type.  The `(n+1)`-th argument
     provides the element's value, and must have the tensor's element type.

     `sparse_tensor.foreach` can also operate on loop-carried variables and returns
     the final values after loop termination. The initial values of the variables are
     passed as additional SSA operands to the "sparse_tensor.foreach" following the n + 1
     SSA values mentioned above (n coordinates and 1 value).

     The region must terminate with a "sparse_tensor.yield" that passes the current
     values of all loop-carried variables to the next iteration, or to the
     result, if at the last iteration. The number and static types of loop-carried
     variables may not change with iterations.

     For example:
     ```mlir
     %c0 = arith.constant 0 : i32
     %ret = sparse_tensor.foreach in %0 init(%c0): tensor<?x?xi32, #DCSR>, i32 -> i32 do {
      ^bb0(%arg1: index, %arg2: index, %arg3: i32, %iter: i32):
        %sum = arith.add %iter, %arg3
        sparse_tensor.yield %sum
     }
     ```

     It is important to note that the generated loop iterates over
     elements in their storage order.  However, regardless of the
     storage scheme used by the tensor, the block is always given
     the dimension-coordinates.

     For example:
     ```mlir
     #COL_MAJOR = #sparse_tensor.encoding<{
       map = (d0, d1) -> (d1 : compressed, d0 : compressed)
     }>

     // foreach on a column-major sparse tensor
     sparse_tensor.foreach in %0 : tensor<2x3xf64, #COL_MAJOR> do {
      ^bb0(%row: index, %col: index, %arg3: f64):
         // [%row, %col] -> [0, 0], [1, 0], [2, 0], [0, 1], [1, 1], [2, 1]
     }

     #ROW_MAJOR = #sparse_tensor.encoding<{
       map = (d0, d1) -> (d0 : compressed, d1 : compressed)
     }>

     // foreach on a row-major sparse tensor
     sparse_tensor.foreach in %0 : tensor<2x3xf64, #ROW_MAJOR> do {
      ^bb0(%row: index, %col: index, %arg3: f64):
         // [%row, %col] -> [0, 0], [0, 1], [1, 0], [1, 1], [2, 0], [2, 1]
     }

     // foreach on a row-major dense tensor but visit column first
     sparse_tensor.foreach in %0 {order=affine_map<(i,j)->(j,i)>}: tensor<2x3xf64> do {
      ^bb0(%row: index, %col: index, %arg3: f64):
         // [%row, %col] -> [0, 0], [1, 0], [2, 0], [0, 1], [1, 1], [2, 1]
     }

     ```
  }];

  let builders = [
    OpBuilder<(ins "Value":$tensor, "ValueRange":$iterArgs, "AffineMapAttr":$order,
      "function_ref<void(OpBuilder &, Location, ValueRange, Value, ValueRange)>")>,
    OpBuilder<(ins "Value":$tensor, "AffineMapAttr":$order,
      "function_ref<void(OpBuilder &, Location, ValueRange, Value, ValueRange)>":$bodyBuilder),
    [{
      build($_builder, $_state, tensor, ValueRange(), order, bodyBuilder);
    }]>,
    OpBuilder<(ins "Value":$tensor,
      "function_ref<void(OpBuilder &, Location, ValueRange, Value, ValueRange)>":$bodyBuilder),
    [{
      build($_builder, $_state, tensor, ValueRange(), nullptr, bodyBuilder);
    }]>,
    OpBuilder<(ins "Value":$tensor, "ValueRange":$iterArgs,
      "function_ref<void(OpBuilder &, Location, ValueRange, Value, ValueRange)>":$bodyBuilder),
    [{
      build($_builder, $_state, tensor, iterArgs, nullptr, bodyBuilder);
    }]>
  ];

  let regions = (region SizedRegion<1>:$region);
  let arguments = (ins AnyRankedTensor:$tensor,
                       Variadic<AnyType>:$initArgs,
                       OptionalAttr<AffineMapAttr>:$order);
  let results = (outs Variadic<AnyType>:$results);
  let assemblyFormat = "`in` $tensor (`init``(`$initArgs^`)`)? attr-dict"
                       "    `:` type($tensor) (`,` type($initArgs)^)?"
                       "  (`->` type($results)^)?  `do` $region";
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// Sparse Tensor Iteration Operations.
//===----------------------------------------------------------------------===//

def ExtractIterSpaceOp : SparseTensor_Op<"extract_iteration_space",
    [Pure, DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "Extracts an iteration space from a sparse tensor between certain levels";
  let description = [{
      Extracts a `!sparse_tensor.iter_space` from a sparse tensor between
      certain (consecutive) levels. For sparse levels, it is usually done by
      loading a postion range from the underlying sparse tensor storage.
      E.g., for a compressed level, the iteration space is extracted by
      [pos[i], pos[i+1]) supposing the the parent iterator points at `i`.

      `tensor`: the input sparse tensor that defines the iteration space.
      `parentIter`: the iterator for the previous level, at which the iteration space
      at the current levels will be extracted.
      `loLvl`, `hiLvl`: the level range between [loLvl, hiLvl) in the input tensor that
      the returned iteration space covers. `hiLvl - loLvl` defines the dimension of the
      iteration space.

      The type of returned the value is must be
      `!sparse_tensor.iter_space<#INPUT_ENCODING, lvls = $loLvl to $hiLvl>`.
      The returned iteration space can then be iterated over by
      `sparse_tensor.iterate` operations to visit every stored element
      (usually nonzeros) in the input sparse tensor.

      Example:
      ```mlir
      // Extracts a 1-D iteration space from a COO tensor at level 1.
      %space = sparse_tensor.iteration.extract_space %sp at %it1 lvls = 1
        : tensor<4x8xf32, #COO>, !sparse_tensor.iterator<#COO, lvls = 0>
       ->!sparse_tensor.iter_space<#COO, lvls = 1>
      ```
  }];

  let arguments = (ins AnySparseTensor:$tensor,
                       Optional<AnySparseIterator>:$parentIter,
                       LevelAttr:$loLvl, LevelAttr:$hiLvl);
  let results = (outs AnySparseIterSpace:$extractedSpace);

  let extraClassDeclaration = [{
    std::pair<Level, Level> getLvlRange() {
      return std::make_pair(getLoLvl(), getHiLvl());
    }
    unsigned getSpaceDim() {
      return getHiLvl() - getLoLvl();
    }
    ArrayRef<::mlir::sparse_tensor::LevelType> getSpaceLvlTypes() {
      return getExtractedSpace().getType().getLvlTypes();
    }
  }];

  let builders = [
    // Construct a 1-D iteration space.
    OpBuilder<(ins "Value":$tensor, "Value":$parentIter,
                   "sparse_tensor::Level":$loLvl),
    [{
      build($_builder, $_state, tensor, parentIter, loLvl, loLvl + 1);
    }]>,
    // Construct a 1-D root iteration space
    OpBuilder<(ins "Value":$tensor),
    [{
      build($_builder, $_state, tensor, nullptr, 0);
    }]>
  ];

  let assemblyFormat = "$tensor (`at` $parentIter^)? `lvls` `=` custom<LevelRange>($loLvl, $hiLvl) "
                       " attr-dict `:` type($tensor) (`,` type($parentIter)^)? "
                       "`->` qualified(type($extractedSpace))";

  let hasVerifier = 1;
}

def ExtractValOp : SparseTensor_Op<"extract_value", [
    Pure,
    TypesMatchWith<"result type matches element type of tensor",
                   "tensor", "result",
                   "::llvm::cast<TensorType>($_self).getElementType()">]> {
  let summary = "Extracts a value from a sparse tensor using an iterator.";
  let description = [{
      The `sparse_tensor.extract_value` operation extracts the value
      pointed to by a sparse iterator from a sparse tensor.

      Example:

      ```mlir
      %val = sparse_tensor.extract_value %sp at %it
           : tensor<?x?xf32, #CSR>, !sparse_tensor.iterator<#CSR, lvl = 1>
      ```
  }];

  let arguments = (ins AnySparseTensor:$tensor, AnySparseIterator:$iterator);
  let results = (outs AnyType:$result);

  let assemblyFormat = "$tensor `at` $iterator attr-dict `:` type($tensor)`,` qualified(type($iterator))";
  let hasVerifier = 1;
}

def IterateOp : SparseTensor_Op<"iterate",
    [RecursiveMemoryEffects, RecursivelySpeculatable,
     DeclareOpInterfaceMethods<LoopLikeOpInterface,
      ["getInitsMutable", "getLoopResults", "getRegionIterArgs",
       "getYieldedValuesMutable"]>,
     DeclareOpInterfaceMethods<RegionBranchOpInterface,
      ["getEntrySuccessorOperands"]>,
     SingleBlockImplicitTerminator<"sparse_tensor::YieldOp">]> {

  let summary = "Iterates over a sparse iteration space";
  let description = [{
      The `sparse_tensor.iterate` operation represents a loop (nest) over
      the provided iteration space extracted from a specific sparse tensor.
      The operation defines an SSA value for a sparse iterator that points
      to the current stored element in the sparse tensor and SSA values
      for coordinates of the stored element. The coordinates are always
      converted to `index` type despite of the underlying sparse tensor
      storage. When coordinates are not used, the SSA values can be skipped
      by `_` symbols, which usually leads to simpler generated code after
      sparsification. For example:

      ```mlir
      // The coordinate for level 0 is not used when iterating over a 2-D
      // iteration space.
      %sparse_tensor.iterate %iterator in %space at(_, %crd_1)
        : !sparse_tensor.iter_space<#CSR, lvls = 0 to 2>
      ```

      `sparse_tensor.iterate` can also operate on loop-carried variables.
      It returns the final values after loop termination.
      The initial values of the variables are passed as additional SSA operands
      to the iterator SSA value and used coordinate SSA values mentioned
      above. The operation region has an argument for the iterator, variadic
      arguments for specified (used) coordiates and followed by one argument
      for each loop-carried variable, representing the value of the variable
      at the current iteration.
      The body region must contain exactly one block that terminates with
      `sparse_tensor.yield`.

      The results of an `sparse_tensor.iterate` hold the final values after
      the last iteration. If the `sparse_tensor.iterate` defines any values,
      a yield must be explicitly present.
      The number and types of the `sparse_tensor.iterate` results must match
      the initial values in the iter_args binding and the yield operands.


      A nested `sparse_tensor.iterate` example that prints all the coordinates
      stored in the sparse input:

      ```mlir
      func.func @nested_iterate(%sp : tensor<4x8xf32, #COO>) {
        // Iterates over the first level of %sp
        %l1 = sparse_tensor.extract_iteration_space %sp lvls = 0
            : tensor<4x8xf32, #COO> -> !sparse_tensor.iter_space<#COO, lvls = 0 to 1>
        %r1 = sparse_tensor.iterate %it1 in %l1 at (%coord0)
            : !sparse_tensor.iter_space<#COO, lvls = 0 to 1>  {
          // Iterates over the second level of %sp
          %l2 = sparse_tensor.extract_iteration_space %sp at %it1 lvls = 1
              : tensor<4x8xf32, #COO>, !sparse_tensor.iterator<#COO, lvls = 0 to 1>
             -> !sparse_tensor.iter_space<#COO, lvls = 1 to 2>
          %r2 = sparse_tensor.iterate %it2 in %l2 at (coord1)
              : !sparse_tensor.iter_space<#COO, lvls = 1 to 2>  {
             vector.print %coord0 : index
             vector.print %coord1 : index
          }
        }
      }

      ```
  }];

  let arguments = (ins AnySparseIterSpace:$iterSpace,
                       Variadic<AnyType>:$initArgs,
                       I64BitSetAttr:$crdUsedLvls);
  let results = (outs Variadic<AnyType>:$results);
  let regions = (region SizedRegion<1>:$region);

  let skipDefaultBuilders = 1;
  let builders = [
    OpBuilder<(ins "Value":$iterSpace, "ValueRange":$initArgs)>,
    OpBuilder<(ins "Value":$iterSpace, "ValueRange":$initArgs, "I64BitSet" :$crdUsedLvls)>
  ];

  let extraClassDeclaration = [{
    unsigned getSpaceDim() {
      return getIterSpace().getType().getSpaceDim();
    }
    BlockArgument getIterator() {
      return getRegion().getArguments().back();
    }
    std::optional<BlockArgument> getLvlCrd(Level lvl) {
      if (getCrdUsedLvls()[lvl]) {
        uint64_t mask = (static_cast<uint64_t>(0x01u) << lvl) - 1;
        return getCrds()[llvm::popcount(mask & getCrdUsedLvls())];
      }
      return std::nullopt;
    }
    Block::BlockArgListType getCrds() {
      // User-provided iteration arguments -> coords -> iterator.
      return getRegion().getArguments().slice(getNumRegionIterArgs(), getCrdUsedLvls().count());
    }
    unsigned getNumRegionIterArgs() {
      return getRegion().getArguments().size() - 1 - getCrdUsedLvls().count();
    }
  }];

  let hasVerifier = 1;
  let hasRegionVerifier = 1;
  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
}

def SparseTensor_CoIterateOp : SparseTensor_Op<"coiterate",
    [AttrSizedOperandSegments,
     SingleBlockImplicitTerminator<"sparse_tensor::YieldOp">,
     RecursiveMemoryEffects]> {
  let summary = "Co-iterates over a set of sparse iteration spaces";
  let description = [{
      The `sparse_tensor.coiterate` operation represents a loop (nest) over
      a set of iteration spaces. The operation can have multiple regions,
      with each of them defining a case to compute a result at the current iterations.
      The case condition is defined solely based on the pattern of specified iterators.
      For example:
      ```mlir
      %ret = sparse_tensor.coiterate (%sp1, %sp2) at(%coord) iter_args(%arg = %init)
           : (!sparse_tensor.iter_space<#CSR, lvls = 0>,
              !sparse_tensor.iter_space<#COO, lvls = 0>)
           -> index
      case %it1, _ {
        // %coord is specifed in space %sp1 but *NOT* specified in space %sp2.
      }
      case %it1, %it2 {
        // %coord is specifed in *BOTH* spaces %sp1 and %sp2.
      }
      ```

      `sparse_tensor.coiterate` can also operate on loop-carried variables.
      It returns the final value for each loop-carried variable after loop termination.
      The initial values of the variables are passed as additional SSA operands
      to the iterator SSA value and used coordinate SSA values.
      Each operation region has variadic arguments for specified (used), one argument
      for each loop-carried variable, representing the value of the variable
      at the current iteration, followed by a list of arguments for iterators.
      The body region must contain exactly one block that terminates with
      `sparse_tensor.yield`.

      The results of an `sparse_tensor.coiterate` hold the final values after
      the last iteration. If the `sparse_tensor.coiterate` defines any values,
      a yield must be explicitly present in every region defined in the operation.
      The number and types of the `sparse_tensor.coiterate` results must match
      the initial values in the iter_args binding and the yield operands.


      A `sparse_tensor.coiterate` example that does elementwise addition between two
      sparse vectors.


      ```mlir
      %ret = sparse_tensor.coiterate (%sp1, %sp2) at(%coord) iter_args(%arg = %init)
           : (!sparse_tensor.iter_space<#CSR, lvls = 0>,
              !sparse_tensor.iter_space<#CSR, lvls = 0>)
           -> tensor<?xindex, #CSR>
      case %it1, _ {
         // v = v1 + 0 = v1
         %v1 = sparse_tensor.extract_value %t1 at %it1 : index
         %yield = sparse_tensor.insert %v1 into %arg[%coord]
         sparse_tensor.yield %yield
      }
      case _, %it2 {
         // v = v2 + 0 = v2
         %v2 = sparse_tensor.extract_value %t2 at %it2 : index
         %yield = sparse_tensor.insert %v1 into %arg[%coord]
         sparse_tensor.yield %yield
      }
      case %it1, %it2 {
         // v = v1 + v2
         %v1 = sparse_tensor.extract_value %t1 at %it1 : index
         %v2 = sparse_tensor.extract_value %t2 at %it2 : index
         %v = arith.addi %v1, %v2 : index
         %yield = sparse_tensor.insert %v into %arg[%coord]
         sparse_tensor.yield %yield
      }
      ```
  }];

  let arguments = (ins Variadic<AnySparseIterSpace>:$iterSpaces,
                       Variadic<AnyType>:$initArgs,
                       I64BitSetAttr:$crdUsedLvls,
                       I64BitSetArrayAttr:$cases);
  let results = (outs Variadic<AnyType>:$results);
  let regions = (region VariadicRegion<SizedRegion<1>>:$caseRegions);

  let builders = [
    OpBuilder<(ins "ValueRange":$iterSpace, "ValueRange":$initArgs, "unsigned":$numCases)>,
  ];

  let extraClassDeclaration = [{
    unsigned getSpaceDim() {
      return llvm::cast<::mlir::sparse_tensor::IterSpaceType>(
                 getIterSpaces().front().getType())
          .getSpaceDim();
    }
    I64BitSet getRegionDefinedSpace(unsigned regionIdx) {
      return I64BitSet(llvm::cast<IntegerAttr>(getCases()[regionIdx])
                           .getValue().getZExtValue());
    }
    auto getRegionDefinedSpaces() {
      return llvm::map_range(getCases().getValue(), [](Attribute attr) {
        return I64BitSet(llvm::cast<IntegerAttr>(attr).getValue().getZExtValue());
      });
    }

    // The block arguments starts with user-provided iteration arguments,
    // follows by referenced coordinates and ends with iterators.
    Block::BlockArgListType getCrds(unsigned regionIdx) {
      return getRegion(regionIdx).getArguments()
          .slice(getNumRegionIterArgs(), getCrdUsedLvls().count());
    }
    unsigned getNumRegionIterArgs() {
      return getInitArgs().size();
    }
    Block::BlockArgListType getRegionIterArgs(unsigned regionIdx) {
      return getRegion(regionIdx).getArguments()
          .take_front(getNumRegionIterArgs());
    }
    Block::BlockArgListType getRegionIterators(unsigned regionIdx) {
      return getRegion(regionIdx).getArguments()
          .take_back(getRegionDefinedSpace(regionIdx).count());
    }
    ValueRange getYieldedValues(unsigned regionIdx);

    // Returns a vector of regions that are the `sub-cases` of the given case region.
    // E.g., `case %it1, _, %it3` is a subcase of `case %it1, %it2, %it3`.
    SmallVector<Region *> getSubCasesOf(unsigned regionIdx);
  }];

  let hasVerifier = 1;
  let hasRegionVerifier = 1;
  let hasCustomAssemblyFormat = 1;
}

//===----------------------------------------------------------------------===//
// Sparse Tensor Debugging and Test-Only Operations.
//===----------------------------------------------------------------------===//

def SparseTensor_PrintOp : SparseTensor_Op<"print"> {
  string summary = "Prints a sparse tensor (for testing and debugging)";
  string description = [{
    Prints the individual components of a sparse tensors (the positions,
    coordinates, and values components) to stdout for testing and debugging
    purposes. This operation lowers to just a few primitives in a light-weight
    runtime support to simplify supporting this operation on new platforms.

    Example:

    ```mlir
    sparse_tensor.print %tensor : tensor<1024x1024xf64, #CSR>
    ```
  }];

  let arguments = (ins AnySparseTensor:$tensor);
  let assemblyFormat = "$tensor attr-dict `:` type($tensor)";
}

def SparseTensor_HasRuntimeLibraryOp
    : SparseTensor_Op<"has_runtime_library", []>, Results<(outs I1:$result)> {
  string summary = "Indicates whether running in runtime/codegen mode";
  string description = [{
    Returns a boolean value that indicates whether the sparsifier runs in
    runtime library mode or not. For testing only! This operation is useful
    for writing test cases that require different code depending on
    runtime/codegen mode.

    Example:

    ```mlir
    %has_runtime = sparse_tensor.has_runtime_library
    scf.if %has_runtime {
      ...
    }
    ```
  }];
  let assemblyFormat = "attr-dict";
}

#endif // SPARSETENSOR_OPS


//===- ArithOps.td - Arith op definitions ------------------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef ARITH_OPS
#define ARITH_OPS

include "mlir/Dialect/Arith/IR/ArithBase.td"
include "mlir/Dialect/Arith/IR/ArithOpsInterfaces.td"
include "mlir/Interfaces/CastInterfaces.td"
include "mlir/Interfaces/ControlFlowInterfaces.td"
include "mlir/Interfaces/InferIntRangeInterface.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/Interfaces/VectorInterfaces.td"
include "mlir/IR/BuiltinAttributeInterfaces.td"
include "mlir/IR/OpAsmInterface.td"
include "mlir/IR/EnumAttr.td"

// Base class for Arith dialect ops. Ops in this dialect have no memory
// effects and can be applied element-wise to vectors and tensors.
class Arith_Op<string mnemonic, list<Trait> traits = []> :
    Op<Arith_Dialect, mnemonic,
       traits #
       [DeclareOpInterfaceMethods<VectorUnrollOpInterface>, NoMemoryEffect] #
       ElementwiseMappable.traits>;

// Base class for integer and floating point arithmetic ops. All ops have one
// result, require operands and results to be of the same type, and can accept
// tensors or vectors of integers or floats.
class Arith_ArithOp<string mnemonic, list<Trait> traits = []> :
    Arith_Op<mnemonic, traits # [SameOperandsAndResultType]>;

// Base class for unary arithmetic operations.
class Arith_UnaryOp<string mnemonic, list<Trait> traits = []> :
    Arith_ArithOp<mnemonic, traits # [Pure]> {
  let assemblyFormat = "$operand attr-dict `:` type($result)";
}

// Base class for binary arithmetic operations.
class Arith_BinaryOp<string mnemonic, list<Trait> traits = []> :
    Arith_ArithOp<mnemonic, traits> {
  let assemblyFormat = "$lhs `,` $rhs attr-dict `:` type($result)";
}

// Base class for integer binary operations.
class Arith_IntBinaryOp<string mnemonic, list<Trait> traits = []> :
    Arith_BinaryOp<mnemonic, traits #
      [DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>]>,
    Arguments<(ins SignlessIntegerLike:$lhs, SignlessIntegerLike:$rhs)>,
    Results<(outs SignlessIntegerLike:$result)>;

// Base class for integer binary operations without undefined behavior.
class Arith_TotalIntBinaryOp<string mnemonic, list<Trait> traits = []> :
    Arith_IntBinaryOp<mnemonic, traits # [Pure]>;

// Base class for floating point unary operations.
class Arith_FloatUnaryOp<string mnemonic, list<Trait> traits = []> :
    Arith_UnaryOp<mnemonic,
      !listconcat([DeclareOpInterfaceMethods<ArithFastMathInterface>],
                  traits)>,
    Arguments<(ins FloatLike:$operand,
      DefaultValuedAttr<
        Arith_FastMathAttr, "::mlir::arith::FastMathFlags::none">:$fastmath)>,
    Results<(outs FloatLike:$result)> {
  let assemblyFormat = [{ $operand (`fastmath` `` $fastmath^)?
                          attr-dict `:` type($result) }];
}

// Base class for floating point binary operations.
class Arith_FloatBinaryOp<string mnemonic, list<Trait> traits = []> :
    Arith_BinaryOp<mnemonic,
      !listconcat([Pure, DeclareOpInterfaceMethods<ArithFastMathInterface>],
                  traits)>,
    Arguments<(ins FloatLike:$lhs, FloatLike:$rhs,
      DefaultValuedAttr<
        Arith_FastMathAttr, "::mlir::arith::FastMathFlags::none">:$fastmath)>,
    Results<(outs FloatLike:$result)> {
  let assemblyFormat = [{ $lhs `,` $rhs (`fastmath` `` $fastmath^)?
                          attr-dict `:` type($result) }];
}

// Checks that tensor input and outputs have identical shapes. This is stricker
// than the verification done in `SameOperandsAndResultShape` that allows for
// tensor dimensions to be 'compatible' (e.g., dynamic dimensions being
// compatible with static ones).
def SameInputOutputTensorDims : PredOpTrait<
    "input and output have the same tensor dimensions",
    AllMatchSameOperatorPred<["in", "out"],
      "(::llvm::isa<::mlir::TensorType>($_self.getType()) ?"
      " ::llvm::cast<::mlir::TensorType>($_self.getType()).getShape() :"
      " ::llvm::ArrayRef<int64_t>{})">>;

// Base class for arithmetic cast operations. Requires a single operand and
// result. If either is a shaped type, then the other must be of the same
// shape.  In the case of tensor types, this also includes the corresponding
// operand/result dimensions being equal.
class Arith_CastOp<string mnemonic, TypeConstraint From, TypeConstraint To,
                   list<Trait> traits = []> :
    Arith_Op<mnemonic, traits # [Pure, SameOperandsAndResultShape,
      SameInputOutputTensorDims, DeclareOpInterfaceMethods<CastOpInterface>]>,
    Arguments<(ins From:$in)>,
    Results<(outs To:$out)> {
  let assemblyFormat = "$in attr-dict `:` type($in) `to` type($out)";
}

// Casts do not accept indices. Type constraint for signless-integer-like types
// excluding indices: signless integers, vectors or tensors thereof.
def SignlessFixedWidthIntegerLike : TypeConstraint<Or<[
        AnySignlessInteger.predicate,
        VectorOfAnyRankOf<[AnySignlessInteger]>.predicate,
        TensorOf<[AnySignlessInteger]>.predicate]>,
    "signless-fixed-width-integer-like">;

// Cast from an integer type to another integer type.
class Arith_IToICastOp<string mnemonic, list<Trait> traits = []> :
    Arith_CastOp<mnemonic, SignlessFixedWidthIntegerLike,
                           SignlessFixedWidthIntegerLike,
                           traits #
                           [DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>]>;
// Cast from an integer type to a floating point type.
class Arith_IToFCastOp<string mnemonic, list<Trait> traits = []> :
    Arith_CastOp<mnemonic, SignlessFixedWidthIntegerLike, FloatLike, traits>;
// Cast from a floating point type to an integer type.
class Arith_FToICastOp<string mnemonic, list<Trait> traits = []> :
    Arith_CastOp<mnemonic, FloatLike, SignlessFixedWidthIntegerLike, traits>;
// Cast from a floating point type to another floating point type.
class Arith_FToFCastOp<string mnemonic, list<Trait> traits = []> :
    Arith_CastOp<mnemonic, FloatLike, FloatLike, traits>;

// Base class for compare operations. Requires two operands of the same type
// and returns a single `BoolLike` result. If the operand type is a vector or
// tensor, then the result will be one of `i1` of the same shape.
class Arith_CompareOp<string mnemonic, list<Trait> traits = []> :
    Arith_Op<mnemonic, traits # [Pure, SameTypeOperands, TypesMatchWith<
    "result type has i1 element type and same shape as operands",
    "lhs", "result", "::getI1SameShape($_self)">]> {
  let results = (outs BoolLike:$result);

  let assemblyFormat = "$predicate `,` $lhs `,` $rhs attr-dict `:` type($lhs)";
}

// Just like `Arith_CompareOp` but also admits 0-D vectors. Introduced
// temporarily to allow gradual transition to 0-D vectors.
class Arith_CompareOpOfAnyRank<string mnemonic, list<Trait> traits = []> :
    Arith_CompareOp<mnemonic, traits> {
  let results = (outs BoolLikeOfAnyRank:$result);
}

class Arith_IntBinaryOpWithOverflowFlags<string mnemonic, list<Trait> traits = []> :
    Arith_BinaryOp<mnemonic, traits #
      [Pure, DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>,
       DeclareOpInterfaceMethods<ArithIntegerOverflowFlagsInterface>]>,
    Arguments<(ins SignlessIntegerLike:$lhs, SignlessIntegerLike:$rhs,
      DefaultValuedAttr<
        Arith_IntegerOverflowAttr,
        "::mlir::arith::IntegerOverflowFlags::none">:$overflowFlags)>,
    Results<(outs SignlessIntegerLike:$result)> {

  let assemblyFormat = [{ $lhs `,` $rhs (`overflow` `` $overflowFlags^)?
                          attr-dict `:` type($result) }];
}

//===----------------------------------------------------------------------===//
// ConstantOp
//===----------------------------------------------------------------------===//

def Arith_ConstantOp : Op<Arith_Dialect, "constant",
    [ConstantLike, Pure,
     DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
     AllTypesMatch<["value", "result"]>,
     DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>]> {
  let summary = "integer or floating point constant";
  let description = [{
    The `constant` operation produces an SSA value equal to some integer or
    floating-point constant specified by an attribute. This is the way MLIR
    forms simple integer and floating point constants.

    Example:

    ```
    // Integer constant
    %1 = arith.constant 42 : i32

    // Equivalent generic form
    %1 = "arith.constant"() {value = 42 : i32} : () -> i32
    ```
  }];

  let arguments = (ins TypedAttrInterface:$value);
  // TODO: Disallow arith.constant to return anything other than a signless
  // integer or float like. Downstream users of Arith should only be
  // working with signless integers, floats, or vectors/tensors thereof.
  // However, it is necessary to allow arith.constant to return vectors/tensors
  // of strings and signed/unsigned integers (for now) as an artefact of
  // splitting the Standard dialect.
  let results = (outs /*SignlessIntegerOrFloatLike*/AnyType:$result);

  let extraClassDeclaration = [{
    /// Whether the constant op can be constructed with a particular value and
    /// type.
    static bool isBuildableWith(Attribute value, Type type);

    /// Build the constant op with `value` and `type` if possible, otherwise
    /// returns null.
    static ConstantOp materialize(OpBuilder &builder, Attribute value,
                                  Type type, Location loc);
  }];

  let hasFolder = 1;
  let assemblyFormat = "attr-dict $value";
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// AddIOp
//===----------------------------------------------------------------------===//

def Arith_AddIOp : Arith_IntBinaryOpWithOverflowFlags<"addi", [Commutative]> {
  let summary = "integer addition operation";
  let description = [{
    Performs N-bit addition on the operands. The operands are interpreted as 
    unsigned bitvectors. The result is represented by a bitvector containing the 
    mathematical value of the addition modulo 2^n, where `n` is the bitwidth. 
    Because `arith` integers use a two's complement representation, this operation 
    is applicable on both signed and unsigned integer operands.

    The `addi` operation takes two operands and returns one result, each of
    these is required to be the same type. This type may be an integer scalar type, 
    a vector whose element type is integer, or a tensor of integers.

    This op supports `nuw`/`nsw` overflow flags which stands stand for
    "No Unsigned Wrap" and "No Signed Wrap", respectively. If the `nuw` and/or
    `nsw` flags are present, and an unsigned/signed overflow occurs
    (respectively), the result is poison.

    Example:

    ```mlir
    // Scalar addition.
    %a = arith.addi %b, %c : i64

    // Scalar addition with overflow flags.
    %a = arith.addi %b, %c overflow<nsw, nuw> : i64

    // SIMD vector element-wise addition.
    %f = arith.addi %g, %h : vector<4xi32>

    // Tensor element-wise addition.
    %x = arith.addi %y, %z : tensor<4x?xi8>
    ```
  }];
  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// AddUIExtendedOp
//===----------------------------------------------------------------------===//

def Arith_AddUIExtendedOp : Arith_Op<"addui_extended", [Pure, Commutative,
    AllTypesMatch<["lhs", "rhs", "sum"]>]> {
  let summary = [{
    extended unsigned integer addition operation returning sum and overflow bit
  }];

  let description = [{
    Performs (N+1)-bit addition on zero-extended operands. Returns two results:
    the N-bit sum (same type as both operands), and the overflow bit
    (boolean-like), where `1` indicates unsigned addition overflow, while `0`
    indicates no overflow.

    Example:

    ```mlir
    // Scalar addition.
    %sum, %overflow = arith.addui_extended %b, %c : i64, i1

    // Vector element-wise addition.
    %d:2 = arith.addui_extended %e, %f : vector<4xi32>, vector<4xi1>

    // Tensor element-wise addition.
    %x:2 = arith.addui_extended %y, %z : tensor<4x?xi8>, tensor<4x?xi1>
    ```
  }];

  let arguments = (ins SignlessIntegerLike:$lhs, SignlessIntegerLike:$rhs);
  let results = (outs SignlessIntegerLike:$sum, BoolLike:$overflow);
  let assemblyFormat = [{
    $lhs `,` $rhs attr-dict `:` type($sum) `,` type($overflow)
  }];

  let builders = [
    OpBuilder<(ins "Value":$lhs, "Value":$rhs), [{
      build($_builder, $_state, lhs.getType(), ::getI1SameShape(lhs.getType()),
            lhs, rhs);
    }]>
  ];

  let hasFolder = 1;
  let hasCanonicalizer = 1;

  let extraClassDeclaration = [{
    std::optional<SmallVector<int64_t, 4>> getShapeForUnroll();
  }];
}

//===----------------------------------------------------------------------===//
// SubIOp
//===----------------------------------------------------------------------===//

def Arith_SubIOp : Arith_IntBinaryOpWithOverflowFlags<"subi"> {
  let summary = [{
    Integer subtraction operation.
  }];
  let description = [{
    Performs N-bit subtraction on the operands. The operands are interpreted as unsigned
    bitvectors. The result is represented by a bitvector containing the mathematical
    value of the subtraction modulo 2^n, where `n` is the bitwidth. Because `arith`
    integers use a two's complement representation, this operation is applicable on
    both signed and unsigned integer operands.

    The `subi` operation takes two operands and returns one result, each of
    these is required to be the same type. This type may be an integer scalar type,
    a vector whose element type is integer, or a tensor of integers.

    This op supports `nuw`/`nsw` overflow flags which stands stand for
    "No Unsigned Wrap" and "No Signed Wrap", respectively. If the `nuw` and/or
    `nsw` flags are present, and an unsigned/signed overflow occurs
    (respectively), the result is poison.

    Example:

    ```mlir
    // Scalar subtraction.
    %a = arith.subi %b, %c : i64

    // Scalar subtraction with overflow flags.
    %a = arith.subi %b, %c overflow<nsw, nuw> : i64

    // SIMD vector element-wise subtraction.
    %f = arith.subi %g, %h : vector<4xi32>

    // Tensor element-wise subtraction.
    %x = arith.subi %y, %z : tensor<4x?xi8>
    ```
  }];
  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// MulIOp
//===----------------------------------------------------------------------===//

def Arith_MulIOp : Arith_IntBinaryOpWithOverflowFlags<"muli",
  [Commutative, DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>]
> {
  let summary = [{
    Integer multiplication operation.
  }];
  let description = [{
    Performs N-bit multiplication on the operands. The operands are interpreted as
    unsigned bitvectors. The result is represented by a bitvector containing the
    mathematical value of the multiplication modulo 2^n, where `n` is the bitwidth.
    Because `arith` integers use a two's complement representation, this operation is
    applicable on both signed and unsigned integer operands.

    The `muli` operation takes two operands and returns one result, each of
    these is required to be the same type. This type may be an integer scalar type,
    a vector whose element type is integer, or a tensor of integers.

    This op supports `nuw`/`nsw` overflow flags which stands stand for
    "No Unsigned Wrap" and "No Signed Wrap", respectively. If the `nuw` and/or
    `nsw` flags are present, and an unsigned/signed overflow occurs
    (respectively), the result is poison.

    Example:

    ```mlir
    // Scalar multiplication.
    %a = arith.muli %b, %c : i64

    // Scalar multiplication with overflow flags.
    %a = arith.muli %b, %c overflow<nsw, nuw> : i64

    // SIMD vector element-wise multiplication.
    %f = arith.muli %g, %h : vector<4xi32>

    // Tensor element-wise multiplication.
    %x = arith.muli %y, %z : tensor<4x?xi8>
    ```
  }];
  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// MulSIExtendedOp
//===----------------------------------------------------------------------===//

def Arith_MulSIExtendedOp : Arith_Op<"mulsi_extended", [Pure, Commutative,
    AllTypesMatch<["lhs", "rhs", "low", "high"]>]> {
  let summary = [{
    extended signed integer multiplication operation
  }];

  let description = [{
    Performs (2*N)-bit multiplication on sign-extended operands. Returns two
    N-bit results: the low and the high halves of the product. The low half has
    the same value as the result of regular multiplication `arith.muli` with
    the same operands.

    Example:

    ```mlir
    // Scalar multiplication.
    %low, %high = arith.mulsi_extended %a, %b : i32

    // Vector element-wise multiplication.
    %c:2 = arith.mulsi_extended %d, %e : vector<4xi32>

    // Tensor element-wise multiplication.
    %x:2 = arith.mulsi_extended %y, %z : tensor<4x?xi8>
    ```
  }];

  let arguments = (ins SignlessIntegerLike:$lhs, SignlessIntegerLike:$rhs);
  let results = (outs SignlessIntegerLike:$low, SignlessIntegerLike:$high);

  let assemblyFormat = "$lhs `,` $rhs attr-dict `:` type($lhs)";

  let hasFolder = 1;
  let hasCanonicalizer = 1;

  let extraClassDeclaration = [{
    std::optional<SmallVector<int64_t, 4>> getShapeForUnroll();
  }];
}

//===----------------------------------------------------------------------===//
// MulUIExtendedOp
//===----------------------------------------------------------------------===//

def Arith_MulUIExtendedOp : Arith_Op<"mului_extended", [Pure, Commutative,
    AllTypesMatch<["lhs", "rhs", "low", "high"]>]> {
  let summary = [{
    extended unsigned integer multiplication operation
  }];

  let description = [{
    Performs (2*N)-bit multiplication on zero-extended operands. Returns two
    N-bit results: the low and the high halves of the product. The low half has
    the same value as the result of regular multiplication `arith.muli` with
    the same operands.

    Example:

    ```mlir
    // Scalar multiplication.
    %low, %high = arith.mului_extended %a, %b : i32

    // Vector element-wise multiplication.
    %c:2 = arith.mului_extended %d, %e : vector<4xi32>

    // Tensor element-wise multiplication.
    %x:2 = arith.mului_extended %y, %z : tensor<4x?xi8>
    ```
  }];

  let arguments = (ins SignlessIntegerLike:$lhs, SignlessIntegerLike:$rhs);
  let results = (outs SignlessIntegerLike:$low, SignlessIntegerLike:$high);

  let assemblyFormat = "$lhs `,` $rhs attr-dict `:` type($lhs)";

  let hasFolder = 1;
  let hasCanonicalizer = 1;

  let extraClassDeclaration = [{
    std::optional<SmallVector<int64_t, 4>> getShapeForUnroll();
  }];
}

//===----------------------------------------------------------------------===//
// DivUIOp
//===----------------------------------------------------------------------===//

def Arith_DivUIOp : Arith_IntBinaryOp<"divui", [ConditionallySpeculatable]> {
  let summary = "unsigned integer division operation";
  let description = [{
    Unsigned integer division. Rounds towards zero. Treats the leading bit as
    the most significant, i.e. for `i16` given two's complement representation,
    `6 / -2 = 6 / (2^16 - 2) = 0`.

    Division by zero is undefined behavior. When applied to `vector` and 
    `tensor` values, the behavior is undefined if _any_ elements are divided by 
    zero.

    Example:

    ```mlir
    // Scalar unsigned integer division.
    %a = arith.divui %b, %c : i64

    // SIMD vector element-wise division.
    %f = arith.divui %g, %h : vector<4xi32>

    // Tensor element-wise integer division.
    %x = arith.divui %y, %z : tensor<4x?xi8>
    ```
  }];

  let extraClassDeclaration = [{
    /// Interface method for ConditionallySpeculatable.
    Speculation::Speculatability getSpeculatability();
  }];

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// DivSIOp
//===----------------------------------------------------------------------===//

def Arith_DivSIOp : Arith_IntBinaryOp<"divsi", [ConditionallySpeculatable]> {
  let summary = "signed integer division operation";
  let description = [{
    Signed integer division. Rounds towards zero. Treats the leading bit as
    sign, i.e. `6 / -2 = -3`.

    Divison by zero, or signed division overflow (minimum value divided by -1) 
    is undefined behavior. When applied to `vector` and `tensor` values, the 
    behavior is undefined if _any_ of its elements are divided by zero or has a 
    signed division overflow.

    Example:

    ```mlir
    // Scalar signed integer division.
    %a = arith.divsi %b, %c : i64

    // SIMD vector element-wise division.
    %f = arith.divsi %g, %h : vector<4xi32>

    // Tensor element-wise integer division.
    %x = arith.divsi %y, %z : tensor<4x?xi8>
    ```
  }];

  let extraClassDeclaration = [{
    /// Interface method for ConditionallySpeculatable.
    Speculation::Speculatability getSpeculatability();
  }];

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// CeilDivUIOp
//===----------------------------------------------------------------------===//

def Arith_CeilDivUIOp : Arith_IntBinaryOp<"ceildivui",
                                          [ConditionallySpeculatable]> {
  let summary = "unsigned ceil integer division operation";
  let description = [{
    Unsigned integer division. Rounds towards positive infinity. Treats the
    leading bit as the most significant, i.e. for `i16` given two's complement
    representation, `6 / -2 = 6 / (2^16 - 2) = 1`. 

    Division by zero is undefined behavior. When applied to `vector` and 
    `tensor` values, the behavior is undefined if _any_ elements are divided by 
    zero.

    Example:

    ```mlir
    // Scalar unsigned integer division.
    %a = arith.ceildivui %b, %c : i64
    ```
  }];

  let extraClassDeclaration = [{
    /// Interface method for ConditionallySpeculatable.
    Speculation::Speculatability getSpeculatability();
  }];

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// CeilDivSIOp
//===----------------------------------------------------------------------===//

def Arith_CeilDivSIOp : Arith_IntBinaryOp<"ceildivsi",
                                          [ConditionallySpeculatable]> {
  let summary = "signed ceil integer division operation";
  let description = [{
    Signed integer division. Rounds towards positive infinity, i.e. `7 / -2 = -3`.

    Divison by zero, or signed division overflow (minimum value divided by -1) 
    is undefined behavior. When applied to `vector` and `tensor` values, the 
    behavior is undefined if _any_ of its elements are divided by zero or has a 
    signed division overflow.

    Example:

    ```mlir
    // Scalar signed integer division.
    %a = arith.ceildivsi %b, %c : i64
    ```
  }];

  let extraClassDeclaration = [{
    /// Interface method for ConditionallySpeculatable.
    Speculation::Speculatability getSpeculatability();
  }];

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// FloorDivSIOp
//===----------------------------------------------------------------------===//

def Arith_FloorDivSIOp : Arith_TotalIntBinaryOp<"floordivsi"> {
  let summary = "signed floor integer division operation";
  let description = [{
    Signed integer division. Rounds towards negative infinity, i.e. `5 / -2 = -3`.

    Divison by zero, or signed division overflow (minimum value divided by -1) 
    is undefined behavior. When applied to `vector` and `tensor` values, the 
    behavior is undefined if _any_ of its elements are divided by zero or has a 
    signed division overflow.

    Example:

    ```mlir
    // Scalar signed integer division.
    %a = arith.floordivsi %b, %c : i64

    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// RemUIOp
//===----------------------------------------------------------------------===//

def Arith_RemUIOp : Arith_TotalIntBinaryOp<"remui"> {
  let summary = "unsigned integer division remainder operation";
  let description = [{
    Unsigned integer division remainder. Treats the leading bit as the most
    significant, i.e. for `i16`, `6 % -2 = 6 % (2^16 - 2) = 6`.

    Division by zero is undefined behavior. When applied to `vector` and 
    `tensor` values, the behavior is undefined if _any_ elements are divided by 
    zero.

    Example:

    ```mlir
    // Scalar unsigned integer division remainder.
    %a = arith.remui %b, %c : i64

    // SIMD vector element-wise division remainder.
    %f = arith.remui %g, %h : vector<4xi32>

    // Tensor element-wise integer division remainder.
    %x = arith.remui %y, %z : tensor<4x?xi8>
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// RemSIOp
//===----------------------------------------------------------------------===//

def Arith_RemSIOp : Arith_TotalIntBinaryOp<"remsi"> {
  let summary = "signed integer division remainder operation";
  let description = [{
    Signed integer division remainder. Treats the leading bit as sign, i.e. `6 %
    -2 = 0`.

    Division by zero is undefined behavior. When applied to `vector` and 
    `tensor` values, the behavior is undefined if _any_ elements are divided by 
    zero.

    Example:

    ```mlir
    // Scalar signed integer division remainder.
    %a = arith.remsi %b, %c : i64

    // SIMD vector element-wise division remainder.
    %f = arith.remsi %g, %h : vector<4xi32>

    // Tensor element-wise integer division remainder.
    %x = arith.remsi %y, %z : tensor<4x?xi8>
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// AndIOp
//===----------------------------------------------------------------------===//

def Arith_AndIOp : Arith_TotalIntBinaryOp<"andi", [Commutative, Idempotent]> {
  let summary = "integer binary and";
  let description = [{
    The `andi` operation takes two operands and returns one result, each of
    these is required to be the same type. This type may be an integer scalar
    type, a vector whose element type is integer, or a tensor of integers. It
    has no standard attributes.

    Example:

    ```mlir
    // Scalar integer bitwise and.
    %a = arith.andi %b, %c : i64

    // SIMD vector element-wise bitwise integer and.
    %f = arith.andi %g, %h : vector<4xi32>

    // Tensor element-wise bitwise integer and.
    %x = arith.andi %y, %z : tensor<4x?xi8>
    ```
  }];
  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// OrIOp
//===----------------------------------------------------------------------===//

def Arith_OrIOp : Arith_TotalIntBinaryOp<"ori", [Commutative, Idempotent]> {
  let summary = "integer binary or";
  let description = [{
    The `ori` operation takes two operands and returns one result, each of these
    is required to be the same type. This type may be an integer scalar type, a
    vector whose element type is integer, or a tensor of integers. It has no
    standard attributes.

    Example:

    ```mlir
    // Scalar integer bitwise or.
    %a = arith.ori %b, %c : i64

    // SIMD vector element-wise bitwise integer or.
    %f = arith.ori %g, %h : vector<4xi32>

    // Tensor element-wise bitwise integer or.
    %x = arith.ori %y, %z : tensor<4x?xi8>
    ```
  }];
  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// XOrIOp
//===----------------------------------------------------------------------===//

def Arith_XOrIOp : Arith_TotalIntBinaryOp<"xori", [Commutative]> {
  let summary = "integer binary xor";
  let description = [{
    The `xori` operation takes two operands and returns one result, each of
    these is required to be the same type. This type may be an integer scalar
    type, a vector whose element type is integer, or a tensor of integers. It
    has no standard attributes.

    Example:

    ```mlir
    // Scalar integer bitwise xor.
    %a = arith.xori %b, %c : i64

    // SIMD vector element-wise bitwise integer xor.
    %f = arith.xori %g, %h : vector<4xi32>

    // Tensor element-wise bitwise integer xor.
    %x = arith.xori %y, %z : tensor<4x?xi8>
    ```
  }];
  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// ShLIOp
//===----------------------------------------------------------------------===//

def Arith_ShLIOp : Arith_IntBinaryOpWithOverflowFlags<"shli"> {
  let summary = "integer left-shift";
  let description = [{
    The `shli` operation shifts the integer value of the first operand to the left 
    by the integer value of the second operand. The second operand is interpreted as 
    unsigned. The low order bits are filled with zeros. If the value of the second 
    operand is greater or equal than the bitwidth of the first operand, then the
    operation returns poison.

    This op supports `nuw`/`nsw` overflow flags which stands stand for
    "No Unsigned Wrap" and "No Signed Wrap", respectively. If the `nuw` and/or
    `nsw` flags are present, and an unsigned/signed overflow occurs
    (respectively), the result is poison.

    Example:

    ```mlir
    %1 = arith.constant 5 : i8  // %1 is 0b00000101
    %2 = arith.constant 3 : i8
    %3 = arith.shli %1, %2 : i8 // %3 is 0b00101000
    %4 = arith.shli %1, %2 overflow<nsw, nuw> : i8  
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// ShRUIOp
//===----------------------------------------------------------------------===//

def Arith_ShRUIOp : Arith_TotalIntBinaryOp<"shrui"> {
  let summary = "unsigned integer right-shift";
  let description = [{
    The `shrui` operation shifts an integer value of the first operand to the right 
    by the value of the second operand. The first operand is interpreted as unsigned,
    and the second operand is interpreted as unsigned. The high order bits are always 
    filled with zeros. If the value of the second operand is greater or equal than the
    bitwidth of the first operand, then the operation returns poison.

    Example:

    ```mlir
    %1 = arith.constant 160 : i8               // %1 is 0b10100000
    %2 = arith.constant 3 : i8
    %3 = arith.shrui %1, %2 : (i8, i8) -> i8   // %3 is 0b00010100
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// ShRSIOp
//===----------------------------------------------------------------------===//

def Arith_ShRSIOp : Arith_TotalIntBinaryOp<"shrsi"> {
  let summary = "signed integer right-shift";
  let description = [{
    The `shrsi` operation shifts an integer value of the first operand to the right 
    by the value of the second operand. The first operand is interpreted as signed, 
    and the second operand is interpreter as unsigned. The high order bits in the 
    output are filled with copies of the most-significant bit of the shifted value 
    (which means that the sign of the value is preserved). If the value of the second 
    operand is greater or equal than bitwidth of the first operand, then the operation
    returns poison.

    Example:

    ```mlir
    %1 = arith.constant 160 : i8               // %1 is 0b10100000
    %2 = arith.constant 3 : i8
    %3 = arith.shrsi %1, %2 : (i8, i8) -> i8   // %3 is 0b11110100
    %4 = arith.constant 96 : i8                   // %4 is 0b01100000
    %5 = arith.shrsi %4, %2 : (i8, i8) -> i8   // %5 is 0b00001100
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// NegFOp
//===----------------------------------------------------------------------===//

def Arith_NegFOp : Arith_FloatUnaryOp<"negf"> {
  let summary = "floating point negation";
  let description = [{
    The `negf` operation computes the negation of a given value. It takes one
    operand and returns one result of the same type. This type may be a float
    scalar type, a vector whose element type is float, or a tensor of floats.
    It has no standard attributes.

    Example:

    ```mlir
    // Scalar negation value.
    %a = arith.negf %b : f64

    // SIMD vector element-wise negation value.
    %f = arith.negf %g : vector<4xf32>

    // Tensor element-wise negation value.
    %x = arith.negf %y : tensor<4x?xf8>
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// AddFOp
//===----------------------------------------------------------------------===//

def Arith_AddFOp : Arith_FloatBinaryOp<"addf", [Commutative]> {
  let summary = "floating point addition operation";
  let description = [{
    The `addf` operation takes two operands and returns one result, each of
    these is required to be the same type. This type may be a floating point
    scalar type, a vector whose element type is a floating point type, or a
    floating point tensor.

    Example:

    ```mlir
    // Scalar addition.
    %a = arith.addf %b, %c : f64

    // SIMD vector addition, e.g. for Intel SSE.
    %f = arith.addf %g, %h : vector<4xf32>

    // Tensor addition.
    %x = arith.addf %y, %z : tensor<4x?xbf16>
    ```

    TODO: In the distant future, this will accept optional attributes for fast
    math, contraction, rounding mode, and other controls.
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// SubFOp
//===----------------------------------------------------------------------===//

def Arith_SubFOp : Arith_FloatBinaryOp<"subf"> {
  let summary = "floating point subtraction operation";
  let description = [{
    The `subf` operation takes two operands and returns one result, each of
    these is required to be the same type. This type may be a floating point
    scalar type, a vector whose element type is a floating point type, or a
    floating point tensor.

    Example:

    ```mlir
    // Scalar subtraction.
    %a = arith.subf %b, %c : f64

    // SIMD vector subtraction, e.g. for Intel SSE.
    %f = arith.subf %g, %h : vector<4xf32>

    // Tensor subtraction.
    %x = arith.subf %y, %z : tensor<4x?xbf16>
    ```

    TODO: In the distant future, this will accept optional attributes for fast
    math, contraction, rounding mode, and other controls.
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// MaximumFOp
//===----------------------------------------------------------------------===//

def Arith_MaximumFOp : Arith_FloatBinaryOp<"maximumf", [Commutative]> {
  let summary = "floating-point maximum operation";
  let description = [{
    Returns the maximum of the two arguments, treating -0.0 as less than +0.0.
    If one of the arguments is NaN, then the result is also NaN.

    Example:

    ```mlir
    // Scalar floating-point maximum.
    %a = arith.maximumf %b, %c : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// MaxNumFOp
//===----------------------------------------------------------------------===//

def Arith_MaxNumFOp : Arith_FloatBinaryOp<"maxnumf", [Commutative]> {
  let summary = "floating-point maximum operation";
  let description = [{
    Returns the maximum of the two arguments.
    If the arguments are -0.0 and +0.0, then the result is either of them.
    If one of the arguments is NaN, then the result is the other argument.

    Example:

    ```mlir
    // Scalar floating-point maximum.
    %a = arith.maxnumf %b, %c : f64
    ```
  }];
  let hasFolder = 1;
}


//===----------------------------------------------------------------------===//
// MaxSIOp
//===----------------------------------------------------------------------===//

def Arith_MaxSIOp : Arith_TotalIntBinaryOp<"maxsi", [Commutative]> {
  let summary = "signed integer maximum operation";
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// MaxUIOp
//===----------------------------------------------------------------------===//

def Arith_MaxUIOp : Arith_TotalIntBinaryOp<"maxui", [Commutative]> {
  let summary = "unsigned integer maximum operation";
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// MinimumFOp
//===----------------------------------------------------------------------===//

def Arith_MinimumFOp : Arith_FloatBinaryOp<"minimumf", [Commutative]> {
  let summary = "floating-point minimum operation";
  let description = [{
    Returns the minimum of the two arguments, treating -0.0 as less than +0.0.
    If one of the arguments is NaN, then the result is also NaN.

    Example:

    ```mlir
    // Scalar floating-point minimum.
    %a = arith.minimumf %b, %c : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// MinNumFOp
//===----------------------------------------------------------------------===//

def Arith_MinNumFOp : Arith_FloatBinaryOp<"minnumf", [Commutative]> {
  let summary = "floating-point minimum operation";
  let description = [{
    Returns the minimum of the two arguments.
    If the arguments are -0.0 and +0.0, then the result is either of them.
    If one of the arguments is NaN, then the result is the other argument.

    Example:

    ```mlir
    // Scalar floating-point minimum.
    %a = arith.minnumf %b, %c : f64
    ```
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// MinSIOp
//===----------------------------------------------------------------------===//

def Arith_MinSIOp : Arith_TotalIntBinaryOp<"minsi", [Commutative]> {
  let summary = "signed integer minimum operation";
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// MinUIOp
//===----------------------------------------------------------------------===//

def Arith_MinUIOp : Arith_TotalIntBinaryOp<"minui", [Commutative]> {
  let summary = "unsigned integer minimum operation";
  let hasFolder = 1;
}


//===----------------------------------------------------------------------===//
// MulFOp
//===----------------------------------------------------------------------===//

def Arith_MulFOp : Arith_FloatBinaryOp<"mulf", [Commutative]> {
  let summary = "floating point multiplication operation";
  let description = [{
    The `mulf` operation takes two operands and returns one result, each of
    these is required to be the same type. This type may be a floating point
    scalar type, a vector whose element type is a floating point type, or a
    floating point tensor.

    Example:

    ```mlir
    // Scalar multiplication.
    %a = arith.mulf %b, %c : f64

    // SIMD pointwise vector multiplication, e.g. for Intel SSE.
    %f = arith.mulf %g, %h : vector<4xf32>

    // Tensor pointwise multiplication.
    %x = arith.mulf %y, %z : tensor<4x?xbf16>
    ```

    TODO: In the distant future, this will accept optional attributes for fast
    math, contraction, rounding mode, and other controls.
  }];
  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// DivFOp
//===----------------------------------------------------------------------===//

def Arith_DivFOp : Arith_FloatBinaryOp<"divf"> {
  let summary = "floating point division operation";
  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// RemFOp
//===----------------------------------------------------------------------===//

def Arith_RemFOp : Arith_FloatBinaryOp<"remf"> {
  let summary = "floating point division remainder operation";
  let description = [{
    Returns the floating point division remainder.
    The remainder has the same sign as the dividend (lhs operand).
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// ExtUIOp
//===----------------------------------------------------------------------===//

def Arith_ExtUIOp : Arith_IToICastOp<"extui"> {
  let summary = "integer zero extension operation";
  let description = [{
    The integer zero extension operation takes an integer input of
    width M and an integer destination type of width N. The destination
    bit-width must be larger than the input bit-width (N > M).
    The top-most (N - M) bits of the output are filled with zeros.

    Example:

    ```mlir
      %1 = arith.constant 5 : i3      // %1 is 0b101
      %2 = arith.extui %1 : i3 to i6  // %2 is 0b000101
      %3 = arith.constant 2 : i3      // %3 is 0b010
      %4 = arith.extui %3 : i3 to i6  // %4 is 0b000010

      %5 = arith.extui %0 : vector<2 x i32> to vector<2 x i64>
    ```
  }];

  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// ExtSIOp
//===----------------------------------------------------------------------===//

def Arith_ExtSIOp : Arith_IToICastOp<"extsi"> {
  let summary = "integer sign extension operation";

  let description = [{
    The integer sign extension operation takes an integer input of
    width M and an integer destination type of width N. The destination
    bit-width must be larger than the input bit-width (N > M).
    The top-most (N - M) bits of the output are filled with copies
    of the most-significant bit of the input.

    Example:

    ```mlir
    %1 = arith.constant 5 : i3      // %1 is 0b101
    %2 = arith.extsi %1 : i3 to i6  // %2 is 0b111101
    %3 = arith.constant 2 : i3      // %3 is 0b010
    %4 = arith.extsi %3 : i3 to i6  // %4 is 0b000010

    %5 = arith.extsi %0 : vector<2 x i32> to vector<2 x i64>
    ```
  }];

  let hasFolder = 1;
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// ExtFOp
//===----------------------------------------------------------------------===//

def Arith_ExtFOp : Arith_FToFCastOp<"extf", [DeclareOpInterfaceMethods<ArithFastMathInterface>]> {
  let summary = "cast from floating-point to wider floating-point";
  let description = [{
    Cast a floating-point value to a larger floating-point-typed value.
    The destination type must to be strictly wider than the source type.
    When operating on vectors, casts elementwise.
  }];
  let hasVerifier = 1;
  let hasFolder = 1;

  let arguments = (ins FloatLike:$in,
                       OptionalAttr<Arith_FastMathAttr>:$fastmath);
  let results = (outs FloatLike:$out);

  let assemblyFormat = [{ $in (`fastmath` `` $fastmath^)?
                          attr-dict `:` type($in) `to` type($out) }];
}

//===----------------------------------------------------------------------===//
// TruncIOp
//===----------------------------------------------------------------------===//

def Arith_TruncIOp : Arith_IToICastOp<"trunci"> {
  let summary = "integer truncation operation";
  let description = [{
    The integer truncation operation takes an integer input of
    width M and an integer destination type of width N. The destination
    bit-width must be smaller than the input bit-width (N < M).
    The top-most (N - M) bits of the input are discarded.

    Example:

    ```mlir
      %1 = arith.constant 21 : i5     // %1 is 0b10101
      %2 = arith.trunci %1 : i5 to i4 // %2 is 0b0101
      %3 = arith.trunci %1 : i5 to i3 // %3 is 0b101

      %5 = arith.trunci %0 : vector<2 x i32> to vector<2 x i16>
    ```
  }];

  let hasFolder = 1;
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// TruncFOp
//===----------------------------------------------------------------------===//

def Arith_TruncFOp :
    Arith_Op<"truncf",
      [Pure, SameOperandsAndResultShape, SameInputOutputTensorDims,
       DeclareOpInterfaceMethods<ArithRoundingModeInterface>,
       DeclareOpInterfaceMethods<ArithFastMathInterface>,
       DeclareOpInterfaceMethods<CastOpInterface>]>,
    Arguments<(ins FloatLike:$in,
                   OptionalAttr<Arith_RoundingModeAttr>:$roundingmode,
                   OptionalAttr<Arith_FastMathAttr>:$fastmath)>,
    Results<(outs FloatLike:$out)> {
  let summary = "cast from floating-point to narrower floating-point";
  let description = [{
    Truncate a floating-point value to a smaller floating-point-typed value.
    The destination type must be strictly narrower than the source type.
    If the value cannot be exactly represented, it is rounded using the
    provided rounding mode or the default one if no rounding mode is provided.
    When operating on vectors, casts elementwise.
  }];
  let builders = [
    OpBuilder<(ins "Type":$out, "Value":$in), [{
      $_state.addOperands(in);
      $_state.addTypes(out);
    }]>
  ];

  let hasFolder = 1;
  let hasVerifier = 1;
  let assemblyFormat = [{ $in ($roundingmode^)?
                          (`fastmath` `` $fastmath^)?
                          attr-dict `:` type($in) `to` type($out) }];
}

//===----------------------------------------------------------------------===//
// UIToFPOp
//===----------------------------------------------------------------------===//

def Arith_UIToFPOp : Arith_IToFCastOp<"uitofp"> {
  let summary = "cast from unsigned integer type to floating-point";
  let description = [{
    Cast from a value interpreted as unsigned integer to the corresponding
    floating-point value. If the value cannot be exactly represented, it is
    rounded using the default rounding mode. When operating on vectors, casts
    elementwise.
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// SIToFPOp
//===----------------------------------------------------------------------===//

def Arith_SIToFPOp : Arith_IToFCastOp<"sitofp"> {
  let summary = "cast from integer type to floating-point";
  let description = [{
    Cast from a value interpreted as a signed integer to the corresponding
    floating-point value. If the value cannot be exactly represented, it is
    rounded using the default rounding mode. When operating on vectors, casts
    elementwise.
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// FPToUIOp
//===----------------------------------------------------------------------===//

def Arith_FPToUIOp : Arith_FToICastOp<"fptoui"> {
  let summary = "cast from floating-point type to integer type";
  let description = [{
    Cast from a value interpreted as floating-point to the nearest (rounding
    towards zero) unsigned integer value. When operating on vectors, casts
    elementwise.
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// FPToSIOp
//===----------------------------------------------------------------------===//

def Arith_FPToSIOp : Arith_FToICastOp<"fptosi"> {
  let summary = "cast from floating-point type to integer type";
  let description = [{
    Cast from a value interpreted as floating-point to the nearest (rounding
    towards zero) signed integer value. When operating on vectors, casts
    elementwise.
  }];
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// IndexCastOp
//===----------------------------------------------------------------------===//

// Index cast can convert between memrefs of signless integers and indices too.
def IndexCastTypeConstraint : TypeConstraint<Or<[
        SignlessIntegerLike.predicate,
        MemRefOf<[AnySignlessInteger, Index]>.predicate]>,
    "signless-integer-like or memref of signless-integer">;

def Arith_IndexCastOp
  : Arith_CastOp<"index_cast", IndexCastTypeConstraint, IndexCastTypeConstraint,
                 [DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>]> {
  let summary = "cast between index and integer types";
  let description = [{
    Casts between scalar or vector integers and corresponding 'index' scalar or
    vectors. Index is an integer of platform-specific bit width. If casting to
    a wider integer, the value is sign-extended. If casting to a narrower
    integer, the value is truncated.
  }];

  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// IndexCastUIOp
//===----------------------------------------------------------------------===//

def Arith_IndexCastUIOp
  : Arith_CastOp<"index_castui", IndexCastTypeConstraint, IndexCastTypeConstraint,
                 [DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>]> {
  let summary = "unsigned cast between index and integer types";
  let description = [{
    Casts between scalar or vector integers and corresponding 'index' scalar or
    vectors. Index is an integer of platform-specific bit width. If casting to
    a wider integer, the value is zero-extended. If casting to a narrower
    integer, the value is truncated.
  }];

  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// BitcastOp
//===----------------------------------------------------------------------===//

// Bitcast can convert between memrefs of signless integers, indices, and
// floats too.
def BitcastTypeConstraint : TypeConstraint<Or<[
        SignlessIntegerOrFloatLike.predicate,
        MemRefOf<[AnySignlessInteger, Index, AnyFloat]>.predicate]>,
    "signless-integer-or-float-like or memref of signless-integer or float">;

def Arith_BitcastOp : Arith_CastOp<"bitcast", BitcastTypeConstraint,
                                              BitcastTypeConstraint> {
  let summary = "bitcast between values of equal bit width";
  let description = [{
    Bitcast an integer or floating point value to an integer or floating point
    value of equal bit width. When operating on vectors, casts elementwise.

    Note that this implements a logical bitcast independent of target
    endianness. This allows constant folding without target information and is
    consitent with the bitcast constant folders in LLVM (see
    https://github.com/llvm/llvm-project/blob/18c19414eb/llvm/lib/IR/ConstantFold.cpp#L168)
    For targets where the source and target type have the same endianness (which
    is the standard), this cast will also change no bits at runtime, but it may
    still require an operation, for example if the machine has different
    floating point and integer register files. For targets that have a different
    endianness for the source and target types (e.g. float is big-endian and
    integer is little-endian) a proper lowering would add operations to swap the
    order of words in addition to the bitcast.
  }];

  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// CmpIOp
//===----------------------------------------------------------------------===//

def Arith_CmpIOp
  : Arith_CompareOpOfAnyRank<"cmpi",
                             [DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>]> {
  let summary = "integer comparison operation";
  let description = [{
    The `cmpi` operation is a generic comparison for integer-like types. Its two
    arguments can be integers, vectors or tensors thereof as long as their types
    match. The operation produces an i1 for the former case, a vector or a
    tensor of i1 with the same shape as inputs in the other cases.

    Its first argument is an attribute that defines which type of comparison is
    performed. The following comparisons are supported:

    -   equal (mnemonic: `"eq"`; integer value: `0`)
    -   not equal (mnemonic: `"ne"`; integer value: `1`)
    -   signed less than (mnemonic: `"slt"`; integer value: `2`)
    -   signed less than or equal (mnemonic: `"sle"`; integer value: `3`)
    -   signed greater than (mnemonic: `"sgt"`; integer value: `4`)
    -   signed greater than or equal (mnemonic: `"sge"`; integer value: `5`)
    -   unsigned less than (mnemonic: `"ult"`; integer value: `6`)
    -   unsigned less than or equal (mnemonic: `"ule"`; integer value: `7`)
    -   unsigned greater than (mnemonic: `"ugt"`; integer value: `8`)
    -   unsigned greater than or equal (mnemonic: `"uge"`; integer value: `9`)

    The result is `1` if the comparison is true and `0` otherwise. For vector or
    tensor operands, the comparison is performed elementwise and the element of
    the result indicates whether the comparison is true for the operand elements
    with the same indices as those of the result.

    Note: while the custom assembly form uses strings, the actual underlying
    attribute has integer type (or rather enum class in C++ code) as seen from
    the generic assembly form. String literals are used to improve readability
    of the IR by humans.

    This operation only applies to integer-like operands, but not floats. The
    main reason being that comparison operations have diverging sets of
    attributes: integers require sign specification while floats require various
    floating point-related particularities, e.g., `-ffast-math` behavior,
    IEEE754 compliance, etc
    ([rationale](../Rationale/Rationale.md#splitting-floating-point-vs-integer-operations)).
    The type of comparison is specified as attribute to avoid introducing ten
    similar operations, taking into account that they are often implemented
    using the same operation downstream
    ([rationale](../Rationale/Rationale.md#specifying-comparison-kind-as-attribute)). The
    separation between signed and unsigned order comparisons is necessary
    because of integers being signless. The comparison operation must know how
    to interpret values with the foremost bit being set: negatives in two's
    complement or large positives
    ([rationale](../Rationale/Rationale.md#specifying-sign-in-integer-comparison-operations)).

    Example:

    ```mlir
    // Custom form of scalar "signed less than" comparison.
    %x = arith.cmpi slt, %lhs, %rhs : i32

    // Generic form of the same operation.
    %x = "arith.cmpi"(%lhs, %rhs) {predicate = 2 : i64} : (i32, i32) -> i1

    // Custom form of vector equality comparison.
    %x = arith.cmpi eq, %lhs, %rhs : vector<4xi64>

    // Generic form of the same operation.
    %x = "arith.cmpi"(%lhs, %rhs) {predicate = 0 : i64}
        : (vector<4xi64>, vector<4xi64>) -> vector<4xi1>
    ```
  }];

  let arguments = (ins Arith_CmpIPredicateAttr:$predicate,
                       SignlessIntegerLikeOfAnyRank:$lhs,
                       SignlessIntegerLikeOfAnyRank:$rhs);

  let extraClassDeclaration = [{
    static arith::CmpIPredicate getPredicateByName(StringRef name);
  }];

  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// CmpFOp
//===----------------------------------------------------------------------===//

def Arith_CmpFOp : Arith_CompareOp<"cmpf",
    [DeclareOpInterfaceMethods<ArithFastMathInterface>]> {
  let summary = "floating-point comparison operation";
  let description = [{
    The `cmpf` operation compares its two operands according to the float
    comparison rules and the predicate specified by the respective attribute.
    The predicate defines the type of comparison: (un)orderedness, (in)equality
    and signed less/greater than (or equal to) as well as predicates that are
    always true or false.  The operands must have the same type, and this type
    must be a float type, or a vector or tensor thereof.  The result is an i1,
    or a vector/tensor thereof having the same shape as the inputs. Unlike cmpi,
    the operands are always treated as signed. The u prefix indicates
    *unordered* comparison, not unsigned comparison, so "une" means unordered or
    not equal. For the sake of readability by humans, custom assembly form for
    the operation uses a string-typed attribute for the predicate.  The value of
    this attribute corresponds to lower-cased name of the predicate constant,
    e.g., "one" means "ordered not equal".  The string representation of the
    attribute is merely a syntactic sugar and is converted to an integer
    attribute by the parser.

    Example:

    ```mlir
    %r1 = arith.cmpf oeq, %0, %1 : f32
    %r2 = arith.cmpf ult, %0, %1 : tensor<42x42xf64>
    %r3 = "arith.cmpf"(%0, %1) {predicate: 0} : (f8, f8) -> i1
    ```
  }];

  let arguments = (ins Arith_CmpFPredicateAttr:$predicate,
                       FloatLike:$lhs,
                       FloatLike:$rhs,
                       DefaultValuedAttr<
                         Arith_FastMathAttr, "::mlir::arith::FastMathFlags::none">:$fastmath);

  let extraClassDeclaration = [{
    static arith::CmpFPredicate getPredicateByName(StringRef name);
  }];

  let hasFolder = 1;
  let hasCanonicalizer = 1;
  let assemblyFormat = [{ $predicate `,` $lhs `,` $rhs (`fastmath` `` $fastmath^)?
                          attr-dict `:` type($lhs)}];
}

//===----------------------------------------------------------------------===//
// SelectOp
//===----------------------------------------------------------------------===//

class BooleanConditionOrMatchingShape<string condition, string result> :
    PredOpTrait<
      condition # " is signless i1 or has matching shape",
      Or<[TypeIsPred<condition, I1>,
          AllShapesMatch<[condition, result]>.predicate]>>;

def SelectOp : Arith_Op<"select", [Pure,
    AllTypesMatch<["true_value", "false_value", "result"]>,
    BooleanConditionOrMatchingShape<"condition", "result">,
    DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRangesFromOptional"]>,
    DeclareOpInterfaceMethods<SelectLikeOpInterface>,
  ] # ElementwiseMappable.traits> {
  let summary = "select operation";
  let description = [{
    The `arith.select` operation chooses one value based on a binary condition
    supplied as its first operand.

    If the value of the first operand (the condition) is `1`, then the second
    operand is returned, and the third operand is ignored, even if it was poison.

    If the value of the first operand (the condition) is `0`, then the third
    operand is returned, and the second operand is ignored, even if it was poison.

    If the value of the first operand (the condition) is poison, then the
    operation returns poison.

    The operation applies to vectors and tensors elementwise given the _shape_
    of all operands is identical. The choice is made for each element
    individually based on the value at the same position as the element in the
    condition operand. If an i1 is provided as the condition, the entire vector
    or tensor is chosen.

    Example:

    ```mlir
    // Custom form of scalar selection.
    %x = arith.select %cond, %true, %false : i32

    // Generic form of the same operation.
    %x = "arith.select"(%cond, %true, %false) : (i1, i32, i32) -> i32

    // Element-wise vector selection.
    %vx = arith.select %vcond, %vtrue, %vfalse : vector<42xi1>, vector<42xf32>

    // Full vector selection.
    %vx = arith.select %cond, %vtrue, %vfalse : vector<42xf32>
    ```
  }];

  let arguments = (ins BoolLike:$condition,
                       AnyType:$true_value,
                       AnyType:$false_value);
  let results = (outs AnyType:$result);

  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;

  // FIXME: Switch this to use the declarative assembly format.
  let hasCustomAssemblyFormat = 1;
}

#endif // ARITH_OPS


//===-- GPUOps.td - GPU dialect operation definitions ------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// Defines some operations of the GPU dialect.
//
//===----------------------------------------------------------------------===//

#ifndef GPU_OPS
#define GPU_OPS

include "mlir/Dialect/DLTI/DLTIBase.td"
include "mlir/Dialect/GPU/IR/GPUBase.td"
include "mlir/Dialect/GPU/IR/CompilationAttrInterfaces.td"
include "mlir/Dialect/GPU/IR/CompilationAttrs.td"
include "mlir/Dialect/GPU/IR/GPUDeviceMappingAttr.td"
include "mlir/Dialect/GPU/IR/ParallelLoopMapperAttr.td"
include "mlir/IR/CommonTypeConstraints.td"
include "mlir/IR/EnumAttr.td"
include "mlir/IR/RegionKindInterface.td"
include "mlir/IR/SymbolInterfaces.td"
include "mlir/Interfaces/ControlFlowInterfaces.td"
include "mlir/Interfaces/DataLayoutInterfaces.td"
include "mlir/IR/OpAsmInterface.td"
include "mlir/Interfaces/FunctionInterfaces.td"
include "mlir/Interfaces/InferIntRangeInterface.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"

//===----------------------------------------------------------------------===//
// GPU Dialect operations.
//===----------------------------------------------------------------------===//

class GPU_Op<string mnemonic, list<Trait> traits = []> :
    Op<GPU_Dialect, mnemonic, traits>;

def GPU_Dimension : I32EnumAttr<"Dimension",
    "a dimension, either 'x', 'y', or 'z'",
    [
      I32EnumAttrCase<"x", 0>,
      I32EnumAttrCase<"y", 1>,
      I32EnumAttrCase<"z", 2>
    ]>{
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::gpu";
}
def GPU_DimensionAttr : EnumAttr<GPU_Dialect, GPU_Dimension, "dim">;

class GPU_IndexOp<string mnemonic, list<Trait> traits = []> :
    GPU_Op<mnemonic, !listconcat(traits, [
        Pure,
        DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>,
        DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>])>,
    Arguments<(ins GPU_DimensionAttr:$dimension,
                   OptionalAttr<IndexAttr>:$upper_bound)>, Results<(outs Index)> {
  let assemblyFormat = "$dimension (`upper_bound` $upper_bound^)? attr-dict";
  let extraClassDefinition = [{
    void $cppClass::getAsmResultNames(
        llvm::function_ref<void(mlir::Value, mlir::StringRef)> setNameFn) {
      auto dimStr = stringifyDimension(getDimensionAttr().getValue());
      auto opName = getOperationName();
      opName.consume_front("gpu.");
      SmallString<8> resultName({opName, "_", dimStr});
      setNameFn(getResult(),resultName);
    }
  }];
  let builders = [
    OpBuilder<(ins "::mlir::gpu::Dimension":$dimension), [{
      build($_builder, $_state, dimension, /*upperBound=*/nullptr);
    }]>,
    OpBuilder<(ins "::mlir::Type":$resultType, "::mlir::gpu::Dimension":$dimension), [{
      build($_builder, $_state, resultType, dimension, /*upperBound=*/nullptr);
    }]>
  ];
}

def GPU_ClusterDimOp : GPU_IndexOp<"cluster_dim"> {
  let description = [{
    Returns the number of cluster identifiers per grid along
    the x, y, or z `dimension`.

    Example:

    ```mlir
    %cDimX = gpu.cluster_dim x
    ```

    If `upper_bound` is set, then executing (a lowering of) this operation in an
    environment where the clusters per grid is greater than `upper_bound` causes
    undefined behavior.

    There is an implicit upper bound of `kMaxDim` (currently uint32_t::max).
  }];
}

def GPU_ClusterDimBlocksOp : GPU_IndexOp<"cluster_dim_blocks"> {
  let description = [{
    Returns the number of thread blocks in the cluster along
    the x, y, or z `dimension`.

    Example:

    ```mlir
    %cDimBlocksX = gpu.cluster_dim_blocks x
    ```

    If `upper_bound` is set, then executing (a lowering of) this operation in an
    environment where the thread blocks per cluster  is greater than `upper_bound`
    causes undefined behavior.

    There is an implicit upper bound of `kMaxClusterDim` (currently 8).
  }];
}

def GPU_ClusterIdOp : GPU_IndexOp<"cluster_id"> {
  let description = [{
    Returns the cluster id, i.e. the index of the current cluster within the
    grid along the x, y, or z `dimension`.

    Example:

    ```mlir
    %cIdY = gpu.cluster_id y
    ```

    If `upper_bound` is set, then executing (a lowering of) this operation in an
    environment where the number of clusters in the grid along `dimension` is
    greater than `upper_bound` causes undefined behavior.

    There is an implicit upper bound of `kMaxDim` (currently uint32_t::max).
  }];
}

def GPU_ClusterBlockIdOp : GPU_IndexOp<"cluster_block_id"> {
  let description = [{
    Returns the block id within the cluster along the x, y, or z `dimension`.

    Example:

    ```mlir
    %cBlockIdY = gpu.cluster_block_id y
    ```

    If `upper_bound` is set, then executing (a lowering of) this operation in an
    environment where the number of thread blocks per cluster  along `dimension`
    is greater than `upper_bound` causes undefined behavior.

    There is an implicit upper bound of `kMaxClusterDim` (currently 8).
  }];
}

def GPU_BlockDimOp : GPU_IndexOp<"block_dim"> {
  let description = [{
    Returns the number of threads in the thread block (aka the block size) along
    the x, y, or z `dimension`.

    Example:

    ```mlir
    %bDimX = gpu.block_dim x
    ```

    If `known_block_size` is set on an this operation's enclosing `gpu.func`,
    or `gpu.known_block_size` is set on an enclosing `FunctionOpInterface`
    implementor, or if the enclosing `gpu.launch` specifies a constant size for
    `dimension`'s blocks, these contextual facts may be used to infer that this
    operation has a constant value, though such a transformation will not be
    performed by canonicalization or the default constant folder. Executions which
    cause that constant-value assumption to be false incur undefined behavior.

    If `upper_bound` is set, executions where the bblock size along `dimension`
    exceeds `upper_bound` cause undefined behavior.

    There is an implicit upper bound of `kMaxDim` (currently uint32_t::max).
  }];
}
def GPU_BlockIdOp : GPU_IndexOp<"block_id"> {
  let description = [{
    Returns the block id, i.e. the index of the current block within the grid
    along the x, y, or z `dimension`.

    Example:

    ```mlir
    %bIdY = gpu.block_id y
    ```

    If `upper_bound` is set, or if one can be inferred from `known_grid_size`-type
    annotations in context, executions where the block index in `dimension` would
    be greater than or equal to that bound cause undefined behavior. `upper_bound`
    takes priority over bounds inferrable from context.

    There is an implicit upper bound of `kMaxDim` (currently uint32_t::max).
  }];
}
def GPU_GridDimOp : GPU_IndexOp<"grid_dim"> {
  let description = [{
    Returns the number of thread blocks in the grid along the x, y, or z
    `dimension`.

    Example:

    ```mlir
    %gDimZ = gpu.grid_dim z
    ```


    If `known_grid_size` is set on an this operation's enclosing `gpu.func`,
    or `gpu.known_grid_size` is set on an enclosing `FunctionOpInterface`
    implementor, or if the enclosing `gpu.launch` specifies a constant size for
    `dimension`'s grid length, these contextual facts may be used to infer that this
    operation has a constant value, though such a transformation will not be
    performed by canonicalization or the default constant folder. Executions which
    cause that constant-value assumption to be false incur undefined behavior.

    If `upper_bound` is set, executions where the grid size in `dimension` would
    exceed `upper_bound` cause undefined behavior.

    There is an implicit upper bound of `kMaxDim` (currently uint32_t::max).
  }];
}
def GPU_ThreadIdOp : GPU_IndexOp<"thread_id"> {
  let description = [{
    Returns the thread id, i.e. the index of the current thread within the block
    along the x, y, or z `dimension`.

    Example:

    ```mlir
    %tIdX = gpu.thread_id x
    ```

    If `upper_bound` is set, or if one can be inferred from `known_block_size`-type
    annotations in context, executions where the thread index would be greater
    than or equal to that bound cause undefined behavior.

    There is an implicit upper bound of `kMaxDim` (currently uint32_t::max).
  }];
}

def GPU_LaneIdOp : GPU_Op<"lane_id", [
      Pure, DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>]> {
  let description = [{
    Returns the lane id within the subgroup (warp/wave).

    Example:
    ```mlir
    %laneId = gpu.lane_id
    ```

    If `upper_bound` is set, executions with more than `upper_bound` lanes per
    subgroup cause undefined behavior. In the abscence of `upper_bound`,
    the lane id is still assumed to be non-negative and less than the
    target-independent `kMaxSubgroupSize` (currently 128).
  }];
  let arguments = (ins OptionalAttr<IndexAttr>:$upper_bound);
  let results = (outs Index:$result);
  let assemblyFormat = "(`upper_bound` $upper_bound^)? attr-dict";
}

def GPU_SubgroupIdOp : GPU_Op<"subgroup_id", [
      Pure, DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>]>,
    Arguments<(ins OptionalAttr<IndexAttr>:$upper_bound)>,
    Results<(outs Index:$result)> {
  let description = [{
    Returns the subgroup id, i.e., the index of the current subgroup within the
    workgroup.

    Example:

    ```mlir
    %sgId = gpu.subgroup_id : index
    ```

    Executions where there are more than `upper_bound` subgroups per workgroup
    cause undefined behavior. There is an implicit upper bound of `kMaxDim`
    (currently uint32_t::max).
  }];

  let assemblyFormat = "(`upper_bound` $upper_bound^)? attr-dict `:` type($result)";
}

def GPU_GlobalIdOp : GPU_IndexOp<"global_id"> {
  let description = [{
    Returns the unique global workitem/thread id, i.e., the unique index of the
    current workitem/thread within all workgroups / grid along the x, y, or z
    `dimension`.

    Example:

    ```mlir
    %gidX = gpu.global_id x
    %gidX = gpu.global_id x upper_bound 65536
    ```

    The `upper_bound` attribute defines an upper bound analogously to the ones on
    `thread_id` and `block_id`. If one is not set, the bound may be inferred from
    a combination of `known_block_size` and `known_grid_size`-type annotations.
  }];
}


def GPU_NumSubgroupsOp : GPU_Op<"num_subgroups", [
      Pure, DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>]>,
    Arguments<(ins OptionalAttr<IndexAttr>:$upper_bound)>,
    Results<(outs Index:$result)> {
  let description = [{
    Returns the number of subgroups within a workgroup.

    Example:

    ```mlir
    %numSg = gpu.num_subgroups : index
    ```

    If `upper_bound` is set, executions with more than `upper_bound` subgroups
    per workgroup cause undefined behavior. There is a default upper bound of
    `kMaxDim` (currently uint32_t::max).
  }];

  let assemblyFormat = "(`upper_bound` $upper_bound^)? attr-dict `:` type($result)";
}

def GPU_SubgroupSizeOp : GPU_Op<"subgroup_size", [
      Pure, DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>]>,
    Arguments<(ins OptionalAttr<IndexAttr>:$upper_bound)>,
    Results<(outs Index:$result)> {
  let description = [{
    Returns the number of threads within a subgroup.

    Example:

    ```mlir
    %sgSz = gpu.subgroup_size : index
    ```

    Executions where the number of threads per subgroup exceed `upper_bound` cause
    undefined behavior. When no `upper_bound` is specified, range analyses and
    similar machinery assume the default bound of `kMaxSubgroupSize`, currently
    128.
  }];

  let assemblyFormat = "(`upper_bound` $upper_bound^)? attr-dict `:` type($result)";
}

def GPU_OptionalDimSizeHintAttr : ConfinedAttr<OptionalAttr<DenseI32ArrayAttr>,
  [AttrConstraint<Or<[IsNullAttr.predicate, DenseArrayCount<3>.predicate]>,
    "with 3 elements (if present)">]>;

def GPU_GPUFuncOp : GPU_Op<"func", [
    HasParent<"GPUModuleOp">, AutomaticAllocationScope, FunctionOpInterface,
    IsolatedFromAbove, AffineScope
  ]> {
  let summary = "Function executable on a GPU";

  let description = [{
    Defines a function that can be executed on a GPU. This supports memory
    attribution and its body has a particular execution model.

    GPU functions are either kernels (as indicated by the `kernel` attribute) or
    regular functions. The former can be launched from the host side, while the
    latter are device side only.

    The memory attribution defines SSA values that correspond to memory buffers
    allocated in the memory hierarchy of the GPU (see below).

    The operation has one attached region that corresponds to the body of the
    function. The region arguments consist of the function arguments without
    modification, followed by buffers defined in memory annotations. The body of
    a GPU function, when launched, is executed by multiple work items. There are
    no guarantees on the order in which work items execute, or on the connection
    between them. In particular, work items are not necessarily executed in
    lock-step. Synchronization ops such as "gpu.barrier" should be used to
    coordinate work items. Declarations of GPU functions, i.e. not having the
    body region, are not supported.

    A function may optionally be annotated with the block and/or grid sizes
    that will be used when it is launched using the `known_block_size` and
    `known_grid_size` attributes, respectively. If set, these attributes must
    be arrays of three 32-bit integers giving the x, y, and z launch dimensions.
    Launching a kernel that has these annotations, or that calls a function with
    these annotations, using a block size or grid size other than what is specified
    is undefined behavior. These attributes may be set on non-`gpu.func` functions
    by using `gpu.known_block_size` or `gpu.known_grid_size`, but this carries
    the risk that they will de discarded.

    Syntax:

    ```
    op ::= `gpu.func` symbol-ref-id `(` argument-list `)` (`->`
    function-result-list)?
           memory-attribution `kernel`? function-attributes? region

    memory-attribution ::= (`workgroup` `(` ssa-id-and-type-list `)`)?
                           (`private` `(` ssa-id-and-type-list `)`)?
    ```

    Example:

    ```mlir
    gpu.func @foo(%arg0: index)
        workgroup(%workgroup: memref<32xf32, 3>)
        private(%private: memref<1xf32, 5>)
        kernel
        attributes {qux: "quux"} {
      gpu.return
    }
    ```

    The generic form illustrates the concept

    ```mlir
    "gpu.func"(%arg: index) {sym_name: "foo", kernel, qux: "quux"} ({
    ^bb0(%arg0: index, %workgroup: memref<32xf32, 3>,
         %private: memref<1xf32, 5>):
      "gpu.return"() : () -> ()
    }) : (index) -> ()
    ```

    Note the non-default memory spaces used in memref types in memory
    attribution.
  }];

  let arguments = (ins TypeAttrOf<FunctionType>:$function_type,
                       OptionalAttr<DictArrayAttr>:$arg_attrs,
                       OptionalAttr<DictArrayAttr>:$res_attrs,
                       OptionalAttr<DictArrayAttr>:$workgroup_attrib_attrs,
                       OptionalAttr<DictArrayAttr>:$private_attrib_attrs,
                       GPU_OptionalDimSizeHintAttr:$known_block_size,
                       GPU_OptionalDimSizeHintAttr:$known_grid_size);
  let regions = (region AnyRegion:$body);

  let skipDefaultBuilders = 1;

  let builders = [
    OpBuilder<(ins "StringRef":$name, "FunctionType":$type,
      CArg<"TypeRange", "{}">:$workgroupAttributions,
      CArg<"TypeRange", "{}">:$privateAttributions,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>
  ];

  let extraClassDeclaration = [{
    /// Returns `true` if the GPU function defined by this Op is a kernel, i.e.
    /// it is intended to be launched from host.
    bool isKernel() {
      return (*this)->getAttrOfType<UnitAttr>(
          GPUDialect::getKernelFuncAttrName()) != nullptr;
    }

    /// Returns the number of buffers located in the workgroup memory.
    unsigned getNumWorkgroupAttributions() {
      auto attr = (*this)->getAttrOfType<IntegerAttr>(
          getNumWorkgroupAttributionsAttrName());
      return attr ? attr.getInt() : 0;
    }

    /// Return the index of the first workgroup attribution in the block argument
    /// list.
    unsigned getFirstWorkgroupAttributionIndex() {
      return getFunctionType().getNumInputs();
    }

    /// Returns a list of block arguments that correspond to buffers located in
    /// the workgroup memory
    ArrayRef<BlockArgument> getWorkgroupAttributions() {
      auto begin =
          std::next(getBody().args_begin(), getFirstWorkgroupAttributionIndex());
      auto end = std::next(begin, getNumWorkgroupAttributions());
      return {begin, end};
    }

    /// Adds a new block argument that corresponds to buffers located in
    /// workgroup memory.
    BlockArgument addWorkgroupAttribution(Type type, Location loc);

    /// Get the workgroup attribution attribute dictionary for the attribution
    /// at index `index`, counted from the start of the workgroup attributions.
    DictionaryAttr getworkgroupAttributionAttrs(unsigned index);

    /// Set the workgroup attribution attribute dictionary for the attribution
    /// at index `index`, counted from the start of the workgroup attributions.
    void setworkgroupAttributionAttrs(unsigned index, DictionaryAttr value);

    /// Get an attribute for a workgroup attribution. `index` is counted
    /// from the start of the workgroup attributions, not the start of the block.
    Attribute getWorkgroupAttributionAttr(unsigned index, StringAttr name);
    Attribute getWorkgroupAttributionAttr(unsigned index, StringRef name) {
      return getWorkgroupAttributionAttr(index, StringAttr::get((*this)->getContext(), name));
    }

    /// Set an attribute for a workgroup attribution. `index` is counted
    /// from the start of the workgroup attributions, not the start of the block.
    /// A null `value` removes an attributino attribute.
    void setWorkgroupAttributionAttr(unsigned index, StringAttr name, Attribute value);
    void setWorkgroupAttributionAttr(unsigned index, StringRef name, Attribute value) {
      return setWorkgroupAttributionAttr(index, StringAttr::get((*this)->getContext(), name), value);
    }

    /// Returns the number of buffers located in the private memory.
    unsigned getNumPrivateAttributions() {
      return getBody().getNumArguments() - getFunctionType().getNumInputs() -
          getNumWorkgroupAttributions();
    }

    /// Returns the index of the first private buffer in the block argument list.
    unsigned getFirstPrivateAttributionIndex() {
      // Buffers on the private memory always come after buffers on the workgroup
      // memory.
      return getFunctionType().getNumInputs() + getNumWorkgroupAttributions();
    }

    /// Returns a list of block arguments that correspond to buffers located in
    /// the private memory.
    ArrayRef<BlockArgument> getPrivateAttributions() {
      auto begin =
          std::next(getBody().args_begin(), getFirstPrivateAttributionIndex());
      return {begin, getBody().args_end()};
    }

    /// Adds a new block argument that corresponds to buffers located in
    /// private memory.
    BlockArgument addPrivateAttribution(Type type, Location loc);

    /// Get the private attribution attribute dictionary for the attribution
    /// at index `index`, counted from the start of the private attributions.
    DictionaryAttr getPrivateAttributionAttrs(unsigned index);

    /// Set the private attribution attribute dictionary for the attribution
    /// at index `index`, counted from the start of the private attributions.
    void setPrivateAttributionAttrs(unsigned index, DictionaryAttr value);

    /// Get an attribute for a private attribution. `index` is counted
    /// from the start of the private attributions, not the start of the block.
    Attribute getPrivateAttributionAttr(unsigned index, StringAttr name);
    Attribute getPrivateAttributionAttr(unsigned index, StringRef name) {
      return getPrivateAttributionAttr(index, StringAttr::get((*this)->getContext(), name));
    }

    /// Set an attribute for a private attribution. `index` is counted
    /// from the start of the private attributions, not the start of the block.
    /// A null `value` removes an attribute.
    void setPrivateAttributionAttr(unsigned index, StringAttr name, Attribute value);
    void setPrivateAttributionAttr(unsigned index, StringRef name, Attribute value) {
      return setPrivateAttributionAttr(index, StringAttr::get((*this)->getContext(), name), value);
    }

    /// Returns the name of the attribute containing the number of buffers
    /// located in the workgroup memory.
    static StringRef getNumWorkgroupAttributionsAttrName() {
      return "workgroup_attributions";
    }

    /// Returns the argument types of this function.
    ArrayRef<Type> getArgumentTypes() { return getFunctionType().getInputs(); }

    /// Returns the result types of this function.
    ArrayRef<Type> getResultTypes() { return getFunctionType().getResults(); }

    Region *getCallableRegion() { return &getBody(); }

    /// Returns the keywords used in the custom syntax for this Op.
    static StringRef getWorkgroupKeyword() { return "workgroup"; }
    static StringRef getPrivateKeyword() { return "private"; }
    static StringRef getKernelKeyword() { return "kernel"; }

    /// Hook for FunctionOpInterface verifier.
    LogicalResult verifyType();

    /// Verifies the body of the function.
    LogicalResult verifyBody();
  }];
  let hasCustomAssemblyFormat = 1;
}

def GPU_DynamicSharedMemoryOp : GPU_Op<"dynamic_shared_memory", [Pure]>
{
  let summary = "Get the memref for dynamic shared memory";

  let description = [{
    This operation provides a memref pointer to the start of dynamic shared
    memory, often referred to as workgroup memory. It's important to note that
    this dynamic shared memory needs to be allocated at kernel launch. One can
    conveniently utilize `the dynamic_shared_memory_size` parameter of
    `gpu.launch` for this purpose.

    Examples:
    ```mlir
    %0 = gpu.dynamic.shared.memory : memref<?xi8, #gpu.address_space<workgroup>>
    %1 = memref.view %0[%c8192][] : memref<?xi8, #gpu.address_space<workgroup>>
                            to memref<32x64xf32, #gpu.address_space<workgroup>>
    %2 = memref.view %0[%c16384][] : memref<?xi8, #gpu.address_space<workgroup>>
                            to memref<32x64xf32, #gpu.address_space<workgroup>>
    ```
  }];
  let arguments = (ins);
  let results = (outs Arg<MemRefRankOf<[I8], [1]>>:$resultMemref);
  let assemblyFormat = [{ attr-dict `:` type($resultMemref) }];
  let hasVerifier = 1;
}

def LaunchIndx : AnyTypeOf<[Index, I32, I64]>;

def GPU_LaunchFuncOp :GPU_Op<"launch_func", [
      GPU_AsyncOpInterface, AttrSizedOperandSegments,
      AllTypesMatch<["gridSizeX", "gridSizeY", "gridSizeZ", "blockSizeX",
                     "blockSizeY", "blockSizeZ"]>]>,
    Arguments<(ins Variadic<GPU_AsyncToken>:$asyncDependencies,
               SymbolRefAttr:$kernel,
               LaunchIndx:$gridSizeX,
               LaunchIndx:$gridSizeY,
               LaunchIndx:$gridSizeZ,
               LaunchIndx:$blockSizeX,
               LaunchIndx:$blockSizeY,
               LaunchIndx:$blockSizeZ,
               Optional<LaunchIndx>:$clusterSizeX,
               Optional<LaunchIndx>:$clusterSizeY,
               Optional<LaunchIndx>:$clusterSizeZ,
               Optional<I32>:$dynamicSharedMemorySize,
               Variadic<AnyType>:$kernelOperands,
               Optional<AnyType>:$asyncObject)>,
    Results<(outs Optional<GPU_AsyncToken>:$asyncToken)> {
  let summary = "Launches a function as a GPU kernel";

  let description = [{
    Launch a kernel function on the specified grid of thread blocks.
    `gpu.launch` operations are lowered to `gpu.launch_func` operations by
    outlining the kernel body into a function in a dedicated module, which
    reflects the separate compilation process. The kernel function is required
    to have the `gpu.kernel` attribute. The module containing the kernel
    function is required to be a gpu.module. And finally, the module containing
    the kernel module (which thus cannot be the top-level module) is required
    to have the `gpu.container_module` attribute. The `gpu.launch_func`
    operation has a symbol attribute named `kernel` to identify the fully
    specified kernel function to launch (both the gpu.module and func).

    The `gpu.launch_func` supports async dependencies: the kernel does not start
    executing until the ops producing those async dependencies have completed.

    By the default, the host implicitly blocks until kernel execution has
    completed. If the `async` keyword is present, the host does not block but
    instead a `!gpu.async.token` is returned. Other async GPU ops can take this
    token as dependency.

    The operation requires at least the grid and block sizes along the x,y,z
    dimensions as arguments. When a lower-dimensional kernel is required,
    unused sizes must be explicitly set to `1`.

    The remaining operands are optional. The first optional operand corresponds
    to the amount of dynamic shared memory a kernel's workgroup should be
    allocated; when this operand is not present, a zero size is assumed.

    The remaining operands if present are passed as arguments to the kernel
    function.

    The `gpu.launch_func` also supports kernel launching with clusters if
    supported by the target architecture. The cluster size can be set by
    `clusterSizeX`, `clusterSizeY`, and `clusterSizeZ` arguments. When these
    arguments are present, the Op launches a kernel that clusters the given
    thread blocks. This feature is exclusive to certain architectures.

    Example:

    ```mlir
    module attributes {gpu.container_module} {

      // This module creates a separate compilation unit for the GPU compiler.
      gpu.module @kernels {
        func.func @kernel_1(%arg0 : f32, %arg1 : memref<?xf32, 1>)
            attributes { nvvm.kernel = true } {

          // Operations that produce block/thread IDs and dimensions are
          // injected when outlining the `gpu.launch` body to a function called
          // by `gpu.launch_func`.
          %tIdX = gpu.thread_id x
          %tIdY = gpu.thread_id y
          %tIdZ = gpu.thread_id z

          %bDimX = gpu.block_dim x
          %bDimY = gpu.block_dim y
          %bDimZ = gpu.block_dim z

          %bIdX = gpu.block_id x
          %bIdY = gpu.block_id y
          %bIdZ = gpu.block_id z

          %gDimX = gpu.grid_dim x
          %gDimY = gpu.grid_dim y
          %gDimZ = gpu.grid_dim z

          // (Optional)  Cluster size only for support architectures
          %cIdX = gpu.cluster_id x
          %cIdY = gpu.cluster_id y
          %cIdZ = gpu.cluster_id z

          %cDimX = gpu.cluster_dim x
          %cDimY = gpu.cluster_dim y
          %cDimZ = gpu.cluster_dim z

          "some_op"(%bx, %tx) : (index, index) -> ()
          %42 = load %arg1[%bx] : memref<?xf32, 1>
        }
      }

      %t0 = gpu.wait async
      gpu.launch_func
          async                           // (Optional) Don't block host, return token.
          [%t0]                           // (Optional) Execute only after %t0 has completed.
          @kernels::@kernel_1             // Kernel function.
          clusters in (%cst, %cst, %cst)  // (Optional) Cluster size only for support architectures.
          blocks in (%cst, %cst, %cst)    // Grid size.
          threads in (%cst, %cst, %cst)   // Block size.
          dynamic_shared_memory_size %s   // (Optional) Amount of dynamic shared
                                          // memory to allocate for a workgroup.
          args(%arg0 : f32,               // (Optional) Kernel arguments.
               %arg1 : memref<?xf32, 1>)
    }
    ```
  }];

  let skipDefaultBuilders = 1;

  let builders = [
    OpBuilder<(ins "GPUFuncOp":$kernelFunc, "KernelDim3":$gridSize,
      "KernelDim3":$blockSize, "Value":$dynamicSharedMemorySize,
      "ValueRange":$kernelOperands,
      CArg<"Type", "nullptr">:$asyncTokenType,
      CArg<"ValueRange", "{}">:$asyncDependencies,
      CArg<"std::optional<KernelDim3>", "std::nullopt">:$clusterSize)>,
    OpBuilder<(ins "SymbolRefAttr":$kernel, "KernelDim3":$gridSize,
      "KernelDim3":$blockSize, "Value":$dynamicSharedMemorySize,
      "ValueRange":$kernelOperands,
      "Type":$asyncTokenType,
      CArg<"ValueRange", "{}">:$asyncDependencies,
      CArg<"std::optional<KernelDim3>", "std::nullopt">:$clusterSize)>,
    OpBuilder<(ins "SymbolRefAttr":$kernel, "KernelDim3":$gridSize,
      "KernelDim3":$blockSize, "Value":$dynamicSharedMemorySize,
      "ValueRange":$kernelOperands,
      CArg<"Value", "nullptr">:$asyncObject,
      CArg<"std::optional<KernelDim3>", "std::nullopt">:$clusterSize)>
  ];

  let extraClassDeclaration = [{
    /// The name of the kernel's containing module.
    StringAttr getKernelModuleName();

    /// The name of the kernel.
    StringAttr getKernelName();

    /// Returns true if cluster size is specified.
    bool hasClusterSize() {
      if (getClusterSizeX() && getClusterSizeY() && getClusterSizeZ())
        return true;
      return false;
    }

    /// The number of operands passed to the kernel function.
    unsigned getNumKernelOperands();

    /// The i-th operand passed to the kernel function.
    Value getKernelOperand(unsigned i);

    /// Get the SSA values passed as operands to specify the cluster size.
    /// When the cluster sizes are not specified, it asserts.
    KernelDim3 getClusterSizeOperandValues();

    /// Get the SSA values passed as operands to specify the grid size.
    KernelDim3 getGridSizeOperandValues();

    /// Get the SSA values passed as operands to specify the block size.
    KernelDim3 getBlockSizeOperandValues();

    // This needs to quietly verify if attributes with names defined below are
    // present since it is run before the verifier of this op.
    friend LogicalResult GPUDialect::verifyOperationAttribute(Operation *,
                                                              NamedAttribute);
  }];

  let assemblyFormat = [{
      custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
      (`<` $asyncObject^ `:` type($asyncObject) `>`)?
      $kernel
      ( `clusters` `in` ` ` `(` $clusterSizeX^ `,` $clusterSizeY `,` $clusterSizeZ `)` )?
      `blocks` `in` ` ` `(` $gridSizeX `,` $gridSizeY `,` $gridSizeZ `)`
      `threads` `in` ` ` `(` $blockSizeX `,` $blockSizeY `,` $blockSizeZ `)`
      custom<LaunchDimType>(type($gridSizeX), ref($clusterSizeX), type($clusterSizeX), type($clusterSizeY), type($clusterSizeZ))
      (`dynamic_shared_memory_size` $dynamicSharedMemorySize^)?
      custom<LaunchFuncOperands>($kernelOperands, type($kernelOperands)) attr-dict
  }];
  let hasVerifier = 1;
}

def GPU_LaunchOp : GPU_Op<"launch", [
      AutomaticAllocationScope, AttrSizedOperandSegments, GPU_AsyncOpInterface,
      DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>,
      RecursiveMemoryEffects]>,
    Arguments<(ins Variadic<GPU_AsyncToken>:$asyncDependencies,
               Index:$gridSizeX, Index:$gridSizeY, Index:$gridSizeZ,
               Index:$blockSizeX, Index:$blockSizeY, Index:$blockSizeZ,
               Optional<Index>:$clusterSizeX,
               Optional<Index>:$clusterSizeY,
               Optional<Index>:$clusterSizeZ,
               Optional<I32>:$dynamicSharedMemorySize,
               OptionalAttr<SymbolRefAttr>:$kernelFunc,
               OptionalAttr<SymbolRefAttr>:$kernelModule)>,
    Results<(outs Optional<GPU_AsyncToken>:$asyncToken)> {
  let summary = "GPU kernel launch operation";

  let description = [{
    Launch a kernel on the specified grid of thread blocks. The body of the
    kernel is defined by the single region that this operation contains. The
    operation takes an optional list of async dependencies followed by six
    operands and an optional operand.

    The `async` keyword indicates the kernel should be launched asynchronously;
    the operation returns a new !gpu.async.token when the keyword is specified.
    The kernel launched does not start executing until the ops producing its
    async dependencies (optional operands) have completed.

    The first three operands (following any async dependencies) are grid sizes
    along the x,y,z dimensions and the following three are block sizes along the
    x,y,z dimensions. When a lower-dimensional kernel is required, unused sizes
    must be explicitly set to `1`.  The last operand is optional and corresponds
    to the amount of dynamic shared memory a kernel's workgroup should be
    allocated; when this operand is not present, a zero size is assumed.

    The body region has at least _twelve_ arguments, or _eighteen_ if cluster
    dimensions are present, grouped as follows:

    -   three optional arguments that contain cluster identifiers along x,y,z
        dimensions;
    -   three arguments that contain block identifiers along x,y,z dimensions;
    -   three arguments that contain thread identifiers along x,y,z dimensions;
    -   operands of the `gpu.launch` operation as is (i.e. the operands for
        grid and block sizes).
    -   a variadic number of Workgroup memory attributions.
    -   a variadic number of Private memory attributions.

    The `kernelFunc` and `kernelModule` attributes are optional and specifies
    the kernel name and a module in which the kernel should be outlined. 

    Syntax:

    ```
    operation ::= `gpu.launch` (`async` (`[` ssa-id-list `]`)? )?
                             ( `clusters` `(` ssa-id-list `)` `in` ssa-reassignment )?
                             `blocks` `(` ssa-id-list `)` `in` ssa-reassignment
                             `threads` `(` ssa-id-list `)` `in` ssa-reassignment
                             (dynamic_shared_memory_size ssa-use)?
                             memory-attribution
                             region attr-dict?
    ssa-reassignment ::= `(` ssa-id `=` ssa-use (`,` ssa-id `=` ssa-use)* `)`
    memory-attribution ::= (`workgroup` `(` ssa-id-and-type-list `)`)?
                           (`private` `(` ssa-id-and-type-list `)`)?
    ```

    Example:

    ```mlir
    gpu.launch blocks(%bx, %by, %bz) in (%sz_bx = %0, %sz_by = %1, %sz_bz = %2)
               threads(%tx, %ty, %tz) in (%sz_tx = %3, %sz_ty = %4, %sz_tz = %5) {
      // Block and thread identifiers, as well as block/grid sizes are
      // immediately usable inside body region.
      "some_op"(%bx, %tx) : (index, index) -> ()
      // Assuming %val1 is defined outside the gpu.launch region.
      %42 = load %val1[%bx] : memref<?xf32, 1>
    }

    // Generic syntax explains how the pretty syntax maps to the IR structure.
    "gpu.launch"(%cst, %cst, %c1,  // Grid sizes.
                 %cst, %c1, %c1)   // Block sizes.

        {/*attributes*/}
        // All sizes and identifiers have "index" size.
        : (index, index, index, index, index, index) -> () {
    // The operation passes block and thread identifiers, followed by grid and
    // block sizes.
    ^bb0(%bx : index, %by : index, %bz : index,
         %tx : index, %ty : index, %tz : index,
         %num_bx : index, %num_by : index, %num_bz : index,
         %num_tx : index, %num_ty : index, %num_tz : index)
      "some_op"(%bx, %tx) : (index, index) -> ()
      %3 = "memref.load"(%val1, %bx) : (memref<?xf32, 1>, index) -> f32
    }

    // Launch with memory attributions.
    gpu.launch blocks(%bx, %by, %bz) in (%sz_bx = %0, %sz_by = %1, %sz_bz = %2)
               threads(%tx, %ty, %tz) in (%sz_tx = %3, %sz_ty = %4, %sz_tz = %5)
               workgroup(%workgroup: memref<32xf32, 3>)
               private(%private: memref<1xf32, 5>) {
      // Block and thread identifiers, as well as block/grid sizes are
      // immediately usable inside body region.
      "some_op"(%bx, %tx) : (index, index) -> ()
      // Assuming %val1 is defined outside the gpu.launch region.
      %42 = load %workgroup[%bx] : memref<32xf32, 3>
    }

    // Launch with clusters.
    gpu.launch clusters(%cx, %cy, %cz) in (%sz_cx = %0, %sz_cy = %1, %sz_cz = %2)
               blocks(%bx, %by, %bz) in (%sz_bx = %3, %sz_by = %4, %sz_bz = %5)
               threads(%tx, %ty, %tz) in (%sz_tx = %6, %sz_ty = %7, %sz_tz = %8)
    {
      // Cluster, block and thread identifiers, as well as cluster/block/grid
      // sizes are immediately usable inside body region.
      "some_op"(%cx, %bx, %tx) : (index, index, index) -> ()
    }
    ```

    Rationale: using operation/block arguments gives analyses a clear way of
    understanding that a value has additional semantics (e.g., we will need to
    know what value corresponds to threadIdx.x for coalescing). We can recover
    these properties by analyzing the operations producing values, but it is
    easier just to have that information by construction.
  }];

  let regions = (region AnyRegion:$body);

  let skipDefaultBuilders = 1;

  let builders = [
    OpBuilder<(ins "Value":$gridSizeX, "Value":$gridSizeY,
      "Value":$gridSizeZ, "Value":$blockSizeX, "Value":$blockSizeY,
      "Value":$blockSizeZ,
      CArg<"Value", "nullptr">:$dynamicSharedMemorySize,
      CArg<"Type", "nullptr">:$asyncTokenType,
      CArg<"ValueRange", "{}">:$asyncDependencies,
      CArg<"TypeRange", "{}">:$workgroupAttributions,
      CArg<"TypeRange", "{}">:$privateAttributions,
      CArg<"Value", "nullptr">:$clusterSizeX,
      CArg<"Value", "nullptr">:$clusterSizeY,
      CArg<"Value", "nullptr">:$clusterSizeZ)>
  ];

  let extraClassDeclaration = [{
    /// Get the SSA values corresponding to kernel block identifiers.
    KernelDim3 getBlockIds();
    /// Get the SSA values corresponding to kernel thread identifiers.
    KernelDim3 getThreadIds();
    /// Get the SSA values corresponding to kernel cluster identifiers.
    std::optional<KernelDim3> getClusterIds();
    /// Get the SSA values corresponding to kernel grid size.
    KernelDim3 getGridSize();
    /// Get the SSA values corresponding to kernel block size.
    KernelDim3 getBlockSize();
    /// Get the SSA values corresponding to kernel cluster size.
    std::optional<KernelDim3> getClusterSize();

    /// Get the SSA values passed as operands to specify the grid size.
    KernelDim3 getGridSizeOperandValues();
    /// Get the SSA values passed as operands to specify the block size.
    KernelDim3 getBlockSizeOperandValues();
    /// Get the SSA values passed as operands to specify the cluster size.
    std::optional<KernelDim3> getClusterSizeOperandValues();

    static StringRef getBlocksKeyword() { return "blocks"; }
    static StringRef getClustersKeyword() { return "clusters"; }
    static StringRef getThreadsKeyword() { return "threads"; }
    static StringRef getDynamicSharedMemorySizeKeyword() {
      return "dynamic_shared_memory_size";
    }

    /// The number of launch configuration operands, placed at the leading
    /// positions of the operand list.
    static constexpr unsigned kNumConfigOperands = 6;

    /// The number of region attributes containing the launch configuration,
    /// placed in the leading positions of the argument list.
    static constexpr unsigned kNumConfigRegionAttributes = 12;

    /// Returns true if cluster size is specified.
    bool hasClusterSize() {
      if (getClusterSizeX() && getClusterSizeY() && getClusterSizeZ())
        return true;
      return false;
    }
    /// Returns the number of operands including cluster size
    unsigned getNumConfigOperands() {
      return kNumConfigOperands + (hasClusterSize() ? 3 : 0);
    }
    /// Returns the number of region attributes including cluster size
    unsigned getNumConfigRegionAttributes() {
      return kNumConfigRegionAttributes + (hasClusterSize() ? 6 : 0);
    }

    /// Returns the keywords used in the custom syntax for this Op.
    static StringRef getWorkgroupKeyword() { return "workgroup"; }
    static StringRef getPrivateKeyword() { return "private"; }

    /// Returns the number of buffers located in the workgroup memory.
    unsigned getNumWorkgroupAttributions() {
      auto attr = (*this)->getAttrOfType<IntegerAttr>(
          getNumWorkgroupAttributionsAttrName());
      return attr ? attr.getInt() : 0;
    }

    /// Returns a list of block arguments that correspond to buffers located in
    /// the workgroup memory
    ArrayRef<BlockArgument> getWorkgroupAttributions() {
      auto begin =
          std::next(getBody().args_begin(), getNumConfigRegionAttributes());
      auto end = std::next(begin, getNumWorkgroupAttributions());
      return {begin, end};
    }

    /// Adds a new block argument that corresponds to buffers located in
    /// workgroup memory.
    BlockArgument addWorkgroupAttribution(Type type, Location loc);

    /// Returns the number of buffers located in the private memory.
    unsigned getNumPrivateAttributions() {
      return getBody().getNumArguments() - getNumConfigRegionAttributes() -
          getNumWorkgroupAttributions();
    }

    /// Returns a list of block arguments that correspond to buffers located in
    /// the private memory.
    ArrayRef<BlockArgument> getPrivateAttributions() {
      // Buffers on the private memory always come after buffers on the workgroup
      // memory.
      auto begin =
          std::next(getBody().args_begin(),
                    getNumConfigRegionAttributes() + getNumWorkgroupAttributions());
      return {begin, getBody().args_end()};
    }

    /// Adds a new block argument that corresponds to buffers located in
    /// private memory.
    BlockArgument addPrivateAttribution(Type type, Location loc);

    /// Returns the name of the attribute containing the number of buffers
    /// located in the workgroup memory.
    static StringRef getNumWorkgroupAttributionsAttrName() {
      return "workgroup_attributions";
    }
  }];

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasRegionVerifier = 1;
  let hasVerifier = 1;
}

def GPU_PrintfOp : GPU_Op<"printf", [MemoryEffects<[MemWrite]>]>,
  Arguments<(ins StrAttr:$format,
                Variadic<AnyTypeOf<[AnyInteger, Index, AnyFloat]>>:$args)> {
  let summary = "Device-side printf, as in CUDA or OpenCL, for debugging";
  let description = [{
    `gpu.printf` takes a literal format string `format` and an arbitrary number of
    scalar arguments that should be printed.

    The format string is a C-style printf string, subject to any restrictions
    imposed by one's target platform.
  }];
  let assemblyFormat = [{
    $format attr-dict ($args^ `:` type($args))?
  }];
}

def GPU_ReturnOp : GPU_Op<"return", [HasParent<"GPUFuncOp">, Pure,
                                     Terminator]>,
    Arguments<(ins Variadic<AnyType>:$operands)>, Results<(outs)> {
  let summary = "Terminator for GPU functions.";
  let description = [{
    A terminator operation for regions that appear in the body of  `gpu.func`
    functions. The operands to the `gpu.return` are the result values returned
    by an invocation of the `gpu.func`.
  }];

  let builders = [OpBuilder<(ins), [{ // empty}]>];

  let assemblyFormat = "attr-dict ($operands^ `:` type($operands))?";
  let hasVerifier = 1;
}

def GPU_TerminatorOp : GPU_Op<"terminator", [HasParent<"LaunchOp">,
                                             Pure, Terminator]>,
    Arguments<(ins)>, Results<(outs)> {
  let summary = "Terminator for GPU launch regions.";
  let description = [{
    A terminator operation for regions that appear in the body of `gpu.launch`
    operation.  These regions are not expected to return any value so the
    terminator takes no operands.
  }];

  let assemblyFormat = "attr-dict";
}

def GPU_YieldOp : GPU_Op<"yield", [Pure, ReturnLike, Terminator]>,
    Arguments<(ins Variadic<AnyType>:$values)> {
  let summary = "GPU yield operation";
  let description = [{
    gpu.yield` is a special terminator operation for blocks inside regions
    in gpu ops. It returns values to the immediately enclosing gpu op.

    Example:

    ```mlir
    gpu.yield %f0, %f1 : f32, f32
    ```
  }];

  let builders = [
    OpBuilder<(ins), [{ /* nothing to do */ }]>
  ];

  let assemblyFormat = "attr-dict ($values^ `:` type($values))?";
}

// These mirror the reduction combining kinds from the vector dialect.
def GPU_AllReduceOpAdd : I32EnumAttrCase<"ADD", 0, "add">;
def GPU_AllReduceOpMul : I32EnumAttrCase<"MUL", 1, "mul">;
def GPU_AllReduceOpMinUI : I32EnumAttrCase<"MINUI", 2, "minui">;
def GPU_AllReduceOpMinSI : I32EnumAttrCase<"MINSI", 3, "minsi">;
// Follows the `arith.minnumf` semantics.
def GPU_AllReduceOpMinnumF : I32EnumAttrCase<"MINNUMF", 4, "minnumf">;
def GPU_AllReduceOpMaxUI : I32EnumAttrCase<"MAXUI", 5, "maxui">;
def GPU_AllReduceOpMaxSI : I32EnumAttrCase<"MAXSI", 6, "maxsi">;
// Follows the `arith.maxnumf` semantics.
def GPU_AllReduceOpMaxnumF : I32EnumAttrCase<"MAXNUMF", 7, "maxnumf">;
def GPU_AllReduceOpAnd : I32EnumAttrCase<"AND", 8, "and">;
def GPU_AllReduceOpOr  : I32EnumAttrCase<"OR",  9, "or">;
def GPU_AllReduceOpXor : I32EnumAttrCase<"XOR", 10, "xor">;
// Follows the `arith.minimumf` semantics.
def GPU_AllReduceOpMinimumF : I32EnumAttrCase<"MINIMUMF", 11, "minimumf">;
// Follows the `arith.maximumf` semantics.
def GPU_AllReduceOpMaximumF : I32EnumAttrCase<"MAXIMUMF", 12, "maximumf">;

def GPU_AllReduceOperation : I32EnumAttr<"AllReduceOperation",
    "built-in reduction operations supported by gpu.allreduce.",
    [
      GPU_AllReduceOpAdd,
      GPU_AllReduceOpMul,
      GPU_AllReduceOpMinUI,
      GPU_AllReduceOpMinSI,
      GPU_AllReduceOpMinnumF,
      GPU_AllReduceOpMaxUI,
      GPU_AllReduceOpMaxSI,
      GPU_AllReduceOpMaxnumF,
      GPU_AllReduceOpAnd,
      GPU_AllReduceOpOr,
      GPU_AllReduceOpXor,
      GPU_AllReduceOpMinimumF,
      GPU_AllReduceOpMaximumF
    ]>{
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::gpu";
}

def AnyIntegerOrFloat : AnyTypeOf<[AnySignlessInteger, AnyFloat], "Integer or Float">;

def GPU_AllReduceOperationAttr : EnumAttr<GPU_Dialect, GPU_AllReduceOperation,
                                          "all_reduce_op">;

def GPU_AllReduceOp : GPU_Op<"all_reduce",
    [SameOperandsAndResultType, IsolatedFromAbove]> {
  let summary = "Reduce values among workgroup.";
  let description = [{
    The `all_reduce` op reduces the value of every work item across a local
    workgroup. The result is equal for all work items of a workgroup.

    For example, both

    ```mlir
    %1 = gpu.all_reduce add %0 {} : (f32) -> (f32)
    %2 = gpu.all_reduce %0 {
    ^bb(%lhs : f32, %rhs : f32):
      %sum = arith.addf %lhs, %rhs : f32
      "gpu.yield"(%sum) : (f32) -> ()
    } : (f32) -> (f32)
    ```

    compute the sum of each work item's %0 value. The first version specifies
    the accumulation as operation, whereas the second version specifies the
    accumulation as code region. The reduction operation must be one of:
    *  Integer types: `add`, `mul`, `minui`, `minsi`, `maxui`, `maxsi`, `and`,
       `or`, `xor`
    *  Floating point types: `add`, `mul`, `minnumf`, `maxnumf`, `minimumf`,
       `maximumf`

    If `uniform` flag is set either none or all work items of a workgroup
    need to execute this op in convergence.
  }];

  let arguments = (ins
    AnyIntegerOrFloat:$value,
    OptionalAttr<GPU_AllReduceOperationAttr>:$op,
    UnitAttr:$uniform
  );
  let results = (outs AnyIntegerOrFloat:$result);

  let regions = (region AnyRegion:$body);
  let assemblyFormat = [{ custom<AllReduceOperation>($op) $value
                          (`uniform` $uniform^)? $body attr-dict
                          `:` functional-type(operands, results) }];

  let hasFolder = 1;
  let hasRegionVerifier = 1;
}

def AnyIntegerOrFloatOr1DVector :
  AnyTypeOf<[AnyIntegerOrFloat, VectorOfRankAndType<[1], [AnyIntegerOrFloat]>]>;

def GPU_SubgroupReduceOp : GPU_Op<"subgroup_reduce", [SameOperandsAndResultType]> {
  let summary = "Reduce values among subgroup.";
  let description = [{
    The `subgroup_reduce` op reduces the values of lanes (work items) across a
    subgroup.

    The subgroup is divided into clusters starting at lane index 0. Within each
    cluster, there are `size` lanes, and the lane index advances by `stride`.
    A reduction is done for each cluster in parallel: every lane in the cluster
    is reduced, and the result is equal for all lanes in the cluster. If `size`
    is omitted, there is a single cluster covering the entire subgroup. If
    `stride` is omitted, the stride is 1 (the cluster's lanes are contiguous).

    When the reduced value is of a vector type, each vector element is reduced
    independently. Only 1-d vector types are allowed.

    Example:

    ```mlir
    %1 = gpu.subgroup_reduce add %a : (f32) -> f32
    %2 = gpu.subgroup_reduce add %b : (vector<4xf16>) -> vector<4xf16>
    %3 = gpu.subgroup_reduce add %c cluster(size = 4) : (f32) -> f32
    %3 = gpu.subgroup_reduce add %c cluster(size = 4, stride = 2) : (f32) -> f32
    ```

    If `uniform` flag is set either none or all lanes of a subgroup need to execute
    this op in convergence.

    The reduction operation must be one of:
    *  Integer types: `add`, `mul`, `minui`, `minsi`, `maxui`, `maxsi`, `and`,
       `or`, `xor`
    *  Floating point types: `add`, `mul`, `minnumf`, `maxnumf`, `minimumf`,
       `maximumf`
  }];

  let arguments = (ins
    AnyIntegerOrFloatOr1DVector:$value,
    GPU_AllReduceOperationAttr:$op,
    UnitAttr:$uniform,
    OptionalAttr<I32Attr>:$cluster_size,
    DefaultValuedAttr<I32Attr,"1">:$cluster_stride
  );
  let results = (outs AnyIntegerOrFloatOr1DVector:$result);

  let builders = [
    OpBuilder<(ins "Value":$value,
               "::mlir::gpu::AllReduceOperation":$op,
               "bool":$uniform), [{
      build($_builder, $_state, value, op, uniform, std::nullopt);
    }]>,
    OpBuilder<(ins "Value":$value,
               "::mlir::gpu::AllReduceOperation":$op,
               "bool":$uniform,
               "std::optional<uint32_t>":$cluster_size), [{
      build($_builder, $_state, value, op, uniform,
            cluster_size ? $_builder.getI32IntegerAttr(*cluster_size) : nullptr);
    }]>,
    OpBuilder<(ins "Value":$value,
               "::mlir::gpu::AllReduceOperation":$op,
               "bool":$uniform,
               "std::optional<uint32_t>":$cluster_size,
               "uint32_t":$cluster_stride), [{
      build($_builder, $_state, value, op, uniform,
            cluster_size ? $_builder.getI32IntegerAttr(*cluster_size) : nullptr,
            cluster_stride);
    }]>
  ];

  let assemblyFormat = [{ custom<AllReduceOperation>($op) $value
                          (`uniform` $uniform^)?
                          (`cluster` `(` `size` `=` $cluster_size^ (`,` `stride` `=` $cluster_stride^)? `)`)?
                          attr-dict
                          `:` functional-type(operands, results) }];

  let hasFolder = 1;
  let hasVerifier = 1;
}

def GPU_ShuffleOpXor  : I32EnumAttrCase<"XOR",  0, "xor">;
def GPU_ShuffleOpDown : I32EnumAttrCase<"DOWN", 1, "down">;
def GPU_ShuffleOpUp   : I32EnumAttrCase<"UP",   2, "up">;
def GPU_ShuffleOpIdx  : I32EnumAttrCase<"IDX",  3, "idx">;

def GPU_ShuffleMode : I32EnumAttr<"ShuffleMode",
    "Indexing modes supported by gpu.shuffle.",
    [
      GPU_ShuffleOpXor, GPU_ShuffleOpUp, GPU_ShuffleOpDown, GPU_ShuffleOpIdx,
    ]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::gpu";
}
def GPU_ShuffleModeAttr : EnumAttr<GPU_Dialect, GPU_ShuffleMode,
                                   "shuffle_mode">;

def GPU_ShuffleOp : GPU_Op<
    "shuffle", [Pure, AllTypesMatch<["value", "shuffleResult"]>]>,
    Arguments<(ins AnyIntegerOrFloatOr1DVector:$value, I32:$offset, I32:$width,
               GPU_ShuffleModeAttr:$mode)>,
    Results<(outs AnyIntegerOrFloatOr1DVector:$shuffleResult, I1:$valid)> {
  let summary = "Shuffles values within a subgroup.";
  let description = [{
    The "shuffle" op moves values to a across lanes (a.k.a., invocations,
    work items) within the same subgroup. The `width` argument specifies the
    number of lanes that participate in the shuffle, and must be uniform
    across all lanes. Further, the first `width` lanes of the subgroup must
    be active.

    The intepretation of the `offset` arguments depends on the selected
    `mode`.

    Returns the `shuffleResult` and `true` if the current lane id is smaller
    than `width`, and an unspecified value and `false` otherwise.

    `xor` example:

    ```mlir
    %1, %2 = gpu.shuffle xor %0, %offset, %width : f32
    ```

    For lane `k`, returns the value `%0` from lane `k ^ offset`. Every lane
    trades value with exactly one other lane.

    `down` example:

    ```mlir
    %cst1 = arith.constant 1 : i32
    %3, %4 = gpu.shuffle down %0, %cst1, %width : f32
    ```

    For lane `k`, returns the value from lane `(k + 1) % width`.

    `up` example:

    ```mlir
    %cst1 = arith.constant 1 : i32
    %5, %6 = gpu.shuffle up %0, %cst1, %width : f32
    ```

    For lane `k`, returns the value from lane `(k - 1) % width`.

    `idx` example:

    ```mlir
    %cst0 = arith.constant 0 : i32
    %7, %8 = gpu.shuffle idx %0, %cst0, %width : f32
    ```

    Broadcasts the value from lane 0 to all lanes.
  }];

  let assemblyFormat = [{
    $mode $value `,` $offset `,` $width attr-dict `:` type($value)
  }];

  let builders = [
    // Helper function that creates a shuffle with constant offset/width.
    OpBuilder<(ins "Value":$value, "int32_t":$offset, "int32_t":$width,
                   "ShuffleMode":$mode)>
  ];
}

def GPU_BarrierOp : GPU_Op<"barrier"> {
  let summary = "Synchronizes all work items of a workgroup.";
  let description = [{
    The "barrier" op synchronizes all work items of a workgroup. It is used
    to coordinate communication between the work items of the workgroup.

    ```mlir
    gpu.barrier
    ```

    waits until all work items in the workgroup have reached this point
    and all memory accesses made by these work items prior to the op are
    visible to all work items in the workgroup. Data hazards between work items
    accessing the same memory can be avoided by synchronizing work items
    in-between these accesses.

    Either none or all work items of a workgroup need to execute this op
    in convergence.
  }];
  let assemblyFormat = "attr-dict";
  let hasCanonicalizer = 1;
}

def GPU_GPUModuleOp : GPU_Op<"module", [
      DataLayoutOpInterface, HasDefaultDLTIDataLayout, IsolatedFromAbove,
      NoRegionArguments, SymbolTable, Symbol] # GraphRegionNoTerminator.traits> {
  let summary = "A top level compilation unit containing code to be run on a GPU.";
  let description = [{
    GPU module contains code that is intended to be run on a GPU. A host device
    can launch this code through a gpu.launc_func that creates a fully
    qualified symbol through the gpu.module's symbol and a gpu.func symbol
    contained in the gpu.module.

    The module's top-level scope is modeled by a single region with a single
    block. GPU modules are required to have a name that is used for symbol
    resolution by the gpu.launch_func operation.

    Using an op with a region to define a GPU module enables "embedding" GPU
    modules with SIMT execution models in other dialects in a clean manner and
    allows filtering of code regions to execute passes on only code intended to
    or not intended to be run on the separate device.

    Modules can contain zero or more target attributes. These attributes encode
    how to transform modules into binary strings and are used by the
    `gpu-module-to-binary` pass to transform modules into GPU binaries.

    Modules can contain an optional `OffloadingTranslationAttr` attribute. This
    attribute will be used during the `gpu-module-to-binary` pass to specify the
    `OffloadingTranslationAttr` used when creating the `gpu.binary` operation.

    ```
    gpu.module @symbol_name {
      gpu.func {}
        ...
    }
    // Module with offloading handler and target attributes.
    gpu.module @symbol_name2 <#gpu.select_object<1>> [
        #nvvm.target,
        #rocdl.target<chip = "gfx90a">] {
      gpu.func {}
        ...
    }
    ```
  }];
  let builders = [
    OpBuilder<(ins "StringRef":$name,
                   CArg<"ArrayAttr", "{}">:$targets,
                   CArg<"Attribute", "{}">:$handler)>,
    OpBuilder<(ins "StringRef":$name,
                   "ArrayRef<Attribute>":$targets,
                   CArg<"Attribute", "{}">:$handler)>
  ];

  let arguments = (ins
      SymbolNameAttr:$sym_name,
      OptionalAttr<GPUNonEmptyTargetArrayAttr>:$targets,
      OptionalAttr<OffloadingTranslationAttr>:$offloadingHandler);
  let regions = (region SizedRegion<1>:$bodyRegion);
  let assemblyFormat = [{
    $sym_name
    (`<` $offloadingHandler^ `>`)?
    ($targets^)?
    attr-dict-with-keyword $bodyRegion
  }];

  // We need to ensure the block inside the region is properly terminated;
  // the auto-generated builders do not guarantee that.
  let skipDefaultBuilders = 1;

  let extraClassDeclaration = [{
    /// Checks if `target` is in the `targets` list.
    bool hasTarget(Attribute target);

    /// Sets the targets of the module.
    void setTargets(ArrayRef<TargetAttrInterface> targets);
  }];
}

def GPU_BinaryOp : GPU_Op<"binary", [Symbol]>, Arguments<(ins
      SymbolNameAttr:$sym_name,
      OptionalAttr<OffloadingTranslationAttr>:$offloadingHandler,
      ConfinedAttr<GPUObjectArrayAttr, [ArrayMinCount<1>]>:$objects)
    > {
  let summary = "An Op for storing serialized GPU binary objects.";
  let description = [{
    GPU binaries provide a semantic mechanism for storing GPU objects,
    e.g. the result of compiling a GPU module to an object file.

    This operation has 3 arguments:
     - The name of the binary.
     - An optional attribute implementing the offloading LLVM translation interface.
     - An array of GPU object attributes.

    During translation, the offloading attribute will be called for translating
    GPU `binary` and `launch_func` operations. The default offloading handler is:
    `#gpu.select_object`, this handler selects the first object from the array
    and embeds it as a string.

    Examples:
    ```
      // Selects the first object.
      gpu.binary @myobject [#gpu.object<...>, #gpu.object<...>]
      // Uses the `#foo.my_handler` for handling the binary during translation.
      gpu.binary @myobject <#foo.my_handler> [#gpu.object<...>, #gpu.object<...>]
      // Selects the object with the `#rocdl.target` target attribute.
      gpu.binary @myobject <#gpu.select_object<#rocdl.target>> [#gpu.object<...>, #gpu.object<#rocdl.target, ...>]
    ```
  }];
  let builders = [
    OpBuilder<(ins "StringRef":$name,
                   "Attribute":$offloadingHandler,
                   "ArrayAttr":$objects)>,
    OpBuilder<(ins "StringRef":$name,
                   "Attribute":$offloadingHandler,
                   "ArrayRef<Attribute>":$objects)>
  ];
  let skipDefaultBuilders = 1;
  let assemblyFormat = [{
    $sym_name custom<OffloadingHandler>($offloadingHandler) attr-dict $objects
  }];
}

def GPU_HostRegisterOp : GPU_Op<"host_register">,
    Arguments<(ins AnyUnrankedMemRef:$value)> {
  let summary = "Registers a memref for access from device.";
  let description = [{
    This op maps the provided host buffer into the device address space.

    This operation may not be supported in every environment, there is not yet a
    way to check at runtime whether this feature is supported.

    Writes from the host are guaranteed to be visible to device kernels that are
    launched afterwards. Writes from the device are guaranteed to be visible on
    the host after synchronizing with the device kernel completion.
  }];

  let assemblyFormat = "$value attr-dict `:` type($value)";
}

def GPU_HostUnregisterOp : GPU_Op<"host_unregister">,
    Arguments<(ins AnyUnrankedMemRef:$value)> {
  let summary = "Unregisters a memref for access from device.";
  let description = [{
      This op unmaps the provided host buffer from the device address space.

      This operation may not be supported in every environment, there is not yet a
          way to check at runtime whether this feature is supported.
  }];

  let assemblyFormat = "$value attr-dict `:` type($value)";
}

def GPU_WaitOp : GPU_Op<"wait", [GPU_AsyncOpInterface]> {
  let summary = "Wait for async gpu ops to complete.";
  let description = [{
    This op synchronizes the host or the device with a list of dependent ops.

    If the op contains the `async` keyword, it returns a new async token which
    is synchronized with the op arguments. This new token is merely a shortcut
    to the argument list, and one could replace the uses of the result with the
    arguments for the same effect. The async version of this op is primarily
    used to make each async token have a single use during lowering and
    thereby make forks in async execution explicit. Example usage:

    ```mlir
    %t0 = gpu.foo async : !gpu.async.token
    %t1 = gpu.bar async : !gpu.async.token
    %t2 = gpu.wait async [%t0, %t1]
    // gpu.baz doesn't run until gpu.foo and gpu.bar have both completed, just
    // as if the async dependencies were [%t0, %t1].
    %t3 = gpu.baz async [%t2]
    ```

    If the op does not contain the `async` keyword, it does not return a new
    async token but blocks until all ops producing the async dependency tokens
    finished execution. All dependent memory operations are visible to the host
    once this op completes. Example usage:

    ```mlir
    %t0 = gpu.foo async : !gpu.async.token
    %t1 = gpu.bar async : !gpu.async.token
    // The gpu.wait op blocks until gpu.foo and gpu.bar have completed.
    gpu.wait [%t0, %t1]
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies);
  let results = (outs Optional<GPU_AsyncToken>:$asyncToken);

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies) attr-dict
  }];

  let hasCanonicalizer = 1;
}

def GPU_AllocOp : GPU_Op<"alloc", [
    GPU_AsyncOpInterface,
    AttrSizedOperandSegments
  ]> {

  let summary = "GPU memory allocation operation.";
  let description = [{
    The `gpu.alloc` operation allocates a region of memory on the GPU. It is
    similar to the `memref.alloc` op, but supports asynchronous GPU execution.

    The op does not execute before all async dependencies have finished
    executing.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it also returns a !gpu.async.token.

    If the `host_shared` keyword is present, the memory will be allocated in a
    memory accessible both on host and on device.

    Example:

    ```mlir
    %memref, %token = gpu.alloc async [%dep] host_shared (%width) : memref<64x?xf32, 1>
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                   Variadic<Index>:$dynamicSizes, Variadic<Index>:$symbolOperands,
                   UnitAttr:$hostShared);
  let results = (outs Res<AnyMemRef, "", [MemAllocAt<0, FullEffect>]>:$memref,
                 Optional<GPU_AsyncToken>:$asyncToken);

  let extraClassDeclaration = [{
    MemRefType getType() { return ::llvm::cast<MemRefType>(getMemref().getType()); }
  }];

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies) (` ` `host_shared` $hostShared^)? ` `
    `(` $dynamicSizes `)` (`` `[` $symbolOperands^ `]`)? attr-dict `:` type($memref)
  }];

  let hasVerifier = 1;
  let hasCanonicalizer = 1;
}

def GPU_DeallocOp : GPU_Op<"dealloc", [GPU_AsyncOpInterface]> {

  let summary = "GPU memory deallocation operation";

  let description = [{
    The `gpu.dealloc` operation frees the region of memory referenced by a
    memref which was originally created by the `gpu.alloc` operation. It is
    similar to the `memref.dealloc` op, but supports asynchronous GPU execution.

    The op does not execute before all async dependencies have finished
    executing.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token.

    Example:

    ```mlir
    %token = gpu.dealloc async [%dep] %memref : memref<8x64xf32, 1>
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                   Arg<AnyMemRef, "", [MemFreeAt<0, FullEffect>]>:$memref);
  let results = (outs Optional<GPU_AsyncToken>:$asyncToken);

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $memref attr-dict `:` type($memref)
  }];
}

def GPU_MemcpyOp : GPU_Op<"memcpy", [GPU_AsyncOpInterface]> {

  let summary = "GPU memcpy operation";

  let description = [{
    The `gpu.memcpy` operation copies the content of one memref to another.

    The op does not execute before all async dependencies have finished
    executing.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token.

    Example:

    ```mlir
    %token = gpu.memcpy async [%dep] %dst, %src : memref<?xf32, 1>, memref<?xf32>
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                   Arg<AnyMemRef, "", [MemWriteAt<0, FullEffect>]>:$dst,
                   Arg<AnyMemRef, "", [MemReadAt<0, FullEffect>]>:$src);
  let results = (outs Optional<GPU_AsyncToken>:$asyncToken);

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $dst`,` $src `:` type($dst)`,` type($src) attr-dict
  }];
  let hasFolder = 1;
  let hasVerifier = 1;
  let hasCanonicalizer = 1;
}

def GPU_MemsetOp : GPU_Op<"memset",
  [GPU_AsyncOpInterface, AllElementTypesMatch<["dst", "value"]>]> {

  let summary = "GPU memset operation";

  let description = [{
    The `gpu.memset` operation sets the content of memref to a scalar value.

    The op does not execute before all async dependencies have finished
    executing.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token.

    Example:

    ```mlir
    %token = gpu.memset async [%dep] %dst, %value : memref<?xf32, 1>, f32
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                   Arg<AnyMemRef, "", [MemWriteAt<0, FullEffect>]>:$dst,
                   Arg<AnyType, "">:$value);
  let results = (outs Optional<GPU_AsyncToken>:$asyncToken);

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $dst`,` $value `:` type($dst)`,` type($value) attr-dict
  }];
  let hasFolder = 1;
}

def GPU_SetDefaultDeviceOp : GPU_Op<"set_default_device",
                                    [MemoryEffects<[MemWrite]>]>,
    Arguments<(ins I32:$devIndex)> {
  let summary = "Set default GPU for operations after this by index";
  let description = [{
    Operation that sets the current default GPU, using a zero-based index
    into the set of GPUs on the system. The default GPU setting may be
    thread-local.
  }];
  let assemblyFormat = "attr-dict $devIndex";
}

def GPU_SubgroupMmaLoadMatrixOp : GPU_Op<"subgroup_mma_load_matrix",
    [MemoryEffects<[MemRead]>]>{

  let summary = "GPU warp synchronous matrix load";

  let description = [{
    The `gpu.subgroup_mma_load_matrix` operation loads a matrix collectively
    using all the threads in a subgroup.

    This operation takes a memref as its first operand: it is the source matrix
    from which data is to be loaded. The op returns a `!gpu.mma_matrix`. The
    source memref can be in global memory or shared memory. The load address is
    determined using `indices`. The matrix being loaded into is the result.  The
    `leadDimension` attribute specifies the leading dimension size of the source
    matrix which eventually allows the lowering to determine the size of each
    row.  If the `transpose` attribute is present then the op does a transposed load.

    For integer types, the resulting `!gpu.mma_matrix` type needs to specify the
    signedness of the data if the matrix type is an `A` or `B` operand for
    `gpu.subgroup_mma_compute`.

    This op is often meant to be used along with `gpu.subgroup_mma_store_matrix` and
    `gpu.subgroup_mma_compute`.

    Example:

    ```mlir
     %0 = gpu.subgroup_mma_load_matrix src[%i,%j] : {leadDimension = 32 : i32}
          : memref<32x32xf16, 3>, !gpu.mma_matrix<16x16xf16, "AOp">
    ```
  }];

  let arguments = (ins Arg<GPU_MMAMemRef, "",
                          [MemReadAt<0, FullEffect>]>:$srcMemref,
                  Variadic<Index>:$indices,
                  IndexAttr:$leadDimension,
                  OptionalAttr<UnitAttr>:$transpose);

  let results = (outs GPU_MMAMatrix:$res);

  let assemblyFormat = [{
    $srcMemref`[`$indices`]` attr-dict `:` type($srcMemref) `->` type($res)
  }];
  let hasVerifier = 1;
}

def GPU_SubgroupMmaStoreMatrixOp : GPU_Op<"subgroup_mma_store_matrix",
    [MemoryEffects<[MemWrite]>]>{

  let summary = "GPU warp synchronous matrix store";

  let description = [{
    The `gpu.subgroup_mma_store_matrix` operation stores a matrix collectively
    using all the threads in a subgroup.

    This operation takes a `!gpu.mma_matrix` and a memref as operands.
    `!gpu.mma_matrix` is the source value containing the data to be stored into the
    destination memref which can be in global or shared memory.  The store address
    is determined using the indices provided. The `leadDimension` attribute
    specifies the leading dimension of the destination matrix. If the
    `transpose` attribute is present then the op does a transposed store.

    This op is often meant to be used along with `gpu.subgroup_mma_load_matrix` and
    `gpu.subgroup_mma_compute`.

    Example:

    ```mlir
    gpu.subgroup_mma_store_matrix %D, %sg[%i,%j] : { leadDimension = 32 : i32}
                    : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x32xf16, 3>
    ```
  }];

  let arguments = (ins Arg<MMAMatrixOf<[SI8, UI8, I32, F16, F32]>>:$src,
                  Arg<GPU_MMAMemRef, "",[MemWriteAt<0, FullEffect>]>:$dstMemref,
                  Variadic<Index>:$indices,
                  IndexAttr:$leadDimension,
                  OptionalAttr<UnitAttr>:$transpose);

  let assemblyFormat = [{
    $src`,` $dstMemref`[`$indices`]` attr-dict `:` type($src)`,` type($dstMemref)
  }];
  let hasVerifier = 1;
}

def GPU_SubgroupMmaComputeOp
    : GPU_Op<"subgroup_mma_compute", [Pure, AllTypesMatch<["opC", "res"]>]> {

  let summary = "GPU warp synchronous matrix multiply accumulate";

  let description = [{
    The `gpu.subgroup_mma_compute` operation performs a matrix-multiply accumulate (mma)
    operation using all the threads in a subgroup.

    This operation takes three `!gpu.mma_matrix`s as arguments: these hold `A`,
    `B` and `C`operands for the mma operation. The operation performed is represented
    as `C += A * B`. The op returns a `!gpu.mma_matrix` which contains the result of
    the operation held by all threads in a subgroup. `a_transpose` or
    `b_transpose` if present, signify that the respective operand was loaded in a
    transposed manner. The transpose operands are required to map to correct
    underlying intrisics but they currently do not seem to affect correctness
    even if they are absent given that the operands were loaded correctly using
    the `transpose` attribute in `gpu.subgroup_mma_load_matrix` op.

    For integer types, the `A` and `B` matrices carry their signedness with their
    types. The accumulator type is expected to be signless and imply a signed integer
    with a greater width than the other two operands.

    This op is meant to be used along with `gpu.subgroup_mma_store_matrix` and
    `gpu.subgroup_mma_load_matrix` ops.

    Example:

    ```mlir
    %D = gpu.subgroup_mma_compute_matrix %A, %B, %C :
      !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp">>
      -> !gpu.mma_matrix<16x16xf16, "COp">
    ```
  }];

  let arguments = (ins Arg<MMAMatrixOf<[SI8, UI8, F16, F32]>>:$opA,
                  Arg<MMAMatrixOf<[SI8, UI8, F16, F32]>>:$opB,
                  Arg<MMAMatrixOf<[I32, F16, F32]>>:$opC,
                  OptionalAttr<UnitAttr>:$a_transpose,
                  OptionalAttr<UnitAttr>:$b_transpose);

  let results = (outs GPU_MMAMatrix : $res);

  let assemblyFormat = [{
    $opA`,` $opB`,` $opC attr-dict `:` type($opA)`,` type($opB) `->` type($res)
  }];
  let hasVerifier = 1;
}

def GPU_SubgroupMmaConstantMatrixOp : GPU_Op<"subgroup_mma_constant_matrix",
    [Pure,
     TypesMatchWith<"value type matches element type of mma_matrix",
                    "res", "value",
                    "::llvm::cast<gpu::MMAMatrixType>($_self).getElementType()">]>{

  let summary = "GPU warp synchronous constant matrix";

  let description = [{
    The `gpu.subgroup_mma_constant_matrix` creates a `!gpu.mma_matrix` with
    constant elements.

    The operation takes a scalar input and return a `!gpu.mma_matrix` where
    each element of is equal to the operand constant. The destination
    mma_matrix type must have elememt type equal to the constant type. Since
    the layout of `!gpu.mma_matrix` is opaque this only support setting all the
    elements to the same value.

    This op is meant to be used along with `gpu.subgroup_mma_compute`.

    Example:

    ```mlir
     %0 = gpu.subgroup_mma_constant_matrix %a :
       !gpu.mma_matrix<16x16xf16, "AOp">
     %1 = gpu.subgroup_mma_constant_matrix %b :
       !gpu.mma_matrix<16x16xf32, "COp">
    ```
  }];

  let arguments = (ins AnyTypeOf<[SI8, UI8, I32, F16, F32]>:$value);

  let results = (outs GPU_MMAMatrix:$res);

  let extraClassDeclaration = [{
    gpu::MMAMatrixType getType() {
      return ::llvm::cast<gpu::MMAMatrixType>(getRes().getType());
    }
  }];

  let assemblyFormat = [{
    $value attr-dict `:` type($res)
  }];
}

def GPU_ElementwiseOpAddF  : I32EnumAttrCase<"ADDF", 0, "addf">;
def GPU_ElementwiseOpMulF  : I32EnumAttrCase<"MULF", 1, "mulf">;
def GPU_ElementwiseOpSUBF  : I32EnumAttrCase<"SUBF", 2, "subf">;
def GPU_ElementwiseOpMaxF : I32EnumAttrCase<"MAXF", 3, "maxf">;
def GPU_ElementwiseOpMinF : I32EnumAttrCase<"MINF", 4, "minf">;
def GPU_ElementwiseOpDivF : I32EnumAttrCase<"DIVF", 5, "divf">;
def GPU_ElementwiseOpAddI  : I32EnumAttrCase<"ADDI", 6, "addi">;
def GPU_ElementwiseOpMulI  : I32EnumAttrCase<"MULI", 7, "muli">;
def GPU_ElementwiseOpSUBI  : I32EnumAttrCase<"SUBI", 8, "subi">;
def GPU_ElementwiseOpDivS : I32EnumAttrCase<"DIVS", 9, "divs">;
def GPU_ElementwiseOpDivU : I32EnumAttrCase<"DIVU", 10, "divu">;
def GPU_ElementwiseOpNEGF : I32EnumAttrCase<"NEGATEF", 11, "negatef">;
def GPU_ElementwiseOpNEGS : I32EnumAttrCase<"NEGATES", 12, "negates">;
def GPU_ElementwiseOpEXTF : I32EnumAttrCase<"EXTF", 13, "extf">;

def MMAElementWise : I32EnumAttr<"MMAElementwiseOp",
  "elementwise operation to apply to mma matrix", [
    GPU_ElementwiseOpAddF,
    GPU_ElementwiseOpMulF,
    GPU_ElementwiseOpSUBF,
    GPU_ElementwiseOpMaxF,
    GPU_ElementwiseOpMinF,
    GPU_ElementwiseOpDivF,
    GPU_ElementwiseOpAddI,
    GPU_ElementwiseOpMulI,
    GPU_ElementwiseOpSUBI,
    GPU_ElementwiseOpDivS,
    GPU_ElementwiseOpDivU,
    GPU_ElementwiseOpNEGF,
    GPU_ElementwiseOpNEGS,
    GPU_ElementwiseOpEXTF
  ]> {
  let genSpecializedAttr = 0;
  let cppNamespace = "::mlir::gpu";
}
def MMAElementWiseAttr : EnumAttr<GPU_Dialect, MMAElementWise,
                                  "mma_element_wise">;

def GPU_SubgroupMmaElementwiseOp : GPU_Op<"subgroup_mma_elementwise",
    [Pure,
     AllTypesMatch<["args"]>]>{

  let summary = "GPU warp elementwise operation on a matrix";

  let description = [{
    The `gpu.subgroup_mma_elementwise` takes `!gpu.mma_matrix` inputs and
    compute a new `!gpu.mma_matrix` by applying an elementwise operation to each
    element.

    Since the operation is elementwise and the matrix type must match, the
    matrix elements are processed independently of the matrix layout.

    This op is meant to be used along with `gpu.subgroup_mma_compute`.

    Example:

    ```mlir
     %0 =  %A, %B { opType = "ADD" } :
      (!gpu.mma_matrix<16x16xf16, "COp">, !gpu.mma_matrix<16x16xf16, "COp">)
      -> !gpu.mma_matrix<16x16xf16, "COp">
    ```
  }];

  let arguments = (ins Variadic<GPU_MMAMatrix>:$args,
                       MMAElementWiseAttr:$opType);

  let results = (outs GPU_MMAMatrix:$res);

  let extraClassDeclaration = [{
    gpu::MMAMatrixType getType() {
      return ::llvm::cast<gpu::MMAMatrixType>(getRes().getType());
    }
  }];

  let assemblyFormat = [{
    $opType $args attr-dict `:` functional-type($args, $res)
  }];
}

//
// Operation on sparse matrices, called from the host
// (currently lowers to cuSparse for CUDA only, no ROCM lowering).
//

def GPU_CreateDnTensorOp : GPU_Op<"create_dn_tensor", [GPU_AsyncOpInterface, AttrSizedOperandSegments]> {
  let summary = "Create dense tensor operation";
  let description = [{
    The `gpu.create_dn_tensor` operation initializes a dense tensor from
    the given values buffer and sizes. The buffer must already be copied
    from the host to the device prior to using this operation. The
    operation returns a handle to the dense tensor descriptor.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token in addition to the environment.

    Example:

    ```mlir
    %dmat, %token = gpu.create_dn_tensor async [%dep] %mem, %dims : index, index into memref<?xf64>
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                       AnyMemRef:$memref,
                       Variadic<Index>:$dims);
  let results = (outs Res<GPU_SparseDnTensorHandle>:$dnTensor, Optional<GPU_AsyncToken>:$asyncToken);

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $memref `,` $dims attr-dict `:` type($dims) `into` type($memref)
  }];
}

def GPU_DestroyDnTensorOp : GPU_Op<"destroy_dn_tensor", [GPU_AsyncOpInterface]> {
  let summary = "Destroy dense tensor operation";
  let description = [{
    The `gpu.destroy_dn_tensor` operation releases all resources of a dense
    tensor represented by a handle that was previously created by a
    `gpu.create_dn_tensor` operation.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token in addition to the environment.

    Example:

    ```mlir
    %token = gpu.destroy_dn_tensor async [%dep] %dnTensor
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                       Arg<GPU_SparseDnTensorHandle>:$dnTensor);
  let results = (outs Optional<GPU_AsyncToken>:$asyncToken);

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $dnTensor attr-dict
  }];
}

def GPU_CreateCooOp : GPU_Op<"create_coo", [GPU_AsyncOpInterface]> {
  let summary = "Create sparse matrix in COO format operation";
  let description = [{
    The `gpu.create_coo` operation initializes a sparse matrix in COO format
    with the given sizes from the given index and values buffers. The buffers
    must already be copied from the host to the device prior to using this
    operation. The operation returns a handle to the sparse matrix descriptor.
    Note that this operation builds the COO in SoA format.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token in addition to the environment.

    Example:

    ```mlir
    %spmat, %token = gpu.create_coo async [%dep] %rows, %cols, %nnz, %rowIdx,
        %colIdx, %values : memref<?xindex>, memref<?xindex>, memref<?xf64>
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                       Index:$rows,
                       Index:$cols,
                       Index:$nnz,
                       AnyMemRef:$rowIdxs,
                       AnyMemRef:$colIdxs,
                       AnyMemRef:$values);
  let results = (outs Res<GPU_SparseSpMatHandle>:$spmat,
                      Optional<GPU_AsyncToken>:$asyncToken);

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $rows `,` $cols `,` $nnz `,` $rowIdxs `,` $colIdxs `,` $values attr-dict
    `:` type($rowIdxs) `,` type($colIdxs) `,` type($values)
  }];
}

def GPU_CreateCooAoSOp : GPU_Op<"create_coo_aos", [GPU_AsyncOpInterface]> {
  let summary = "Create sparse matrix in COO format operation (AoS)";
  let description = [{
    The `gpu.create_coo_aos` operation initializes a sparse matrix in COO format
    with the given sizes from the given index and values buffers. The buffers
    must already be copied from the host to the device prior to using this
    operation. The operation returns a handle to the sparse matrix descriptor.
    Unlike the default `gpu.create_coo` operation, this operation builds the
    COO format from a single index buffer in AoS format (note that this
    feature has been deprecated in cuSparse 11.2).

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token in addition to the environment.

    Example:

    ```mlir
    %spmat, %token = gpu.create_coo_aos async [%dep] %rows, %cols, %nnz, %idxs,
        %values : memref<?xindex>, memref<?xf64>
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                   Index:$rows,
                   Index:$cols,
                   Index:$nnz,
                   AnyMemRef:$idxs,
                   AnyMemRef:$values);
  let results = (outs Res<GPU_SparseSpMatHandle>:$spmat,
                      Optional<GPU_AsyncToken>:$asyncToken);

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $rows `,` $cols `,` $nnz `,` $idxs `,` $values attr-dict
    `:` type($idxs) `,` type($values)
  }];
}

def GPU_CreateCsrOp : GPU_Op<"create_csr", [GPU_AsyncOpInterface]> {
  let summary = "Create sparse matrix in CSR format operation";
  let description = [{
    The `gpu.create_csr` operation initializes a sparse matrix in CSR format
    with the given sizes from the given position, index, and values buffers.
    The buffers must already be copied from the host to the device prior to
    using this operation. The operation returns a handle to the sparse
    matrix descriptor.

    The CSR format has exactly the same memory layout as its transpose
    in CSC format (and vice versa).

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token in addition to the environment.

    Example:

    ```mlir
    %spmat, %token = gpu.create_csr async [%dep] %rows, %cols, %nnz, %rowPos,
        %colIdx, %values : memref<?xindex>, memref<?xindex>, memref<?xf64>
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                   Index:$rows,
                   Index:$cols,
                   Index:$nnz,
                   AnyMemRef:$rowPos,
                   AnyMemRef:$colIdxs,
                   AnyMemRef:$values);
  let results = (outs Res<GPU_SparseSpMatHandle>:$spmat,
                      Optional<GPU_AsyncToken>:$asyncToken);

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $rows `,` $cols `,` $nnz `,` $rowPos `,` $colIdxs `,` $values attr-dict
    `:` type($rowPos) `,` type($colIdxs) `,` type($values)
  }];
}

def GPU_CreateCscOp : GPU_Op<"create_csc", [GPU_AsyncOpInterface]> {
  let summary = "Create sparse matrix in CSC format operation";
  let description = [{
    The `gpu.create_csc` operation initializes a sparse matrix in CSC format
    with the given sizes from the given position, index, and values buffers.
    The buffers must already be copied from the host to the device prior to
    using this operation. The operation returns a handle to the sparse
    matrix descriptor.

    The CSC format has exactly the same memory layout as its transpose
    in CSR format (and vice versa).

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token in addition to the environment.

    Example:

    ```mlir
    %spmat, %token = gpu.create_csc async [%dep] %rows, %cols, %nnz, %colPos,
        %rowIdx, %values : memref<?xindex>, memref<?xindex>, memref<?xf64>
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                   Index:$rows,
                   Index:$cols,
                   Index:$nnz,
                   AnyMemRef:$colPos,
                   AnyMemRef:$rowIdxs,
                   AnyMemRef:$values);
  let results = (outs Res<GPU_SparseSpMatHandle>:$spmat,
                      Optional<GPU_AsyncToken>:$asyncToken);

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $rows `,` $cols `,` $nnz `,` $colPos `,` $rowIdxs `,` $values attr-dict
    `:` type($colPos) `,` type($rowIdxs) `,` type($values)
  }];
}

def GPU_CreateBsrOp : GPU_Op<"create_bsr", [GPU_AsyncOpInterface]> {
  let summary = "Create sparse matrix in BSR format operation";
  let description = [{
    The `gpu.create_bsr` operation initializes a sparse matrix in BSR format
    with the given sizes for the matrix and blocks from the given position,
    index, and values buffers. The buffers must already be copied from the
    host to the device prior to using this operation. The operation returns
    a handle to the sparse matrix descriptor.

    The BSR format is similar to CSR, where the column indices represent
    two-dimensional blocks instead of a single matrix entry. Note that this
    operation (currently) only supports storage with **square** blocks,
    i.e., `rBlockSize == cBlockSize`.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token in addition to the environment.

    Example:

    ```mlir
    %spmat, %token = gpu.create_bsr async [%dep]
       %brows, %bcols, %bnnz, %rBlockSize, %cBlockSize,
       %bRowPos, %bColIdxs, %values : memref<?xindex>, memref<?xindex>, memref<?xf64>
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                   Index:$brows,
                   Index:$bcols,
                   Index:$bnnz,
                   Index:$rBlockSize,
                   Index:$cBlockSize,
                   AnyMemRef:$bRowPos,
                   AnyMemRef:$bColIdxs,
                   AnyMemRef:$values);
  let results = (outs Res<GPU_SparseSpMatHandle>:$spmat,
                      Optional<GPU_AsyncToken>:$asyncToken);

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $brows `,` $bcols `,` $bnnz `,` $rBlockSize `,` $cBlockSize `,`
    $bRowPos `,` $bColIdxs `,` $values attr-dict
    `:` type($bRowPos) `,` type($bColIdxs) `,` type($values)
  }];
}

def GPU_Prune2To4SpMatFlag : I32EnumAttr<"Prune2To4SpMatFlag",
  "pruning strategy for 2:4 sparse matrix",
  [
    I32EnumAttrCase<"NONE", 0>,
    I32EnumAttrCase<"PRUNE_ONLY", 1>,
    I32EnumAttrCase<"PRUNE_AND_CHECK", 2>,
  ]> {
    let genSpecializedAttr = 0;
    let cppNamespace = GPU_Dialect.cppNamespace;
}

def GPU_Prune2To4SpMatFlagAttr : EnumAttr<GPU_Dialect, GPU_Prune2To4SpMatFlag,
                                   "prune_2to4_spmat_flag">{
  let defaultValue = "Prune2To4SpMatFlag::PRUNE_AND_CHECK";
}


def GPU_Create2To4SpMatOp : GPU_Op<"create_2to4_spmat", [GPU_AsyncOpInterface]> {
  let summary = "Create sparse matrix with 2:4 sparsity operation";
  let description = [{
    The `gpu.create_2to4_spmat` operation initializes a sparse matrix in dense
    format with 2:4 sparsity.
    The buffers must already be copied from the host to the device prior to
    using this operation. The operation returns a handle to the sparse
    matrix descriptor.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token in addition to the environment.

    Example:

    ```mlir
    %spmat, %token = gpu.create_2to4_spmat async [%dep] {PRUNE_AND_CHECK} %rows, %cols, %mem: memref<?xf64>
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                       Index:$rows,
                       Index:$cols,
                       GPU_Prune2To4SpMatFlagAttr:$pruneFlag,
                       AnyMemRef:$memref);
  let results = (outs Res<GPU_SparseSpMatHandle>:$spMat,
                      Optional<GPU_AsyncToken>:$asyncToken);

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
     `{` $pruneFlag `}` $rows `,` $cols `,` $memref attr-dict `:` type($memref)
  }];
}

def GPU_DestroySpMatOp : GPU_Op<"destroy_sp_mat", [GPU_AsyncOpInterface]> {
  let summary = "Destroy sparse matrix operation";
  let description = [{
    The `gpu.destroy_sp_mat` operation releases all resources of a sparse
    matrix represented by a handle that was previously created by a
    one of the sparse matrix creation operations.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token in addition to the environment.

    Example:

    ```mlir
    %token = gpu.destroy_sp_mat async [%dep] %spmat
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                       Arg<GPU_SparseSpMatHandle>:$spmat);
  let results = (outs Optional<GPU_AsyncToken>:$asyncToken);

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies) $spmat attr-dict
  }];
}

// To avoid coupling this dialect with cusparse.h specifics, we hardcoded magic
// literals in this enum. Note that this should be kept in sync with
// cusparseOperation_t in cusparse.h:
// typedef enum {
// CUSPARSE_OPERATION_NON_TRANSPOSE       = 0,
// CUSPARSE_OPERATION_TRANSPOSE           = 1,
// CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE = 2
// } cusparseOperation_t;
// TODO: find a proper way to keep them in sync?
def GPU_TransposeMode : I32EnumAttr<"TransposeMode",
    "transpose mode of sparse matrix supported by sparse tensor ops",
    [
      I32EnumAttrCase<"NON_TRANSPOSE", 0>,
      I32EnumAttrCase<"TRANSPOSE", 1>,
      I32EnumAttrCase<"CONJUGATE_TRANSPOSE", 2>,
    ]> {
      let genSpecializedAttr = 0;
      let cppNamespace = GPU_Dialect.cppNamespace;
}

def GPU_TransposeModeAttr : EnumAttr<GPU_Dialect, GPU_TransposeMode,
                                   "mat_transpose_mode">{
  let defaultValue = "TransposeMode::NON_TRANSPOSE";
}

def GPU_SpMVBufferSizeOp : GPU_Op<"spmv_buffer_size", [GPU_AsyncOpInterface]> {
  let summary = "Precompute buffersize for SpMV operation";
  let description = [{
    The `gpu.spmv_buffer_size` operation returns the buffer size required
    to perform the SpMV operation on the given sparse matrix and dense vectors.
    The operation expects handles returned by previous sparse operations
    to construct an environment and the operands for SpMV.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token in addition to the environment.

    The matrix arguments can also be associated with one of the following
    operators: NON_TRANSPOSE, TRANSPOSE, CONJUGATE_TRANSPOSE. The default value
    is NON_TRANSPOSE.

    Example:

    ```mlir
    %buffersz, %token = gpu.spmv_buffer_size async [%dep] %spmatA{TRANSPOSE}, %dnX, %dnY into f32
    ```
  }];
  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                       GPU_TransposeModeAttr:$modeA,
                       GPU_SparseSpMatHandle:$spmatA,
                       GPU_SparseDnTensorHandle:$dnX,
                       GPU_SparseDnTensorHandle:$dnY,
                       TypeAttr:$computeType);
  let results = (outs Res<Index>:$bufferSz,
                      Optional<GPU_AsyncToken>:$asyncToken);

  let builders = [OpBuilder<(ins
      "Type":$bufferSz,
      "Type":$asyncToken,
      "ValueRange":$asyncDependencies,
      "Value":$spmatA,
      "Value":$dnX,
      "Value":$dnY,
      "Type":$computeType)
      , [{
    auto modeA = gpu::TransposeMode::NON_TRANSPOSE;
    return build($_builder, $_state, bufferSz, asyncToken, asyncDependencies,
                 modeA, spmatA, dnX, dnY, computeType);}]>
  ];

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $spmatA (`{` $modeA^ `}`)? `,` $dnX `,` $dnY attr-dict  `into` $computeType
  }];
}

def GPU_SpMVOp : GPU_Op<"spmv", [GPU_AsyncOpInterface]> {
  let summary = "SpMV operation";
  let description = [{
    The `gpu.spmv` operation performs the SpMV operation on the given sparse matrix,
    dense vectors, and buffer.  The operation expects handles returned by previous
    sparse operations to construct an environment and the operands for SpMV. The
    buffer must have been allocated on the device.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token in addition to the environment.

    The matrix arguments can also be associated with one of the following
    operators: NON_TRANSPOSE, TRANSPOSE, CONJUGATE_TRANSPOSE. The default value
    is NON_TRANSPOSE.

    Example:

    ```mlir
    %token = gpu.spmv async [%dep] %spmatA{TRANSPOSE}, %dnX, %dnY : memref<?xf64> into bf16
    ```
  }];
  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                       GPU_TransposeModeAttr:$modeA,
                       GPU_SparseSpMatHandle:$spmatA,
                       GPU_SparseDnTensorHandle:$dnX,
                       GPU_SparseDnTensorHandle:$dnY,
                       TypeAttr:$computeType,
                       AnyMemRef:$buffer);
  let results = (outs Optional<GPU_AsyncToken>:$asyncToken);

  let builders = [OpBuilder<(ins
      "Type":$asyncToken,
      "ValueRange":$asyncDependencies,
      "Value":$spmatA,
      "Value":$dnX,
      "Value":$dnY,
      "Type":$computeType,
      "Value":$buffer), [{
    auto modeA = gpu::TransposeMode::NON_TRANSPOSE;
    return build($_builder, $_state, asyncToken, asyncDependencies, modeA,
                 spmatA, dnX, dnY, computeType, buffer);}]>
  ];

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $spmatA (`{` $modeA^ `}`)? `,` $dnX `,` $dnY `,` $buffer attr-dict `:` type($buffer) `into` $computeType
  }];
}

def GPU_SpMMBufferSizeOp : GPU_Op<"spmm_buffer_size", [GPU_AsyncOpInterface, AttrSizedResultSegments]> {
  let summary = "Precompute buffersize for SpMM operation";
  let description = [{
    The `gpu.spmm_buffer_size` operation returns the buffer size required
    to perform the SpMM operation on the given sparse and dense matrix.
    The operation expects handles returned by previous sparse operations
    to construct an environment and the operands for SpMM.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token in addition to the environment.

    The matrix arguments can also be associated with one of the following
    operators: NON_TRANSPOSE, TRANSPOSE, CONJUGATE_TRANSPOSE. The default value
    is NON_TRANSPOSE.

    Example:

    ```mlir
    %bufferszs, %token = gpu.spmm_buffer_size async [%dep] %spmatA{TRANSPOSE}, %dnmatB{TRANSPOSE}, %dnmatC : i64 into f32
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                       GPU_TransposeModeAttr:$modeA,
                       GPU_TransposeModeAttr:$modeB,
                       GPU_SparseSpMatHandle:$spmatA,
                       GPU_SparseDnTensorHandle:$dnmatB,
                       GPU_SparseDnTensorHandle:$dnmatC,
                       TypeAttr:$computeType);
  let results = (outs Variadic<Index>:$bufferSzs,
                      Optional<GPU_AsyncToken>:$asyncToken);

  let builders = [OpBuilder<(ins
      "Type":$bufferSzs,
      "Type":$asyncToken,
      "ValueRange":$asyncDependencies,
      "Value":$spmatA,
      "Value":$dnmatB,
      "Value":$dnmatC,
      "Type":$computeType), [{
    auto modeA = gpu::TransposeMode::NON_TRANSPOSE;
    auto modeB = gpu::TransposeMode::NON_TRANSPOSE;
    return build($_builder, $_state, bufferSzs, asyncToken, asyncDependencies,
                 modeA, modeB, spmatA, dnmatB, dnmatC, computeType);}]>
  ];

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $spmatA (`{` $modeA^ `}`)? `,` $dnmatB (`{` $modeB^ `}`)? `,` $dnmatC attr-dict `:` type($bufferSzs) `into` $computeType
  }];
}

def GPU_SpMMOp : GPU_Op<"spmm", [GPU_AsyncOpInterface, AttrSizedOperandSegments]> {
  let summary = "SpMM operation";
  let description = [{
    The `gpu.spmm` operation performs the SpMM operation on the given sparse and
    dense matrix, and buffer.  The operation expects handles returned by previous
    sparse operations to construct an environment and the operands for SpMM. The
    buffer must have been allocated on the device.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token in addition to the environment.

    The matrix arguments can also be associated with one of the following
    operators: NON_TRANSPOSE, TRANSPOSE, CONJUGATE_TRANSPOSE. The default value
    is NON_TRANSPOSE.

    Example:

    ```mlir
    %token = gpu.spmm async [%dep] %spmatA{TRANSPOSE}, %dnmatB{TRANSPOSE}, %dnmatC, %buffers : type($buffers) into f32
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                       GPU_TransposeModeAttr:$modeA,
                       GPU_TransposeModeAttr:$modeB,
                       GPU_SparseSpMatHandle:$spmatA,
                       GPU_SparseDnTensorHandle:$dnmatB,
                       GPU_SparseDnTensorHandle:$dnmatC,
                       TypeAttr:$computeType,
                       Variadic<AnyMemRef>:$buffers);
  let results = (outs Optional<GPU_AsyncToken>:$asyncToken);

  let builders = [OpBuilder<(ins
      "Type":$asyncToken,
      "ValueRange":$asyncDependencies,
      "Value":$spmatA,
      "Value":$dnmatB,
      "Value":$dnmatC,
      "Type":$computeType,
      "ValueRange":$buffers), [{
    auto modeA = gpu::TransposeMode::NON_TRANSPOSE;
    auto modeB = gpu::TransposeMode::NON_TRANSPOSE;
    return build($_builder, $_state, asyncToken, asyncDependencies, modeA,
                 modeB, spmatA, dnmatB, dnmatC, computeType, buffers);}]>
  ];

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $spmatA (`{` $modeA^ `}`)? `,` $dnmatB (`{` $modeB^ `}`)? `,` $dnmatC `,` $buffers attr-dict `:` type($buffers) `into` $computeType
  }];
}

def GPU_SDDMMBufferSizeOp : GPU_Op<"sddmm_buffer_size", [GPU_AsyncOpInterface]> {
  let summary = "Precompute buffersize for SDDMM operation";
  let description = [{
    The `gpu.sddmm_buffer_size` operation returns the buffer size required
    to perform the SDDMM operation on the given sparse and dense matrices.
    The operation expects handles returned by previous sparse operations
    to construct an environment and the operands for SDDMM.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token in addition to the environment.

    Example:

    ```mlir
    %buffersz, %token = gpu.sddmm_buffer_size async [%dep] %dnmatA{TRANSPOSE}, %dnmatB{TRANSPOSE}, %spmatC into f32
    ```

    The matrix arguments can also be associated with one of the following
    operators: NON_TRANSPOSE, TRANSPOSE, CONJUGATE_TRANSPOSE. The default value
    is NON_TRANSPOSE.
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                   GPU_TransposeModeAttr:$modeA,
                   GPU_TransposeModeAttr:$modeB,
                   GPU_SparseDnTensorHandle:$dnmatA,
                   GPU_SparseDnTensorHandle:$dnmatB,
                   GPU_SparseSpMatHandle:$spmatC,
                   TypeAttr:$computeType);
  let results = (outs Res<Index>:$bufferSz, Optional<GPU_AsyncToken>:$asyncToken);

  let builders = [OpBuilder<(ins
      "Type":$bufferSz,
      "Type":$asyncToken,
      "ValueRange":$asyncDependencies,
      "Value":$dnmatA,
      "Value":$dnmatB,
      "Value":$spmatC,
      "Type":$computeType), [{
    auto modeA = gpu::TransposeMode::NON_TRANSPOSE;
    auto modeB = gpu::TransposeMode::NON_TRANSPOSE;
    return build($_builder, $_state, bufferSz, asyncToken, asyncDependencies,
                 modeA, modeB, dnmatA, dnmatB, spmatC, computeType);}]>
  ];

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $dnmatA (`{` $modeA^ `}`)? `,` $dnmatB (`{` $modeB^ `}`)? `,` $spmatC attr-dict `into` $computeType
  }];
}

def GPU_SDDMMOp : GPU_Op<"sddmm", [GPU_AsyncOpInterface]> {
  let summary = "SDDMM operation";
  let description = [{
    The `gpu.sddmm` operation performs the SDDMM operation on the given sparse and
    dense matrices, and buffer.  The operation expects handles returned by previous
    sparse operations to construct an environment and the operands for SDDMM. The
    buffer must have been allocated on the device.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a !gpu.async.token in addition to the environment.

    Example:

    ```mlir
    %token = gpu.sddmm async [%dep] %dnmatA{TRANSPOSE}, %dnmatB{TRANSPOSE}, %spmatC, %buffer into f32
    ```

    The matrix arguments can also be associated with one of the following
    operators: NON_TRANSPOSE, TRANSPOSE, CONJUGATE_TRANSPOSE. The default value
    is NON_TRANSPOSE.
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                   GPU_TransposeModeAttr:$modeA,
                   GPU_TransposeModeAttr:$modeB,
                   GPU_SparseDnTensorHandle:$dnmatA,
                   GPU_SparseDnTensorHandle:$dnmatB,
                   GPU_SparseSpMatHandle:$spmatC,
                   TypeAttr:$computeType,
                   AnyMemRef:$buffer);
  let results = (outs Optional<GPU_AsyncToken>:$asyncToken);

  let builders = [OpBuilder<(ins
    "Type":$asyncToken,
    "ValueRange":$asyncDependencies,
    "Value":$dnmatA,
    "Value":$dnmatB,
    "Value":$spmatC,
    "Type":$computeType,
    "Value":$buffer), [{
  auto modeA = gpu::TransposeMode::NON_TRANSPOSE;
  auto modeB = gpu::TransposeMode::NON_TRANSPOSE;
  return build($_builder, $_state, asyncToken, asyncDependencies, modeA,
                modeB, dnmatA, dnmatB, spmatC, computeType, buffer);}]>
  ];

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $dnmatA (`{` $modeA^ `}`)? `,` $dnmatB (`{` $modeB^ `}`)? `,` $spmatC `,` $buffer attr-dict `:` type($buffer) `into` $computeType
  }];
}

def GPU_SpGEMMWorkEstimationOrComputeKind : I32EnumAttr<"SpGEMMWorkEstimationOrComputeKind",
    "choose whether spgemm_work_estimation_or_compute does work estimation or compute",
    [
      I32EnumAttrCase<"WORK_ESTIMATION", 0>,
      I32EnumAttrCase<"COMPUTE", 1>,
    ]> {
      let genSpecializedAttr = 0;
      let cppNamespace = GPU_Dialect.cppNamespace;
}

def GPU_SpGEMMWorkEstimationOrComputeKindAttr : EnumAttr<GPU_Dialect,
    GPU_SpGEMMWorkEstimationOrComputeKind,
    "spgemm_work_estimation_or_compute_kind"> {}

def GPU_SpGEMMCreateDescrOp : GPU_Op<"spgemm_create_descr", [GPU_AsyncOpInterface]> {
  let summary = "SpGEMM Create Descr operation";
  let description = [{
    The `gpu.spgemm_create_descr` creates a descriptor for the SpGEMM operation.
    The descriptor describes the SpGEMM operation and stores the internal data
    throughout the computation. It needs to be passed as an argument to
    spgemm_* operations.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a `!gpu.async.token` in addition to the environment.

    Example:

    ```mlir
    %desc, %token = gpu.spgemm_create_descr async [%dep]
    ```
  }];
  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies);
  let results = (outs GPU_SparseSpGEMMOpHandle:$desc,
                      Optional<GPU_AsyncToken>:$asyncToken);
  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    attr-dict
  }];
}

def GPU_SpGEMMDestroyDescrOp : GPU_Op<"spgemm_destroy_descr", [GPU_AsyncOpInterface]> {
  let summary = "SpGEMM Destroy Descr operation";
  let description = [{
    The `gpu.spgemm_destroy_descr` destroys the SpGEMM operation descriptor.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a `!gpu.async.token` in addition to the environment.

    Example:

    ```mlir
    %token = gpu.spgemm_destroy_descr async [%dep] %desc
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                       GPU_SparseSpGEMMOpHandle:$desc);
  let results = (outs Optional<GPU_AsyncToken>:$asyncToken);
  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $desc attr-dict
  }];
}

def GPU_SpGEMMWorkEstimationOrComputeOp : GPU_Op<"spgemm_work_estimation_or_compute", [GPU_AsyncOpInterface]> {
  let summary = "SpGEMM work estimation operation";
  let description = [{
    The `gpu.spgemm_work_estimation_or_compute` is used to call
    cusparseSpGEMM_workEstimation or cusparseSpGEMM_compute. Both of them are
    for both determining the buffer size and performing the actual computation.
    The operation expects handles returned by previous sparse operations to
    construct an environment and the operands for SpGEMM.
    The buffer must have been allocated on the device.

    C' = alpha * op(A) * op(B) + beta * C

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a `!gpu.async.token` in addition to the environment.

    Example:

    ```mlir
    %bufferSz, %token = gpu.spgemm_work_estimation_or_compute async [%dep] {COMPUTE}
                          %desc, %spmatA{NON_TRANSPOSE}, %spmatB{NON_TRANSPOSE},
                          %spmatC, %spgemmDesc, %c0, %alloc: f32 into
                          memref<0xi8>
    ```

    The matrix arguments can also be associated with one of the following
    operators: NON_TRANSPOSE, TRANSPOSE, CONJUGATE_TRANSPOSE. The default value
    is NON_TRANSPOSE.
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                       GPU_SparseSpGEMMOpHandle:$desc,
                       GPU_TransposeModeAttr:$modeA,
                       GPU_TransposeModeAttr:$modeB,
                       GPU_SparseSpMatHandle:$spmatA,
                       GPU_SparseSpMatHandle:$spmatB,
                       GPU_SparseSpMatHandle:$spmatC,
                       TypeAttr:$computeType,
                       Index:$bufferSz,
                       AnyMemRef:$buffer,
                       GPU_SpGEMMWorkEstimationOrComputeKindAttr:$kind);
  let results = (outs Res<Index>:$bufferSzNew,
                      Optional<GPU_AsyncToken>:$asyncToken);

  let builders = [OpBuilder<(ins
    "Type":$bufferSzNew,
    "Type":$asyncToken,
    "ValueRange":$asyncDependencies,
    "Value":$desc,
    "Value":$spmatA,
    "Value":$spmatB,
    "Value":$spmatC,
    "Type":$computeType,
    "Value":$bufferSz,
    "Value":$buffer), [{
  auto modeA = gpu::TransposeMode::NON_TRANSPOSE;
  auto modeB = gpu::TransposeMode::NON_TRANSPOSE;
  auto kind = gpu::SpGEMMWorkEstimationOrComputeKind::WORK_ESTIMATION;
  return build($_builder, $_state, bufferSzNew, asyncToken, asyncDependencies, desc,
               modeA, modeB, spmatA, spmatB, spmatC, computeType, bufferSz, buffer, kind);}]>
  ];

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    `{` $kind `}` $spmatA (`{` $modeA^ `}`)? `,` $spmatB (`{` $modeB^ `}`)? `,` $spmatC `,` $desc `,` $bufferSz `,` $buffer  attr-dict `:` $computeType `into` type($buffer)
  }];
}

def GPU_SpGEMMCopyOp : GPU_Op<"spgemm_copy", [GPU_AsyncOpInterface]> {
  let summary = "SpGEMM copy operation";
  let description = [{
    The `gpu.spgemm_copy` operation copies the sparse matrix result of
    a SpGEMM computation.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a `!gpu.async.token` in addition to the environment.

    Example:

    ```mlir
    gpu.spgemm_copy %spmatA, %spmatB, %spmatC, %spgemmDesc: f32
    ```

    The matrix arguments can also be associated with one of the following
    operators: NON_TRANSPOSE, TRANSPOSE, CONJUGATE_TRANSPOSE. The default value
    is NON_TRANSPOSE.
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                       GPU_SparseSpGEMMOpHandle:$desc,
                       GPU_TransposeModeAttr:$modeA,
                       GPU_TransposeModeAttr:$modeB,
                       GPU_SparseSpMatHandle:$spmatA,
                       GPU_SparseSpMatHandle:$spmatB,
                       GPU_SparseSpMatHandle:$spmatC,
                       TypeAttr:$computeType);
  let results = (outs Optional<GPU_AsyncToken>:$asyncToken);

  let builders = [OpBuilder<(ins
    "Type":$asyncToken,
    "ValueRange":$asyncDependencies,
    "Value":$desc,
    "Value":$spmatA,
    "Value":$spmatB,
    "Value":$spmatC,
    "Type":$computeType), [{
  auto modeA = gpu::TransposeMode::NON_TRANSPOSE;
  auto modeB = gpu::TransposeMode::NON_TRANSPOSE;
  return build($_builder, $_state, asyncToken, asyncDependencies, desc,
               modeA, modeB, spmatA, spmatB, spmatC, computeType);}]>
  ];

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $spmatA (`{` $modeA^ `}`)? `,` $spmatB (`{` $modeB^ `}`)? `,` $spmatC `,` $desc attr-dict `:` $computeType
  }];
}

def GPU_SpMatGetSizeOp : GPU_Op<"spmat_get_size", [GPU_AsyncOpInterface]> {
  let summary = "SpMat get size operation";
  let description = [{
    The `gpu.spmat_get_size` operation retrieves the number of rows, number of
    columns, and number of non-zero elements of a sparse matrix.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a `!gpu.async.token` in addition to the environment.

    Example:

    ```mlir
    %rows, %cols, %nnz, %token = gpu.spmat_get_size async [%dep] %spmatC
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                       GPU_SparseSpMatHandle:$spmat);
  let results = (outs Index:$rows,
                      Index:$cols,
                      Index:$nnz,
                      Optional<GPU_AsyncToken>:$asyncToken);

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
    $spmat attr-dict
  }];
}

def GPU_SetCsrPointersOp : GPU_Op<"set_csr_pointers", [GPU_AsyncOpInterface]> {
  let summary = "SpGEMM get size operation";
  let description = [{
    The `gpu.set_csr_pointers` assigns the given positions, coordinates,
    and values buffer that reside on the device directly to the given sparse
    matrix descriptor in csr format.

    If the `async` keyword is present, the op is executed asynchronously (i.e.
    it does not block until the execution has finished on the device). In
    that case, it returns a `!gpu.async.token` in addition to the environment.

    Example:

    ```mlir
    %token = gpu.set_csr_pointers async [%dep] %positions, %coordinates, %values
          : memref<?xf32>, memref<?xindex>, memref<?xindex>
    ```
  }];

  let arguments = (ins Variadic<GPU_AsyncToken>:$asyncDependencies,
                       Arg<GPU_SparseSpMatHandle>:$spmat,
                       AnyMemRef:$positions,
                       AnyMemRef:$coordinates,
		       AnyMemRef:$values);
  let results = (outs Optional<GPU_AsyncToken>:$asyncToken);

  let assemblyFormat = [{
    custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)
      $spmat `,` $positions `,` $coordinates `,` $values attr-dict
        `:` type($positions) `,` type($coordinates) `,` type($values)
  }];
}

def GPU_WarpExecuteOnLane0Op : GPU_Op<"warp_execute_on_lane_0",
      [DeclareOpInterfaceMethods<RegionBranchOpInterface, ["areTypesCompatible"]>,
       SingleBlockImplicitTerminator<"gpu::YieldOp">,
       RecursiveMemoryEffects]> {
  let summary = "Executes operations in the associated region on thread #0 of a"
                "SPMD program";
  let description = [{
    `warp_execute_on_lane_0` is an operation used to bridge the gap between
    vector programming and SPMD programming model like GPU SIMT. It allows to
    trivially convert a region of vector code meant to run on a multiple threads
    into a valid SPMD region and then allows incremental transformation to
    distribute vector operations on the threads.

    Any code present in the region would only be executed on first thread/lane
    based on the `laneid` operand. The `laneid` operand is an integer ID between
    [0, `warp_size`). The `warp_size` attribute indicates the number of lanes in
    a warp.

    Operands are vector values distributed on all lanes that may be used by
    the single lane execution. The matching region argument is a vector of all
    the values of those lanes available to the single active lane. The
    distributed dimension is implicit based on the shape of the operand and
    argument. the properties of the distribution may be described by extra
    attributes (e.g. affine map).

    Return values are distributed on all lanes using laneId as index. The
    vector is distributed based on the shape ratio between the vector type of
    the yield and the result type.
    If the shapes are the same this means the value is broadcasted to all lanes.
    In the future the distribution can be made more explicit using affine_maps
    and will support having multiple Ids.

    Therefore the `warp_execute_on_lane_0` operations allow to implicitly copy
    between lane0 and the lanes of the warp. When distributing a vector
    from lane0 to all the lanes, the data are distributed in a block cyclic way.
    For example `vector<64xf32>` gets distributed on 32 threads and map to
    `vector<2xf32>` where thread 0 contains vector[0] and vector[1].

    During lowering values passed as operands and return value need to be
    visible to different lanes within the warp. This would usually be done by
    going through memory.

    The region is *not* isolated from above. For values coming from the parent
    region not going through operands only the lane 0 value will be accesible so
    it generally only make sense for uniform values.

    Example:
    ```
    // Execute in parallel on all threads/lanes.
    gpu.warp_execute_on_lane_0 (%laneid)[32] {
      // Serial code running only on thread/lane 0.
      ...
    }
    // Execute in parallel on all threads/lanes.
    ```

    This may be lowered to an scf.if region as below:
    ```
      // Execute in parallel on all threads/lanes.
      %cnd = arith.cmpi eq, %laneid, %c0 : index
      scf.if %cnd {
        // Serial code running only on thread/lane 0.
        ...
      }
      // Execute in parallel on all threads/lanes.
    ```

    When the region has operands and/or return values:
    ```
    // Execute in parallel on all threads/lanes.
    %0 = gpu.warp_execute_on_lane_0(%laneid)[32]
    args(%v0 : vector<4xi32>) -> (vector<1xf32>) {
    ^bb0(%arg0 : vector<128xi32>) :
      // Serial code running only on thread/lane 0.
      ...
      gpu.yield %1 : vector<32xf32>
    }
    // Execute in parallel on all threads/lanes.
    ```

    values at the region boundary would go through memory:
    ```
    // Execute in parallel on all threads/lanes.
    ...
    // Store the data from each thread into memory and Synchronization.
    %tmp0 = memreg.alloc() : memref<128xf32>
    %tmp1 = memreg.alloc() : memref<32xf32>
    %cnd = arith.cmpi eq, %laneid, %c0 : index
    vector.store %v0, %tmp0[%laneid] : memref<128xf32>, vector<4xf32>
    some_synchronization_primitive
    scf.if %cnd {
      // Serialized code running only on thread 0.
      // Load the data from all the threads into a register from thread 0. This
      // allow threads 0 to access data from all the threads.
      %arg0 = vector.load %tmp0[%c0] : memref<128xf32>, vector<128xf32>
      ...
      // Store the data from thread 0 into memory.
      vector.store %1, %tmp1[%c0] : memref<32xf32>, vector<32xf32>
    }
    // Synchronization and load the data in a block cyclic way so that the
    // vector is distributed on all threads.
    some_synchronization_primitive
    %0 = vector.load %tmp1[%laneid] : memref<32xf32>, vector<32xf32>
    // Execute in parallel on all threads/lanes.
    ```

  }];

  let hasVerifier = 1;
  let hasCustomAssemblyFormat = 1;
  let arguments = (ins Index:$laneid, I64Attr:$warp_size,
                       Variadic<AnyType>:$args);
  let results = (outs Variadic<AnyType>:$results);
  let regions = (region SizedRegion<1>:$warpRegion);

  let skipDefaultBuilders = 1;
  let builders = [
    OpBuilder<(ins "TypeRange":$resultTypes, "Value":$laneid,
                   "int64_t":$warpSize)>,
    // `blockArgTypes` are different than `args` types as they are they
    // represent all the `args` instances visibile to lane 0. Therefore we need
    // to explicit pass the type.
    OpBuilder<(ins "TypeRange":$resultTypes, "Value":$laneid,
                   "int64_t":$warpSize, "ValueRange":$args,
                   "TypeRange":$blockArgTypes)>
  ];

  let extraClassDeclaration = [{
    bool isDefinedOutsideOfRegion(Value value) {
      return !getRegion().isAncestor(value.getParentRegion());
    }
  }];
}

#endif // GPU_OPS


//===- LinalgOps.td - Linalg dialect ops -------------------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This is the operation definition file for linear algebra operations.
//
//===----------------------------------------------------------------------===//

#ifndef LINALG_OPS
#define LINALG_OPS

include "mlir/Dialect/Linalg/IR/LinalgBase.td"
include "mlir/Dialect/Linalg/IR/LinalgInterfaces.td"
include "mlir/Interfaces/ControlFlowInterfaces.td"
include "mlir/Interfaces/DestinationStyleOpInterface.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/LoopLikeInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/Interfaces/TilingInterface.td"
include "mlir/Interfaces/ViewLikeInterface.td"

// Base class for Linalg dialect ops that do not correspond to library calls.
class Linalg_Op<string mnemonic, list<Trait> traits = []> :
    Op<Linalg_Dialect, mnemonic, traits>;

def Linalg_YieldOp : Linalg_Op<"yield", [Pure, ReturnLike, Terminator]>,
    Arguments<(ins Variadic<AnyType>:$values)> {
  let summary = "Linalg yield operation";
  let description = [{
    `linalg.yield` is a special terminator operation for blocks inside regions
    in `linalg` generic ops. It returns values to the immediately enclosing
    `linalg` generic op.

    Example:

    ```mlir
    linalg.yield %f0, %f1 : f32, f32
    ```
  }];
  let builders = [OpBuilder<(ins), [{ /* nothing to do */ }]>];
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

def Linalg_IndexOp : Linalg_Op<"index", [Pure]>,
    Arguments<(ins ConfinedAttr<I64Attr, [IntMinValue<0>]>:$dim)>,
    Results<(outs Index:$result)> {
  let summary = "linalg index operation";
  let description = [{
    The `linalg.index` operation returns the iteration index of the immediately
    enclosing linalg structured operation for the iteration dimension `dim`. The
    `dim` attribute specifies the position of the accessed dimension in the
    indexing map domain.

    Example:

    ```mlir
    #map = affine_map<(i, j) -> (i, j)>
    linalg.generic {indexing_maps = [#map, #map],
                    iterator_types = ["parallel", "parallel"]}
      outs(%I, %J : memref<?x?xindex>, memref<?x?xindex>) {
      ^bb0(%arg0 : index, %arg1 : index):
      // Access the outer iteration dimension i
      %i = linalg.index 0 : index
      // Access the inner iteration dimension j
      %j = linalg.index 1 : index
      linalg.yield %i, %j : index, index
    }
    ```

    This may lower to IR resembling:

    ```mlir
    %0 = dim %I, %c0 : memref<?x?xindex>
    %1 = dim %I, %c1 : memref<?x?xindex>
    scf.for %i = %c0 to %0 step %c1 {
      scf.for %j = %c0 to %1 step %c1 {
        store %i, %I[%i, %j] : memref<?x?xindex>
        store %j, %J[%i, %j] : memref<?x?xindex>
      }
    }
    ```
  }];

  let assemblyFormat = [{ $dim attr-dict `:` type($result) }];
  let hasVerifier = 1;
}

def Linalg_SoftmaxOp : Linalg_Op<"softmax",
    [DestinationStyleOpInterface,
     PredOpTrait<"input and output have same element type", TCopVTEtIsSameAs<0, 1>>,
     DeclareOpInterfaceMethods<ReifyRankedShapedTypeOpInterface>,
     DeclareOpInterfaceMethods<AggregatedOpInterface, ["decomposeOperation"]>,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     DeclareOpInterfaceMethods<TilingInterface,
      ["getIterationDomain",
       "getLoopIteratorTypes",
       "getResultTilePosition",
       "getTiledImplementation"]>]> {
  let summary = "Softmax operator";
  let description = [{
    linalg.softmax computes a numerically stable version of softmax.

    For a given input tensor and a specified dimension `d`, compute:
      1. the max `m` along that dimension `d`
      2. f(x) = exp(x - m)
      3. sum f(x) along dimension d to get l(x).
      4. compute the final result f(x) / l(x).

    This is an aggregate linalg operation that further reduces to a small DAG of
    structured operations.

    Warning: Regarding the tiling capabilities, the implementation doesn't
    check that the provided dimensions make sense. This is the responsability
    of the transformation calling the tiling to ensure that the provided
    sizes for each dimension make sense with respect to the semantic of
    softmax.
  }];

  let arguments = (ins AnyShaped:$input,
                       AnyShaped:$output,
                       I64Attr:$dimension
  );

  let results = (outs Variadic<AnyRankedTensor>:$result);
  let hasFolder = 1;
  let assemblyFormat = [{
    attr-dict
    `dimension` `(` $dimension `)`
    `ins` `(` $input `:` type($input) `)`
    `outs` `(` $output `:` type($output) `)`
    (`->` type($result)^)?
  }];

  let extraClassDeclaration = [{
    ShapedType getInputOperandType() {
      return cast<ShapedType>(getInput().getType());
    }
    ShapedType getOutputOperandType() {
      return cast<ShapedType>(getOutput().getType());
    }
    int64_t getInputOperandRank() {
      return getInputOperandType().getRank();
    }
    int64_t getOutputOperandRank() {
      return getOutputOperandType().getRank();
    }
    MutableOperandRange getDpsInitsMutable() { return getOutputMutable(); }
  }];
  let hasVerifier = 1;
}

def Linalg_WinogradFilterTransformOp : Linalg_Op<"winograd_filter_transform",
    [AllElementTypesMatch<["filter", "output"]>,
     DeclareOpInterfaceMethods<TilingInterface,
      ["getIterationDomain",
       "getLoopIteratorTypes",
       "getResultTilePosition",
       "getTiledImplementation"]>]> {
  let summary = "Winograd filter transform operator";
  let description = [{
    Winograd Conv2D algorithm will convert linalg Conv2D operator into batched
    matrix multiply. Before the matrix multiply, it will convert filter and
    input into a format suitable for batched matrix multiply. After the matrix
    multiply, it will convert output to the final result tensor.

    The algorithm F(m x m, r x r) is

    Y = A^T x [(G x g x G^T) @ (B^T x d x B)] x A

    The size of output Y is m x m. The size of filter g is r x r. The size of
    input d is (m + r - 1) x (m + r - 1). A^T, A, G^T, G, B^T, and B are
    transformation matrices.

    This operator is defined to represent the high level concept of filter
    transformation (G x g x G^T) in the Winograd Conv2D algorithm.
  }];

  let arguments = (ins TensorRankOf<[AnyType], [4]>:$filter,
                       TensorRankOf<[AnyType], [4]>:$output,
                       I64Attr:$m,
                       I64Attr:$r
  );

  let results = (outs TensorRankOf<[AnyType], [4]>:$result);
  let assemblyFormat = [{
    attr-dict
    `m` `(` $m `)`
    `r` `(` $r `)`
    `ins` `(` $filter `:` type($filter) `)`
    `outs` `(` $output `:` type($output) `)`
    `->` type($result)
  }];
  let extraClassDeclaration = [{
    ShapedType getFilterOperandType() {
      return cast<ShapedType>(getFilter().getType());
    }
    ShapedType getOutputOperandType() {
      return cast<ShapedType>(getOutput().getType());
    }
    int64_t getFilterOperandRank() {
      return getFilterOperandType().getRank();
    }
    int64_t getOutputOperandRank() {
      return getOutputOperandType().getRank();
    }
    int64_t getFilterFDim() {
      return 0;
    }
    int64_t getFilterHDim() {
      return 1;
    }
    int64_t getFilterWDim() {
      return 2;
    }
    int64_t getFilterCDim() {
      return 3;
    }
  }];
  let hasVerifier = 1;
}

def Linalg_WinogradInputTransformOp : Linalg_Op<"winograd_input_transform",
    [AllElementTypesMatch<["input", "output"]>,
     DeclareOpInterfaceMethods<TilingInterface,
      ["getIterationDomain",
       "getLoopIteratorTypes",
       "getResultTilePosition",
       "getTiledImplementation"]>]> {
  let summary = "Winograd input transform operator";
  let description = [{
    Winograd Conv2D algorithm will convert linalg Conv2D operator into batched
    matrix multiply. Before the matrix multiply, it will convert filter and
    input into a format suitable for batched matrix multiply. After the matrix
    multiply, it will convert output to the final result tensor.

    The algorithm F(m x m, r x r) is

    Y = A^T x [(G x g x G^T) @ (B^T x d x B)] x A

    The size of output Y is m x m. The size of filter g is r x r. The size of
    input d is (m + r - 1) x (m + r - 1). A^T, A, G^T, G, B^T, and B are
    transformation matrices.

    This operator is defined to represent the high level concept of input
    transformation (B^T x d x B) in the Winograd Conv2D algorithm.
  }];

  let arguments = (ins TensorRankOf<[AnyType], [4]>:$input,
                       TensorRankOf<[AnyType], [6]>:$output,
                       I64Attr:$m,
                       I64Attr:$r
  );

  let results = (outs TensorRankOf<[AnyType], [6]>:$result);
  let assemblyFormat = [{
    attr-dict
    `m` `(` $m `)`
    `r` `(` $r `)`
    `ins` `(` $input `:` type($input) `)`
    `outs` `(` $output `:` type($output) `)`
    `->` type($result)
  }];
  let extraClassDeclaration = [{
    ShapedType getInputOperandType() {
      return cast<ShapedType>(getInput().getType());
    }
    ShapedType getOutputOperandType() {
      return cast<ShapedType>(getOutput().getType());
    }
    int64_t getInputOperandRank() {
      return getInputOperandType().getRank();
    }
    int64_t getOutputOperandRank() {
      return getOutputOperandType().getRank();
    }
    int64_t getInputNDim() {
      return 0;
    }
    int64_t getInputHDim() {
      return 1;
    }
    int64_t getInputWDim() {
      return 2;
    }
    int64_t getInputCDim() {
      return 3;
    }
    int64_t getOutputAlphaHDim() {
      return 0;
    }
    int64_t getOutputAlphaWDim() {
      return 1;
    }
    int64_t getOutputTileHDim() {
      return 2;
    }
    int64_t getOutputTileWDim() {
      return 3;
    }
    int64_t getOutputNDim() {
      return 4;
    }
    int64_t getOutputCDim() {
      return 5;
    }
  }];
  let hasVerifier = 1;
}

def Linalg_WinogradOutputTransformOp : Linalg_Op<"winograd_output_transform",
    [AllElementTypesMatch<["value", "output"]>, DestinationStyleOpInterface,
     DeclareOpInterfaceMethods<TilingInterface,
      ["getIterationDomain",
       "getLoopIteratorTypes",
       "getResultTilePosition",
       "getTiledImplementation"]>]> {
  let summary = "Winograd output transform operator";
  let description = [{
    Winograd Conv2D algorithm will convert linalg Conv2D operator into batched
    matrix multiply. Before the matrix multiply, it will convert filter and
    input into a format suitable for batched matrix multiply. After the matrix
    multiply, it will convert output to the final result tensor.

    The algorithm F(m x m, r x r) is

    Y = A^T x [(G x g x G^T) @ (B^T x d x B)] x A

    The size of output Y is m x m. The size of filter g is r x r. The size of
    input d is (m + r - 1) x (m + r - 1). A^T, A, G^T, G, B^T, and B are
    transformation matrices.

    This operator is defined to represent the high level concept of output
    transformation (A^T x y x A) in the Winograd Conv2D algorithm.
  }];

  let arguments = (ins TensorRankOf<[AnyType], [6]>:$value,
                       TensorRankOf<[AnyType], [4]>:$output,
                       I64Attr:$m,
                       I64Attr:$r
  );

  let results = (outs TensorRankOf<[AnyType], [4]>:$result);
  let assemblyFormat = [{
    attr-dict
    `m` `(` $m `)`
    `r` `(` $r `)`
    `ins` `(` $value `:` type($value) `)`
    `outs` `(` $output `:` type($output) `)`
    `->` type($result)
  }];
  let extraClassDeclaration = [{
    ShapedType getValueOperandType() {
      return cast<ShapedType>(getValue().getType());
    }
    ShapedType getOutputOperandType() {
      return cast<ShapedType>(getOutput().getType());
    }
    int64_t getValueOperandRank() {
      return getValueOperandType().getRank();
    }
    int64_t getOutputOperandRank() {
      return getOutputOperandType().getRank();
    }
    int64_t getValueAlphaHDim() {
      return 0;
    }
    int64_t getValueAlphaWDim() {
      return 1;
    }
    int64_t getValueTileHDim() {
      return 2;
    }
    int64_t getValueTileWDim() {
      return 3;
    }
    int64_t getValueNDim() {
      return 4;
    }
    int64_t getValueFDim() {
      return 5;
    }
    int64_t getOutputNDim() {
      return 0;
    }
    int64_t getOutputHDim() {
      return 1;
    }
    int64_t getOutputWDim() {
      return 2;
    }
    int64_t getOutputFDim() {
      return 3;
    }
    MutableOperandRange getDpsInitsMutable() { return getOutputMutable(); }
  }];
  let hasVerifier = 1;
}

#endif // LINALG_OPS


//===- ComplexOps.td - Complex op definitions ----------------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef COMPLEX_OPS
#define COMPLEX_OPS

include "mlir/Dialect/Arith/IR/ArithBase.td"
include "mlir/Dialect/Arith/IR/ArithOpsInterfaces.td"
include "mlir/Dialect/Complex/IR/ComplexBase.td"
include "mlir/IR/OpAsmInterface.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"

class Complex_Op<string mnemonic, list<Trait> traits = []>
    : Op<Complex_Dialect, mnemonic, traits>;

// Base class for standard arithmetic operations on complex numbers with a
// floating-point element type. These operations take two operands and return
// one result, all of which must be complex numbers of the same type.
class ComplexArithmeticOp<string mnemonic, list<Trait> traits = []> :
    Complex_Op<mnemonic, traits # [Pure, SameOperandsAndResultType,
    Elementwise, DeclareOpInterfaceMethods<ArithFastMathInterface>]> {
  let arguments = (ins Complex<AnyFloat>:$lhs, Complex<AnyFloat>:$rhs, DefaultValuedAttr<
        Arith_FastMathAttr, "::mlir::arith::FastMathFlags::none">:$fastmath);
  let results = (outs Complex<AnyFloat>:$result);
  let assemblyFormat = "$lhs `,` $rhs (`fastmath` `` $fastmath^)? attr-dict `:` type($result)";
}

// Base class for standard unary operations on complex numbers with a
// floating-point element type. These operations take one operand and return
// one result; the operand must be a complex number.
class ComplexUnaryOp<string mnemonic, list<Trait> traits = []> :
    Complex_Op<mnemonic, traits # [Pure, Elementwise, DeclareOpInterfaceMethods<ArithFastMathInterface>]> {
  let arguments = (ins Complex<AnyFloat>:$complex, DefaultValuedAttr<
        Arith_FastMathAttr, "::mlir::arith::FastMathFlags::none">:$fastmath);
  let assemblyFormat = "$complex (`fastmath` `` $fastmath^)? attr-dict `:` type($complex)";
}

//===----------------------------------------------------------------------===//
// AbsOp
//===----------------------------------------------------------------------===//

def AbsOp : ComplexUnaryOp<"abs",
    [TypesMatchWith<"complex element type matches result type",
                    "complex", "result",
                    "::llvm::cast<ComplexType>($_self).getElementType()">]> {
  let summary = "computes absolute value of a complex number";
  let description = [{
    The `abs` op takes a single complex number and computes its absolute value.

    Example:

    ```mlir
    %a = complex.abs %b : complex<f32>
    ```
  }];
  let results = (outs AnyFloat:$result);
}

//===----------------------------------------------------------------------===//
// AddOp
//===----------------------------------------------------------------------===//

def AddOp : ComplexArithmeticOp<"add"> {
  let summary = "complex addition";
  let description = [{
    The `add` operation takes two complex numbers and returns their sum.

    Example:

    ```mlir
    %a = complex.add %b, %c : complex<f32>
    ```
  }];

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Atan2
//===----------------------------------------------------------------------===//

def Atan2Op : ComplexArithmeticOp<"atan2"> {
  let summary = "complex 2-argument arctangent";
  let description = [{
    For complex numbers it is expressed using complex logarithm
    atan2(y, x) = -i * log((x + i * y) / sqrt(x**2 + y**2))

    Example:

    ```mlir
    %a = complex.atan2 %b, %c : complex<f32>
    ```
  }];
}


//===----------------------------------------------------------------------===//
// Bitcast
//===----------------------------------------------------------------------===//

def BitcastOp : Complex_Op<"bitcast", [Pure]> {

  let summary = "computes bitcast between complex and equal arith types";
  let description = [{

    Example:

    ```mlir
         %a = complex.bitcast %b : complex<f32> -> i64
    ```
  }];
  let assemblyFormat = "$operand attr-dict `:` type($operand) `to` type($result)";
  let arguments = (ins AnyType:$operand);
  let results = (outs AnyType:$result);

  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// ConstantOp
//===----------------------------------------------------------------------===//

def ConstantOp : Complex_Op<"constant", [
    ConstantLike, Pure,
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>
  ]> {
  let summary = "complex number constant operation";
  let description = [{
    The `complex.constant` operation creates a constant complex number from an
    attribute containing the real and imaginary parts.

    Example:

    ```mlir
    %a = complex.constant [0.1, -1.0] : complex<f64>
    ```
  }];

  let arguments = (ins ArrayAttr:$value);
  let results = (outs AnyComplex:$complex);

  let assemblyFormat = "$value attr-dict `:` type($complex)";
  let hasFolder = 1;
  let hasVerifier = 1;

  let extraClassDeclaration = [{
    /// Returns true if a constant operation can be built with the given value
    /// and result type.
    static bool isBuildableWith(Attribute value, Type type);
  }];
}

//===----------------------------------------------------------------------===//
// CosOp
//===----------------------------------------------------------------------===//

def CosOp : ComplexUnaryOp<"cos", [SameOperandsAndResultType]> {
  let summary = "computes cosine of a complex number";
  let description = [{
    The `cos` op takes a single complex number and computes the cosine of
    it, i.e. `cos(x)`, where `x` is the input value.

    Example:

    ```mlir
    %a = complex.cos %b : complex<f32>
    ```
  }];

  let results = (outs Complex<AnyFloat>:$result);
}

//===----------------------------------------------------------------------===//
// CreateOp
//===----------------------------------------------------------------------===//

def CreateOp : Complex_Op<"create",
    [Pure,
     AllTypesMatch<["real", "imaginary"]>,
     TypesMatchWith<"complex element type matches real operand type",
                    "complex", "real",
                    "::llvm::cast<ComplexType>($_self).getElementType()">,
     TypesMatchWith<"complex element type matches imaginary operand type",
                    "complex", "imaginary",
                    "::llvm::cast<ComplexType>($_self).getElementType()">]> {

  let summary = "complex number creation operation";
  let description = [{
    The `complex.create` operation creates a complex number from two
    floating-point operands, the real and the imaginary part.

    Example:

    ```mlir
    %a = complex.create %b, %c : complex<f32>
    ```
  }];

  let arguments = (ins AnyFloat:$real, AnyFloat:$imaginary);
  let results = (outs Complex<AnyFloat>:$complex);

  let assemblyFormat = "$real `,` $imaginary attr-dict `:` type($complex)";
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// DivOp
//===----------------------------------------------------------------------===//

def DivOp : ComplexArithmeticOp<"div"> {
  let summary = "complex division";
  let description = [{
    The `div` operation takes two complex numbers and returns result of their
    division:

    ```mlir
    %a = complex.div %b, %c : complex<f32>
    ```
  }];

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// EqualOp
//===----------------------------------------------------------------------===//

def EqualOp : Complex_Op<"eq",
    [Pure, AllTypesMatch<["lhs", "rhs"]>, Elementwise]> {
  let summary = "computes whether two complex values are equal";
  let description = [{
    The `eq` op takes two complex numbers and returns whether they are equal.

    Example:

    ```mlir
    %a = complex.eq %b, %c : complex<f32>
    ```
  }];

  let arguments = (ins Complex<AnyFloat>:$lhs, Complex<AnyFloat>:$rhs);
  let results = (outs I1:$result);

  let assemblyFormat = "$lhs `,` $rhs  attr-dict `:` type($lhs)";
}

//===----------------------------------------------------------------------===//
// ExpOp
//===----------------------------------------------------------------------===//

def ExpOp : ComplexUnaryOp<"exp", [SameOperandsAndResultType]> {
  let summary = "computes exponential of a complex number";
  let description = [{
    The `exp` op takes a single complex number and computes the exponential of
    it, i.e. `exp(x)` or `e^(x)`, where `x` is the input value.
    `e` denotes Euler's number and is approximately equal to 2.718281.

    Example:

    ```mlir
    %a = complex.exp %b : complex<f32>
    ```
  }];

  let results = (outs Complex<AnyFloat>:$result);

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Expm1Op
//===----------------------------------------------------------------------===//

def Expm1Op : ComplexUnaryOp<"expm1", [SameOperandsAndResultType]> {
  let summary = "computes exponential of a complex number minus 1";
  let description = [{
    complex.expm1(x) := complex.exp(x) - 1

    Example:

    ```mlir
    %a = complex.expm1 %b : complex<f32>
    ```
  }];

  let results = (outs Complex<AnyFloat>:$result);
}

//===----------------------------------------------------------------------===//
// ImOp
//===----------------------------------------------------------------------===//

def ImOp : ComplexUnaryOp<"im",
    [TypesMatchWith<"complex element type matches result type",
                    "complex", "imaginary",
                    "::llvm::cast<ComplexType>($_self).getElementType()">]> {
  let summary = "extracts the imaginary part of a complex number";
  let description = [{
    The `im` op takes a single complex number and extracts the imaginary part.

    Example:

    ```mlir
    %a = complex.im %b : complex<f32>
    ```
  }];

  let results = (outs AnyFloat:$imaginary);
  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// LogOp
//===----------------------------------------------------------------------===//

def LogOp : ComplexUnaryOp<"log", [SameOperandsAndResultType]> {
  let summary = "computes natural logarithm of a complex number";
  let description = [{
    The `log` op takes a single complex number and computes the natural
    logarithm of it, i.e. `log(x)` or `log_e(x)`, where `x` is the input value.
    `e` denotes Euler's number and is approximately equal to 2.718281.

    Example:

    ```mlir
    %a = complex.log %b : complex<f32>
    ```
  }];

  let results = (outs Complex<AnyFloat>:$result);

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// Log1pOp
//===----------------------------------------------------------------------===//

def Log1pOp : ComplexUnaryOp<"log1p", [SameOperandsAndResultType]> {
  let summary = "computes natural logarithm of a complex number";
  let description = [{
    The `log` op takes a single complex number and computes the natural
    logarithm of one plus the given value, i.e. `log(1 + x)` or `log_e(1 + x)`,
    where `x` is the input value. `e` denotes Euler's number and is
    approximately equal to 2.718281.

    Example:

    ```mlir
    %a = complex.log1p %b : complex<f32>
    ```
  }];

  let results = (outs Complex<AnyFloat>:$result);
}

//===----------------------------------------------------------------------===//
// MulOp
//===----------------------------------------------------------------------===//

def MulOp : ComplexArithmeticOp<"mul"> {
  let summary = "complex multiplication";
  let description = [{
    The `mul` operation takes two complex numbers and returns their product:

    ```mlir
    %a = complex.mul %b, %c : complex<f32>
    ```
  }];

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// NegOp
//===----------------------------------------------------------------------===//

def NegOp : ComplexUnaryOp<"neg", [SameOperandsAndResultType]> {
  let summary = "Negation operator";
  let description = [{
    The `neg` op takes a single complex number `complex` and returns `-complex`.

    Example:

    ```mlir
    %a = complex.neg %b : complex<f32>
    ```
  }];

  let results = (outs Complex<AnyFloat>:$result);

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// NotEqualOp
//===----------------------------------------------------------------------===//

def NotEqualOp : Complex_Op<"neq",
    [Pure, AllTypesMatch<["lhs", "rhs"]>, Elementwise]> {
  let summary = "computes whether two complex values are not equal";
  let description = [{
    The `neq` op takes two complex numbers and returns whether they are not
    equal.

    Example:

    ```mlir
    %a = complex.neq %b, %c : complex<f32>
    ```
  }];

  let arguments = (ins Complex<AnyFloat>:$lhs, Complex<AnyFloat>:$rhs);
  let results = (outs I1:$result);

  let assemblyFormat = "$lhs `,` $rhs  attr-dict `:` type($lhs)";
}

//===----------------------------------------------------------------------===//
// PowOp
//===----------------------------------------------------------------------===//

def PowOp : ComplexArithmeticOp<"pow"> {
  let summary = "complex power function";
  let description = [{
    The `sqrt` operation takes a complex number raises it to the given complex
    exponent.

    Example:

    ```mlir
    %a = complex.pow %b, %c : complex<f32>
    ```
  }];
}

//===----------------------------------------------------------------------===//
// ReOp
//===----------------------------------------------------------------------===//

def ReOp : ComplexUnaryOp<"re",
    [TypesMatchWith<"complex element type matches result type",
                    "complex", "real",
                    "::llvm::cast<ComplexType>($_self).getElementType()">]> {
  let summary = "extracts the real part of a complex number";
  let description = [{
    The `re` op takes a single complex number and extracts the real part.

    Example:

    ```mlir
    %a = complex.re %b : complex<f32>
    ```
  }];

  let results = (outs AnyFloat:$real);
  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// RsqrtOp
//===----------------------------------------------------------------------===//

def RsqrtOp : ComplexUnaryOp<"rsqrt", [SameOperandsAndResultType]> {
  let summary = "complex reciprocal of square root";
  let description = [{
    The `rsqrt` operation computes reciprocal of square root.

    Example:

    ```mlir
    %a = complex.rsqrt %b : complex<f32>
    ```
  }];

  let results = (outs Complex<AnyFloat>:$result);
}

//===----------------------------------------------------------------------===//
// SignOp
//===----------------------------------------------------------------------===//

def SignOp : ComplexUnaryOp<"sign", [SameOperandsAndResultType]> {
  let summary = "computes sign of a complex number";
  let description = [{
    The `sign` op takes a single complex number and computes the sign of
    it, i.e. `y = sign(x) = x / |x|` if `x != 0`, otherwise `y = 0`.

    Example:

    ```mlir
    %a = complex.sign %b : complex<f32>
    ```
  }];

  let results = (outs Complex<AnyFloat>:$result);
}

//===----------------------------------------------------------------------===//
// SinOp
//===----------------------------------------------------------------------===//

def SinOp : ComplexUnaryOp<"sin", [SameOperandsAndResultType]> {
  let summary = "computes sine of a complex number";
  let description = [{
    The `sin` op takes a single complex number and computes the sine of
    it, i.e. `sin(x)`, where `x` is the input value.

    Example:

    ```mlir
    %a = complex.sin %b : complex<f32>
    ```
  }];

  let results = (outs Complex<AnyFloat>:$result);
}

//===----------------------------------------------------------------------===//
// SqrtOp
//===----------------------------------------------------------------------===//

def SqrtOp : ComplexUnaryOp<"sqrt", [SameOperandsAndResultType]> {
  let summary = "complex square root";
  let description = [{
    The `sqrt` operation takes a complex number and returns its square root.

    Example:

    ```mlir
    %a = complex.sqrt %b : complex<f32>
    ```
  }];

  let results = (outs Complex<AnyFloat>:$result);
}

//===----------------------------------------------------------------------===//
// SubOp
//===----------------------------------------------------------------------===//

def SubOp : ComplexArithmeticOp<"sub"> {
  let summary = "complex subtraction";
  let description = [{
    The `sub` operation takes two complex numbers and returns their difference.

    Example:

    ```mlir
    %a = complex.sub %b, %c : complex<f32>
    ```
  }];

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// TanhOp
//===----------------------------------------------------------------------===//

def TanhOp : ComplexUnaryOp<"tanh", [SameOperandsAndResultType]> {
  let summary = "complex hyperbolic tangent";
  let description = [{
    The `tanh` operation takes a complex number and returns its hyperbolic
    tangent.

    Example:

    ```mlir
    %a = complex.tanh %b : complex<f32>
    ```
  }];

  let results = (outs Complex<AnyFloat>:$result);
}

//===----------------------------------------------------------------------===//
// TanOp
//===----------------------------------------------------------------------===//

def TanOp : ComplexUnaryOp<"tan", [SameOperandsAndResultType]> {
  let summary = "computes tangent of a complex number";
  let description = [{
    The `tan` op takes a single complex number and computes the tangent of
    it, i.e. `tan(x)`, where `x` is the input value.

    Example:

    ```mlir
    %a = complex.tan %b : complex<f32>
    ```
  }];
  let results = (outs Complex<AnyFloat>:$result);
}

//===----------------------------------------------------------------------===//
// Conj
//===----------------------------------------------------------------------===//

def ConjOp : ComplexUnaryOp<"conj", [SameOperandsAndResultType]> {
  let summary = "Calculate the complex conjugate";
  let description = [{
    The `conj` op takes a single complex number and computes the
    complex conjugate.

    Example:

    ```mlir
    %a = complex.conj %b: complex<f32>
    ```
  }];

  let results = (outs Complex<AnyFloat>:$result);
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// AngleOp
//===----------------------------------------------------------------------===//

def AngleOp : ComplexUnaryOp<"angle",
                           [TypesMatchWith<"complex element type matches result type",
                                           "complex", "result",
                                           "::llvm::cast<ComplexType>($_self).getElementType()">]> {
  let summary = "computes argument value of a complex number";
  let description = [{
    The `angle` op takes a single complex number and computes its argument value with a branch cut along the negative real axis.

    Example:

    ```mlir
         %a = complex.angle %b : complex<f32>
    ```
  }];
  let results = (outs AnyFloat:$result);
}

#endif // COMPLEX_OPS


//===- ControlFlowOps.td - ControlFlow operations ----------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This file contains definitions for the operations within the ControlFlow
// dialect.
//
//===----------------------------------------------------------------------===//

#ifndef MLIR_DIALECTS_CONTROLFLOW_IR_CONTROLFLOWOPS_TD
#define MLIR_DIALECTS_CONTROLFLOW_IR_CONTROLFLOWOPS_TD

include "mlir/IR/EnumAttr.td"
include "mlir/IR/OpAsmInterface.td"
include "mlir/Interfaces/ControlFlowInterfaces.td"
include "mlir/Interfaces/SideEffectInterfaces.td"

def ControlFlow_Dialect : Dialect {
  let name = "cf";
  let cppNamespace = "::mlir::cf";
  let dependentDialects = ["arith::ArithDialect"];
  let description = [{
    This dialect contains low-level, i.e. non-region based, control flow
    constructs. These constructs generally represent control flow directly
    on SSA blocks of a control flow graph.
  }];
}

class CF_Op<string mnemonic, list<Trait> traits = []> :
    Op<ControlFlow_Dialect, mnemonic, traits>;

//===----------------------------------------------------------------------===//
// AssertOp
//===----------------------------------------------------------------------===//

def AssertOp : CF_Op<"assert",
    [DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
  let summary = "Assert operation with message attribute";
  let description = [{
    Assert operation at runtime with single boolean operand and an error
    message attribute.
    If the argument is `true` this operation has no effect. Otherwise, the
    program execution will abort. The provided error message may be used by a
    runtime to propagate the error to the user.

    Example:

    ```mlir
    cf.assert %b, "Expected ... to be true"
    ```
  }];

  let arguments = (ins I1:$arg, StrAttr:$msg);

  let assemblyFormat = "$arg `,` $msg attr-dict";
  let hasCanonicalizeMethod = 1;
}

//===----------------------------------------------------------------------===//
// BranchOp
//===----------------------------------------------------------------------===//

def BranchOp : CF_Op<"br", [
    DeclareOpInterfaceMethods<BranchOpInterface, ["getSuccessorForOperands"]>,
    Pure, Terminator
  ]> {
  let summary = "Branch operation";
  let description = [{
    The `cf.br` operation represents a direct branch operation to a given
    block. The operands of this operation are forwarded to the successor block,
    and the number and type of the operands must match the arguments of the
    target block.

    Example:

    ```mlir
    ^bb2:
      %2 = call @someFn()
      cf.br ^bb3(%2 : tensor<*xf32>)
    ^bb3(%3: tensor<*xf32>):
    ```
  }];

  let arguments = (ins Variadic<AnyType>:$destOperands);
  let successors = (successor AnySuccessor:$dest);

  let builders = [
    OpBuilder<(ins "Block *":$dest,
                   CArg<"ValueRange", "{}">:$destOperands), [{
      $_state.addSuccessors(dest);
      $_state.addOperands(destOperands);
    }]>];

  let extraClassDeclaration = [{
    void setDest(Block *block);

    /// Erase the operand at 'index' from the operand list.
    void eraseOperand(unsigned index);
  }];

  let hasCanonicalizeMethod = 1;
  let assemblyFormat = [{
    $dest (`(` $destOperands^ `:` type($destOperands) `)`)? attr-dict
  }];
}

//===----------------------------------------------------------------------===//
// CondBranchOp
//===----------------------------------------------------------------------===//

def CondBranchOp : CF_Op<"cond_br",
    [AttrSizedOperandSegments,
     DeclareOpInterfaceMethods<BranchOpInterface, ["getSuccessorForOperands"]>,
     Pure, Terminator]> {
  let summary = "Conditional branch operation";
  let description = [{
    The `cf.cond_br` terminator operation represents a conditional branch on a
    boolean (1-bit integer) value. If the bit is set, then the first destination
    is jumped to; if it is false, the second destination is chosen. The count
    and types of operands must align with the arguments in the corresponding
    target blocks.

    The MLIR conditional branch operation is not allowed to target the entry
    block for a region. The two destinations of the conditional branch operation
    are allowed to be the same.

    The following example illustrates a function with a conditional branch
    operation that targets the same block.

    Example:

    ```mlir
    func.func @select(%a: i32, %b: i32, %flag: i1) -> i32 {
      // Both targets are the same, operands differ
      cf.cond_br %flag, ^bb1(%a : i32), ^bb1(%b : i32)

    ^bb1(%x : i32) :
      return %x : i32
    }
    ```
  }];

  let arguments = (ins I1:$condition,
                       Variadic<AnyType>:$trueDestOperands,
                       Variadic<AnyType>:$falseDestOperands);
  let successors = (successor AnySuccessor:$trueDest, AnySuccessor:$falseDest);

  let builders = [
    OpBuilder<(ins "Value":$condition, "Block *":$trueDest,
      "ValueRange":$trueOperands, "Block *":$falseDest,
      "ValueRange":$falseOperands), [{
      build($_builder, $_state, condition, trueOperands, falseOperands, trueDest,
            falseDest);
    }]>,
    OpBuilder<(ins "Value":$condition, "Block *":$trueDest,
      "Block *":$falseDest, CArg<"ValueRange", "{}">:$falseOperands), [{
      build($_builder, $_state, condition, trueDest, ValueRange(), falseDest,
            falseOperands);
    }]>];

  let extraClassDeclaration = [{
    // These are the indices into the dests list.
    enum { trueIndex = 0, falseIndex = 1 };

    // Accessors for operands to the 'true' destination.
    Value getTrueOperand(unsigned idx) {
      assert(idx < getNumTrueOperands());
      return getOperand(getTrueDestOperandIndex() + idx);
    }

    void setTrueOperand(unsigned idx, Value value) {
      assert(idx < getNumTrueOperands());
      setOperand(getTrueDestOperandIndex() + idx, value);
    }

    unsigned getNumTrueOperands()  { return getTrueOperands().size(); }

    /// Erase the operand at 'index' from the true operand list.
    void eraseTrueOperand(unsigned index)  {
      getTrueDestOperandsMutable().erase(index);
    }

    // Accessors for operands to the 'false' destination.
    Value getFalseOperand(unsigned idx) {
      assert(idx < getNumFalseOperands());
      return getOperand(getFalseDestOperandIndex() + idx);
    }
    void setFalseOperand(unsigned idx, Value value) {
      assert(idx < getNumFalseOperands());
      setOperand(getFalseDestOperandIndex() + idx, value);
    }

    operand_range getTrueOperands() { return getTrueDestOperands(); }
    operand_range getFalseOperands() { return getFalseDestOperands(); }

    unsigned getNumFalseOperands() { return getFalseOperands().size(); }

    /// Erase the operand at 'index' from the false operand list.
    void eraseFalseOperand(unsigned index) {
      getFalseDestOperandsMutable().erase(index);
    }

  private:
    /// Get the index of the first true destination operand.
    unsigned getTrueDestOperandIndex() { return 1; }

    /// Get the index of the first false destination operand.
    unsigned getFalseDestOperandIndex() {
      return getTrueDestOperandIndex() + getNumTrueOperands();
    }
  }];

  let hasCanonicalizer = 1;
  let assemblyFormat = [{
    $condition `,`
    $trueDest (`(` $trueDestOperands^ `:` type($trueDestOperands) `)`)? `,`
    $falseDest (`(` $falseDestOperands^ `:` type($falseDestOperands) `)`)?
    attr-dict
  }];
}

//===----------------------------------------------------------------------===//
// SwitchOp
//===----------------------------------------------------------------------===//

def SwitchOp : CF_Op<"switch",
    [AttrSizedOperandSegments,
     DeclareOpInterfaceMethods<BranchOpInterface, ["getSuccessorForOperands"]>,
     Pure, Terminator]> {
  let summary = "Switch operation";
  let description = [{
    The `cf.switch` terminator operation represents a switch on a signless integer
    value. If the flag matches one of the specified cases, then the
    corresponding destination is jumped to. If the flag does not match any of
    the cases, the default destination is jumped to. The count and types of
    operands must align with the arguments in the corresponding target blocks.

    Example:

    ```mlir
    cf.switch %flag : i32, [
      default: ^bb1(%a : i32),
      42: ^bb1(%b : i32),
      43: ^bb3(%c : i32)
    ]
    ```
  }];

  let arguments = (ins
    AnyInteger:$flag,
    Variadic<AnyType>:$defaultOperands,
    VariadicOfVariadic<AnyType, "case_operand_segments">:$caseOperands,
    OptionalAttr<AnyIntElementsAttr>:$case_values,
    DenseI32ArrayAttr:$case_operand_segments
  );
  let successors = (successor
    AnySuccessor:$defaultDestination,
    VariadicSuccessor<AnySuccessor>:$caseDestinations
  );
  let builders = [
    OpBuilder<(ins "Value":$flag,
      "Block *":$defaultDestination,
      "ValueRange":$defaultOperands,
      CArg<"ArrayRef<APInt>", "{}">:$caseValues,
      CArg<"BlockRange", "{}">:$caseDestinations,
      CArg<"ArrayRef<ValueRange>", "{}">:$caseOperands)>,
    OpBuilder<(ins "Value":$flag,
      "Block *":$defaultDestination,
      "ValueRange":$defaultOperands,
      CArg<"ArrayRef<int32_t>", "{}">:$caseValues,
      CArg<"BlockRange", "{}">:$caseDestinations,
      CArg<"ArrayRef<ValueRange>", "{}">:$caseOperands)>,
    OpBuilder<(ins "Value":$flag,
      "Block *":$defaultDestination,
      "ValueRange":$defaultOperands,
      CArg<"DenseIntElementsAttr", "{}">:$caseValues,
      CArg<"BlockRange", "{}">:$caseDestinations,
      CArg<"ArrayRef<ValueRange>", "{}">:$caseOperands)>
  ];

  let assemblyFormat = [{
    $flag `:` type($flag) `,` `[` `\n`
      custom<SwitchOpCases>(ref(type($flag)),$defaultDestination,
                            $defaultOperands,
                            type($defaultOperands),
                            $case_values,
                            $caseDestinations,
                            $caseOperands,
                            type($caseOperands))
   `]`
    attr-dict
  }];

  let extraClassDeclaration = [{
    /// Return the operands for the case destination block at the given index.
    OperandRange getCaseOperands(unsigned index) {
      return getCaseOperands()[index];
    }

    /// Return a mutable range of operands for the case destination block at the
    /// given index.
    MutableOperandRange getCaseOperandsMutable(unsigned index) {
      return getCaseOperandsMutable()[index];
    }
  }];

  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

#endif // MLIR_DIALECTS_CONTROLFLOW_IR_CONTROLFLOWOPS_TD


//===- IndexOps.td - Index operation definitions -----------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef INDEX_OPS
#define INDEX_OPS

include "mlir/Dialect/Index/IR/IndexDialect.td"
include "mlir/Dialect/Index/IR/IndexEnums.td"
include "mlir/Interfaces/CastInterfaces.td"
include "mlir/Interfaces/InferIntRangeInterface.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/IR/OpAsmInterface.td"
include "mlir/IR/OpBase.td"

//===----------------------------------------------------------------------===//
// IndexOp
//===----------------------------------------------------------------------===//

/// Base class for Index dialect operations.
class IndexOp<string mnemonic, list<Trait> traits = []>
    : Op<IndexDialect, mnemonic,
      [DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>] # traits>;

//===----------------------------------------------------------------------===//
// IndexBinaryOp
//===----------------------------------------------------------------------===//

/// Base class for binary Index dialect operations.
class IndexBinaryOp<string mnemonic, list<Trait> traits = []>
    : IndexOp<mnemonic, traits> {
  let arguments = (ins Index:$lhs, Index:$rhs);
  let results = (outs Index:$result);
  let assemblyFormat = "$lhs `,` $rhs attr-dict";
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// AddOp
//===----------------------------------------------------------------------===//

def Index_AddOp : IndexBinaryOp<"add", [Commutative, Pure]> {
  let summary = "index addition";
  let description = [{
    The `index.add` operation takes two index values and computes their sum.

    Example:

    ```mlir
    // c = a + b
    %c = index.add %a, %b
    ```
  }];

  let hasCanonicalizeMethod = 1;
}

//===----------------------------------------------------------------------===//
// SubOp
//===----------------------------------------------------------------------===//

def Index_SubOp : IndexBinaryOp<"sub", [Pure]> {
  let summary = "index subtraction";
  let description = [{
    The `index.sub` operation takes two index values and computes the difference
    of the first from the second operand.

    Example:

    ```mlir
    // c = a - b
    %c = index.sub %a, %b
    ```
  }];
}

//===----------------------------------------------------------------------===//
// MulOp
//===----------------------------------------------------------------------===//

def Index_MulOp : IndexBinaryOp<"mul", [Commutative, Pure]> {
  let summary = "index multiplication";
  let description = [{
    The `index.mul` operation takes two index values and computes their product.

    Example:

    ```mlir
    // c = a * b
    %c = index.mul %a, %b
    ```
  }];

  let hasCanonicalizeMethod = 1;
}

//===----------------------------------------------------------------------===//
// DivSOp
//===----------------------------------------------------------------------===//

def Index_DivSOp : IndexBinaryOp<"divs", [NoMemoryEffect]> {
  let summary = "index signed division";
  let description = [{
    The `index.divs` operation takes two index values and computes their signed
    quotient. Treats the leading bit as the sign and rounds towards zero, i.e.
    `6 / -2 = -3`.

    Note: division by zero and signed division overflow are undefined behaviour.

    Example:

    ```mlir
    // c = a / b
    %c = index.divs %a, %b
    ```
  }];
}

//===----------------------------------------------------------------------===//
// DivUOp
//===----------------------------------------------------------------------===//

def Index_DivUOp : IndexBinaryOp<"divu", [NoMemoryEffect]> {
  let summary = "index unsigned division";
  let description = [{
    The `index.divu` operation takes two index values and computes their
    unsigned quotient. Treats the leading bit as the most significant and rounds
    towards zero, i.e. `6 / -2 = 0`.

    Note: division by zero is undefined behaviour.

    Example:

    ```mlir
    // c = a / b
    %c = index.divu %a, %b
    ```
  }];
}

//===----------------------------------------------------------------------===//
// CeilDivSOp
//===----------------------------------------------------------------------===//

def Index_CeilDivSOp : IndexBinaryOp<"ceildivs", [NoMemoryEffect]> {
  let summary = "index signed ceil division";
  let description = [{
    The `index.ceildivs` operation takes two index values and computes their
    signed quotient. Treats the leading bit as the sign and rounds towards
    positive infinity, i.e. `7 / -2 = -3`.

    Note: division by zero and signed division overflow are undefined behaviour.

    Example:

    ```mlir
    // c = ceil(a / b)
    %c = index.ceildivs %a, %b
    ```
  }];
}

//===----------------------------------------------------------------------===//
// CeilDivUOp
//===----------------------------------------------------------------------===//

def Index_CeilDivUOp : IndexBinaryOp<"ceildivu", [NoMemoryEffect]> {
  let summary = "index unsigned ceil division";
  let description = [{
    The `index.ceildivu` operation takes two index values and computes their
    unsigned quotient. Treats the leading bit as the most significant and rounds
    towards positive infinity, i.e. `6 / -2 = 1`.

    Note: division by zero is undefined behaviour.

    Example:

    ```mlir
    // c = ceil(a / b)
    %c = index.ceildivu %a, %b
    ```
  }];
}

//===----------------------------------------------------------------------===//
// FloorDivSOp
//===----------------------------------------------------------------------===//

def Index_FloorDivSOp : IndexBinaryOp<"floordivs", [NoMemoryEffect]> {
  let summary = "index signed floor division";
  let description = [{
    The `index.floordivs` operation takes two index values and computes their
    signed quotient. Treats the leading bit as the sign and rounds towards
    negative infinity, i.e. `5 / -2 = -3`.

    Note: division by zero and signed division overflow are undefined behaviour.

    Example:

    ```mlir
    // c = floor(a / b)
    %c = index.floordivs %a, %b
    ```
  }];
}

//===----------------------------------------------------------------------===//
// RemSOp
//===----------------------------------------------------------------------===//

def Index_RemSOp : IndexBinaryOp<"rems", [NoMemoryEffect]> {
  let summary = "index signed remainder";
  let description = [{
    The `index.rems` operation takes two index values and computes their signed
    remainder. Treats the leading bit as the sign, i.e. `6 % -2 = 0`.

    Example:

    ```mlir
    // c = a % b
    %c = index.rems %a, %b
    ```
  }];
}

//===----------------------------------------------------------------------===//
// RemUOp
//===----------------------------------------------------------------------===//

def Index_RemUOp : IndexBinaryOp<"remu", [NoMemoryEffect]> {
  let summary = "index unsigned remainder";
  let description = [{
    The `index.remu` operation takes two index values and computes their
    unsigned remainder. Treats the leading bit as the most significant, i.e.
    `6 % -2 = 6`.

    Example:

    ```mlir
    // c = a % b
    %c = index.remu %a, %b
    ```
  }];
}

//===----------------------------------------------------------------------===//
// MaxSOp
//===----------------------------------------------------------------------===//

def Index_MaxSOp : IndexBinaryOp<"maxs", [Commutative, Pure]> {
  let summary = "index signed maximum";
  let description = [{
    The `index.maxs` operation takes two index values and computes their signed
    maximum value. Treats the leading bit as the sign, i.e. `max(-2, 6) = 6`.

    Example:

    ```mlir
    // c = max(a, b)
    %c = index.maxs %a, %b
    ```
  }];

  let hasCanonicalizeMethod = 1;
}

//===----------------------------------------------------------------------===//
// MaxUOp
//===----------------------------------------------------------------------===//

def Index_MaxUOp : IndexBinaryOp<"maxu", [Commutative, Pure]> {
  let summary = "index unsigned maximum";
  let description = [{
    The `index.maxu` operation takes two index values and computes their
    unsigned maximum value. Treats the leading bit as the most significant, i.e.
    `max(15, 6) = 15` or `max(-2, 6) = -2`.

    Example:

    ```mlir
    // c = max(a, b)
    %c = index.maxu %a, %b
    ```
  }];

  let hasCanonicalizeMethod = 1;
}

//===----------------------------------------------------------------------===//
// MinSOp
//===----------------------------------------------------------------------===//

def Index_MinSOp : IndexBinaryOp<"mins", [Commutative, Pure]> {
  let summary = "index signed minimum";
  let description = [{
    The `index.mins` operation takes two index values and computes their signed
    minimum value. Treats the leading bit as the sign, i.e. `min(-2, 6) = -2`.

    Example:

    ```mlir
    // c = min(a, b)
    %c = index.mins %a, %b
    ```
  }];

  let hasCanonicalizeMethod = 1;
}

//===----------------------------------------------------------------------===//
// MinUOp
//===----------------------------------------------------------------------===//

def Index_MinUOp : IndexBinaryOp<"minu", [Commutative, Pure]> {
  let summary = "index unsigned minimum";
  let description = [{
    The `index.minu` operation takes two index values and computes their
    unsigned minimum value. Treats the leading bit as the most significant, i.e.
    `min(15, 6) = 6` or `min(-2, 6) = 6`.

    Example:

    ```mlir
    // c = min(a, b)
    %c = index.minu %a, %b
    ```
  }];

  let hasCanonicalizeMethod = 1;
}

//===----------------------------------------------------------------------===//
// ShlOp
//===----------------------------------------------------------------------===//

def Index_ShlOp : IndexBinaryOp<"shl", [Pure]> {
  let summary = "index shift left";
  let description = [{
    The `index.shl` operation shifts an index value to the left by a variable
    amount. The low order bits are filled with zeroes. The RHS operand is always
    treated as unsigned. If the RHS operand is equal to or greater than the
    index bitwidth, the result is a poison value.

    Example:

    ```mlir
    // c = a << b
    %c = index.shl %a, %b
    ```
  }];
}

//===----------------------------------------------------------------------===//
// ShrSOp
//===----------------------------------------------------------------------===//

def Index_ShrSOp : IndexBinaryOp<"shrs", [Pure]> {
  let summary = "signed index shift right";
  let description = [{
    The `index.shrs` operation shifts an index value to the right by a variable
    amount. The LHS operand is treated as signed. The high order bits are filled
    with copies of the most significant bit. If the RHS operand is equal to or
    greater than the index bitwidth, the result is a poison value.

    Example:

    ```mlir
    // c = a >> b
    %c = index.shrs %a, %b
    ```
  }];
}

//===----------------------------------------------------------------------===//
// ShrUOp
//===----------------------------------------------------------------------===//

def Index_ShrUOp : IndexBinaryOp<"shru", [Pure]> {
  let summary = "unsigned index shift right";
  let description = [{
    The `index.shru` operation shifts an index value to the right by a variable
    amount. The LHS operand is treated as unsigned. The high order bits are
    filled with zeroes. If the RHS operand is equal to or greater than the index
    bitwidth, the result is a poison value.

    Example:

    ```mlir
    // c = a >> b
    %c = index.shru %a, %b
    ```
  }];
}

//===----------------------------------------------------------------------===//
// AndOp
//===----------------------------------------------------------------------===//

def Index_AndOp : IndexBinaryOp<"and", [Commutative, Pure]> {
  let summary = "index bitwise and";
  let description = [{
    The `index.and` operation takes two index values and computes their bitwise
    and.

    Example:

    ```mlir
    // c = a & b
    %c = index.and %a, %b
    ```
  }];

  let hasCanonicalizeMethod = 1;
}

//===----------------------------------------------------------------------===//
// OrOp
//===----------------------------------------------------------------------===//

def Index_OrOp : IndexBinaryOp<"or", [Commutative, Pure]> {
  let summary = "index bitwise or";
  let description = [{
    The `index.or` operation takes two index values and computes their bitwise
    or.

    Example:

    ```mlir
    // c = a | b
    %c = index.or %a, %b
    ```
  }];

  let hasCanonicalizeMethod = 1;
}

//===----------------------------------------------------------------------===//
// XorOp
//===----------------------------------------------------------------------===//

def Index_XOrOp : IndexBinaryOp<"xor", [Commutative, Pure]> {
  let summary = "index bitwise xor";
  let description = [{
    The `index.xor` operation takes two index values and computes their bitwise
    xor.

    Example:

    ```mlir
    // c = a ^ b
    %c = index.xor %a, %b
    ```
  }];

  let hasCanonicalizeMethod = 1;
}

//===----------------------------------------------------------------------===//
// CastSOp
//===----------------------------------------------------------------------===//

def Index_CastSOp : IndexOp<"casts", [Pure,
    DeclareOpInterfaceMethods<CastOpInterface>]> {
  let summary = "index signed cast";
  let description = [{
    The `index.casts` operation enables conversions between values of index type
    and concrete fixed-width integer types. If casting to a wider integer, the
    value is sign-extended. If casting to a narrower integer, the value is
    truncated.

    Example:

    ```mlir
    // Cast to i32
    %0 = index.casts %a : index to i32

    // Cast from i64
    %1 = index.casts %b : i64 to index
    ```
  }];

  let arguments = (ins AnyTypeOf<[AnyInteger, Index]>:$input);
  let results = (outs AnyTypeOf<[AnyInteger, Index]>:$output);
  let assemblyFormat = "$input attr-dict `:` type($input) `to` type($output)";
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// CastUOp
//===----------------------------------------------------------------------===//

def Index_CastUOp : IndexOp<"castu", [Pure,
    DeclareOpInterfaceMethods<CastOpInterface>]> {
  let summary = "index unsigned cast";
  let description = [{
    The `index.castu` operation enables conversions between values of index type
    and concrete fixed-width integer types. If casting to a wider integer, the
    value is zero-extended. If casting to a narrower integer, the value is
    truncated.

    Example:

    ```mlir
    // Cast to i32
    %0 = index.castu %a : index to i32

    // Cast from i64
    %1 = index.castu %b : i64 to index
    ```
  }];

  let arguments = (ins AnyTypeOf<[AnyInteger, Index]>:$input);
  let results = (outs AnyTypeOf<[AnyInteger, Index]>:$output);
  let assemblyFormat = "$input attr-dict `:` type($input) `to` type($output)";
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// CmpOp
//===----------------------------------------------------------------------===//

def Index_CmpOp : IndexOp<"cmp", [Pure]> {
  let summary = "index compare";
  let description = [{
    The `index.cmp` operation takes two index values and compares them according
    to the comparison predicate and returns an `i1`. The following comparisons
    are supported:

    -   `eq`:  equal
    -   `ne`:  not equal
    -   `slt`: signed less than
    -   `sle`: signed less than or equal
    -   `sgt`: signed greater than
    -   `sge`: signed greater than or equal
    -   `ult`: unsigned less than
    -   `ule`: unsigned less than or equal
    -   `ugt`: unsigned greater than
    -   `uge`: unsigned greater than or equal

    The result is `1` if the comparison is true and `0` otherwise.

    Example:

    ```mlir
    // Signed less than comparison.
    %0 = index.cmp slt(%a, %b)

    // Unsigned greater than or equal comparison.
    %1 = index.cmp uge(%a, %b)

    // Not equal comparison.
    %2 = index.cmp ne(%a, %b)
    ```
  }];

  let arguments = (ins IndexCmpPredicateAttr:$pred, Index:$lhs, Index:$rhs);
  let results = (outs I1:$result);
  let assemblyFormat = "`` $pred `(` $lhs `,` $rhs `)` attr-dict";
  let hasFolder = 1;
  let hasCanonicalizeMethod = 1;
}

//===----------------------------------------------------------------------===//
// SizeOfOp
//===----------------------------------------------------------------------===//

def Index_SizeOfOp : IndexOp<"sizeof", [Pure]> {
  let summary = "size in bits of the index type";
  let description = [{
    The `index.sizeof` operation produces an index-typed SSA value equal to the
    size in bits of the `index` type. For example, on 32-bit systems, the result
    is `32 : index`, and on 64-bit systems, the result is `64 : index`.

    Example:

    ```mlir
    %0 = index.sizeof
    ```
  }];

  let results = (outs Index:$result);
  let assemblyFormat = "attr-dict";
}

//===----------------------------------------------------------------------===//
// ConstantOp
//===----------------------------------------------------------------------===//

def Index_ConstantOp : IndexOp<"constant", [
    ConstantLike, Pure,
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>
  ]> {
  let summary = "index constant";
  let description = [{
    The `index.constant` operation produces an index-typed SSA value equal to
    some index-typed integer constant.

    Example:

    ```mlir
    %0 = index.constant 42
    ```
  }];

  let arguments = (ins IndexAttr:$value);
  let results = (outs Index:$result);
  let assemblyFormat = "attr-dict $value";
  let hasFolder = 1;

  let builders = [OpBuilder<(ins "int64_t":$value)>];
}

//===----------------------------------------------------------------------===//
// BoolConstantOp
//===----------------------------------------------------------------------===//

def Index_BoolConstantOp : IndexOp<"bool.constant", [
    ConstantLike, Pure,
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>
  ]> {
  let summary = "boolean constant";
  let description = [{
    The `index.bool.constant` operation produces an bool-typed SSA value equal
    to either `true` or `false`.

    This operation is used to materialize bool constants that arise when folding
    `index.cmp`.

    Example:

    ```mlir
    %0 = index.bool.constant true
    ```
  }];

  let arguments = (ins BoolAttr:$value);
  let results = (outs I1:$result);
  let assemblyFormat = "attr-dict $value";
  let hasFolder = 1;
}

#endif // INDEX_OPS


//===- SCFOps.td - Structured Control Flow operations ------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// Defines MLIR structured control flow operations.
//
//===----------------------------------------------------------------------===//

#ifndef MLIR_DIALECT_SCF_SCFOPS
#define MLIR_DIALECT_SCF_SCFOPS

include "mlir/Interfaces/ControlFlowInterfaces.td"
include "mlir/Interfaces/LoopLikeInterface.td"
include "mlir/IR/RegionKindInterface.td"
include "mlir/Dialect/SCF/IR/DeviceMappingInterface.td"
include "mlir/Interfaces/DestinationStyleOpInterface.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/ParallelCombiningOpInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/Interfaces/ViewLikeInterface.td"

def SCF_Dialect : Dialect {
  let name = "scf";
  let cppNamespace = "::mlir::scf";

  let description = [{
    The `scf` (structured control flow) dialect contains operations that
    represent control flow constructs such as `if` and `for`. Being
    _structured_ means that the control flow has a structure unlike, for
    example, `goto`s or `assert`s. Unstructured control flow operations are
    located in the `cf` (control flow) dialect.

    Originally, this dialect was developed as a common lowering stage for the
    `affine` and `linalg` dialects. Both convert to SCF loops instead of
    targeting branch-based CFGs directly. Typically, `scf` is lowered to `cf`
    and then lowered to some final target like LLVM or SPIR-V.
  }];

  let dependentDialects = ["arith::ArithDialect"];
}

// Base class for SCF dialect ops.
class SCF_Op<string mnemonic, list<Trait> traits = []> :
    Op<SCF_Dialect, mnemonic, traits>;

//===----------------------------------------------------------------------===//
// ConditionOp
//===----------------------------------------------------------------------===//

def ConditionOp : SCF_Op<"condition", [
  HasParent<"WhileOp">,
  DeclareOpInterfaceMethods<RegionBranchTerminatorOpInterface,
    ["getSuccessorRegions"]>,
  Pure,
  Terminator
]> {
  let summary = "loop continuation condition";
  let description = [{
    This operation accepts the continuation (i.e., inverse of exit) condition
    of the `scf.while` construct. If its first argument is true, the "after"
    region of `scf.while` is executed, with the remaining arguments forwarded
    to the entry block of the region. Otherwise, the loop terminates.
  }];

  let arguments = (ins I1:$condition, Variadic<AnyType>:$args);

  let assemblyFormat =
      [{ `(` $condition `)` attr-dict ($args^ `:` type($args))? }];
}

//===----------------------------------------------------------------------===//
// ExecuteRegionOp
//===----------------------------------------------------------------------===//

def ExecuteRegionOp : SCF_Op<"execute_region", [
    DeclareOpInterfaceMethods<RegionBranchOpInterface>]> {
  let summary = "operation that executes its region exactly once";
  let description = [{
    The `scf.execute_region` operation is used to allow multiple blocks within SCF
    and other operations which can hold only one block.  The `scf.execute_region`
    operation executes the region held exactly once and cannot have any operands.
    As such, its region has no arguments. All SSA values that dominate the op can
    be accessed inside the op. The op's region can have multiple blocks and the
    blocks can have multiple distinct terminators. Values returned from this op's
    region define the op's results.

    Example:

    ```mlir
    scf.for %i = 0 to 128 step %c1 {
      %y = scf.execute_region -> i32 {
        %x = load %A[%i] : memref<128xi32>
        scf.yield %x : i32
      }
    }

    affine.for %i = 0 to 100 {
      "foo"() : () -> ()
      %v = scf.execute_region -> i64 {
        cf.cond_br %cond, ^bb1, ^bb2

      ^bb1:
        %c1 = arith.constant 1 : i64
        cf.br ^bb3(%c1 : i64)

      ^bb2:
        %c2 = arith.constant 2 : i64
        cf.br ^bb3(%c2 : i64)

      ^bb3(%x : i64):
        scf.yield %x : i64
      }
      "bar"(%v) : (i64) -> ()
    }
    ```
  }];

  let results = (outs Variadic<AnyType>);

  let regions = (region AnyRegion:$region);

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;

  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// ForOp
//===----------------------------------------------------------------------===//

def ForOp : SCF_Op<"for",
      [AutomaticAllocationScope, DeclareOpInterfaceMethods<LoopLikeOpInterface,
       ["getInitsMutable", "getLoopResults", "getRegionIterArgs",
        "getLoopInductionVars", "getLoopLowerBounds", "getLoopSteps",
        "getLoopUpperBounds", "getYieldedValuesMutable",
        "promoteIfSingleIteration", "replaceWithAdditionalYields",
        "yieldTiledValuesAndReplace"]>,
       AllTypesMatch<["lowerBound", "upperBound", "step"]>,
       ConditionallySpeculatable,
       DeclareOpInterfaceMethods<RegionBranchOpInterface,
        ["getEntrySuccessorOperands"]>,
       SingleBlockImplicitTerminator<"scf::YieldOp">,
       RecursiveMemoryEffects]> {
  let summary = "for operation";
  let description = [{
    The `scf.for` operation represents a loop taking 3 SSA value as operands
    that represent the lower bound, upper bound and step respectively. The
    operation defines an SSA value for its induction variable. It has one
    region capturing the loop body. The induction variable is represented as an
    argument of this region. This SSA value is a signless integer or index.
    The step is a value of same type but required to be positive. The lower and
    upper bounds specify a half-open range: the range includes the lower bound
    but does not include the upper bound.

    The body region must contain exactly one block that terminates with
    `scf.yield`. Calling ForOp::build will create such a region and insert
    the terminator implicitly if none is defined, so will the parsing even in
    cases when it is absent from the custom format. For example:

    ```mlir
    // Index case.
    scf.for %iv = %lb to %ub step %step {
      ... // body
    }
    ...
    // Integer case.
    scf.for %iv_32 = %lb_32 to %ub_32 step %step_32 : i32 {
      ... // body
    }
    ```

    `scf.for` can also operate on loop-carried variables and returns the final
    values after loop termination. The initial values of the variables are
    passed as additional SSA operands to the `scf.for` following the 3 loop
    control SSA values mentioned above (lower bound, upper bound and step). The
    operation region has an argument for the induction variable, followed by
    one argument for each loop-carried variable, representing the value of the
    variable at the current iteration.

    The region must terminate with a `scf.yield` that passes the current
    values of all loop-carried variables to the next iteration, or to the
    `scf.for` result, if at the last iteration. The static type of a
    loop-carried variable may not change with iterations; its runtime type is
    allowed to change. Note, that when the loop-carried variables are present,
    calling ForOp::build will not insert the terminator implicitly. The caller
    must insert `scf.yield` in that case.

    `scf.for` results hold the final values after the last iteration.
    For example, to sum-reduce a memref:

    ```mlir
    func.func @reduce(%buffer: memref<1024xf32>, %lb: index,
                      %ub: index, %step: index) -> (f32) {
      // Initial sum set to 0.
      %sum_0 = arith.constant 0.0 : f32
      // iter_args binds initial values to the loop's region arguments.
      %sum = scf.for %iv = %lb to %ub step %step
          iter_args(%sum_iter = %sum_0) -> (f32) {
        %t = load %buffer[%iv] : memref<1024xf32>
        %sum_next = arith.addf %sum_iter, %t : f32
        // Yield current iteration sum to next iteration %sum_iter or to %sum
        // if final iteration.
        scf.yield %sum_next : f32
      }
      return %sum : f32
    }
    ```

    If the `scf.for` defines any values, a yield must be explicitly present.
    The number and types of the `scf.for` results must match the initial
    values in the `iter_args` binding and the yield operands.

    Another example with a nested `scf.if` (see `scf.if` for details) to
    perform conditional reduction:

    ```mlir
    func.func @conditional_reduce(%buffer: memref<1024xf32>, %lb: index,
                                  %ub: index, %step: index) -> (f32) {
      %sum_0 = arith.constant 0.0 : f32
      %c0 = arith.constant 0.0 : f32
      %sum = scf.for %iv = %lb to %ub step %step
          iter_args(%sum_iter = %sum_0) -> (f32) {
        %t = load %buffer[%iv] : memref<1024xf32>
        %cond = arith.cmpf "ugt", %t, %c0 : f32
        %sum_next = scf.if %cond -> (f32) {
          %new_sum = arith.addf %sum_iter, %t : f32
          scf.yield %new_sum : f32
        } else {
          scf.yield %sum_iter : f32
        }
        scf.yield %sum_next : f32
      }
      return %sum : f32
    }
    ```
  }];
  let arguments = (ins AnySignlessIntegerOrIndex:$lowerBound,
                       AnySignlessIntegerOrIndex:$upperBound,
                       AnySignlessIntegerOrIndex:$step,
                       Variadic<AnyType>:$initArgs);
  let results = (outs Variadic<AnyType>:$results);
  let regions = (region SizedRegion<1>:$region);

  let skipDefaultBuilders = 1;
  let builders = [
    OpBuilder<(ins "Value":$lowerBound, "Value":$upperBound, "Value":$step,
      CArg<"ValueRange", "std::nullopt">:$initArgs,
      CArg<"function_ref<void(OpBuilder &, Location, Value, ValueRange)>",
           "nullptr">)>
  ];

  let extraClassDeclaration = [{
    using BodyBuilderFn =
        function_ref<void(OpBuilder &, Location, Value, ValueRange)>;

    Value getInductionVar() { return getBody()->getArgument(0); }

    /// Return the `index`-th region iteration argument.
    BlockArgument getRegionIterArg(unsigned index) {
      assert(index < getNumRegionIterArgs() &&
        "expected an index less than the number of region iter args");
      return getBody()->getArguments().drop_front(getNumInductionVars())[index];
    }

    void setLowerBound(Value bound) { getOperation()->setOperand(0, bound); }
    void setUpperBound(Value bound) { getOperation()->setOperand(1, bound); }
    void setStep(Value step) { getOperation()->setOperand(2, step); }

    /// Number of induction variables, always 1 for scf::ForOp.
    unsigned getNumInductionVars() { return 1; }
    /// Number of region arguments for loop-carried values
    unsigned getNumRegionIterArgs() {
      return getBody()->getNumArguments() - getNumInductionVars();
    }
    /// Number of operands controlling the loop: lb, ub, step
    unsigned getNumControlOperands() { return 3; }

    /// Returns the step as an `APInt` if it is constant.
    std::optional<APInt> getConstantStep();

    /// Interface method for ConditionallySpeculatable.
    Speculation::Speculatability getSpeculatability();
  }];

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
  let hasRegionVerifier = 1;
}

//===----------------------------------------------------------------------===//
// ForallOp
//===----------------------------------------------------------------------===//

def ForallOp : SCF_Op<"forall", [
       AttrSizedOperandSegments,
       AutomaticAllocationScope,
       DeclareOpInterfaceMethods<LoopLikeOpInterface,
          ["getInitsMutable", "getRegionIterArgs", "getLoopInductionVars", 
           "getLoopLowerBounds", "getLoopUpperBounds", "getLoopSteps",
           "promoteIfSingleIteration", "yieldTiledValuesAndReplace"]>,
       RecursiveMemoryEffects,
       SingleBlockImplicitTerminator<"scf::InParallelOp">,
       DeclareOpInterfaceMethods<RegionBranchOpInterface>,
       DestinationStyleOpInterface,
       HasParallelRegion
     ]> {
  let summary = "evaluate a block multiple times in parallel";
  let description = [{
    `scf.forall` is a target-independent multi-dimensional parallel
    region application operation. It has exactly one block that represents the
    parallel body and it takes index operands that specify lower bounds, upper
    bounds and steps.

    The op also takes a variadic number of tensor operands (`shared_outs`).
    The future buffers corresponding to these tensors are shared among all
    threads. Shared tensors should be accessed via their corresponding block
    arguments. If multiple threads write to a shared buffer in a racy
    fashion, these writes will execute in some unspecified order. Tensors that
    are not shared can be used inside the body (i.e., the op is not isolated
    from above); however, if a use of such a tensor bufferizes to a memory
    write, the tensor is privatized, i.e., a thread-local copy of the tensor is
    used. This ensures that memory side effects of a thread are not visible to
    other threads (or in the parent body), apart from explicitly shared tensors.

    The name "thread" conveys the fact that the parallel execution is mapped
    (i.e. distributed) to a set of virtual threads of execution, one function
    application per thread. Further lowerings are responsible for specifying
    how this is materialized on concrete hardware resources.

    An optional `mapping` is an attribute array that specifies processing units
    with their dimension, how it remaps 1-1 to a set of concrete processing
    element resources (e.g. a CUDA grid dimension or a level of concrete nested
    async parallelism). It is expressed via any attribute that implements the
    device mapping interface. It is the reponsibility of the lowering mechanism
    to interpret the `mapping` attributes in the context of the concrete target
    the op is lowered to, or to ignore it when the specification is ill-formed
    or unsupported for a particular target.

    The only allowed terminator is `scf.forall.in_parallel`.
    `scf.forall` returns one value per `shared_out` operand. The
    actions of the `scf.forall.in_parallel` terminators specify how to combine the
    partial results of all parallel invocations into a full value, in some
    unspecified order. The "destination" of each such op must be a `shared_out`
    block argument of the `scf.forall` op.

    The actions involved in constructing the return values are further described
    by `tensor.parallel_insert_slice`.

    `scf.forall` acts as an implicit synchronization point.

    When the parallel function body has side effects, their order is unspecified
    across threads.

    `scf.forall` can be printed in two different ways depending on
    whether the loop is normalized or not. The loop is 'normalized' when all
    lower bounds are equal to zero and steps are equal to one. In that case,
    `lowerBound` and `step` operands will be omitted during printing.

    Normalized loop example:

    ```mlir
    //
    // Sequential context.
    //
    %matmul_and_pointwise:2 = scf.forall (%thread_id_1, %thread_id_2) in
        (%num_threads_1, %numthread_id_2) shared_outs(%o1 = %C, %o2 = %pointwise)
      -> (tensor<?x?xT>, tensor<?xT>) {
      //
      // Parallel context, each thread with id = (%thread_id_1, %thread_id_2)
      // runs its version of the code.
      //
      %sA = tensor.extract_slice %A[f((%thread_id_1, %thread_id_2))]:
        tensor<?x?xT> to tensor<?x?xT>
      %sB = tensor.extract_slice %B[g((%thread_id_1, %thread_id_2))]:
        tensor<?x?xT> to tensor<?x?xT>
      %sC = tensor.extract_slice %o1[h((%thread_id_1, %thread_id_2))]:
        tensor<?x?xT> to tensor<?x?xT>
      %sD = linalg.matmul
        ins(%sA, %sB : tensor<?x?xT>, tensor<?x?xT>)
        outs(%sC : tensor<?x?xT>)

      %spointwise = subtensor %o2[i((%thread_id_1, %thread_id_2))]:
        tensor<?xT> to tensor<?xT>
      %sE = linalg.add ins(%spointwise : tensor<?xT>) outs(%sD : tensor<?xT>)

      scf.forall.in_parallel {
        tensor.parallel_insert_slice %sD into %o1[h((%thread_id_1, %thread_id_2))]:
          tensor<?x?xT> into tensor<?x?xT>

        tensor.parallel_insert_slice %spointwise into %o2[i((%thread_id_1, %thread_id_2))]:
          tensor<?xT> into tensor<?xT>
      }
    }
    // Implicit synchronization point.
    // Sequential context.
    //
    ```

    Loop with loop bounds example:

    ```mlir
    //
    // Sequential context.
    //
    %pointwise = scf.forall (%i, %j) = (0, 0) to (%dim1, %dim2)
      step (%tileSize1, %tileSize2) shared_outs(%o1 = %out)
      -> (tensor<?x?xT>, tensor<?xT>) {
      //
      // Parallel context.
      //
      %sA = tensor.extract_slice %A[%i, %j][%tileSize1, %tileSize2][1, 1]
        : tensor<?x?xT> to tensor<?x?xT>
      %sB = tensor.extract_slice %B[%i, %j][%tileSize1, %tileSize2][1, 1]
        : tensor<?x?xT> to tensor<?x?xT>
      %sC = tensor.extract_slice %o[%i, %j][%tileSize1, %tileSize2][1, 1]
        : tensor<?x?xT> to tensor<?x?xT>

      %add = linalg.map {"arith.addf"}
        ins(%sA, %sB : tensor<?x?xT>, tensor<?x?xT>)
        outs(%sC : tensor<?x?xT>)

      scf.forall.in_parallel {
        tensor.parallel_insert_slice %add into
          %o[%i, %j][%tileSize1, %tileSize2][1, 1]
          : tensor<?x?xT> into tensor<?x?xT>
      }
    }
    // Implicit synchronization point.
    // Sequential context.
    //
    ```

    Example with mapping attribute:

    ```mlir
    //
    // Sequential context. Here `mapping` is expressed as GPU thread mapping
    // attributes
    //
    %matmul_and_pointwise:2 = scf.forall (%thread_id_1, %thread_id_2) in
        (%num_threads_1, %numthread_id_2) shared_outs(...)
      -> (tensor<?x?xT>, tensor<?xT>) {
      //
      // Parallel context, each thread with id = **(%thread_id_2, %thread_id_1)**
      // runs its version of the code.
      //
       scf.forall.in_parallel {
         ...
      }
    } { mapping = [#gpu.thread<y>, #gpu.thread<x>] }
    // Implicit synchronization point.
    // Sequential context.
    //
    ```

    Example with privatized tensors:

    ```mlir
    %t0 = ...
    %t1 = ...
    %r = scf.forall ... shared_outs(%o = t0) -> tensor<?xf32> {
      // %t0 and %t1 are privatized. %t0 is definitely copied for each thread
      // because the scf.forall op's %t0 use bufferizes to a memory
      // write. In the absence of other conflicts, %t1 is copied only if there
      // are uses of %t1 in the body that bufferize to a memory read and to a
      // memory write.
      "some_use"(%t0)
      "some_use"(%t1)
    }
    ```
  }];
  let arguments = (ins
    Variadic<Index>:$dynamicLowerBound,
    Variadic<Index>:$dynamicUpperBound,
    Variadic<Index>:$dynamicStep,
    DenseI64ArrayAttr:$staticLowerBound,
    DenseI64ArrayAttr:$staticUpperBound,
    DenseI64ArrayAttr:$staticStep,
    Variadic<AnyRankedTensor>:$outputs,
    OptionalAttr<DeviceMappingArrayAttr>:$mapping);

  let results = (outs Variadic<AnyType>:$results);
  let regions = (region SizedRegion<1>:$region);

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;

  // The default builder does not add the proper body BBargs, roll our own.
  let skipDefaultBuilders = 1;
  let builders = [
    // Builder that takes loop bounds.
    OpBuilder<(ins "ArrayRef<OpFoldResult>":$lbs,
       "ArrayRef<OpFoldResult>":$ubs, "ArrayRef<OpFoldResult>":$steps,
       "ValueRange":$outputs, "std::optional<ArrayAttr>":$mapping,
       CArg<"function_ref<void(OpBuilder &, Location, ValueRange)>",
            "nullptr"> :$bodyBuilderFn)>,

    // Builder for normalized loop that takes only upper bounds.
    OpBuilder<(ins "ArrayRef<OpFoldResult>":$ubs,
       "ValueRange":$outputs, "std::optional<ArrayAttr>":$mapping,
       CArg<"function_ref<void(OpBuilder &, Location, ValueRange)>",
            "nullptr"> :$bodyBuilderFn)>,
  ];

  let extraClassDeclaration = [{
    /// Get induction variables.
    SmallVector<Value> getInductionVars() {
      std::optional<SmallVector<Value>> maybeInductionVars = getLoopInductionVars();
      assert(maybeInductionVars.has_value() && "expected values");
      return *maybeInductionVars;
    }
    /// Get lower bounds as OpFoldResult.
    SmallVector<OpFoldResult> getMixedLowerBound() {
      std::optional<SmallVector<OpFoldResult>> maybeLowerBounds = getLoopLowerBounds();
      assert(maybeLowerBounds.has_value() && "expected values");
      return *maybeLowerBounds;
    }

    /// Get upper bounds as OpFoldResult.
    SmallVector<OpFoldResult> getMixedUpperBound() {
      std::optional<SmallVector<OpFoldResult>> maybeUpperBounds = getLoopUpperBounds();
      assert(maybeUpperBounds.has_value() && "expected values");
      return *maybeUpperBounds;
    }

    /// Get steps as OpFoldResult.
    SmallVector<OpFoldResult> getMixedStep() {
      std::optional<SmallVector<OpFoldResult>> maybeSteps = getLoopSteps();
      assert(maybeSteps.has_value() && "expected values");
      return *maybeSteps;
    }

    /// Get lower bounds as values.
    SmallVector<Value> getLowerBound(OpBuilder &b) {
      return getValueOrCreateConstantIndexOp(b, getLoc(), getMixedLowerBound());
    }

    /// Get upper bounds as values.
    SmallVector<Value> getUpperBound(OpBuilder &b) {
      return getValueOrCreateConstantIndexOp(b, getLoc(), getMixedUpperBound());
    }

    /// Get steps as values.
    SmallVector<Value> getStep(OpBuilder &b) {
      return getValueOrCreateConstantIndexOp(b, getLoc(), getMixedStep());
    }

    int64_t getRank() { return getStaticLowerBound().size(); }

    /// Number of operands controlling the loop: lbs, ubs, steps
    unsigned getNumControlOperands() { return 3 * getRank(); }

    /// Number of dynamic operands controlling the loop: lbs, ubs, steps
    unsigned getNumDynamicControlOperands() {
      return getODSOperandIndexAndLength(3).first;
    }

    OpResult getTiedOpResult(OpOperand *opOperand) {
      assert(opOperand->getOperandNumber() >= getNumDynamicControlOperands() &&
             "invalid operand");
      return getOperation()->getOpResult(
          opOperand->getOperandNumber() - getNumDynamicControlOperands());
    }

    /// Return the num_threads operand that is tied to the given thread id
    /// block argument.
    OpOperand *getTiedOpOperand(BlockArgument bbArg) {
      assert(bbArg.getArgNumber() >= getRank() && "invalid bbArg");

      return &getOperation()->getOpOperand(getNumDynamicControlOperands() +
                                           bbArg.getArgNumber() - getRank());
    }

    /// Return the shared_outs operand that is tied to the given OpResult.
    OpOperand *getTiedOpOperand(OpResult opResult) {
      assert(opResult.getDefiningOp() == getOperation() && "invalid OpResult");
      return &getOperation()->getOpOperand(getNumDynamicControlOperands() +
                                           opResult.getResultNumber());
    }

    BlockArgument getTiedBlockArgument(OpOperand *opOperand) {
      assert(opOperand->getOperandNumber() >= getNumDynamicControlOperands() &&
             "invalid operand");

      return getBody()->getArgument(opOperand->getOperandNumber() -
                                    getNumDynamicControlOperands() + getRank());
    }

    ::mlir::Value getInductionVar(int64_t idx) {
      return getInductionVars()[idx];
    }

    ::mlir::Block::BlockArgListType getRegionOutArgs() {
      return getBody()->getArguments().drop_front(getRank());
    }

    /// Checks if the lbs are zeros and steps are ones.
    bool isNormalized();

    // The ensureTerminator method generated by SingleBlockImplicitTerminator is
    // unaware of the fact that our terminator also needs a region to be
    // well-formed. We override it here to ensure that we do the right thing.
    static void ensureTerminator(Region & region, OpBuilder & builder,
                                 Location loc);

    InParallelOp getTerminator();

    // Declare the shared_outs as inits/outs to DestinationStyleOpInterface.
    MutableOperandRange getDpsInitsMutable() { return getOutputsMutable(); }

    /// Returns operations within scf.forall.in_parallel whose destination
    /// operand is the block argument `bbArg`.
    SmallVector<Operation*> getCombiningOps(BlockArgument bbArg);
  }];
}

//===----------------------------------------------------------------------===//
// InParallelOp
//===----------------------------------------------------------------------===//

def InParallelOp : SCF_Op<"forall.in_parallel", [
       Pure,
       Terminator,
       DeclareOpInterfaceMethods<ParallelCombiningOpInterface>,
       HasParent<"ForallOp">,
      ] # GraphRegionNoTerminator.traits> {
  let summary = "terminates a `forall` block";
  let description = [{
    The `scf.forall.in_parallel` is a designated terminator for
    the `scf.forall` operation.

    It has a single region with a single block that contains a flat list of ops.
    Each such op participates in the aggregate formation of a single result of
    the enclosing `scf.forall`.
    The result number corresponds to the position of the op in the terminator.
  }];

  let regions = (region SizedRegion<1>:$region);

  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;

  // The default builder does not add a region with an empty body, add our own.
  let skipDefaultBuilders = 1;
  let builders = [
    OpBuilder<(ins)>,
  ];

  // TODO: Add a `InParallelOpInterface` interface for ops that can
  // appear inside in_parallel.
  let extraClassDeclaration = [{
    ::llvm::SmallVector<::mlir::BlockArgument> getDests();
    ::llvm::iterator_range<::mlir::Block::iterator> getYieldingOps();
    ::mlir::OpResult getParentResult(int64_t idx);
  }];
}

//===----------------------------------------------------------------------===//
// IfOp
//===----------------------------------------------------------------------===//

def IfOp : SCF_Op<"if", [DeclareOpInterfaceMethods<RegionBranchOpInterface, [
    "getNumRegionInvocations", "getRegionInvocationBounds",
    "getEntrySuccessorRegions"]>,
    InferTypeOpAdaptor, SingleBlockImplicitTerminator<"scf::YieldOp">,
    RecursiveMemoryEffects, NoRegionArguments]> {
  let summary = "if-then-else operation";
  let description = [{
    The `scf.if` operation represents an if-then-else construct for
    conditionally executing two regions of code. The operand to an if operation
    is a boolean value. For example:

    ```mlir
    scf.if %b  {
      ...
    } else {
      ...
    }
    ```

    `scf.if` may also produce results. Which values are returned depends on
    which execution path is taken.

    Example:

    ```mlir
    %x, %y = scf.if %b -> (f32, f32) {
      %x_true = ...
      %y_true = ...
      scf.yield %x_true, %y_true : f32, f32
    } else {
      %x_false = ...
      %y_false = ...
      scf.yield %x_false, %y_false : f32, f32
    }
    ```

    The "then" region has exactly 1 block. The "else" region may have 0 or 1
    block. In case the `scf.if` produces results, the "else" region must also
    have exactly 1 block.

    The blocks are always terminated with `scf.yield`. If `scf.if` defines no
    values, the `scf.yield` can be left out, and will be inserted implicitly.
    Otherwise, it must be explicit.

    Example:

    ```mlir
    scf.if %b  {
      ...
    }
    ```

    The types of the yielded values must match the result types of the
    `scf.if`.
  }];
  let arguments = (ins I1:$condition);
  let results = (outs Variadic<AnyType>:$results);
  let regions = (region SizedRegion<1>:$thenRegion,
                        MaxSizedRegion<1>:$elseRegion);

  let skipDefaultBuilders = 1;
  let builders = [
    OpBuilder<(ins "TypeRange":$resultTypes, "Value":$cond)>,
    OpBuilder<(ins "TypeRange":$resultTypes, "Value":$cond,
      "bool":$addThenBlock, "bool":$addElseBlock)>,
    OpBuilder<(ins "Value":$cond, "bool":$withElseRegion)>,
    OpBuilder<(ins "TypeRange":$resultTypes, "Value":$cond,
      "bool":$withElseRegion)>,
    OpBuilder<(ins "Value":$cond,
      CArg<"function_ref<void(OpBuilder &, Location)>",
           "buildTerminatedBody">:$thenBuilder,
      CArg<"function_ref<void(OpBuilder &, Location)>",
           "nullptr">:$elseBuilder)>,
  ];

  let extraClassDeclaration = [{
    OpBuilder getThenBodyBuilder(OpBuilder::Listener *listener = nullptr) {
      Block* body = getBody(0);
      return getResults().empty() ? OpBuilder::atBlockTerminator(body, listener)
                                  : OpBuilder::atBlockEnd(body, listener);
    }
    OpBuilder getElseBodyBuilder(OpBuilder::Listener *listener = nullptr) {
      Block* body = getBody(1);
      return getResults().empty() ? OpBuilder::atBlockTerminator(body, listener)
                                  : OpBuilder::atBlockEnd(body, listener);
    }
    Block* thenBlock();
    YieldOp thenYield();
    Block* elseBlock();
    YieldOp elseYield();
  }];
  let hasFolder = 1;
  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// ParallelOp
//===----------------------------------------------------------------------===//

def ParallelOp : SCF_Op<"parallel",
    [AutomaticAllocationScope,
     AttrSizedOperandSegments,
     DeclareOpInterfaceMethods<LoopLikeOpInterface, ["getLoopInductionVars",
          "getLoopLowerBounds", "getLoopUpperBounds", "getLoopSteps"]>,
     RecursiveMemoryEffects,
     DeclareOpInterfaceMethods<RegionBranchOpInterface>,
     SingleBlockImplicitTerminator<"scf::ReduceOp">,
     HasParallelRegion]> {
  let summary = "parallel for operation";
  let description = [{
    The `scf.parallel` operation represents a loop nest taking 4 groups of SSA
    values as operands that represent the lower bounds, upper bounds, steps and
    initial values, respectively. The operation defines a variadic number of
    SSA values for its induction variables. It has one region capturing the
    loop body. The induction variables are represented as an argument of this
    region. These SSA values always have type index, which is the size of the
    machine word. The steps are values of type index, required to be positive.
    The lower and upper bounds specify a half-open range: the range includes
    the lower bound but does not include the upper bound. The initial values
    have the same types as results of `scf.parallel`. If there are no results,
    the keyword `init` can be omitted.

    Semantically we require that the iteration space can be iterated in any
    order, and the loop body can be executed in parallel. If there are data
    races, the behavior is undefined.

    The parallel loop operation supports reduction of values produced by
    individual iterations into a single result. This is modeled using the
    `scf.reduce` terminator operation (see `scf.reduce` for details). The i-th
    result of an `scf.parallel` operation is associated with the i-th initial
    value operand, the i-th operand of the `scf.reduce` operation (the value to
    be reduced) and the i-th region of the `scf.reduce` operation (the reduction
    function). Consequently, we require that the number of results of an
    `scf.parallel` op matches the number of initial values and the the number of
    reductions in the `scf.reduce` terminator.

    The body region must contain exactly one block that terminates with a
    `scf.reduce` operation. If an `scf.parallel` op has no reductions, the
    terminator has no operands and no regions. The `scf.parallel` parser will
    automatically insert the terminator for ops that have no reductions if it is
    absent.

    Example:

    ```mlir
    %init = arith.constant 0.0 : f32
    %r:2 = scf.parallel (%iv) = (%lb) to (%ub) step (%step) init (%init, %init)
        -> f32, f32 {
      %elem_to_reduce1 = load %buffer1[%iv] : memref<100xf32>
      %elem_to_reduce2 = load %buffer2[%iv] : memref<100xf32>
      scf.reduce(%elem_to_reduce1, %elem_to_reduce2 : f32, f32) {
        ^bb0(%lhs : f32, %rhs: f32):
          %res = arith.addf %lhs, %rhs : f32
          scf.reduce.return %res : f32
      }, {
        ^bb0(%lhs : f32, %rhs: f32):
          %res = arith.mulf %lhs, %rhs : f32
          scf.reduce.return %res : f32
      }
    }
    ```
  }];

  let arguments = (ins Variadic<Index>:$lowerBound,
                       Variadic<Index>:$upperBound,
                       Variadic<Index>:$step,
                       Variadic<AnyType>:$initVals);
  let results = (outs Variadic<AnyType>:$results);
  let regions = (region SizedRegion<1>:$region);

  let skipDefaultBuilders = 1;
  let builders = [
    OpBuilder<(ins "ValueRange":$lowerBounds, "ValueRange":$upperBounds,
      "ValueRange":$steps, "ValueRange":$initVals,
      CArg<"function_ref<void (OpBuilder &, Location, ValueRange, ValueRange)>",
           "nullptr">:$bodyBuilderFn)>,
    OpBuilder<(ins "ValueRange":$lowerBounds, "ValueRange":$upperBounds,
      "ValueRange":$steps,
      CArg<"function_ref<void (OpBuilder &, Location, ValueRange)>",
           "nullptr">:$bodyBuilderFn)>,
  ];

  let extraClassDeclaration = [{
    /// Get induction variables.
    SmallVector<Value> getInductionVars() {
      std::optional<SmallVector<Value>> maybeInductionVars = getLoopInductionVars();;
      assert(maybeInductionVars.has_value() && "expected values");
      return *maybeInductionVars;
    }
    unsigned getNumLoops() { return getStep().size(); }
    unsigned getNumReductions() { return getInitVals().size(); }
  }];

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// ReduceOp
//===----------------------------------------------------------------------===//

def ReduceOp : SCF_Op<"reduce", [
    Terminator, HasParent<"ParallelOp">, RecursiveMemoryEffects,
    DeclareOpInterfaceMethods<RegionBranchTerminatorOpInterface>]> {
  let summary = "reduce operation for scf.parallel";
  let description = [{
    The `scf.reduce` operation is the terminator for `scf.parallel` operations. It can model
    an arbitrary number of reductions. It has one region per reduction. Each
    region has one block with two arguments which have the same type as the
    corresponding operand of `scf.reduce`. The operands of the op are the values
    that should be reduce; one value per reduction.

    The i-th reduction (i.e., the i-th region and the i-th operand) corresponds
    the i-th initial value and the i-th result of the enclosing `scf.parallel`
    op.

    The `scf.reduce` operation contains regions whose entry blocks expect two
    arguments of the same type as the corresponding operand. As the iteration
    order of the enclosing parallel loop and hence reduction order is
    unspecified, the results of the reductions may be non-deterministic unless
    the reductions are associative and commutative.

    The result of a reduction region (`scf.reduce.return` operand) must have the
    same type as the corresponding `scf.reduce` operand and the corresponding
    `scf.parallel` initial value.

    Example:

    ```mlir
    %operand = arith.constant 1.0 : f32
    scf.reduce(%operand : f32) {
      ^bb0(%lhs : f32, %rhs: f32):
        %res = arith.addf %lhs, %rhs : f32
        scf.reduce.return %res : f32
    }
    ```
  }];

  let skipDefaultBuilders = 1;
  let builders = [
    OpBuilder<(ins "ValueRange":$operands)>,
    OpBuilder<(ins)>
  ];

  let arguments = (ins Variadic<AnyType>:$operands);
  let assemblyFormat = [{
    (`(` $operands^ `:` type($operands) `)`)? $reductions attr-dict
  }];
  let regions = (region VariadicRegion<SizedRegion<1>>:$reductions);
  let hasRegionVerifier = 1;
}

//===----------------------------------------------------------------------===//
// ReduceReturnOp
//===----------------------------------------------------------------------===//

def ReduceReturnOp :
    SCF_Op<"reduce.return", [HasParent<"ReduceOp">, Pure, Terminator]> {
  let summary = "terminator for reduce operation";
  let description = [{
    The `scf.reduce.return` operation is a special terminator operation for the block inside
    `scf.reduce` regions. It terminates the region. It should have the same
    operand type as the corresponding operand of the enclosing `scf.reduce` op.

    Example:

    ```mlir
    scf.reduce.return %res : f32
    ```
  }];

  let arguments = (ins AnyType:$result);
  let assemblyFormat = "$result attr-dict `:` type($result)";
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// WhileOp
//===----------------------------------------------------------------------===//

def WhileOp : SCF_Op<"while",
    [DeclareOpInterfaceMethods<RegionBranchOpInterface,
        ["getEntrySuccessorOperands"]>,
     DeclareOpInterfaceMethods<LoopLikeOpInterface,
        ["getRegionIterArgs", "getYieldedValuesMutable"]>,
     RecursiveMemoryEffects, SingleBlock]> {
  let summary = "a generic 'while' loop";
  let description = [{
    This operation represents a generic "while"/"do-while" loop that keeps
    iterating as long as a condition is satisfied. There is no restriction on
    the complexity of the condition. It consists of two regions (with single
    block each): "before" region and "after" region. The names of regions
    indicates whether they execute before or after the condition check.
    Therefore, if the main loop payload is located in the "before" region, the
    operation is a "do-while" loop. Otherwise, it is a "while" loop.

    The "before" region terminates with a special operation, `scf.condition`,
    that accepts as its first operand an `i1` value indicating whether to
    proceed to the "after" region (value is `true`) or not. The two regions
    communicate by means of region arguments. Initially, the "before" region
    accepts as arguments the operands of the `scf.while` operation and uses them
    to evaluate the condition. It forwards the trailing, non-condition operands
    of the `scf.condition` terminator either to the "after" region if the
    control flow is transferred there or to results of the `scf.while` operation
    otherwise. The "after" region takes as arguments the values produced by the
    "before" region and uses `scf.yield` to supply new arguments for the
    "before" region, into which it transfers the control flow unconditionally.

    A simple "while" loop can be represented as follows.

    ```mlir
    %res = scf.while (%arg1 = %init1) : (f32) -> f32 {
      // "Before" region.
      // In a "while" loop, this region computes the condition.
      %condition = call @evaluate_condition(%arg1) : (f32) -> i1

      // Forward the argument (as result or "after" region argument).
      scf.condition(%condition) %arg1 : f32

    } do {
    ^bb0(%arg2: f32):
      // "After" region.
      // In a "while" loop, this region is the loop body.
      %next = call @payload(%arg2) : (f32) -> f32

      // Forward the new value to the "before" region.
      // The operand types must match the types of the `scf.while` operands.
      scf.yield %next : f32
    }
    ```

    A simple "do-while" loop can be represented by reducing the "after" block
    to a simple forwarder.

    ```mlir
    %res = scf.while (%arg1 = %init1) : (f32) -> f32 {
      // "Before" region.
      // In a "do-while" loop, this region contains the loop body.
      %next = call @payload(%arg1) : (f32) -> f32

      // And also evaluates the condition.
      %condition = call @evaluate_condition(%arg1) : (f32) -> i1

      // Loop through the "after" region.
      scf.condition(%condition) %next : f32

    } do {
    ^bb0(%arg2: f32):
      // "After" region.
      // Forwards the values back to "before" region unmodified.
      scf.yield %arg2 : f32
    }
    ```

    Note that the types of region arguments need not to match with each other.
    The op expects the operand types to match with argument types of the
    "before" region; the result types to match with the trailing operand types
    of the terminator of the "before" region, and with the argument types of the
    "after" region. The following scheme can be used to share the results of
    some operations executed in the "before" region with the "after" region,
    avoiding the need to recompute them.

    ```mlir
    %res = scf.while (%arg1 = %init1) : (f32) -> i64 {
      // One can perform some computations, e.g., necessary to evaluate the
      // condition, in the "before" region and forward their results to the
      // "after" region.
      %shared = call @shared_compute(%arg1) : (f32) -> i64

      // Evaluate the condition.
      %condition = call @evaluate_condition(%arg1, %shared) : (f32, i64) -> i1

      // Forward the result of the shared computation to the "after" region.
      // The types must match the arguments of the "after" region as well as
      // those of the `scf.while` results.
      scf.condition(%condition) %shared : i64

    } do {
    ^bb0(%arg2: i64) {
      // Use the partial result to compute the rest of the payload in the
      // "after" region.
      %res = call @payload(%arg2) : (i64) -> f32

      // Forward the new value to the "before" region.
      // The operand types must match the types of the `scf.while` operands.
      scf.yield %res : f32
    }
    ```

    The custom syntax for this operation is as follows.

    ```
    op ::= `scf.while` assignments `:` function-type region `do` region
           `attributes` attribute-dict
    initializer ::= /* empty */ | `(` assignment-list `)`
    assignment-list ::= assignment | assignment `,` assignment-list
    assignment ::= ssa-value `=` ssa-value
    ```
  }];

  let arguments = (ins Variadic<AnyType>:$inits);
  let results = (outs Variadic<AnyType>:$results);
  let regions = (region SizedRegion<1>:$before, SizedRegion<1>:$after);

  let builders = [
    OpBuilder<(ins "TypeRange":$resultTypes, "ValueRange":$inits,
      "function_ref<void(OpBuilder &, Location, ValueRange)>":$beforeBuilder,
      "function_ref<void(OpBuilder &, Location, ValueRange)>":$afterBuilder)>
  ];

  let extraClassDeclaration = [{
    using BodyBuilderFn =
        function_ref<void(OpBuilder &, Location, ValueRange)>;

    ConditionOp getConditionOp();
    YieldOp getYieldOp();

    Block::BlockArgListType getBeforeArguments();
    Block::BlockArgListType getAfterArguments();
    Block *getBeforeBody() { return &getBefore().front(); }
    Block *getAfterBody() { return &getAfter().front(); }
  }];

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// IndexSwitchOp
//===----------------------------------------------------------------------===//

def IndexSwitchOp : SCF_Op<"index_switch", [RecursiveMemoryEffects,
    SingleBlockImplicitTerminator<"scf::YieldOp">,
    DeclareOpInterfaceMethods<RegionBranchOpInterface,
                              ["getRegionInvocationBounds",
                               "getEntrySuccessorRegions"]>]> {
  let summary = "switch-case operation on an index argument";
  let description = [{
    The `scf.index_switch` is a control-flow operation that branches to one of
    the given regions based on the values of the argument and the cases. The
    argument is always of type `index`.

    The operation always has a "default" region and any number of case regions
    denoted by integer constants. Control-flow transfers to the case region
    whose constant value equals the value of the argument. If the argument does
    not equal any of the case values, control-flow transfer to the "default"
    region.

    Example:

    ```mlir
    %0 = scf.index_switch %arg0 : index -> i32
    case 2 {
      %1 = arith.constant 10 : i32
      scf.yield %1 : i32
    }
    case 5 {
      %2 = arith.constant 20 : i32
      scf.yield %2 : i32
    }
    default {
      %3 = arith.constant 30 : i32
      scf.yield %3 : i32
    }
    ```
  }];

  let arguments = (ins Index:$arg, DenseI64ArrayAttr:$cases);
  let results = (outs Variadic<AnyType>:$results);
  let regions = (region SizedRegion<1>:$defaultRegion,
                        VariadicRegion<SizedRegion<1>>:$caseRegions);

  let assemblyFormat = [{
    $arg attr-dict (`->` type($results)^)?
    custom<SwitchCases>($cases, $caseRegions) `\n`
    `` `default` $defaultRegion
  }];

  let extraClassDeclaration = [{
    /// Get the number of cases.
    unsigned getNumCases();

    /// Get the default region body.
    Block &getDefaultBlock();

    /// Get the body of a case region.
    Block &getCaseBlock(unsigned idx);
  }];

  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// YieldOp
//===----------------------------------------------------------------------===//

def YieldOp : SCF_Op<"yield", [Pure, ReturnLike, Terminator,
    ParentOneOf<["ExecuteRegionOp", "ForOp", "IfOp", "IndexSwitchOp",
                 "WhileOp"]>]> {
  let summary = "loop yield and termination operation";
  let description = [{
    The `scf.yield` operation yields an SSA value from the SCF dialect op region and
    terminates the regions. The semantics of how the values are yielded is
    defined by the parent operation.
    If `scf.yield` has any operands, the operands must match the parent
    operation's results.
    If the parent operation defines no values, then the `scf.yield` may be
    left out in the custom syntax and the builders will insert one implicitly.
    Otherwise, it has to be present in the syntax to indicate which values are
    yielded.
  }];

  let arguments = (ins Variadic<AnyType>:$results);
  let builders = [OpBuilder<(ins), [{ /* nothing to do */ }]>];

  let assemblyFormat =
      [{  attr-dict ($results^ `:` type($results))? }];
}

#endif // MLIR_DIALECT_SCF_SCFOPS


//===-- ROCDLOps.td - ROCDL IR dialect op definition file --*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This is the ROCDL IR operation definition file.
//
//===----------------------------------------------------------------------===//

#ifndef ROCDLIR_OPS
#define ROCDLIR_OPS

include "mlir/Dialect/GPU/IR/CompilationAttrInterfaces.td"
include "mlir/Dialect/LLVMIR/LLVMOpBase.td"
include "mlir/Interfaces/SideEffectInterfaces.td"

//===----------------------------------------------------------------------===//
// ROCDL dialect definitions
//===----------------------------------------------------------------------===//

def ROCDL_Dialect : Dialect {
  let name = "rocdl";
  let cppNamespace = "::mlir::ROCDL";
  let dependentDialects = ["LLVM::LLVMDialect"];
  let hasOperationAttrVerify = 1;

  let extraClassDeclaration = [{
    /// Get the name of the attribute used to annotate external kernel
    /// functions.
    static StringRef getKernelFuncAttrName() { return "rocdl.kernel"; }
    static constexpr ::llvm::StringLiteral getFlatWorkGroupSizeAttrName() {
      return ::llvm::StringLiteral("rocdl.flat_work_group_size");
    }
    static constexpr ::llvm::StringLiteral getReqdWorkGroupSizeAttrName() {
      return ::llvm::StringLiteral("rocdl.reqd_work_group_size");
    }
    /// MLIR's gpu-related infrastructure effectively assume uniform workgroup
    /// sizes, so this attribute defaults to "true" on `rocdl.kernel` functions.
    /// It is provided here to allow overriding this assumption.
    static constexpr ::llvm::StringLiteral getUniformWorkGroupSizeAttrName() {
      return ::llvm::StringLiteral("rocdl.uniform_work_group_size");
    }

    /// The address space value that represents global memory.
    static constexpr unsigned kGlobalMemoryAddressSpace = 1;
    /// The address space value that represents shared memory.
    static constexpr unsigned kSharedMemoryAddressSpace = 3;
    /// The address space value that represents private memory.
    static constexpr unsigned kPrivateMemoryAddressSpace = 5;
  }];

  let discardableAttrs = (ins
     "::mlir::UnitAttr":$kernel,
     "::mlir::DenseI32ArrayAttr":$reqd_work_group_size,
     "::mlir::StringAttr":$flat_work_group_size,
     "::mlir::IntegerAttr":$max_flat_work_group_size,
     "::mlir::IntegerAttr":$waves_per_eu,
     "::mlir::BoolAttr":$unsafe_fp_atomics,
     // Correspond to LLVM metadata of the same name
     "::mlir::UnitAttr":$last_use,
     "::mlir::UnitAttr":$no_remote_memory,
     "::mlir::UnitAttr":$no_fine_grained_memory,
     "::mlir::UnitAttr":$ignore_denormal_mode
  );

  let useDefaultAttributePrinterParser = 1;
}

//===----------------------------------------------------------------------===//
// ROCDL attribute definitions
//===----------------------------------------------------------------------===//

class ROCDL_Attr<string attrName, string attrMnemonic, list<Trait> traits = []>
    : AttrDef<ROCDL_Dialect, attrName, traits> {
  let mnemonic = attrMnemonic;
}


//===----------------------------------------------------------------------===//
// ROCDL op definitions
//===----------------------------------------------------------------------===//

class ROCDL_Op<string mnemonic, list<Trait> traits = []> :
  LLVM_OpBase<ROCDL_Dialect, mnemonic, traits> {
}

class ROCDL_IntrPure1Op<string mnemonic> :
  LLVM_IntrOpBase<ROCDL_Dialect, mnemonic,
  "amdgcn_" # !subst(".", "_", mnemonic), [], [], [Pure], 1>;

class ROCDL_IntrOp<string mnemonic, list<int> overloadedResults,
  list<int> overloadedOperands, list<Trait> traits, int numResults,
  int requiresAccessGroup = 0, int requiresAliasAnalysis = 0, list<int> immArgPositions = [],
  list<string> immArgAttrNames = []> :
  LLVM_IntrOpBase<ROCDL_Dialect,  mnemonic,
    "amdgcn_" # !subst(".", "_", mnemonic), overloadedResults,
    overloadedOperands, traits, numResults, requiresAccessGroup,
    requiresAliasAnalysis, 0, 0, immArgPositions, immArgAttrNames>;

//===----------------------------------------------------------------------===//
// ROCDL special register op definitions
//===----------------------------------------------------------------------===//

class ROCDL_SpecialIdRegisterOp<string mnemonic> :
    ROCDL_IntrPure1Op<mnemonic>,
    Arguments<(ins OptionalAttr<LLVM_ConstantRangeAttr>:$range)> {
  string llvmBuilder = baseLlvmBuilder # setRangeRetAttrCode # baseLlvmBuilderCoda;
  string mlirBuilder = baseMlirBuilder # importRangeRetAttrCode # baseMlirBuilderCoda;

  let assemblyFormat = "(`range` $range^)? attr-dict `:` type($res)";

    // Temporaly builder until Nvidia ops also support range attributes.
  let builders = [
    OpBuilder<(ins "Type":$resultType), [{
      build($_builder, $_state, resultType, ::mlir::LLVM::ConstantRangeAttr{});
    }]>
  ];
}

class ROCDL_DimGetterFunctionOp<string mnemonic, string device_function,
                             int parameter, list<Trait> traits = []> :
  ROCDL_Op<mnemonic, !listconcat(traits, [Pure])>,
  Results<(outs LLVM_Type:$res)>, Arguments<(ins OptionalAttr<LLVM_ConstantRangeAttr>:$range)> {
  string llvmBuilder = "$res = createDimGetterFunctionCall(builder, op, \""
  # device_function # "\", " # parameter # ");";
  let assemblyFormat = "(`range` $range^)? attr-dict `:` type($res)";

  // Temporaly builder until Nvidia ops also support range attributes.
  let builders = [
    OpBuilder<(ins "Type":$resultType), [{
      build($_builder, $_state, resultType, ::mlir::LLVM::ConstantRangeAttr{});
    }]>
  ];
}

//===----------------------------------------------------------------------===//
// Wave-level primitives

class ROCDL_MbcntOp<string mnemonic> :
    ROCDL_IntrPure1Op<"mbcnt." # mnemonic>,
  Arguments<(ins I32:$in0, I32:$in1)> {
  let assemblyFormat = [{
    $in0 `,` $in1  attr-dict `:` `(` type($in0) `,` type($in1) `)` `->` type($res)
   }];
}

def ROCDL_MbcntLoOp : ROCDL_MbcntOp<"lo">;
def ROCDL_MbcntHiOp : ROCDL_MbcntOp<"hi">;

def ROCDL_DsSwizzleOp :
ROCDL_Op<"ds_swizzle">,
Results<(outs I32:$res)>,
Arguments<(ins I32:$src,
               I32:$offset)>
{
  string llvmBuilder = [{
    $res = createIntrinsicCall(builder, llvm::Intrinsic::amdgcn_ds_swizzle, {$src, $offset});
  }];
  let assemblyFormat = [{
    $src `,` $offset  attr-dict `:` `(` type($src) `,` type($offset) `)` `->` type($res)
   }];
}

def ROCDL_DsBpermuteOp :
ROCDL_Op<"ds_bpermute">,
Results<(outs I32:$res)>,
Arguments<(ins I32:$index,
               I32:$src)>
{
  string llvmBuilder = [{
    $res = createIntrinsicCall(builder, llvm::Intrinsic::amdgcn_ds_bpermute, {$index, $src});
  }];
  let assemblyFormat = [{
    $index `,` $src  attr-dict `:` `(` type($index) `,` type($src) `)` `->` type($res)
   }];
}

def ROCDL_BallotOp :
  ROCDL_Op<"ballot">,
  Results<(outs LLVM_Type:$res)>,
  Arguments<(ins I1:$pred)> {
  let summary = "Vote across thread group";

  let description = [{
      Ballot provides a bit mask containing the 1-bit predicate value from each lane.
      The nth bit of the result contains the 1 bit contributed by the nth warp lane.
  }];

  string llvmBuilder = [{
      $res = createIntrinsicCall(builder,
            llvm::Intrinsic::amdgcn_ballot, {$pred}, {$_resultType});
  }];

  let assemblyFormat = "$pred attr-dict `:` type($res)";
}

def ROCDL_ReadlaneOp : ROCDL_IntrOp<"readlane", [], [0], [AllTypesMatch<["res", "src0"]>], 1>,
  Arguments<(ins LLVM_Type:$src0,
                 I32:$src1)> {
  let results = (outs LLVM_Type:$res);
  let summary = "Get the value in the specific lane.";

  let description = [{
    Get the value in lane `src1` from input `src0`.
  }];

  let assemblyFormat = [{
    $src0 `,` $src1  attr-dict `:` `(` type($src0) `,` type($src1) `)` `->` type($res)
   }];
}

//===----------------------------------------------------------------------===//
// Thread index and Block index

def ROCDL_ThreadIdXOp : ROCDL_SpecialIdRegisterOp<"workitem.id.x">;
def ROCDL_ThreadIdYOp : ROCDL_SpecialIdRegisterOp<"workitem.id.y">;
def ROCDL_ThreadIdZOp : ROCDL_SpecialIdRegisterOp<"workitem.id.z">;

def ROCDL_BlockIdXOp : ROCDL_SpecialIdRegisterOp<"workgroup.id.x">;
def ROCDL_BlockIdYOp : ROCDL_SpecialIdRegisterOp<"workgroup.id.y">;
def ROCDL_BlockIdZOp : ROCDL_SpecialIdRegisterOp<"workgroup.id.z">;

//===----------------------------------------------------------------------===//
// Thread range and Block range

def ROCDL_BlockDimXOp : ROCDL_DimGetterFunctionOp<"workgroup.dim.x",
                                               "__ockl_get_local_size", 0>;

def ROCDL_BlockDimYOp : ROCDL_DimGetterFunctionOp<"workgroup.dim.y",
                                               "__ockl_get_local_size", 1>;

def ROCDL_BlockDimZOp : ROCDL_DimGetterFunctionOp<"workgroup.dim.z",
                                               "__ockl_get_local_size", 2>;

def ROCDL_GridDimXOp : ROCDL_DimGetterFunctionOp<"grid.dim.x",
                                               "__ockl_get_num_groups", 0>;

def ROCDL_GridDimYOp : ROCDL_DimGetterFunctionOp<"grid.dim.y",
                                               "__ockl_get_num_groups", 1>;

def ROCDL_GridDimZOp : ROCDL_DimGetterFunctionOp<"grid.dim.z",
                                               "__ockl_get_num_groups", 2>;

//===----------------------------------------------------------------------===//
// Synchronization primitives

// Emits the waintcnt instruction. The bitfield's semantics depend
// on the target chipset
def ROCDL_WaitcntOp : ROCDL_Op<"waitcnt">, Arguments<(ins I32Attr:$bitfield)> {
  string llvmBuilder = [{
    createIntrinsicCall(builder, llvm::Intrinsic::amdgcn_s_waitcnt,
      {builder.getInt32($bitfield)});
  }];
  let assemblyFormat = "attr-dict $bitfield";
}

def ROCDL_SBarrierOp : ROCDL_Op<"s.barrier"> {
  string llvmBuilder = [{
    createIntrinsicCall(builder, llvm::Intrinsic::amdgcn_s_barrier);
  }];
  let assemblyFormat = "attr-dict";
}

def ROCDL_BarrierOp : ROCDL_Op<"barrier"> {
  string llvmBuilder = [{
    llvm::LLVMContext &llvmContext = builder.getContext();
    builder.CreateFence(llvm::AtomicOrdering::Release,
                        llvmContext.getOrInsertSyncScopeID("workgroup"));
    createIntrinsicCall(builder, llvm::Intrinsic::amdgcn_s_barrier);
    builder.CreateFence(llvm::AtomicOrdering::Acquire,
                        llvmContext.getOrInsertSyncScopeID("workgroup"));
  }];
  let assemblyFormat = "attr-dict";
}

def ROCDL_BarrierSignalOp : ROCDL_IntrOp<"s.barrier.signal", [], [], [], 0, 0, 0, [0], ["id"]>,
  Arguments<(ins I32Attr:$id)> {
  let results = (outs);
  let assemblyFormat = "$id attr-dict";
}

def ROCDL_BarrierWaitOp : ROCDL_IntrOp<"s.barrier.wait", [], [], [], 0, 0, 0, [0], ["id"]>,
  Arguments<(ins I16Attr:$id)> {
  let results = (outs);
  let assemblyFormat = "$id attr-dict";
  string llvmBuilder =
    "createIntrinsicCall(builder, llvm::Intrinsic::amdgcn_s_barrier_wait,builder.getInt16(op.getId()));";
}

def ROCDL_WaitDscntOp: ROCDL_IntrOp<"s.wait.dscnt", [], [], [], 0, 0, 0, [0], ["id"]>,
  Arguments<(ins I16Attr:$id)> {
  let results = (outs);
  let assemblyFormat = "$id attr-dict";
}

def ROCDL_SetPrioOp : ROCDL_IntrOp<"s.setprio", [], [], [], 0>,
  Arguments<(ins I16Attr:$priority)> {
  let results = (outs);
  let assemblyFormat = "$priority attr-dict";
  string llvmBuilder =
    "createIntrinsicCall(builder, llvm::Intrinsic::amdgcn_s_setprio,builder.getInt16(op.getPriority()));";
}

def ROCDL_SchedBarrier : ROCDL_IntrOp<"sched.barrier", [], [], [], 0>,
  Arguments<(ins I32Attr:$mask)> {
  let results = (outs);
  let assemblyFormat = "$mask attr-dict";
  string llvmBuilder =
    "createIntrinsicCall(builder, llvm::Intrinsic::amdgcn_sched_barrier,builder.getInt32(op.getMask()));";
}

def ROCDL_SchedGroupBarrier : ROCDL_IntrOp<"sched.group.barrier", [], [], [], 0>,
  Arguments<(ins I32Attr:$mask, I32Attr:$size, I32Attr:$groupId)> {
  let results = (outs);
  let assemblyFormat = "$mask `,` $size `,` $groupId attr-dict";
  string llvmBuilder = [{
    createIntrinsicCall(builder,
      llvm::Intrinsic::amdgcn_sched_group_barrier,
      {builder.getInt32(op.getMask()), builder.getInt32(op.getSize()), builder.getInt32(op.getGroupId())});
  }];
}

def ROCDL_IglpOpt : ROCDL_IntrOp<"iglp.opt", [], [], [], 0>,
  Arguments<(ins I32Attr:$variant)> {
  let results = (outs);
  let assemblyFormat = "$variant attr-dict";
  string llvmBuilder =
    "createIntrinsicCall(builder, llvm::Intrinsic::amdgcn_iglp_opt,builder.getInt32(op.getVariant()));";
}

//===---------------------------------------------------------------------===//
// Xdlops intrinsics

class ROCDL_Mfma_IntrOp<string mnemonic, list<Trait> traits = []> :
  LLVM_IntrOpBase<ROCDL_Dialect, mnemonic,
                  "amdgcn_" # !subst(".","_", mnemonic),
                  [], [], traits, 1>,
  Arguments<(ins Variadic<LLVM_Type>:$args)> {
  let assemblyFormat =
    "$args attr-dict `:` functional-type($args, $res)";
}

// Available on all CDNA.
def ROCDL_mfma_f32_32x32x1f32 : ROCDL_Mfma_IntrOp<"mfma.f32.32x32x1f32">;
def ROCDL_mfma_f32_16x16x1f32 : ROCDL_Mfma_IntrOp<"mfma.f32.16x16x1f32">;
def ROCDL_mfma_f32_4x4x1f32 : ROCDL_Mfma_IntrOp<"mfma.f32.4x4x1f32">;
def ROCDL_mfma_f32_32x32x2f32 : ROCDL_Mfma_IntrOp<"mfma.f32.32x32x2f32">;
def ROCDL_mfma_f32_16x16x4f32 : ROCDL_Mfma_IntrOp<"mfma.f32.16x16x4f32">;
def ROCDL_mfma_f32_32x32x4f16 : ROCDL_Mfma_IntrOp<"mfma.f32.32x32x4f16">;
def ROCDL_mfma_f32_16x16x4f16 : ROCDL_Mfma_IntrOp<"mfma.f32.16x16x4f16">;
def ROCDL_mfma_f32_4x4x4f16 : ROCDL_Mfma_IntrOp<"mfma.f32.4x4x4f16">;
def ROCDL_mfma_f32_32x32x8f16 : ROCDL_Mfma_IntrOp<"mfma.f32.32x32x8f16">;
def ROCDL_mfma_f32_16x16x16f16 : ROCDL_Mfma_IntrOp<"mfma.f32.16x16x16f16">;
def ROCDL_mfma_i32_32x32x4i8 : ROCDL_Mfma_IntrOp<"mfma.i32.32x32x4i8">;
def ROCDL_mfma_i32_16x16x4i8 : ROCDL_Mfma_IntrOp<"mfma.i32.16x16x4i8">;
def ROCDL_mfma_i32_4x4x4i8 : ROCDL_Mfma_IntrOp<"mfma.i32.4x4x4i8">;
def ROCDL_mfma_i32_32x32x8i8 : ROCDL_Mfma_IntrOp<"mfma.i32.32x32x8i8">;
def ROCDL_mfma_i32_16x16x16i8 : ROCDL_Mfma_IntrOp<"mfma.i32.16x16x16i8">;
def ROCDL_mfma_f32_32x32x2bf16 : ROCDL_Mfma_IntrOp<"mfma.f32.32x32x2bf16">;
def ROCDL_mfma_f32_16x16x2bf16 : ROCDL_Mfma_IntrOp<"mfma.f32.16x16x2bf16">;
def ROCDL_mfma_f32_4x4x2bf16 : ROCDL_Mfma_IntrOp<"mfma.f32.4x4x2bf16">;
def ROCDL_mfma_f32_32x32x4bf16 : ROCDL_Mfma_IntrOp<"mfma.f32.32x32x4bf16">;
def ROCDL_mfma_f32_16x16x8bf16 : ROCDL_Mfma_IntrOp<"mfma.f32.16x16x8bf16">;
// New in gfx90a.
def ROCDL_mfma_f32_32x32x4bf16_1k : ROCDL_Mfma_IntrOp<"mfma.f32.32x32x4bf16.1k">;
def ROCDL_mfma_f32_16x16x4bf16_1k : ROCDL_Mfma_IntrOp<"mfma.f32.16x16x4bf16.1k">;
def ROCDL_mfma_f32_4x4x4bf16_1k : ROCDL_Mfma_IntrOp<"mfma.f32.4x4x4bf16.1k">;
def ROCDL_mfma_f32_32x32x8bf16_1k : ROCDL_Mfma_IntrOp<"mfma.f32.32x32x8bf16.1k">;
def ROCDL_mfma_f32_16x16x16bf16_1k : ROCDL_Mfma_IntrOp<"mfma.f32.16x16x16bf16.1k">;
// Note: in gfx940, unlike in gfx90a, the f64 xdlops use the "blgp" argument as a
// NEG bitfield. See IntrinsicsAMDGPU.td for more info.
def ROCDL_mfma_f64_16x16x4f64 : ROCDL_Mfma_IntrOp<"mfma.f64.16x16x4f64">;
def ROCDL_mfma_f64_4x4x4f64 : ROCDL_Mfma_IntrOp<"mfma.f64.4x4x4f64">;
// New in gfx940.
def ROCDL_mfma_i32_16x16x32_i8 : ROCDL_Mfma_IntrOp<"mfma.i32.16x16x32.i8">;
def ROCDL_mfma_i32_32x32x16_i8 : ROCDL_Mfma_IntrOp<"mfma.i32.32x32x16.i8">;
def ROCDL_mfma_f32_16x16x8_xf32 : ROCDL_Mfma_IntrOp<"mfma.f32.16x16x8.xf32">;
def ROCDL_mfma_f32_32x32x4_xf32 : ROCDL_Mfma_IntrOp<"mfma.f32.32x32x4.xf32">;
// fp8, only on gfx940
def ROCDL_mfma_f32_16x16x32_bf8_bf8 : ROCDL_Mfma_IntrOp<"mfma.f32.16x16x32.bf8.bf8">;
def ROCDL_mfma_f32_16x16x32_bf8_fp8 : ROCDL_Mfma_IntrOp<"mfma.f32.16x16x32.bf8.fp8">;
def ROCDL_mfma_f32_16x16x32_fp8_bf8 : ROCDL_Mfma_IntrOp<"mfma.f32.16x16x32.fp8.bf8">;
def ROCDL_mfma_f32_16x16x32_fp8_fp8 : ROCDL_Mfma_IntrOp<"mfma.f32.16x16x32.fp8.fp8">;
def ROCDL_mfma_f32_32x32x16_bf8_bf8 : ROCDL_Mfma_IntrOp<"mfma.f32.32x32x16.bf8.bf8">;
def ROCDL_mfma_f32_32x32x16_bf8_fp8 : ROCDL_Mfma_IntrOp<"mfma.f32.32x32x16.bf8.fp8">;
def ROCDL_mfma_f32_32x32x16_fp8_bf8 : ROCDL_Mfma_IntrOp<"mfma.f32.32x32x16.fp8.bf8">;
def ROCDL_mfma_f32_32x32x16_fp8_fp8 : ROCDL_Mfma_IntrOp<"mfma.f32.32x32x16.fp8.fp8">;

//===---------------------------------------------------------------------===//
// WMMA intrinsics
class ROCDL_Wmma_IntrOp<string mnemonic, list<int> overloadedOperands,
                        list<Trait> traits = []> :
  LLVM_IntrOpBase<ROCDL_Dialect, mnemonic,
                  "amdgcn_" # !subst(".","_", mnemonic),
                  [0], overloadedOperands, traits, 1>,
  Arguments<(ins Variadic<LLVM_Type>:$args)> {
  let assemblyFormat =
    "$args attr-dict `:` functional-type($args, $res)";
}

// Available from gfx11
def ROCDL_wmma_f32_16x16x16_f16 : ROCDL_Wmma_IntrOp<"wmma.f32.16x16x16.f16", [0]>;
def ROCDL_wmma_f32_16x16x16_bf16 : ROCDL_Wmma_IntrOp<"wmma.f32.16x16x16.bf16", [0]>;
def ROCDL_wmma_f16_16x16x16_f16 : ROCDL_Wmma_IntrOp<"wmma.f16.16x16x16.f16", [0]>;
def ROCDL_wmma_bf16_16x16x16_bf16 : ROCDL_Wmma_IntrOp<"wmma.bf16.16x16x16.bf16", [0]>;
def ROCDL_wmma_i32_16x16x16_iu8 : ROCDL_Wmma_IntrOp<"wmma.i32.16x16x16.iu8", [1]>;
def ROCDL_wmma_i32_16x16x16_iu4 : ROCDL_Wmma_IntrOp<"wmma.i32.16x16x16.iu4", [1]>;
// Available from gfx12
def ROCDL_wmma_f32_16x16x16_fp8 : ROCDL_Wmma_IntrOp<"wmma.f32.16x16x16.fp8_fp8", [1]>;
def ROCDL_wmma_f32_16x16x16_bf8 : ROCDL_Wmma_IntrOp<"wmma.f32.16x16x16.bf8_bf8", [1]>;

//===---------------------------------------------------------------------===//
// Operations on raw buffer resources (stride of 0, bounds checks either off or in
// raw buffer mode).
//===---------------------------------------------------------------------===//

def ROCDLBufferRsrc : LLVM_PointerInAddressSpace<8>;

def ROCDL_MakeBufferRsrcOp :
  ROCDL_IntrOp<"make.buffer.rsrc", [], [0], [Pure], 1>,
  Arguments<(ins LLVM_AnyPointer:$base,
                 I16:$stride,
                 I32:$numRecords,
                 I32:$flags)> {
  let results = (outs ROCDLBufferRsrc:$res);
  let assemblyFormat = "operands attr-dict `:` type($base) `to` type($res)";
}

def ROCDL_RawPtrBufferLoadOp :
  ROCDL_IntrOp<"raw.ptr.buffer.load", [0], [], [], 1, 0, 1> {
  dag args = (ins Arg<ROCDLBufferRsrc, "", [MemRead]>:$rsrc,
                  I32:$offset,
                  I32:$soffset,
                  I32:$aux);
  let arguments = !con(args, aliasAttrs);
  let assemblyFormat = "operands attr-dict `:` type($res)";
  let extraClassDefinition = [{
    ::llvm::SmallVector<::mlir::Value> $cppClass::getAccessedOperands() {
      return {getRes()};
    }
  }];
}

def ROCDL_RawPtrBufferStoreOp :
  ROCDL_IntrOp<"raw.ptr.buffer.store", [], [0], [], 0, 0, 1> {
  dag args = (ins LLVM_Type:$vdata,
                  Arg<ROCDLBufferRsrc, "", [MemWrite]>:$rsrc,
                  I32:$offset,
                  I32:$soffset,
                  I32:$aux);
  let arguments = !con(args, aliasAttrs);
  let assemblyFormat = "operands attr-dict `:` type($vdata)";
  let extraClassDefinition = [{
    ::llvm::SmallVector<::mlir::Value> $cppClass::getAccessedOperands() {
      return {getRsrc()};
    }
  }];

}

def ROCDL_RawPtrBufferAtomicCmpSwap :
  ROCDL_IntrOp<"raw.ptr.buffer.atomic.cmpswap",
    [0], [], [AllTypesMatch<["res", "src", "cmp"]>], 1, 0, 1> {
  dag args = (ins LLVM_Type:$src,
                  LLVM_Type:$cmp,
                  Arg<ROCDLBufferRsrc, "", [MemRead, MemWrite]>:$rsrc,
                  I32:$offset,
                  I32:$soffset,
                  I32:$aux);
  let arguments = !con(args, aliasAttrs);
  let assemblyFormat = "operands attr-dict `:` type($res)";
  let extraClassDefinition = [{
    ::llvm::SmallVector<::mlir::Value> $cppClass::getAccessedOperands() {
      return {getRsrc()};
    }
  }];
}

class ROCDL_RawPtrBufferAtomicNoRet<string op> :
  ROCDL_IntrOp<"raw.ptr.buffer.atomic." # op, [], [0], [], 0, 0, 1> {
  dag args = (ins LLVM_Type:$vdata,
                  Arg<ROCDLBufferRsrc, "", [MemRead, MemWrite]>:$rsrc,
                  I32:$offset,
                  I32:$soffset,
                  I32:$aux);
  let arguments = !con(args, aliasAttrs);
  let assemblyFormat = "operands attr-dict `:` type($vdata)";
  let extraClassDefinition = [{
    ::llvm::SmallVector<::mlir::Value> $cppClass::getAccessedOperands() {
      return {getRsrc()};
    }
  }];
}

def ROCDL_RawPtrBufferAtomicFmaxOp : ROCDL_RawPtrBufferAtomicNoRet<"fmax">;
def ROCDL_RawPtrBufferAtomicSmaxOp : ROCDL_RawPtrBufferAtomicNoRet<"smax">;
def ROCDL_RawPtrBufferAtomicUminOp : ROCDL_RawPtrBufferAtomicNoRet<"umin">;
// Note: not supported on all architectures
def ROCDL_RawPtrBufferAtomicFaddOp : ROCDL_RawPtrBufferAtomicNoRet<"fadd">;

//===---------------------------------------------------------------------===//
// Raw buffer load/store intrinsics

def ROCDL_RawBufferLoadOp :
  ROCDL_Op<"raw.buffer.load">,
  Results<(outs LLVM_Type:$res)>,
  Arguments<(ins LLVM_Type:$rsrc,
                 LLVM_Type:$offset,
                 LLVM_Type:$soffset,
                 LLVM_Type:$aux)> {
  string llvmBuilder = [{
      $res = createIntrinsicCall(builder,
          llvm::Intrinsic::amdgcn_raw_buffer_load, {$rsrc, $offset,
          $soffset, $aux}, {$_resultType});
  }];
  let hasCustomAssemblyFormat = 1;
}

def ROCDL_RawBufferStoreOp :
  ROCDL_Op<"raw.buffer.store">,
  Arguments<(ins LLVM_Type:$vdata,
                 LLVM_Type:$rsrc,
                 LLVM_Type:$offset,
                 LLVM_Type:$soffset,
                 LLVM_Type:$aux)>{
  string llvmBuilder = [{
    auto vdataType = moduleTranslation.convertType(op.getVdata().getType());
    createIntrinsicCall(builder,
          llvm::Intrinsic::amdgcn_raw_buffer_store, {$vdata, $rsrc,
          $offset, $soffset, $aux}, {vdataType});
  }];
  let hasCustomAssemblyFormat = 1;
}

def ROCDL_RawBufferAtomicCmpSwap :
  ROCDL_Op<"raw.buffer.atomic.cmpswap", [AllTypesMatch<["res", "src", "cmp"]>]>,
  Results<(outs LLVM_Type:$res)>,
  Arguments<(ins LLVM_Type:$src,
                 LLVM_Type:$cmp,
                 LLVM_Type:$rsrc,
                 I32:$offset,
                 I32:$soffset,
                 I32:$aux)>{
  string llvmBuilder = [{
      $res = createIntrinsicCall(builder,
          llvm::Intrinsic::amdgcn_raw_buffer_atomic_cmpswap, {$src, $cmp, $rsrc,
            $offset, $soffset, $aux}, {$_resultType});
  }];
  let assemblyFormat = [{
    attr-dict `(` operands `)` `:` type($res) `,` type($rsrc)
  }];
}

//===---------------------------------------------------------------------===//
// MI-100 and MI-200 buffer atomic floating point add intrinsic

def ROCDL_RawBufferAtomicFAddOp :
  ROCDL_Op<"raw.buffer.atomic.fadd">,
  Arguments<(ins LLVM_Type:$vdata,
                 LLVM_Type:$rsrc,
                 LLVM_Type:$offset,
                 LLVM_Type:$soffset,
                 LLVM_Type:$aux)>{
  string llvmBuilder = [{
      auto vdataType = moduleTranslation.convertType(op.getVdata().getType());
      createIntrinsicCall(builder,
          llvm::Intrinsic::amdgcn_raw_buffer_atomic_fadd, {$vdata, $rsrc,
            $offset, $soffset, $aux}, {vdataType});
  }];
  let hasCustomAssemblyFormat = 1;
}

//===---------------------------------------------------------------------===//
// Buffer atomic floating point max intrinsic. GFX9 does not support fp32.

def ROCDL_RawBufferAtomicFMaxOp :
  ROCDL_Op<"raw.buffer.atomic.fmax">,
  Arguments<(ins LLVM_Type:$vdata,
                 LLVM_Type:$rsrc,
                 LLVM_Type:$offset,
                 LLVM_Type:$soffset,
                 LLVM_Type:$aux)>{
  string llvmBuilder = [{
      auto vdataType = moduleTranslation.convertType(op.getVdata().getType());
      createIntrinsicCall(builder,
          llvm::Intrinsic::amdgcn_raw_buffer_atomic_fmax, {$vdata, $rsrc,
            $offset, $soffset, $aux}, {vdataType});
  }];
  let hasCustomAssemblyFormat = 1;
}

//===---------------------------------------------------------------------===//
// Buffer atomic signed integer max intrinsic.

def ROCDL_RawBufferAtomicSMaxOp :
  ROCDL_Op<"raw.buffer.atomic.smax">,
  Arguments<(ins LLVM_Type:$vdata,
                 LLVM_Type:$rsrc,
                 LLVM_Type:$offset,
                 LLVM_Type:$soffset,
                 LLVM_Type:$aux)>{
  string llvmBuilder = [{
      auto vdataType = moduleTranslation.convertType(op.getVdata().getType());
      createIntrinsicCall(builder,
          llvm::Intrinsic::amdgcn_raw_buffer_atomic_smax, {$vdata, $rsrc,
            $offset, $soffset, $aux}, {vdataType});
  }];
  let hasCustomAssemblyFormat = 1;
}

//===---------------------------------------------------------------------===//
// Buffer atomic unsigned integer min intrinsic.

def ROCDL_RawBufferAtomicUMinOp :
  ROCDL_Op<"raw.buffer.atomic.umin">,
  Arguments<(ins LLVM_Type:$vdata,
                 LLVM_Type:$rsrc,
                 LLVM_Type:$offset,
                 LLVM_Type:$soffset,
                 LLVM_Type:$aux)>{
  string llvmBuilder = [{
      auto vdataType = moduleTranslation.convertType(op.getVdata().getType());
      createIntrinsicCall(builder,
          llvm::Intrinsic::amdgcn_raw_buffer_atomic_umin, {$vdata, $rsrc,
            $offset, $soffset, $aux}, {vdataType});
  }];
  let hasCustomAssemblyFormat = 1;
}

// DPP Update intrinsic
def ROCDL_DPPUpdateOp : ROCDL_IntrOp<"update.dpp", [], [0],
    [AllTypesMatch<["res", "src", "old"]>], 1>,
  Arguments<(ins LLVM_Type:$old, LLVM_Type:$src, I32Attr:$dppCtrl, I32Attr:$rowMask,
      I32Attr:$bankMask, I1Attr:$boundCtrl)> {
  let results = (outs LLVM_Type:$res);
  let assemblyFormat = [{
    attr-dict $old `,` $src `with` $dppCtrl `,` $rowMask `,` $bankMask `,` $boundCtrl `:` type($src)
  }];
  string llvmBuilder = [{
      auto vdataType = moduleTranslation.convertType(op.getSrc().getType());
      llvm::Value *args[] = {
        moduleTranslation.lookupValue(op.getOld()),
        moduleTranslation.lookupValue(op.getSrc()),
          builder.getInt32(op.getDppCtrl()),
          builder.getInt32(op.getRowMask()),
          builder.getInt32(op.getBankMask()),
          builder.getInt1(op.getBoundCtrl())
      };
      $res = createIntrinsicCall(builder,
        llvm::Intrinsic::amdgcn_update_dpp, args, {vdataType});
  }];
}

//===---------------------------------------------------------------------===//
// 16-bit float intrinsics
//===---------------------------------------------------------------------===//
def ROCDL_CvtPkRtz:
    ROCDL_IntrOp<"cvt.pkrtz", [], [], [Pure], 1>,
    Arguments<(ins F32:$srcA, F32:$srcB)> {
  let summary = "Convert two f32 input into a vector<2xf16>";
  let description = [{
    Convert two f32 values into a packed vector<2xf16>.
  }];
  let assemblyFormat = [{
    attr-dict $srcA `,` $srcB `:` type($res)
  }];
}

//===---------------------------------------------------------------------===//
// 8-bit float intrinsics
//===---------------------------------------------------------------------===//
def ROCDL_CvtF32Bf8Op :
    ROCDL_IntrOp<"cvt.f32.bf8", [], [], [Pure], 1>,
    Arguments<(ins I32:$srcA, I32:$byteSel)> {
  let summary = "Convert bf8 to f32";
  let description = [{
    Convert 8-bit bf8 value from the `byteSel`th bit of `srcA` to fp32.
  }];
  let assemblyFormat = [{
    attr-dict $srcA `[` $byteSel `]` `:` type($res)
  }];
}

def ROCDL_CvtF32Fp8Op :
    ROCDL_IntrOp<"cvt.f32.fp8", [], [], [Pure], 1>,
    Arguments<(ins I32:$srcA, I32:$byteSel)> {
  let summary = "Convert fp8 to f32";
  let description = [{
    Convert 8-bit fp8 value from the `byteSel`th bit of `srcA` to fp32.
  }];
  let assemblyFormat = [{
    attr-dict $srcA `[` $byteSel `]` `:` type($res)
  }];
}

def ROCDL_CvtPkBf8F32Op :
    ROCDL_IntrOp<"cvt.pk.bf8.f32", [], [], [Pure], 1>,
    Arguments<(ins F32:$srcA, F32:$srcB, I32:$old, I1:$wordSel)> {
  let summary = "Convert two f32's to bf8";
  let description = [{
    Convert `srcA` and `srcB` to bf8 and store into the low/high word of
    `old`, preserving the other word.
  }];
  let assemblyFormat = [{
    attr-dict $srcA `,` $srcB `->` $old `[` $wordSel `]` `:` type($res)
  }];
}

def ROCDL_CvtPkFp8F32Op :
    ROCDL_IntrOp<"cvt.pk.fp8.f32", [], [], [Pure], 1>,
    Arguments<(ins F32:$srcA, F32:$srcB, I32:$old, I1:$wordSel)> {
  let summary = "Convert two f32's to fp8";
  let description = [{
    Convert `srcA` and `srcB` to fp8 and store into the low/high word of
    `old`, preserving the other word.
  }];
  let assemblyFormat = [{
    attr-dict $srcA `,` $srcB `->` $old `[` $wordSel `]` `:` type($res)
  }];
}

def ROCDL_CvtSrBf8F32Op :
    ROCDL_IntrOp<"cvt.sr.bf8.f32", [], [], [Pure], 1>,
    Arguments<(ins F32:$srcA, I32:$srcB, I32:$old, I32:$byteSel)> {
  let summary = "Convert f32 to bf8, stochiastic rounding";
  let description = [{
    Convert `srcA` to bf8, adding the rounding factor from `srcB`,
    and store into the `byteSel`th byte of `old`, preserving the others.
  }];
  let assemblyFormat = [{
    attr-dict $srcA `,` $srcB `->` $old `[` $byteSel `]` `:` type($res)
  }];
}

def ROCDL_CvtSrFp8F32Op :
    ROCDL_IntrOp<"cvt.sr.fp8.f32", [], [], [Pure], 1>,
    Arguments<(ins F32:$srcA, I32:$srcB, I32:$old, I32:$byteSel)> {
  let summary = "Convert f32 to fp8, stochiastic rounding";
  let description = [{
    Convert `srcA` to fp8, adding the rounding factor from `srcB`,
    and store into the `byteSel`th byte of `old`, preserving the others.
  }];
  let assemblyFormat = [{
    attr-dict $srcA `,` $srcB `->` $old `[` $byteSel `]` `:` type($res)
  }];
}

//===----------------------------------------------------------------------===//
// ROCDL target attribute.
//===----------------------------------------------------------------------===//

def ROCDL_TargetAttr :
    ROCDL_Attr<"ROCDLTarget", "target"> {
  let description = [{
    ROCDL target attribute for controlling compilation of AMDGPU targets. All
    parameters decay into default values if not present.

    Examples:

    1. Target with default values.
    ```
      gpu.module @mymodule [#rocdl.target] attributes {...} {
        ...
      }
    ```

    2. Target with `gfx90a` chip and fast math.
    ```
      gpu.module @mymodule [#rocdl.target<chip = "gfx90a", flags = {fast, no_wave64}>] {
        ...
      }
    ```
  }];
  let parameters = (ins
    DefaultValuedParameter<"int", "2", "Optimization level to apply.">:$O,
    StringRefParameter<"Target triple.", "\"amdgcn-amd-amdhsa\"">:$triple,
    StringRefParameter<"Target chip.", "\"gfx900\"">:$chip,
    StringRefParameter<"Target chip features.", "\"\"">:$features,
    // Also update the default builder below and rocdl-attach-target in
    // Dialect/GPU/Transforms/Passes.td .
    StringRefParameter<"ABI version.", "\"500\"">:$abi,
    OptionalParameter<"DictionaryAttr", "Target specific flags.">:$flags,
    OptionalParameter<"ArrayAttr", "Files to link to the LLVM module.">:$link
  );
  let assemblyFormat = [{
    (`<` struct($O, $triple, $chip, $features, $abi, $flags, $link)^ `>`)?
  }];
  let builders = [
    AttrBuilder<(ins CArg<"int", "2">:$optLevel,
                     CArg<"StringRef", "\"amdgcn-amd-amdhsa\"">:$triple,
                     CArg<"StringRef", "\"gfx900\"">:$chip,
                     CArg<"StringRef", "\"\"">:$features,
                     CArg<"StringRef", "\"500\"">:$abiVersion,
                     CArg<"DictionaryAttr", "nullptr">:$targetFlags,
                     CArg<"ArrayAttr", "nullptr">:$linkFiles), [{
      return Base::get($_ctxt, optLevel, triple, chip, features, abiVersion,
                       targetFlags, linkFiles);
    }]>
  ];
  let skipDefaultBuilders = 1;
  let genVerifyDecl = 1;
  let extraClassDeclaration = [{
    bool hasFlag(StringRef flag) const;
    bool hasWave64() const;
    bool hasFastMath() const;
    bool hasDaz() const;
    bool hasFiniteOnly() const;
    bool hasUnsafeMath() const;
    bool hasCorrectSqrt() const;
  }];
  let extraClassDefinition = [{
    bool $cppClass::hasFlag(StringRef flag) const {
      if (DictionaryAttr flags = getFlags())
        return flags.get(flag) != nullptr;
      return false;
    }
    bool $cppClass::hasWave64() const {
      return hasFlag("wave64") || !hasFlag("no_wave64");
    }
    bool $cppClass::hasFastMath() const {
      return hasFlag("fast");
    }
    bool $cppClass::hasDaz() const {
      return hasFlag("daz");
    }
    bool $cppClass::hasFiniteOnly() const {
      return hasFlag("finite_only");
    }
    bool $cppClass::hasUnsafeMath() const {
      return hasFlag("unsafe_math");
    }
    bool $cppClass::hasCorrectSqrt() const {
      return !hasFlag("unsafe_sqrt");
    }
  }];
}
#endif // ROCDLIR_OPS


//===-- SparseTensorAttrDefs.td - attributes definitions ---*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef SPARSETENSOR_ATTRDEFS
#define SPARSETENSOR_ATTRDEFS

include "mlir/IR/AttrTypeBase.td"
include "mlir/IR/EnumAttr.td"
include "mlir/Dialect/SparseTensor/IR/SparseTensorBase.td"
include "mlir/IR/TensorEncoding.td"

// All of the sparse tensor attributes will extend this class.
class SparseTensor_Attr<string name,
                        list<Trait> traits = []>
    : AttrDef<SparseTensor_Dialect, name, traits>;

//===----------------------------------------------------------------------===//
// A simple bitset attribute wrapped around a single int64_t to encode a set of
// sparse tensor levels.
//===----------------------------------------------------------------------===//

def I64BitSetAttr : TypedAttrBase<I64, "IntegerAttr",
      And<[CPred<"::llvm::isa<::mlir::IntegerAttr>($_self)">,
           CPred<"::llvm::cast<::mlir::IntegerAttr>($_self).getType().isInteger(64)">]>,
      "LevelSet attribute"> {
  let returnType = [{::mlir::sparse_tensor::I64BitSet}];
  let convertFromStorage = [{::mlir::sparse_tensor::I64BitSet($_self.getValue().getZExtValue())}];
}

def I64BitSetArrayAttr :
    TypedArrayAttrBase<I64BitSetAttr, "I64BitSet array attribute">;

//===----------------------------------------------------------------------===//
// These attributes are just like `IndexAttr` except that they clarify whether
// the index refers to a dimension (an axis of the semantic tensor) or a level
// (an axis of the actual storage format).
//===----------------------------------------------------------------------===//

def DimensionAttr :
    TypedAttrBase<
      Index, "IntegerAttr",
      And<[CPred<"::llvm::isa<::mlir::IntegerAttr>($_self)">,
           CPred<"::llvm::isa<::mlir::IndexType>("
                     "::llvm::cast<::mlir::IntegerAttr>($_self).getType())">]>,
      "dimension attribute"> {
  let returnType = [{::mlir::sparse_tensor::Dimension}];
  let convertFromStorage = [{$_self.getValue().getZExtValue()}];
}

def LevelAttr :
    TypedAttrBase<
      Index, "IntegerAttr",
      And<[CPred<"::llvm::isa<::mlir::IntegerAttr>($_self)">,
           CPred<"::llvm::isa<::mlir::IndexType>("
                     "::llvm::cast<::mlir::IntegerAttr>($_self).getType())">]>,
      "level attribute"> {
  let returnType = [{::mlir::sparse_tensor::Level}];
  let convertFromStorage = [{$_self.getValue().getZExtValue()}];
}

//===----------------------------------------------------------------------===//
// Sparse Tensor Dimension Slice Attribute.
//===----------------------------------------------------------------------===//

def SparseTensorDimSliceAttr : SparseTensor_Attr<"SparseTensorDimSlice", []> {
  let mnemonic = "slice";

  let description = [{
    An attribute to encode slice information of a sparse tensor on a particular
    dimension (a tuple of offset, size, stride).
  }];

  let parameters = (
    ins
    "int64_t" : $offset,
    "int64_t" : $size,
    "int64_t" : $stride
  );

  let builders = [
    // The nop slice (i.e., that includes everything).
    AttrBuilder<(ins), [{ return $_get($_ctxt, 0, kDynamic, 1); }]>
  ];

  let extraClassDeclaration = [{
    void print(llvm::raw_ostream &os) const;

    /// Special value for dynamic offset/size/stride.
    static constexpr int64_t kDynamic = -1;
    static constexpr bool isDynamic(int64_t v) { return v == kDynamic; }
    static std::optional<uint64_t> getStatic(int64_t v);
    static std::string getStaticString(int64_t v);

    std::optional<uint64_t> getStaticOffset() const;
    std::optional<uint64_t> getStaticStride() const;
    std::optional<uint64_t> getStaticSize() const;
    bool isCompletelyDynamic() const;
  }];

  let genVerifyDecl = 1;
  let hasCustomAssemblyFormat = 1;
}

//===----------------------------------------------------------------------===//
// Sparse Tensor Type Encoding Attribute.
//===----------------------------------------------------------------------===//

// Sparse tensor encoding attribute.
def SparseTensorEncodingAttr : SparseTensor_Attr<"SparseTensorEncoding",
         [ DeclareAttrInterfaceMethods<VerifiableTensorEncoding> ] > {
  let mnemonic = "encoding";

  let description = [{
    An attribute to encode information on sparsity properties of tensors, inspired
    by the TACO formalization of sparse tensors. This encoding is eventually used
    by a **sparsifier** pass to generate sparse code fully automatically from a
    sparsity-agnostic representation of the computation, i.e., an implicit sparse
    representation is converted to an explicit sparse representation where co-iterating
    loops operate on sparse storage formats rather than tensors with a sparsity
    encoding. Compiler passes that run before this sparsifier pass need to be aware
    of the semantics of tensor types with such a sparsity encoding.

    In this encoding, we use **dimension** to refer to the axes of the semantic tensor,
    and **level** to refer to the axes of the actual storage format, i.e., the
    operational representation of the sparse tensor in memory. The number of
    dimensions is usually the same as the number of levels (such as CSR storage format).
    However, the encoding can also map dimensions to higher-order levels (for example,
    to encode a block-sparse BSR storage format) or to lower-order levels
    (for example, to linearize dimensions as a single level in the storage).

    The encoding contains a map that provides the following:

    - An ordered sequence of dimension specifications, each of which defines:
      - the dimension-size (implicit from the tensor’s dimension-shape)
      - a **dimension-expression**
    - An ordered sequence of level specifications, each of which includes a required
      **level-type**, which defines how the level should be stored. Each level-type
      consists of:
      - a **level-expression**, which defines what is stored
      - a **level-format**
      - a collection of **level-properties** that apply to the level-format

    Each level-expression is an affine expression over dimension-variables. Thus, the
    level-expressions collectively define an affine map from dimension-coordinates to
    level-coordinates. The dimension-expressions collectively define the inverse map,
    which only needs to be provided for elaborate cases where it cannot be inferred
    automatically.

    Each dimension could also have an optional `SparseTensorDimSliceAttr`.
    Within the sparse storage format, we refer to indices that are stored explicitly
    as **coordinates** and offsets into the storage format as **positions**.

    The supported level-formats are the following:

    - **dense** : all entries along this level are stored and linearized.
    - **batch** : all entries along this level are stored but not linearized.
    - **compressed** : only nonzeros along this level are stored
    - **loose_compressed** : as compressed, but allows for free space between regions
    - **singleton** : a variant of the compressed format, where coordinates have no siblings
    - **structured[n, m]** : the compression uses a n:m encoding
      (viz. n out of m consecutive elements are nonzero)

    For a compressed level, each position interval is represented in a compact
    way with a lowerbound `pos(i)` and an upperbound `pos(i+1) - 1`, which implies
    that successive intervals must appear in order without any "holes" in between
    them. The loose compressed format relaxes these constraints by representing each
    position interval with a lowerbound `lo(i)` and an upperbound `hi(i)`, which
    allows intervals to appear in arbitrary order and with elbow room between them.

    By default, each level-type has the property of being unique (no duplicate
    coordinates at that level) and ordered (coordinates appear sorted at that
    level). For singleton levels, the coordinates are fused with its parents in AoS
    (array of structures) scheme. The following properties can be added to a level-format
    to change this default behavior:

    - **nonunique** : duplicate coordinates may appear at the level
    - **nonordered** : coordinates may appear in arbribratry order
    - **soa** : only applicable to singleton levels, fuses the singleton
      level in SoA (structure of arrays) scheme.

    In addition to the map, the following fields are optional:

    - The required bitwidth for position storage (integral offsets
      into the sparse storage scheme).  A narrow width reduces the memory
      footprint of overhead storage, as long as the width suffices to
      define the total required range (viz. the maximum number of stored
      entries over all indirection levels).  The choices are `8`, `16`,
      `32`, `64`, or, the default, `0` to indicate the native bitwidth.

    - The required bitwidth for coordinate storage (the coordinates
      of stored entries).  A narrow width reduces the memory footprint
      of overhead storage, as long as the width suffices to define
      the total required range (viz. the maximum value of each tensor
      coordinate over all levels).  The choices are `8`, `16`, `32`,
      `64`, or, the default, `0` to indicate a native bitwidth.

    - The explicit value for the sparse tensor. If explicitVal is set,
      then all the non-zero values in the tensor have the same explicit value.
      The default value Attribute() indicates that it is not set. This
      is useful for binary-valued sparse tensors whose values can either
      be an implicit value (0 by default) or an explicit value (such as 1).
      In this approach, we don't store explicit/implicit values, and metadata
      (such as position and coordinate arrays) alone fully defines the original tensor.
      This yields additional savings for the storage requirements,
      as well as for the computational time, since we skip operating on
      implicit values and can constant fold the explicit values where they are used.

    - The implicit value for the sparse tensor. If implicitVal is set,
      then the "zero" value in the tensor is equal to the implicit value.
      For now, we only support `0` as the implicit value but it could be
      extended in the future. The default value Attribute() indicates that
      the implicit value is `0` (same type as the tensor element type).

    Examples:

    ```mlir
    // Sparse vector.
    #SparseVector = #sparse_tensor.encoding<{
      map = (i) -> (i : compressed)
    }>
    ... tensor<?xf32, #SparseVector> ...

    // Sorted coordinate scheme (arranged in AoS format by default).
    #SortedCOO = #sparse_tensor.encoding<{
      map = (i, j) -> (i : compressed(nonunique), j : singleton)
    }>
    // coordinates = {x_crd, y_crd}[nnz]
    ... tensor<?x?xf64, #SortedCOO> ...

    // Sorted coordinate scheme (arranged in SoA format).
    #SortedCOO = #sparse_tensor.encoding<{
      map = (i, j) -> (i : compressed(nonunique), j : singleton(soa))
    }>
    // coordinates = {x_crd[nnz], y_crd[nnz]}
    ... tensor<?x?xf64, #SortedCOO> ...

    // Batched sorted coordinate scheme, with high encoding.
    #BCOO = #sparse_tensor.encoding<{
      map = (i, j, k) -> (i : dense, j : compressed(nonunique, high), k : singleton)
    }>
    ... tensor<10x10xf32, #BCOO> ...

    // Compressed sparse row.
    #CSR = #sparse_tensor.encoding<{
      map = (i, j) -> (i : dense, j : compressed)
    }>
    ... tensor<100x100xbf16, #CSR> ...

    // Doubly compressed sparse column storage with specific bitwidths.
    #DCSC = #sparse_tensor.encoding<{
      map = (i, j) -> (j : compressed, i : compressed),
      posWidth = 32,
      crdWidth = 8
    }>
    ... tensor<8x8xf64, #DCSC> ...

    // Doubly compressed sparse column storage with specific
    // explicit and implicit values.
    #DCSC = #sparse_tensor.encoding<{
      map = (i, j) -> (j : compressed, i : compressed),
      explicitVal = 1 : i64,
      implicitVal = 0 : i64
    }>
    ... tensor<8x8xi64, #DCSC> ...

    // Block sparse row storage (2x3 blocks).
    #BSR = #sparse_tensor.encoding<{
      map = ( i, j ) ->
      ( i floordiv 2 : dense,
        j floordiv 3 : compressed,
        i mod 2      : dense,
        j mod 3      : dense
      )
    }>
    ... tensor<20x30xf32, #BSR> ...

    // Same block sparse row storage (2x3 blocks) but this time
    // also with a redundant reverse mapping, which can be inferred.
    #BSR_explicit = #sparse_tensor.encoding<{
      map = { ib, jb, ii, jj }
            ( i = ib * 2 + ii,
              j = jb * 3 + jj) ->
      ( ib = i floordiv 2 : dense,
        jb = j floordiv 3 : compressed,
        ii = i mod 2 : dense,
        jj = j mod 3 : dense)
    }>
    ... tensor<20x30xf32, #BSR_explicit> ...

    // ELL format.
    // In the simple format for matrix, one array stores values and another
    // array stores column indices. The arrays have the same number of rows
    // as the original matrix, but only have as many columns as
    // the maximum number of nonzeros on a row of the original matrix.
    // There are many variants for ELL such as jagged diagonal scheme.
    // To implement ELL, map provides a notion of "counting a
    // dimension", where every stored element with the same coordinate
    // is mapped to a new slice. For instance, ELL storage of a 2-d
    // tensor can be defined with the mapping (i, j) -> (#i, i, j)
    // using the notation of [Chou20]. Lacking the # symbol in MLIR's
    // affine mapping, we use a free symbol c to define such counting,
    // together with a constant that denotes the number of resulting
    // slices. For example, the mapping [c](i, j) -> (c * 3 * i, i, j)
    // with the level-types ["dense", "dense", "compressed"] denotes ELL
    // storage with three jagged diagonals that count the dimension i.
    #ELL = #sparse_tensor.encoding<{
      map = [c](i, j) -> (c * 3 * i : dense, i : dense, j : compressed)
    }>
    ... tensor<?x?xf64, #ELL> ...

    // CSR slice (offset = 0, size = 4, stride = 1 on the first dimension;
    // offset = 0, size = 8, and a dynamic stride on the second dimension).
    #CSR_SLICE = #sparse_tensor.encoding<{
      map = (i : #sparse_tensor<slice(0, 4, 1)>,
             j : #sparse_tensor<slice(0, 8, ?)>) ->
            (i : dense, j : compressed)
    }>
    ... tensor<?x?xf64, #CSR_SLICE> ...

    ```
  }];

  //
  // Data in sparse tensor encoding.
  //
  let parameters = (
    ins
    // A level-type for each level of the sparse storage
    // (consists of a level-format combined with level-properties).
    ArrayRefParameter<
      "::mlir::sparse_tensor::LevelType",
      "level-types"
      >: $lvlTypes,

    // A mapping from dimension-coordinates to level-coordinates.
    "AffineMap":$dimToLvl,

    // A mapping from level-coordinates to dimension-coordinates.
    "AffineMap":$lvlToDim,

    // The required bitwidth for position storage.
    "unsigned":$posWidth,

    // The required bitwidth for coordinate storage.
    "unsigned":$crdWidth,

    // The required explicit value.
    "::mlir::Attribute":$explicitVal,

    // The required implicit value.
    "::mlir::Attribute":$implicitVal,

    // A slice attribute for each dimension of the tensor type.
    ArrayRefParameter<
      "::mlir::sparse_tensor::SparseTensorDimSliceAttr",
      "per dimension slice metadata"
      >: $dimSlices
  );

  let builders = [
    AttrBuilder<(ins "ArrayRef<::mlir::sparse_tensor::LevelType>":$lvlTypes,
                     CArg<"AffineMap", "{}">:$dimToLvl,
                     CArg<"AffineMap", "{}">:$lvlToDim,
                     CArg<"unsigned", "0">:$posWidth,
                     CArg<"unsigned", "0">:$crdWidth,
                     CArg<"::mlir::Attribute", "{}">:$explicitVal,
                     CArg<"::mlir::Attribute", "{}">:$implicitVal), [{
      if (!dimToLvl) {
        dimToLvl = ::mlir::AffineMap::getMultiDimIdentityMap(lvlTypes.size(), $_ctxt);
      }
      if (!lvlToDim) {
        lvlToDim = ::mlir::sparse_tensor::inferLvlToDim(dimToLvl, $_ctxt);
      }
      return $_get($_ctxt, lvlTypes, dimToLvl, lvlToDim, posWidth, crdWidth,
        explicitVal, implicitVal,
        ArrayRef<::mlir::sparse_tensor::SparseTensorDimSliceAttr>{});
    }]>
  ];

  let extraClassDeclaration = [{
    //
    // Factory methods.
    //

    /// Constructs a new encoding with the given dimToLvl mapping,
    /// and all other fields inherited from `this`.
    SparseTensorEncodingAttr withDimToLvl(AffineMap dimToLvl) const;
    SparseTensorEncodingAttr withDimToLvl(SparseTensorEncodingAttr enc) const;

    /// Constructs a new encoding with dimToLvl reset to the default/identity,
    /// and all other fields inherited from `this`.
    SparseTensorEncodingAttr withoutDimToLvl() const;

    /// Constructs a new encoding with the given pointer and index
    /// bitwidths, and all other fields inherited from `this`.
    SparseTensorEncodingAttr withBitWidths(unsigned posWidth, unsigned crdWidth) const;

    /// Constructs a new encoding with the pointer and index bitwidths
    /// reset to the default, and all other fields inherited from `this`.
    SparseTensorEncodingAttr withoutBitWidths() const;

    /// Constructs a new encoding with the given explicit value
    /// and all other fields inherited from `this`.
    SparseTensorEncodingAttr withExplicitVal(Attribute explicitVal) const;

    /// Constructs a new encoding with the explicit value
    /// reset to the default, and all other fields inherited from `this`.
    SparseTensorEncodingAttr withoutExplicitVal() const;

    /// Constructs a new encoding with the given implicit value
    /// and all other fields inherited from `this`.
    SparseTensorEncodingAttr withImplicitVal(Attribute implicitVal) const;

    /// Constructs a new encoding with the implicit value
    /// reset to the default, and all other fields inherited from `this`.
    SparseTensorEncodingAttr withoutImplicitVal() const;

    /// Constructs a new encoding with the given dimSlices, and all
    /// other fields inherited from `this`.
    SparseTensorEncodingAttr withDimSlices(ArrayRef<::mlir::sparse_tensor::SparseTensorDimSliceAttr> dimSlices) const;

    /// Constructs a new encoding with the dimSlices reset to the default,
    /// and all other fields inherited from `this`.
    SparseTensorEncodingAttr withoutDimSlices() const;

    //
    // Rank methods.
    //

    /// Returns the expected number of tensor dimensions.  Asserts that
    /// the encoding is non-null (since no fixed result is valid for every
    /// dense-tensor).
    ::mlir::sparse_tensor::Dimension getDimRank() const;

    /// Returns the number of storage levels.  Asserts that the encoding
    /// is non-null (since no fixed result is valid for every dense-tensor).
    ::mlir::sparse_tensor::Level getLvlRank() const;

    uint64_t getBatchLvlRank() const;

    //
    // lvlTypes methods.
    //

    /// Safely looks up the level-type for the requested level.  (Returns
    /// `LevelType::Dense` for the null encoding, since dense-tensors
    /// are always all-dense.)
    ::mlir::sparse_tensor::LevelType getLvlType(::mlir::sparse_tensor::Level l) const;

    bool isDenseLvl(::mlir::sparse_tensor::Level l) const { return isDenseLT(getLvlType(l)); }
    bool isCompressedLvl(::mlir::sparse_tensor::Level l) const { return isCompressedLT(getLvlType(l)); }
    bool isSingletonLvl(::mlir::sparse_tensor::Level l) const { return isSingletonLT(getLvlType(l)); }
    bool isLooseCompressedLvl(::mlir::sparse_tensor::Level l) const { return isLooseCompressedLT(getLvlType(l)); }
    bool isNOutOfMLvl(::mlir::sparse_tensor::Level l) const { return isNOutOfMLT(getLvlType(l)); }
    bool isOrderedLvl(::mlir::sparse_tensor::Level l) const { return isOrderedLT(getLvlType(l)); }
    bool isUniqueLvl(::mlir::sparse_tensor::Level l) const { return isUniqueLT(getLvlType(l)); }

    /// Returns true if every level is dense.  Also returns true for
    /// the null encoding (since dense-tensors are always all-dense).
    bool isAllDense() const;

    /// Returns true if every level is ordered.  Also returns true for
    /// the null encoding (since dense-tensors are always all-ordered).
    bool isAllOrdered() const;

    //
    // Storage type methods.
    //

    /// Returns the coordinate-overhead MLIR type, defaulting to `IndexType`.
    Type getCrdElemType() const;

    /// Returns the position-overhead MLIR type, defaulting to `IndexType`.
    Type getPosElemType() const;

    /// Returns the coordinate-memref MLIR type, an optional tensorDimShape is
    /// used to refine the leading batch dimensions (if any).
    MemRefType getCrdMemRefType(
      std::optional<ArrayRef<int64_t>> tensorDimShape = std::nullopt) const;

    /// Returns the position-memref MLIR type, an optional tensorDimShape is
    /// used to refine the leading batch dimensions (if any).
    MemRefType getPosMemRefType(
      std::optional<ArrayRef<int64_t>> tensorDimShape = std::nullopt) const;

    //
    // dimToLvl methods.
    //

    /// Returns true if the dimToLvl mapping is the identity.
    /// Also returns true for the null encoding (since dense-tensors
    /// always have the identity mapping).
    bool isIdentity() const;

    /// Returns true if the dimToLvl mapping is a permutation.
    /// Also returns true for the null encoding (since dense-tensors
    /// always have the identity mapping).
    bool isPermutation() const;

    //
    // dimSlices methods.
    //

    bool isSlice() const;

    ::mlir::sparse_tensor::SparseTensorDimSliceAttr getDimSlice(::mlir::sparse_tensor::Dimension dim) const;

    std::optional<uint64_t> getStaticDimSliceOffset(::mlir::sparse_tensor::Dimension dim) const;
    std::optional<uint64_t> getStaticDimSliceStride(::mlir::sparse_tensor::Dimension dim) const;
    std::optional<uint64_t> getStaticLvlSliceOffset(::mlir::sparse_tensor::Level lvl) const;
    std::optional<uint64_t> getStaticLvlSliceStride(::mlir::sparse_tensor::Level lvl) const;

    //
    // Helper function to translate between level/dimension space.
    //

    SmallVector<int64_t> translateShape(::mlir::ArrayRef<int64_t> srcShape, ::mlir::sparse_tensor::CrdTransDirectionKind) const;
    ValueRange translateCrds(::mlir::OpBuilder &builder, ::mlir::Location loc, ::mlir::ValueRange crds, ::mlir::sparse_tensor::CrdTransDirectionKind) const;

    //
    // COO methods.
    //

    /// Returns the starting level of this sparse tensor type for a
    /// trailing COO region that spans **at least** two levels. If
    /// no such COO region is found, then returns the level-rank.
    ///
    /// DEPRECATED: use getCOOSegment instead;
    Level getAoSCOOStart() const;

    /// Returns a list of COO segments in the sparse tensor types.
    SmallVector<COOSegment> getCOOSegments() const;

    //
    // Printing methods.
    //

    void printSymbols(AffineMap &map, AsmPrinter &printer) const;
    void printDimensions(AffineMap &map, AsmPrinter &printer, ArrayRef<::mlir::sparse_tensor::SparseTensorDimSliceAttr> dimSlices) const;
    void printLevels(AffineMap &map, AsmPrinter &printer, ArrayRef<::mlir::sparse_tensor::LevelType> lvlTypes) const;
  }];

  let genVerifyDecl = 1;
  let hasCustomAssemblyFormat = 1;
}

//===----------------------------------------------------------------------===//
// Sparse Tensor Storage Specifier Enum Attribute.
//===----------------------------------------------------------------------===//

// The C++ enum for Storage Specifier kind.
def SparseTensorStorageSpecifierKindEnum
    : I32EnumAttr<"StorageSpecifierKind", "sparse tensor storage specifier kind", [
        I32EnumAttrCase<"LvlSize",    0, "lvl_sz">,
        I32EnumAttrCase<"PosMemSize", 1, "pos_mem_sz">,
        I32EnumAttrCase<"CrdMemSize", 2, "crd_mem_sz">,
        I32EnumAttrCase<"ValMemSize", 3, "val_mem_sz">,
        I32EnumAttrCase<"DimOffset",  4, "dim_offset">,
        I32EnumAttrCase<"DimStride",  5, "dim_stride">,
      ]> {
  let genSpecializedAttr = 0;
  let cppNamespace = SparseTensor_Dialect.cppNamespace;
}

// Define the enum StorageSpecifier kind attribute.
def SparseTensorStorageSpecifierKindAttr
    : EnumAttr<SparseTensor_Dialect, SparseTensorStorageSpecifierKindEnum,
               "SparseTensorStorageSpecifierKind"> {
   let mnemonic = "kind";
}

//===----------------------------------------------------------------------===//
// Sparse Tensor Traits.
//===----------------------------------------------------------------------===//

def IsSparseTensorPred
  : CPred<"!!::mlir::sparse_tensor::getSparseTensorEncoding($_self)">;

def IsSparseTensorSlicePred
  : CPred<"!!::mlir::sparse_tensor::getSparseTensorEncoding($_self) && "
          "  ::mlir::sparse_tensor::getSparseTensorEncoding($_self).isSlice()">;

class SparseTensorOf<list<Type> allowedTypes>
  : RankedTensorOf<allowedTypes, [IsSparseTensorPred], "sparse tensor">;

class SparseTensorSliceOf<list<Type> allowedTypes>
  : RankedTensorOf<allowedTypes, [IsSparseTensorSlicePred], "sparse tensor slice">;

class ScalarLikeOf<list<Type> allowedTypes>
  : AnyTypeOf<[0DTensorOf<allowedTypes>, AnyTypeOf<allowedTypes>], "scalar like">;

def AnySparseTensor : SparseTensorOf<[AnyType]>;
def AnySparseTensorSlice : SparseTensorSliceOf<[AnyType]>;
def AnyIndexingScalarLike : ScalarLikeOf<[AnySignlessIntegerOrIndex]>;

//===----------------------------------------------------------------------===//
// Sparse Tensor Sorting Algorithm Attribute.
//===----------------------------------------------------------------------===//

// Currently, we only provide four implementations, and expose the
// implementations via attribute algorithm. In the future, if we will
// need to support both stable and non-stable quick sort, we may add
// quick_sort_nonstable enum to the attribute. Alternative, we may use
// two attributes, (stable|nonstable, algorithm), to specify a sorting
// implementation.
//
// --------------------------------------------------------------------------
// |           | hybrid_qsort| insertion_sort | qsort       | heap_sort.    |
// |non-stable | Impl        | X              |  Impl       | Impl          |
// |stable     | X           | Impl           |  Not Impl   | X             |
// --------------------------------------------------------------------------

// The C++ enum for sparse tensor sort kind.
def SparseTensorSortKindEnum
    : I32EnumAttr<"SparseTensorSortKind", "sparse tensor sort algorithm", [
        I32EnumAttrCase<"HybridQuickSort",    0, "hybrid_quick_sort">,
        I32EnumAttrCase<"InsertionSortStable", 1, "insertion_sort_stable">,
        I32EnumAttrCase<"QuickSort", 2, "quick_sort">,
        I32EnumAttrCase<"HeapSort", 3, "heap_sort">,
      ]> {
  let genSpecializedAttr = 0;
  let cppNamespace = SparseTensor_Dialect.cppNamespace;
}

// Define the enum sparse tensor sort kind attribute.
def SparseTensorSortKindAttr
    : EnumAttr<SparseTensor_Dialect, SparseTensorSortKindEnum,
               "SparseTensorSortAlgorithm"> {
}


//===----------------------------------------------------------------------===//
// Sparse Tensor Coordinate Translation Direction Attribute.
//===----------------------------------------------------------------------===//

// The C++ enum for sparse tensor coordinate translation direction enum.
def SparseTensorCrdTransDirectionEnum
    : I32EnumAttr<"CrdTransDirectionKind", "sparse tensor coordinate translation direction", [
        I32EnumAttrCase<"dim2lvl", 0, "dim_to_lvl">,
        I32EnumAttrCase<"lvl2dim", 1, "lvl_to_dim">,
      ]> {
  let genSpecializedAttr = 0;
  let cppNamespace = SparseTensor_Dialect.cppNamespace;
}

// The C++ enum for sparse tensor coordinate translation direction attribute.
def SparseTensorCrdTransDirectionAttr
    : EnumAttr<SparseTensor_Dialect, SparseTensorCrdTransDirectionEnum,
               "CrdTransDirection"> {
}

#endif // SPARSETENSOR_ATTRDEFS


//===- VectorOps.td - Vector op definitions ---------------*- tablegen -*-====//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// Defines MLIR vector operations.
//
//===----------------------------------------------------------------------===//

#ifndef MLIR_DIALECT_VECTOR_IR_VECTOR_OPS
#define MLIR_DIALECT_VECTOR_IR_VECTOR_OPS

include "mlir/Dialect/Arith/IR/ArithBase.td"
include "mlir/Dialect/Arith/IR/ArithOpsInterfaces.td"
include "mlir/Dialect/Vector/Interfaces/MaskableOpInterface.td"
include "mlir/Dialect/Vector/Interfaces/MaskingOpInterface.td"
include "mlir/Dialect/Vector/IR/Vector.td"
include "mlir/Dialect/Vector/IR/VectorAttributes.td"
include "mlir/Interfaces/ControlFlowInterfaces.td"
include "mlir/Interfaces/DestinationStyleOpInterface.td"
include "mlir/Interfaces/InferIntRangeInterface.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/Interfaces/VectorInterfaces.td"
include "mlir/Interfaces/ViewLikeInterface.td"
include "mlir/IR/BuiltinAttributes.td"
include "mlir/IR/EnumAttr.td"

// TODO: Add an attribute to specify a different algebra with operators other
// than the current set: {*, +}.
def Vector_ContractionOp :
  Vector_Op<"contract", [
      Pure,
      PredOpTrait<"lhs and rhs have same element type", TCopVTEtIsSameAs<0, 1>>,
      PredOpTrait<"third operand acc and result have same element type",
                  TCresVTEtIsSameAsOpBase<0, 2>>,
      DeclareOpInterfaceMethods<MaskableOpInterface>,
      DeclareOpInterfaceMethods<VectorUnrollOpInterface, ["getShapeForUnroll"]>
    ]>,
    Arguments<(ins AnyVectorOfNonZeroRank:$lhs, AnyVectorOfNonZeroRank:$rhs, AnyType:$acc,
               ArrayAttr:$indexing_maps,
               Vector_IteratorTypeArrayAttr:$iterator_types,
               DefaultValuedAttr<Vector_CombiningKindAttr,
                                 "CombiningKind::ADD">:$kind)>,
    Results<(outs AnyType)> {
  let summary = "vector contraction operation";
  let description = [{
    Computes the sum of products of vector elements along contracting
    dimension pairs from 2 vectors of rank M and N respectively, adds this
    intermediate result to the accumulator argument of rank K, and returns a
    vector result of rank K (where K = num_lhs_free_dims + num_rhs_free_dims +
    num_batch_dims (see dimension type descriptions below)). For K = 0 (no
    free or batch dimensions), the accumulator and output are a scalar.

    If operands and the result have types of different bitwidths, operands are
    promoted to have the same bitwidth as the result before performing the
    contraction. For integer types, only signless integer types are supported,
    and the promotion happens via sign extension.

    An iterator type attribute list must be specified, where each element of
    the list represents an iterator with one of the following types:

    *   "reduction": reduction dimensions are present in the lhs and rhs
        arguments but not in the output (and accumulator
        argument). These are the dimensions along which the vector
        contraction op computes the sum of products, and
        contracting dimension pair dimension sizes must match
        between lhs/rhs.

    *   "parallel": Batch dimensions are iterator type "parallel", and
        are non-contracting dimensions present in the lhs, rhs and
        output. The lhs/rhs co-iterate along the batch dimensions,
        which should be expressed in their indexing maps.

        Free dimensions are iterator type "parallel", and are
        non-contraction, non-batch dimensions accessed by either the
        lhs or rhs (but not both). The lhs and rhs free dimensions
        are unrelated to each other and do not co-iterate, which
        should be expressed in their indexing maps.

    An indexing map attribute list must be specified with an entry for lhs, rhs
    and acc arguments. An indexing map attribute specifies a mapping from each
    iterator in the iterator type list, to each dimension of an N-D vector.

    An optional kind attribute may be used to specify the combining function
    between the intermediate result and accumulator argument of rank K. This
    attribute can take the values `add`/`mul`/`minsi`/`minui`/`maxsi`/`maxui`
    /`and`/`or`/`xor` for integers, and `add`/`mul`/`minnumf`/`maxnumf`
    /`minimumf`/`maximumf` for floats. The default is `add`.

    Example:

    ```mlir
    // Simple DOT product (K = 0).
    #contraction_accesses = [
     affine_map<(i) -> (i)>,
     affine_map<(i) -> (i)>,
     affine_map<(i) -> ()>
    ]
    #contraction_trait = {
      indexing_maps = #contraction_accesses,
      iterator_types = ["reduction"]
    }
    %3 = vector.contract #contraction_trait %0, %1, %2
      : vector<10xf32>, vector<10xf32> into f32

    // 2D vector contraction with one contracting dimension (matmul, K = 2).
    #contraction_accesses = [
      affine_map<(i, j, k) -> (i, k)>,
      affine_map<(i, j, k) -> (k, j)>,
      affine_map<(i, j, k) -> (i, j)>
    ]
    #contraction_trait = {
      indexing_maps = #contraction_accesses,
      iterator_types = ["parallel", "parallel", "reduction"]
    }

    %3 = vector.contract #contraction_trait %0, %1, %2
      : vector<4x3xf32>, vector<3x7xf32> into vector<4x7xf32>

    // 4D to 3D vector contraction with two contracting dimensions and
    // one batch dimension (K = 3).
    #contraction_accesses = [
      affine_map<(b0, f0, f1, c0, c1) -> (c0, b0, c1, f0)>,
      affine_map<(b0, f0, f1, c0, c1) -> (b0, c1, c0, f1)>,
      affine_map<(b0, f0, f1, c0, c1) -> (b0, f0, f1)>
    ]
    #contraction_trait = {
      indexing_maps = #contraction_accesses,
      iterator_types = ["parallel", "parallel", "parallel",
                        "reduction", "reduction"]
    }

    %4 = vector.contract #contraction_trait %0, %1, %2
        : vector<7x8x16x15xf32>, vector<8x16x7x5xf32> into vector<8x15x5xf32>

    // Vector contraction with mixed typed. lhs/rhs have different element
    // types than accumulator/result.
    %5 = vector.contract #contraction_trait %0, %1, %2
      : vector<10xf16>, vector<10xf16> into f32

    // Contract with max (K = 0).
    #contraction_accesses = [
     affine_map<(i) -> (i)>,
     affine_map<(i) -> (i)>,
     affine_map<(i) -> ()>
    ]
    #contraction_trait = {
      indexing_maps = #contraction_accesses,
      iterator_types = ["reduction"],
      kind = #vector.kind<maxnumf>
    }
    %6 = vector.contract #contraction_trait %0, %1, %2
      : vector<10xf32>, vector<10xf32> into f32
    ```
  }];
  let builders = [
    OpBuilder<(ins "Value":$lhs, "Value":$rhs, "Value":$acc,
      "ArrayAttr":$indexingMaps, "ArrayAttr":$iteratorTypes)>,
    OpBuilder<(ins "Value":$lhs, "Value":$rhs, "Value":$acc,
      "ArrayRef<ArrayRef<AffineExpr>>":$indexingExprs,
      "ArrayRef<IteratorType>":$iteratorTypes)>,
    OpBuilder<(ins "Value":$lhs, "Value":$rhs, "Value":$acc,
      "ArrayAttr":$indexingMaps, "ArrayAttr":$iteratorTypes,
      "CombiningKind":$kind)>
  ];
  let extraClassDeclaration = [{
    VectorType getLhsType() {
      return ::llvm::cast<VectorType>(getLhs().getType());
    }
    VectorType getRhsType() {
      return ::llvm::cast<VectorType>(getRhs().getType());
    }
    Type getAccType() { return getAcc().getType(); }
    Type getResultType() { return getResult().getType(); }
    SmallVector<StringRef> getTraitAttrNames();
    static unsigned getAccOperandIndex() { return 2; }

    llvm::SmallVector<::mlir::AffineMap, 4> getIndexingMapsArray() {
      return llvm::to_vector<4>(getIndexingMaps().getAsValueRange<::mlir::AffineMapAttr>());
    }

    // Returns the bounds of each dimension in the iteration space spanned
    // by the iterator types of this operation.
    void getIterationBounds(SmallVectorImpl<int64_t> &iterationBounds);

    // Returns a list of index maps, where there is a list entry for each
    // op indexing map attribute (i.e. one for each input and output, with
    // the output listed last). Each index map, maps from this operations
    // iteration space, to vector dimensions of the maps input/output.
    void getIterationIndexMap(
      std::vector<DenseMap<int64_t, int64_t>> &iterationIndexMap);

    std::vector<std::pair<int64_t, int64_t>> getContractingDimMap();
    std::vector<std::pair<int64_t, int64_t>> getBatchDimMap();

    static CombiningKind getDefaultKind() {
      return CombiningKind::ADD;
    }

    SmallVector<IteratorType> getIteratorTypesArray() {
      auto range =
          getIteratorTypes()
              .template getAsValueRange<IteratorTypeAttr, IteratorType>();
      return {range.begin(), range.end()};
    }
  }];

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

def Vector_ReductionOp :
  Vector_Op<"reduction", [Pure,
     PredOpTrait<"source operand and result have same element type",
                 TCresVTEtIsSameAsOpBase<0, 0>>,
     OptionalTypesMatchWith<"dest and acc have the same type",
                            "dest", "acc", "::llvm::cast<Type>($_self)">,
     DeclareOpInterfaceMethods<ArithFastMathInterface>,
     DeclareOpInterfaceMethods<MaskableOpInterface>,
     DeclareOpInterfaceMethods<VectorUnrollOpInterface, ["getShapeForUnroll"]>
    ]>,
    Arguments<(ins Vector_CombiningKindAttr:$kind,
               AnyVectorOfAnyRank:$vector,
               Optional<AnyType>:$acc,
               DefaultValuedAttr<
                 Arith_FastMathAttr,
                 "::mlir::arith::FastMathFlags::none">:$fastmath)>,
    Results<(outs AnyType:$dest)> {
  let summary = "reduction operation";
  let description = [{
    Reduces an 1-D vector "horizontally" into a scalar using the given
    operation: `add`/`mul`/`minsi`/`minui`/`maxsi`/`maxui`/`and`/`or`/`xor` for
    integers, and `add`/`mul`/`minnumf`/`maxnumf`/`minimumf`/`maximumf` for
    floats. Reductions also allow an optional fused accumulator.

    Note that these operations are restricted to 1-D vectors to remain
    close to the corresponding LLVM intrinsics:

    http://llvm.org/docs/LangRef.html#vector-reduction-intrinsics

    Example:

    ```mlir
    %1 = vector.reduction <add>, %0 : vector<16xf32> into f32

    %3 = vector.reduction <xor>, %2 : vector<4xi32> into i32

    %4 = vector.reduction <mul>, %0, %1 : vector<16xf32> into f32
    ```
  }];
  let extraClassDeclaration = [{
    VectorType getSourceVectorType() {
      return ::llvm::cast<VectorType>(getVector().getType());
    }
  }];
  let builders = [
    // Builder that infers the type of `dest`.
    OpBuilder<(ins "CombiningKind":$kind, "Value":$vector, "Value":$acc,
                    CArg<"::mlir::arith::FastMathFlags",
                         "::mlir::arith::FastMathFlags::none">:$fastMathFlags)>,
    // Builder that infers the type of `dest` and has no accumulator.
    OpBuilder<(ins "CombiningKind":$kind, "Value":$vector,
                    CArg<"::mlir::arith::FastMathFlags",
                         "::mlir::arith::FastMathFlags::none">:$fastMathFlags)>
  ];

  let assemblyFormat = "$kind `,` $vector (`,` $acc^)? (`fastmath` `` $fastmath^)?"
                       " attr-dict `:` type($vector) `into` type($dest)";
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

def Vector_MultiDimReductionOp :
  Vector_Op<"multi_reduction", [Pure,
     AllTypesMatch<["dest", "acc"]>,
     PredOpTrait<"source operand and result have same element type",
                 TCresVTEtIsSameAsOpBase<0, 0>>,
     DeclareOpInterfaceMethods<InferTypeOpInterface>,
     DeclareOpInterfaceMethods<MaskableOpInterface>,
     DeclareOpInterfaceMethods<VectorUnrollOpInterface,
                               ["getShapeForUnroll"]>]>,
    Arguments<(ins Vector_CombiningKindAttr:$kind,
                   AnyVectorOfNonZeroRank:$source,
                   AnyType:$acc,
                   DenseI64ArrayAttr:$reduction_dims)>,
    Results<(outs AnyType:$dest)> {
  let summary = "Multi-dimensional reduction operation";
  let description = [{
    Reduces an n-D vector into an (n-k)-D vector (or a scalar when k == n)
    using the given operation: `add`/`mul`/`minsi`/`minui`/`maxsi`/`maxui`
    /`and`/`or`/`xor` for integers, and `add`/`mul`/`minnumf`/`maxnumf`/`minimumf`
    /`maximumf` for floats.
    Takes an initial accumulator operand.

    Example:

    ```mlir
    %1 = vector.multi_reduction <add>, %0, %acc0 [1, 3] :
      vector<4x8x16x32xf32> to vector<4x16xf32>
    %2 = vector.multi_reduction <add>, %1, %acc1 [0, 1] :
      vector<4x16xf32> to f32
    ```
  }];
  let builders = [
    OpBuilder<(ins "Value":$source, "Value":$acc,
                   "ArrayRef<bool>":$reductionMask, "CombiningKind":$kind)>
  ];
  let extraClassDeclaration = [{
    VectorType getSourceVectorType() {
      return ::llvm::cast<VectorType>(getSource().getType());
    }
    Type getDestType() {
      return getDest().getType();
    }

    bool isReducedDim(int64_t d) {
      assert(d >= 0 && d < static_cast<int64_t>(getReductionMask().size()) &&
        "d overflows the number of dims");
      return getReductionMask()[d];
    }

    SmallVector<bool> getReductionMask() {
      SmallVector<bool> res(getSourceVectorType().getRank(), false);
      for (int64_t dim : getReductionDims())
        res[dim] = true;
      return res;
    }
    static SmallVector<bool> getReductionMask(
        ArrayRef<int64_t> reductionDims, unsigned sourceRank) {
      SmallVector<bool> res(sourceRank, false);
      for (auto idx : reductionDims)
        res[idx] = true;
      return res;
    }
  }];
  let assemblyFormat =
    "$kind `,` $source `,` $acc attr-dict $reduction_dims `:` type($source) `to` type($dest)";
  let hasFolder = 1;
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

def Vector_BroadcastOp :
  Vector_Op<"broadcast", [Pure,
     DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>,
     PredOpTrait<"source operand and result have same element type",
                 TCresVTEtIsSameAsOpBase<0, 0>>]>,
    Arguments<(ins AnyType:$source)>,
    Results<(outs AnyVectorOfAnyRank:$vector)> {
  let summary = "broadcast operation";
  let description = [{
    Broadcasts the scalar or k-D vector value in the source operand
    to a n-D result vector such that the broadcast makes sense, i.e.,
    the source operand is duplicated to match the given rank and sizes
    in the result vector. The legality rules are:
    * the source operand must have the same element type as the result type
    * a k-D vector <s_1 x .. x s_k x type> can be broadcast to
      a n-D vector <t_1 x .. x t_n x type> if
       * k <= n, and
       * the sizes in the trailing dimensions n-k < i <= n with j=i+k-n
          match exactly as s_j = t_i or s_j = 1:
       ```
           t_1 x   ..  t_n-k x t_n-k+1 x .. x t_i x .. x t_n
                               s_1     x .. x s_j x .. x s_k
               <duplication>         <potential stretch>
       ```
       * in addition, any scalable unit dimension, `[1]`, must match exactly.

    The source operand is duplicated over all the missing leading dimensions
    and stretched over the trailing dimensions where the source has a non-equal
    dimension of 1. These rules imply that any scalar broadcast (k=0) to any
    shaped vector with the same element type is always legal.

    Example:

    ```mlir
    %0 = arith.constant 0.0 : f32
    %1 = vector.broadcast %0 : f32 to vector<16xf32>
    %2 = vector.broadcast %1 : vector<16xf32> to vector<4x16xf32>
    ```
  }];
  let extraClassDeclaration = [{
    Type getSourceType() { return getSource().getType(); }
    VectorType getResultVectorType() {
      return ::llvm::cast<VectorType>(getVector().getType());
    }

    /// Return the dimensions of the result vector that were formerly ones in the
    /// source tensor and thus correspond to "dim-1" broadcasting.
    llvm::SetVector<int64_t> computeBroadcastedUnitDims();

    /// Broadcast `value` to a vector of `dstShape`, knowing that exactly the
    /// `broadcastedDims` dimensions in the dstShape are broadcasted.
    /// This requires (and asserts) that the broadcast is free of dim-1
    /// broadcasting.
    /// Since vector.broadcast only allows expanding leading dimensions, an extra
    /// vector.transpose may be inserted to make the broadcast possible.
    /// `value`, `dstShape` and `broadcastedDims` must be properly specified or
    /// the helper will assert. This means:
    ///   1. `dstShape` must not be empty.
    ///   2. `broadcastedDims` must be confined to [0 .. rank(value.getResultVectorType)]
    ///   2. `dstShape` trimmed of the dimensions specified in `broadcastedDims`
    //       must match the `value` shape.
    static Value createOrFoldBroadcastOp(
      OpBuilder &b, Value value,
      ArrayRef<int64_t> dstShape,
      const llvm::SetVector<int64_t> &broadcastedDims);
  }];
  let assemblyFormat = "$source attr-dict `:` type($source) `to` type($vector)";
  let hasFolder = 1;
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

def Vector_ShuffleOp
    : Vector_Op<
          "shuffle",
          [Pure,
           PredOpTrait<"first operand v1 and result have same element type",
                       TCresVTEtIsSameAsOpBase<0, 0>>,
           PredOpTrait<"second operand v2 and result have same element type",
                       TCresVTEtIsSameAsOpBase<0, 1>>,
           InferTypeOpAdaptor]>,
      Arguments<(ins AnyFixedVectorOfAnyRank:$v1, AnyFixedVectorOfAnyRank:$v2,
          DenseI64ArrayAttr:$mask)>,
      Results<(outs AnyVectorOfNonZeroRank:$vector)> {
  let summary = "shuffle operation";
  let description = [{
    The shuffle operation constructs a permutation (or duplication) of elements
    from two input vectors, returning a vector with the same element type as
    the input and a length that is the same as the shuffle mask. The two input
    vectors must have the same element type, same rank , and trailing dimension
    sizes and shuffles their values in the
    leading dimension (which may differ in size) according to the given mask.
    The legality rules are:
    * the two operands must have the same element type as the result
      - Either, the two operands and the result must have the same
        rank and trailing dimension sizes, viz. given two k-D operands
                v1 : <s_1 x s_2 x .. x s_k x type> and
                v2 : <t_1 x t_2 x .. x t_k x type>
        we have s_i = t_i for all 1 < i <= k
      - Or, the two operands must be 0-D vectors and the result is a 1-D vector.
    * the mask length equals the leading dimension size of the result
    * numbering the input vector indices left to right across the operands, all
      mask values must be within range, viz. given two k-D operands v1 and v2
      above, all mask values are in the range [0,s_1+t_1)

    Note, scalable vectors are not supported.

    Example:

    ```mlir
    %0 = vector.shuffle %a, %b[0, 3]
               : vector<2xf32>, vector<2xf32>       ; yields vector<2xf32>
    %1 = vector.shuffle %c, %b[0, 1, 2]
               : vector<2x16xf32>, vector<1x16xf32> ; yields vector<3x16xf32>
    %2 = vector.shuffle %a, %b[3, 2, 1, 0]
               : vector<2xf32>, vector<2xf32>       ; yields vector<4xf32>
    %3 = vector.shuffle %a, %b[0, 1]
               : vector<f32>, vector<f32>           ; yields vector<2xf32>
    ```
  }];

  let extraClassDeclaration = [{
    VectorType getV1VectorType() {
      return ::llvm::cast<VectorType>(getV1().getType());
    }
    VectorType getV2VectorType() {
      return ::llvm::cast<VectorType>(getV2().getType());
    }
    VectorType getResultVectorType() {
      return ::llvm::cast<VectorType>(getVector().getType());
    }
  }];

  let assemblyFormat = "operands $mask attr-dict `:` type(operands)";

  let hasFolder = 1;
  let hasVerifier = 1;
  let hasCanonicalizer = 1;
}

def ResultIsDoubleSourceVectorType : TypesMatchWith<
    "type of 'result' is double the width of the inputs",
    "lhs", "result",
    [{
      [&]() -> ::mlir::VectorType {
        auto vectorType = ::llvm::cast<::mlir::VectorType>($_self);
        ::mlir::VectorType::Builder builder(vectorType);
        if (vectorType.getRank() == 0) {
          static constexpr int64_t v2xTyShape[] = {2};
          return builder.setShape(v2xTyShape);
        }
        auto lastDim = vectorType.getRank() - 1;
        return builder.setDim(lastDim, vectorType.getDimSize(lastDim) * 2);
      }()
    }]>;

def Vector_InterleaveOp :
  Vector_Op<"interleave", [Pure, AllTypesMatch<["lhs", "rhs"]>,
    ResultIsDoubleSourceVectorType]> {
  let summary = "constructs a vector by interleaving two input vectors";
  let description = [{
    The interleave operation constructs a new vector by interleaving the
    elements from the trailing (or final) dimension of two input vectors,
    returning a new vector where the trailing dimension is twice the size.

    Note that for the n-D case this differs from the interleaving possible with
    `vector.shuffle`, which would only operate on the leading dimension.

    Another key difference is this operation supports scalable vectors, though
    currently a general LLVM lowering is limited to the case where only the
    trailing dimension is scalable.

    Example:
    ```mlir
    %a = arith.constant dense<[0, 1]> : vector<2xi32>
    %b = arith.constant dense<[2, 3]> : vector<2xi32>
    // The value of `%0` is `[0, 2, 1, 3]`.
    %0 = vector.interleave %a, %b : vector<2xi32> -> vector<4xi32>

    // Examples showing allowed input and result types.
    %1 = vector.interleave %c, %d : vector<f16> -> vector<2xf16>
    %2 = vector.interleave %e, %f : vector<6x3xf32> -> vector<6x6xf32>
    %3 = vector.interleave %g, %h : vector<[4]xi32> -> vector<[8]xi32>
    %4 = vector.interleave %i, %j : vector<2x4x[2]xf64> -> vector<2x4x[4]xf64>
    ```
  }];

  let arguments = (ins AnyVectorOfAnyRank:$lhs, AnyVectorOfAnyRank:$rhs);
  let results = (outs AnyVectorOfNonZeroRank:$result);

  let assemblyFormat = [{
    $lhs `,` $rhs  attr-dict `:` type($lhs) `->` type($result)
  }];

  let extraClassDeclaration = [{
    VectorType getSourceVectorType() {
      return ::llvm::cast<VectorType>(getLhs().getType());
    }
    VectorType getResultVectorType() {
      return ::llvm::cast<VectorType>(getResult().getType());
    }
  }];
}

class ResultIsHalfSourceVectorType<string result> : TypesMatchWith<
  "the trailing dimension of the results is half the width of source trailing dimension",
  "source", result,
  [{
    [&]() -> ::mlir::VectorType {
      auto vectorType = ::llvm::cast<mlir::VectorType>($_self);
      ::mlir::VectorType::Builder builder(vectorType);
      auto lastDim = vectorType.getRank() - 1;
      auto newDimSize = vectorType.getDimSize(lastDim) / 2;;
      if (newDimSize <= 0)
         return vectorType; // (invalid input type)
      return builder.setDim(lastDim, newDimSize);
    }()
  }]
>;

def SourceVectorEvenElementCount : PredOpTrait<
  "the trailing dimension of the source vector has an even number of elements",
  CPred<[{
    [&](){
      auto srcVec = getSourceVectorType();
      return srcVec.getDimSize(srcVec.getRank() - 1) % 2 == 0;
    }()
  }]>
>;

def Vector_DeinterleaveOp :
  Vector_Op<"deinterleave", [Pure,
    SourceVectorEvenElementCount,
    ResultIsHalfSourceVectorType<"res1">,
    AllTypesMatch<["res1", "res2"]>
    ]> {
      let summary = "constructs two vectors by deinterleaving an input vector";
      let description = [{
        The deinterleave operation constructs two vectors from a single input
        vector. The first result vector contains the elements from even indexes
        of the input, and the second contains elements from odd indexes. This is
        the inverse of a `vector.interleave` operation.

        Each output's trailing dimension is half of the size of the input
        vector's trailing dimension. This operation requires the input vector
        to have a rank > 0 and an even number of elements in its trailing
        dimension.

        The operation supports scalable vectors.

        Example:
        ```mlir
        %0, %1 = vector.deinterleave %a
                   : vector<8xi8> -> vector<4xi8>
        %2, %3 = vector.deinterleave %b
                   : vector<2x8xi8> -> vector<2x4xi8>
        %4, %5 = vector.deinterleave %c
                   : vector<2x8x4xi8> -> vector<2x8x2xi8>
        %6, %7 = vector.deinterleave %d
                   : vector<[8]xf32> -> vector<[4]xf32>
        %8, %9 = vector.deinterleave %e
                   : vector<2x[6]xf64> -> vector<2x[3]xf64>
        %10, %11 = vector.deinterleave %f
                   : vector<2x4x[6]xf64> -> vector<2x4x[3]xf64>
        ```
      }];

      let arguments = (ins AnyVectorOfNonZeroRank:$source);
      let results = (outs AnyVectorOfNonZeroRank:$res1, AnyVectorOfNonZeroRank:$res2);

      let assemblyFormat = [{
        $source attr-dict `:` type($source) `->` type($res1)
      }];

      let extraClassDeclaration = [{
        VectorType getSourceVectorType() {
          return ::llvm::cast<VectorType>(getSource().getType());
        }
        VectorType getResultVectorType() {
          return ::llvm::cast<VectorType>(getRes1().getType());
        }
      }];
    }

def Vector_ExtractElementOp :
  Vector_Op<"extractelement", [Pure,
     DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>,
     TypesMatchWith<"result type matches element type of vector operand",
                    "vector", "result",
                    "::llvm::cast<VectorType>($_self).getElementType()">]>,
    Arguments<(ins AnyVectorOfAnyRank:$vector,
                   Optional<AnySignlessIntegerOrIndex>:$position)>,
    Results<(outs AnyType:$result)> {
  let summary = "extractelement operation";
  let description = [{
    Takes a 0-D or 1-D vector and a optional dynamic index position and
    extracts the scalar at that position.

    Note that this instruction resembles vector.extract, but is restricted to
    0-D and 1-D vectors and relaxed to dynamic indices.
    If the vector is 0-D, the position must be std::nullopt.


    It is meant to be closer to LLVM's version:
    https://llvm.org/docs/LangRef.html#extractelement-instruction

    Example:

    ```mlir
    %c = arith.constant 15 : i32
    %1 = vector.extractelement %0[%c : i32]: vector<16xf32>
    %2 = vector.extractelement %z[]: vector<f32>
    ```
  }];
  let assemblyFormat = [{
    $vector `[` ($position^ `:` type($position))? `]` attr-dict `:` type($vector)
  }];

  let builders = [
    // 0-D builder.
    OpBuilder<(ins "Value":$source)>,
  ];
  let extraClassDeclaration = [{
    VectorType getSourceVectorType() {
      return ::llvm::cast<VectorType>(getVector().getType());
    }
  }];
  let hasVerifier = 1;
  let hasFolder = 1;
}

def Vector_ExtractOp :
  Vector_Op<"extract", [Pure,
     DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>,
     PredOpTrait<"operand and result have same element type",
                 TCresVTEtIsSameAsOpBase<0, 0>>,
     InferTypeOpAdaptorWithIsCompatible]> {
  let summary = "extract operation";
  let description = [{
    Takes an n-D vector and a k-D position and extracts the (n-k)-D vector at
    the proper position. Degenerates to an element type if n-k is zero.

    Dynamic indices must be greater or equal to zero and less than the size of
    the corresponding dimension. The result is undefined if any index is
    out-of-bounds.

    Example:

    ```mlir
    %1 = vector.extract %0[3]: vector<8x16xf32> from vector<4x8x16xf32>
    %2 = vector.extract %0[2, 1, 3]: f32 from vector<4x8x16xf32>
    %3 = vector.extract %1[]: vector<f32> from vector<f32>
    %4 = vector.extract %0[%a, %b, %c]: f32 from vector<4x8x16xf32>
    %5 = vector.extract %0[2, %b]: vector<16xf32> from vector<4x8x16xf32>
    ```
  }];

  let arguments = (ins
    AnyVectorOfAnyRank:$vector,
    Variadic<Index>:$dynamic_position,
    DenseI64ArrayAttr:$static_position
  );
  let results = (outs AnyType:$result);

  let builders = [
    OpBuilder<(ins "Value":$source, "int64_t":$position)>,
    OpBuilder<(ins "Value":$source, "OpFoldResult":$position)>,
    OpBuilder<(ins "Value":$source, "ArrayRef<int64_t>":$position)>,
    OpBuilder<(ins "Value":$source, "ArrayRef<OpFoldResult>":$position)>,
  ];

  let extraClassDeclaration = [{
    VectorType getSourceVectorType() {
      return ::llvm::cast<VectorType>(getVector().getType());
    }

    /// Return a vector with all the static and dynamic position indices.
    SmallVector<OpFoldResult> getMixedPosition() {
      OpBuilder builder(getContext());
      return getMixedValues(getStaticPosition(), getDynamicPosition(), builder);
    }

    unsigned getNumIndices() {
      return getStaticPosition().size();
    }

    /// Return "true" if the op has at least one dynamic position.
    bool hasDynamicPosition() {
      return !getDynamicPosition().empty();
    }
  }];

  let assemblyFormat = [{
    $vector ``
    custom<DynamicIndexList>($dynamic_position, $static_position)
    attr-dict `:` type($result) `from` type($vector)
  }];

  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

def Vector_FMAOp :
  Op<Vector_Dialect, "fma", [
       Pure, AllTypesMatch<["lhs", "rhs", "acc", "result"]>,
       DeclareOpInterfaceMethods<VectorUnrollOpInterface, ["getShapeForUnroll"]>
     ] # ElementwiseMappable.traits>,
    Arguments<(ins VectorOfAnyRankOf<[AnyFloat]>:$lhs,
                   VectorOfAnyRankOf<[AnyFloat]>:$rhs,
                   VectorOfAnyRankOf<[AnyFloat]>:$acc)>,
    Results<(outs VectorOfAnyRankOf<[AnyFloat]>:$result)> {
  let summary = "vector fused multiply-add";
  let description = [{
    Multiply-add expressions operate on n-D vectors and compute a fused
    pointwise multiply-and-accumulate: `$result = $lhs * $rhs + $acc`.
    All operands and result have the same vector type. The semantics
    of the operation correspond to those of the `llvm.fma`
    [intrinsic](https://llvm.org/docs/LangRef.html#int-fma). In the
    particular case of lowering to LLVM, this is guaranteed to lower
    to the `llvm.fma.*` intrinsic.

    Example:

    ```mlir
    %3 = vector.fma %0, %1, %2: vector<8x16xf32>
    ```
  }];
  let assemblyFormat = "$lhs `,` $rhs `,` $acc attr-dict `:` type($lhs)";
  let extraClassDeclaration = [{
    VectorType getVectorType() { return ::llvm::cast<VectorType>(getLhs().getType()); }
  }];
}

def Vector_FromElementsOp : Vector_Op<"from_elements", [
    Pure,
    TypesMatchWith<"operand types match result element type",
                   "result", "elements", "SmallVector<Type>("
                   "::llvm::cast<VectorType>($_self).getNumElements(), "
                   "::llvm::cast<VectorType>($_self).getElementType())">]> {
  let summary = "operation that defines a vector from scalar elements";
  let description = [{
    This operation defines a vector from one or multiple scalar elements. The
    number of elements must match the number of elements in the result type.
    All elements must have the same type, which must match the element type of
    the result vector type.

    `elements` are a flattened version of the result vector in row-major order.

    Example:

    ```mlir
    // %f1
    %0 = vector.from_elements %f1 : vector<f32>
    // [%f1, %f2]
    %1 = vector.from_elements %f1, %f2 : vector<2xf32>
    // [[%f1, %f2, %f3], [%f4, %f5, %f6]]
    %2 = vector.from_elements %f1, %f2, %f3, %f4, %f5, %f6 : vector<2x3xf32>
    // [[[%f1, %f2]], [[%f3, %f4]], [[%f5, %f6]]]
    %3 = vector.from_elements %f1, %f2, %f3, %f4, %f5, %f6 : vector<3x1x2xf32>
    ```

    Note, scalable vectors are not supported.
  }];

  let arguments = (ins Variadic<AnyType>:$elements);
  let results = (outs AnyFixedVectorOfAnyRank:$result);
  let assemblyFormat = "$elements attr-dict `:` type($result)";
  let hasCanonicalizer = 1;
}

def Vector_InsertElementOp :
  Vector_Op<"insertelement", [Pure,
     DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>,
     TypesMatchWith<"source operand type matches element type of result",
                    "result", "source",
                    "::llvm::cast<VectorType>($_self).getElementType()">,
     AllTypesMatch<["dest", "result"]>]>,
     Arguments<(ins AnyType:$source, AnyVectorOfAnyRank:$dest,
                    Optional<AnySignlessIntegerOrIndex>:$position)>,
     Results<(outs AnyVectorOfAnyRank:$result)> {
  let summary = "insertelement operation";
  let description = [{
    Takes a scalar source, a 0-D or 1-D destination vector and a dynamic index
    position and inserts the source into the destination at the proper position.

    Note that this instruction resembles vector.insert, but is restricted to 0-D
    and 1-D vectors and relaxed to dynamic indices.

    It is meant to be closer to LLVM's version:
    https://llvm.org/docs/LangRef.html#insertelement-instruction

    Example:

    ```mlir
    %c = arith.constant 15 : i32
    %f = arith.constant 0.0f : f32
    %1 = vector.insertelement %f, %0[%c : i32]: vector<16xf32>
    %2 = vector.insertelement %f, %z[]: vector<f32>
    ```
  }];
  let assemblyFormat = [{
    $source `,` $dest `[` ($position^ `:` type($position))? `]`  attr-dict `:`
    type($result)
  }];

  let builders = [
    // 0-D builder.
    OpBuilder<(ins "Value":$source, "Value":$dest)>,
  ];
  let extraClassDeclaration = [{
    Type getSourceType() { return getSource().getType(); }
    VectorType getDestVectorType() {
      return ::llvm::cast<VectorType>(getDest().getType());
    }
  }];
  let hasVerifier = 1;
  let hasFolder = 1;
}

def Vector_InsertOp :
  Vector_Op<"insert", [Pure,
     DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>,
     PredOpTrait<"source operand and result have same element type",
                 TCresVTEtIsSameAsOpBase<0, 0>>,
     AllTypesMatch<["dest", "result"]>]> {
  let summary = "insert operation";
  let description = [{
    Takes an n-D source vector, an (n+k)-D destination vector and a k-D position
    and inserts the n-D source into the (n+k)-D destination at the proper
    position. Degenerates to a scalar or a 0-d vector source type when n = 0.

    Dynamic indices must be greater or equal to zero and less than the size of
    the corresponding dimension. The result is undefined if any index is
    out-of-bounds.

    Example:

    ```mlir
    %2 = vector.insert %0, %1[3] : vector<8x16xf32> into vector<4x8x16xf32>
    %5 = vector.insert %3, %4[2, 1, 3] : f32 into vector<4x8x16xf32>
    %8 = vector.insert %6, %7[] : f32 into vector<f32>
    %11 = vector.insert %9, %10[%a, %b, %c] : vector<f32> into vector<4x8x16xf32>
    %12 = vector.insert %4, %10[2, %b] : vector<16xf32> into vector<4x8x16xf32>
    ```
  }];

  let arguments = (ins
    AnyType:$source,
    AnyVectorOfAnyRank:$dest,
    Variadic<Index>:$dynamic_position,
    DenseI64ArrayAttr:$static_position
  );
  let results = (outs AnyVectorOfAnyRank:$result);

  let builders = [
    OpBuilder<(ins "Value":$source, "Value":$dest, "int64_t":$position)>,
    OpBuilder<(ins "Value":$source, "Value":$dest, "OpFoldResult":$position)>,
    OpBuilder<(ins "Value":$source, "Value":$dest, "ArrayRef<int64_t>":$position)>,
    OpBuilder<(ins "Value":$source, "Value":$dest, "ArrayRef<OpFoldResult>":$position)>,
  ];

  let extraClassDeclaration = [{
    Type getSourceType() { return getSource().getType(); }
    VectorType getDestVectorType() {
      return ::llvm::cast<VectorType>(getDest().getType());
    }

    /// Return a vector with all the static and dynamic position indices.
    SmallVector<OpFoldResult> getMixedPosition() {
      OpBuilder builder(getContext());
      return getMixedValues(getStaticPosition(), getDynamicPosition(), builder);
    }

    unsigned getNumIndices() {
      return getStaticPosition().size();
    }

    bool hasDynamicPosition() {
      return llvm::any_of(getDynamicPosition(),
                          [](Value operand) { return operand != nullptr; });
    }
  }];

  let assemblyFormat = [{
    $source `,` $dest custom<DynamicIndexList>($dynamic_position, $static_position)
    attr-dict `:` type($source) `into` type($dest)
  }];

  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

def Vector_ScalableInsertOp :
  Vector_Op<"scalable.insert", [Pure,
       AllElementTypesMatch<["source", "dest"]>,
       AllTypesMatch<["dest", "res"]>,
       PredOpTrait<"position is a multiple of the source length.",
        CPred<
          "(getPos() % getSourceVectorType().getNumElements()) == 0"
        >>]>,
     Arguments<(ins VectorOfRank<[1]>:$source,
                    ScalableVectorOfRank<[1]>:$dest,
                    I64Attr:$pos)>,
     Results<(outs ScalableVectorOfRank<[1]>:$res)> {
  let summary = "insert subvector into scalable vector operation";
  // NOTE: This operation is designed to map to `llvm.vector.insert`, and its
  //       documentation should be kept aligned with LLVM IR:
  //       https://llvm.org/docs/LangRef.html#llvm-vector-insert-intrinsic
  let description = [{
    This operations takes a rank-1 fixed-length or scalable subvector and
    inserts it within the destination scalable vector starting from the
    position specificed by `pos`. If the source vector is scalable, the
    insertion position will be scaled by the runtime scaling factor of the
    source subvector.

    The insertion position must be a multiple of the minimum size of the source
    vector. For the operation to be well defined, the source vector must fit in
    the destination vector from the specified position. Since the destination
    vector is scalable and its runtime length is unknown, the validity of the
    operation can't be verified nor guaranteed at compile time.

    Example:

    ```mlir
    %2 = vector.scalable.insert %0, %1[8] : vector<4xf32> into vector<[16]xf32>
    %5 = vector.scalable.insert %3, %4[0] : vector<8xf32> into vector<[4]xf32>
    %8 = vector.scalable.insert %6, %7[0] : vector<[4]xf32> into vector<[8]xf32>
    ```

    Invalid example:
    ```mlir
    %2 = vector.scalable.insert %0, %1[5] : vector<4xf32> into vector<[16]xf32>
    ```
  }];

  let assemblyFormat = [{
    $source `,` $dest `[` $pos `]` attr-dict `:` type($source) `into` type($dest)
  }];

  let extraClassDeclaration = [{
    VectorType getSourceVectorType() {
      return ::llvm::cast<VectorType>(getSource().getType());
    }
    VectorType getDestVectorType() {
      return ::llvm::cast<VectorType>(getDest().getType());
    }
  }];
}

def Vector_ScalableExtractOp :
  Vector_Op<"scalable.extract", [Pure,
       AllElementTypesMatch<["source", "res"]>,
       PredOpTrait<"position is a multiple of the result length.",
        CPred<
          "(getPos() % getResultVectorType().getNumElements()) == 0"
        >>]>,
     Arguments<(ins ScalableVectorOfRank<[1]>:$source,
                    I64Attr:$pos)>,
     Results<(outs VectorOfRank<[1]>:$res)> {
  let summary = "extract subvector from scalable vector operation";
  // NOTE: This operation is designed to map to `llvm.vector.extract`, and its
  //       documentation should be kept aligned with LLVM IR:
  //       https://llvm.org/docs/LangRef.html#llvm-vector-extract-intrinsic
  let description = [{
    Takes rank-1 source vector and a position `pos` within the source
    vector, and extracts a subvector starting from that position.

    The extraction position must be a multiple of the minimum size of the result
    vector. For the operation to be well defined, the destination vector must
    fit within the source vector from the specified position. Since the source
    vector is scalable and its runtime length is unknown, the validity of the
    operation can't be verified nor guaranteed at compile time.

    Example:

    ```mlir
    %1 = vector.scalable.extract %0[8] : vector<4xf32> from vector<[8]xf32>
    %3 = vector.scalable.extract %2[0] : vector<[4]xf32> from vector<[8]xf32>
    ```

    Invalid example:
    ```mlir
    %1 = vector.scalable.extract %0[5] : vector<4xf32> from vector<[16]xf32>
    ```
  }];

  let assemblyFormat = [{
    $source `[` $pos `]` attr-dict `:` type($res) `from` type($source)
  }];

  let extraClassDeclaration = [{
    VectorType getSourceVectorType() {
      return ::llvm::cast<VectorType>(getSource().getType());
    }
    VectorType getResultVectorType() {
      return ::llvm::cast<VectorType>(getRes().getType());
    }
  }];
}

def Vector_InsertStridedSliceOp :
  Vector_Op<"insert_strided_slice", [Pure,
    PredOpTrait<"operand #0 and result have same element type",
                 TCresVTEtIsSameAsOpBase<0, 0>>,
    AllTypesMatch<["dest", "res"]>]>,
    Arguments<(ins AnyVectorOfNonZeroRank:$source, AnyVectorOfNonZeroRank:$dest, I64ArrayAttr:$offsets,
               I64ArrayAttr:$strides)>,
    Results<(outs AnyVectorOfNonZeroRank:$res)> {
  let summary = "strided_slice operation";
  let description = [{
    Takes a k-D source vector, an n-D destination vector (n >= k), n-sized
    `offsets` integer array attribute, a k-sized `strides` integer array attribute
    and inserts the k-D source vector as a strided subvector at the proper offset
    into the n-D destination vector.

    At the moment strides must contain only 1s.

    Returns an n-D vector that is a copy of the n-D destination vector in which
    the last k-D dimensions contain the k-D source vector elements strided at
    the proper location as specified by the offsets.

    Example:

    ```mlir
    %2 = vector.insert_strided_slice %0, %1
        {offsets = [0, 0, 2], strides = [1, 1]}:
      vector<2x4xf32> into vector<16x4x8xf32>
    ```
  }];

  let assemblyFormat = [{
    $source `,` $dest attr-dict `:` type($source) `into` type($dest)
  }];

  let builders = [
    OpBuilder<(ins "Value":$source, "Value":$dest,
      "ArrayRef<int64_t>":$offsets, "ArrayRef<int64_t>":$strides)>
  ];
  let extraClassDeclaration = [{
    VectorType getSourceVectorType() {
      return ::llvm::cast<VectorType>(getSource().getType());
    }
    VectorType getDestVectorType() {
      return ::llvm::cast<VectorType>(getDest().getType());
    }
    bool hasNonUnitStrides() {
      return llvm::any_of(getStrides(), [](Attribute attr) {
        return ::llvm::cast<IntegerAttr>(attr).getInt() != 1;
      });
    }
  }];

  let hasFolder = 1;
  let hasVerifier = 1;
  let hasCanonicalizer = 1;
}

def Vector_OuterProductOp :
  Vector_Op<"outerproduct", [Pure,
    PredOpTrait<"lhs operand and result have same element type",
                TCresVTEtIsSameAsOpBase<0, 0>>,
    PredOpTrait<"rhs operand and result have same element type",
                TCresVTEtIsSameAsOpBase<0, 1>>,
    DeclareOpInterfaceMethods<MaskableOpInterface>]>,
    Arguments<(ins AnyVectorOfNonZeroRank:$lhs, AnyType:$rhs,
               Optional<AnyVectorOfNonZeroRank>:$acc,
               DefaultValuedAttr<Vector_CombiningKindAttr, "CombiningKind::ADD">:$kind)>,
    Results<(outs AnyVectorOfNonZeroRank)> {
  let summary = "vector outerproduct with optional fused add";
  let description = [{
    Takes 2 1-D vectors and returns the 2-D vector containing the outer-product,
    as illustrated below:
    ```
     outer |   [c, d]
     ------+------------
       [a, | [ [a*c, a*d],
        b] |   [b*c, b*d] ]
    ```
    This operation also accepts a 1-D vector lhs and a scalar rhs. In this
    case a simple AXPY operation is performed, which returns a 1-D vector.
    ```
        [a, b] * c = [a*c, b*c]
    ```

    An optional extra vector argument with the same shape as the output
    vector may be specified in which case the operation returns the sum of
    the outer-product and the extra vector. In this multiply-accumulate
    scenario for floating-point arguments, the rounding mode is enforced
    by guaranteeing that a fused-multiply add operation is emitted. When
    lowered to the LLVMIR dialect, this form emits `llvm.intr.fma`, which
    is guaranteed to lower to actual `fma` instructions on x86.

    An optional kind attribute may be specified to be: `add`/`mul`/`minsi`
    /`minui`/`maxsi`/`maxui`/`and`/`or`/`xor` for integers, and `add`/`mul`
    /`minnumf`/`maxnumf`/`minimumf`/`maximumf` for floats. The default is
    `add`.

    Example:

    ```
    %2 = vector.outerproduct %0, %1: vector<4xf32>, vector<8xf32>
    return %2: vector<4x8xf32>

    %3 = vector.outerproduct %0, %1, %2:
      vector<4xf32>, vector<8xf32>, vector<4x8xf32>
    return %3: vector<4x8xf32>

    %4 = vector.outerproduct %0, %1, %2 {kind = #vector.kind<maxnumf>}:
      vector<4xf32>, vector<8xf32>, vector<4x8xf32>
    return %3: vector<4x8xf32>

    %6 = vector.outerproduct %4, %5: vector<10xf32>, f32
    return %6: vector<10xf32>

    ```
  }];
  let builders = [
    // Build an op without mask, use the type of `acc` as the return type.
    OpBuilder<(ins "Value":$lhs, "Value":$rhs, "Value":$acc)>
  ];
  let extraClassDeclaration = [{
    VectorType getOperandVectorTypeLHS() {
      return ::llvm::cast<VectorType>(getLhs().getType());
    }
    Type getOperandTypeRHS() {
      return getRhs().getType();
    }
    VectorType getOperandVectorTypeACC() {
      return getAcc()
        ? ::llvm::cast<VectorType>(getAcc().getType())
        : VectorType();
    }
    VectorType getResultVectorType() {
      return ::llvm::cast<VectorType>(getResult().getType());
    }
    static CombiningKind getDefaultKind() {
      return CombiningKind::ADD;
    }
  }];
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

def Vector_ExtractStridedSliceOp :
  Vector_Op<"extract_strided_slice", [Pure,
    PredOpTrait<"operand and result have same element type",
                 TCresVTEtIsSameAsOpBase<0, 0>>]>,
    Arguments<(ins AnyVectorOfNonZeroRank:$vector, I64ArrayAttr:$offsets,
               I64ArrayAttr:$sizes, I64ArrayAttr:$strides)>,
    Results<(outs AnyVectorOfNonZeroRank)> {
  let summary = "extract_strided_slice operation";
  let description = [{
    Takes an n-D vector, k-D `offsets` integer array attribute, a k-sized
    `sizes` integer array attribute, a k-sized `strides` integer array
    attribute and extracts the n-D subvector at the proper offset.

    At the moment strides must contain only 1s.

    Returns an n-D vector where the first k-D dimensions match the `sizes`
    attribute. The returned subvector contains the elements starting at offset
    `offsets` and ending at `offsets + sizes`.

    Example:

    ```mlir
    %1 = vector.extract_strided_slice %0
        {offsets = [0, 2], sizes = [2, 4], strides = [1, 1]}:
      vector<4x8x16xf32> to vector<2x4x16xf32>

    // TODO: Evolve to a range form syntax similar to:
    %1 = vector.extract_strided_slice %0[0:2:1][2:4:1]
      vector<4x8x16xf32> to vector<2x4x16xf32>
    ```
  }];
  let builders = [
    OpBuilder<(ins "Value":$source, "ArrayRef<int64_t>":$offsets,
      "ArrayRef<int64_t>":$sizes, "ArrayRef<int64_t>":$strides)>
  ];
  let extraClassDeclaration = [{
    VectorType getSourceVectorType() {
      return ::llvm::cast<VectorType>(getVector().getType());
    }
    void getOffsets(SmallVectorImpl<int64_t> &results);
    bool hasNonUnitStrides() {
      return llvm::any_of(getStrides(), [](Attribute attr) {
        return ::llvm::cast<IntegerAttr>(attr).getInt() != 1;
      });
    }
  }];
  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
  let assemblyFormat = "$vector attr-dict `:` type($vector) `to` type(results)";
}

// TODO: Tighten semantics so that masks and inbounds can't be used
// simultaneously within the same transfer op.
def Vector_TransferReadOp :
  Vector_Op<"transfer_read", [
      DeclareOpInterfaceMethods<VectorTransferOpInterface>,
      DeclareOpInterfaceMethods<VectorUnrollOpInterface, ["getShapeForUnroll"]>,
      DeclareOpInterfaceMethods<MaskableOpInterface>,
      DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
      DeclareOpInterfaceMethods<ConditionallySpeculatable>,
      AttrSizedOperandSegments,
      DestinationStyleOpInterface
    ]>,
    Arguments<(ins AnyShaped:$source,
                   Variadic<Index>:$indices,
                   AffineMapAttr:$permutation_map,
                   AnyType:$padding,
                   Optional<VectorOfNonZeroRankOf<[I1]>>:$mask,
                   BoolArrayAttr:$in_bounds)>,
    Results<(outs AnyVectorOfAnyRank:$vector)> {

  let summary = "Reads a supervector from memory into an SSA vector value.";

  let description = [{
    The `vector.transfer_read` op performs a read from a slice within a
    [MemRef](../LangRef.md#memref-type) or a Ranked
    [Tensor](../LangRef.md#tensor-type) supplied as its first operand
    into a [vector](../LangRef.md#vector-type) of the same base elemental type.

    A memref/tensor operand with vector element type, must have its vector
    element type match a suffix (shape and element type) of the vector (e.g.
    memref<3x2x6x4x3xf32>, vector<1x1x4x3xf32>).

    The slice is further defined by a full-rank index within the MemRef/Tensor,
    supplied as the operands `[1 .. 1 + rank(memref/tensor))` that defines the
    starting point of the transfer (e.g. `%A[%i0, %i1, %i2]`).

    The permutation_map [attribute](../LangRef.md#attributes) is an
    [affine-map](Affine.md#affine-maps) which specifies the transposition on the
    slice to match the vector shape. The permutation map may be implicit and
    omitted from parsing and printing if it is the canonical minor identity map
    (i.e. if it does not permute or broadcast any dimension).

    The size of the slice is specified by the size of the vector, given as the
    return type.

    An SSA value `padding` of the same elemental type as the MemRef/Tensor is
    provided to specify a fallback value in the case of out-of-bounds accesses
    and/or masking.

    An optional SSA value `mask` may be specified to mask out elements read from
    the MemRef/Tensor. The `mask` type is an `i1` vector with a shape that
    matches how elements are read from the MemRef/Tensor, *before* any
    permutation or broadcasting. Elements whose corresponding mask element is
    `0` are masked out and replaced with `padding`.

    For every vector dimension, the boolean array attribute `in_bounds`
    specifies if the transfer is guaranteed to be within the source bounds. If
    set to "false", accesses (including the starting point) may run
    out-of-bounds along the respective vector dimension as the index increases.
    Non-vector dimensions *must* always be in-bounds. The `in_bounds` array
    length has to be equal to the vector rank. This attribute has a default
    value: `false` (i.e. "out-of-bounds"). When skipped in the textual IR, the
    default value is assumed. Similarly, the OP printer will omit this
    attribute when all dimensions are out-of-bounds (i.e. the default value is
    used).

    A `vector.transfer_read` can be lowered to a simple load if all dimensions
    are specified to be within bounds and no `mask` was specified.

    This operation is called 'read' by opposition to 'load' because the
    super-vector granularity is generally not representable with a single
    hardware register. A `vector.transfer_read` is thus a mid-level abstraction
    that supports super-vectorization with non-effecting padding for full-tile
    only operations.

    More precisely, let's dive deeper into the permutation_map for the following
    MLIR:

    ```mlir
    vector.transfer_read %A[%expr1, %expr2, %expr3, %expr4]
      { permutation_map : (d0,d1,d2,d3) -> (d2,0,d0) } :
      memref<?x?x?x?xf32>, vector<3x4x5xf32>
    ```

    This operation always reads a slice starting at `%A[%expr1, %expr2, %expr3,
    %expr4]`. The size of the slice can be inferred from the resulting vector
    shape and walking back through the permutation map: 3 along d2 and 5 along
    d0, so the slice is: `%A[%expr1 : %expr1 + 5, %expr2, %expr3:%expr3 + 3, %expr4]`

    That slice needs to be read into a `vector<3x4x5xf32>`. Since the
    permutation map is not full rank, there must be a broadcast along vector
    dimension `1`.

    A notional lowering of vector.transfer_read could generate code resembling:

    ```mlir
    // %expr1, %expr2, %expr3, %expr4 defined before this point
    // alloc a temporary buffer for performing the "gather" of the slice.
    %tmp = memref.alloc() : memref<vector<3x4x5xf32>>
    for %i = 0 to 3 {
      affine.for %j = 0 to 4 {
        affine.for %k = 0 to 5 {
          // Note that this load does not involve %j.
          %a = load %A[%expr1 + %k, %expr2, %expr3 + %i, %expr4] : memref<?x?x?x?xf32>
          // Update the temporary gathered slice with the individual element
          %slice = memref.load %tmp : memref<vector<3x4x5xf32>> -> vector<3x4x5xf32>
          %updated = vector.insert %a, %slice[%i, %j, %k] : f32 into vector<3x4x5xf32>
          memref.store %updated, %tmp : memref<vector<3x4x5xf32>>
    }}}
    // At this point we gathered the elements from the original
    // memref into the desired vector layout, stored in the `%tmp` allocation.
    %vec = memref.load %tmp : memref<vector<3x4x5xf32>> -> vector<3x4x5xf32>
    ```

    On a GPU one could then map `i`, `j`, `k` to blocks and threads. Notice that
    the temporary storage footprint could conceptually be only `3 * 5` values but
    `3 * 4 * 5` values are actually transferred between `%A` and `%tmp`.

    Alternatively, if a notional vector broadcast operation were available, we
    could avoid the loop on `%j` and the lowered code would resemble:

    ```mlir
    // %expr1, %expr2, %expr3, %expr4 defined before this point
    %tmp = memref.alloc() : memref<vector<3x4x5xf32>>
    for %i = 0 to 3 {
      affine.for %k = 0 to 5 {
        %a = load %A[%expr1 + %k, %expr2, %expr3 + %i, %expr4] : memref<?x?x?x?xf32>
        %slice = memref.load %tmp : memref<vector<3x4x5xf32>> -> vector<3x4x5xf32>
        // Here we only store to the first element in dimension one
        %updated = vector.insert %a, %slice[%i, 0, %k] : f32 into vector<3x4x5xf32>
        memref.store %updated, %tmp : memref<vector<3x4x5xf32>>
    }}
    // At this point we gathered the elements from the original
    // memref into the desired vector layout, stored in the `%tmp` allocation.
    // However we haven't replicated them alongside the first dimension, we need
    // to broadcast now.
    %partialVec = load %tmp : memref<vector<3x4x5xf32>> -> vector<3x4x5xf32>
    %vec = broadcast %tmpvec, 1 : vector<3x4x5xf32>
    ```

    where `broadcast` broadcasts from element 0 to all others along the
    specified dimension. This time, the number of loaded element is `3 * 5`
    values.
    An additional `1` broadcast is required. On a GPU this broadcast could be
    implemented using a warp-shuffle if loop `j` were mapped to `threadIdx.x`.

    Syntax
    ```
    operation ::= ssa-id `=` `vector.transfer_read` ssa-use-list
      `{` attribute-entry `} :` memref-type `,` vector-type
    ```

    Example:

    ```mlir
    // Read the slice `%A[%i0, %i1:%i1+256, %i2:%i2+32]` into vector<32x256xf32>
    // and pad with %f0 to handle the boundary case:
    %f0 = arith.constant 0.0f : f32
    affine.for %i0 = 0 to %0 {
      affine.for %i1 = 0 to %1 step 256 {
        affine.for %i2 = 0 to %2 step 32 {
          %v = vector.transfer_read %A[%i0, %i1, %i2], (%f0)
               {permutation_map: (d0, d1, d2) -> (d2, d1)} :
               memref<?x?x?xf32>, vector<32x256xf32>
    }}}

    // or equivalently (rewrite with vector.transpose)
    %f0 = arith.constant 0.0f : f32
    affine.for %i0 = 0 to %0 {
      affine.for %i1 = 0 to %1 step 256 {
        affine.for %i2 = 0 to %2 step 32 {
          %v0 = vector.transfer_read %A[%i0, %i1, %i2], (%f0)
               {permutation_map: (d0, d1, d2) -> (d1, d2)} :
               memref<?x?x?xf32>, vector<256x32xf32>
          %v = vector.transpose %v0, [1, 0] :
              vector<256x32xf32> to vector<32x256f32>
    }}}

    // Read the slice `%A[%i0, %i1]` (i.e. the element `%A[%i0, %i1]`) into
    // vector<128xf32>. The underlying implementation will require a 1-D vector
    // broadcast:
    affine.for %i0 = 0 to %0 {
      affine.for %i1 = 0 to %1 {
        %3 = vector.transfer_read %A[%i0, %i1]
             {permutation_map: (d0, d1) -> (0)} :
             memref<?x?xf32>, vector<128xf32>
      }
    }

    // Read from a memref with vector element type.
    %4 = vector.transfer_read %arg1[%c3, %c3], %vf0
      {permutation_map = (d0, d1)->(d0, d1)}
        : memref<?x?xvector<4x3xf32>>, vector<1x1x4x3xf32>

    // Read from a tensor with vector element type.
    %4 = vector.transfer_read %arg1[%c3, %c3], %vf0
      {permutation_map = (d0, d1)->(d0, d1)}
        : tensor<?x?xvector<4x3xf32>>, vector<1x1x4x3xf32>

    // Special encoding for 0-d transfer with 0-d tensor/memref, vector shape
    // {1} and permutation_map () -> (0).
    %0 = vector.transfer_read %arg0[], %f0 {permutation_map = affine_map<()->(0)>} :
      tensor<f32>, vector<1xf32>
    ```
  }];

  let builders = [
    /// 1. Builder that sets padding to zero and an empty mask (variant with attrs).
    OpBuilder<(ins "VectorType":$vectorType,
                   "Value":$source,
                   "ValueRange":$indices,
                   "AffineMapAttr":$permutationMapAttr,
                   "ArrayAttr":$inBoundsAttr)>,
    /// 2. Builder that sets padding to zero and an empty mask (variant without attrs).
    OpBuilder<(ins "VectorType":$vectorType,
                   "Value":$source,
                   "ValueRange":$indices,
                   "AffineMap":$permutationMap,
                   CArg<"std::optional<ArrayRef<bool>>", "::std::nullopt">:$inBounds)>,
    /// 3. Builder that sets permutation map to 'getMinorIdentityMap'.
    OpBuilder<(ins "VectorType":$vectorType,
                   "Value":$source,
                   "ValueRange":$indices,
                   "Value":$padding,
                   CArg<"std::optional<ArrayRef<bool>>", "::std::nullopt">:$inBounds)>,
    /// 4. Builder that sets padding to zero and permutation map to
    /// 'getMinorIdentityMap'.
    OpBuilder<(ins "VectorType":$vectorType,
                   "Value":$source,
                   "ValueRange":$indices,
                   CArg<"std::optional<ArrayRef<bool>>", "::std::nullopt">:$inBounds)>,
  ];

  let extraClassDeclaration = [{
    // MaskableOpInterface methods.
    bool supportsPassthru() { return true; }

    MutableOperandRange getDpsInitsMutable() {
      return MutableOperandRange(getOperation(), /*start=*/0, /*length=*/0);
    }
  }];

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

// TODO: Tighten semantics so that masks and inbounds can't be used
// simultaneously within the same transfer op.
def Vector_TransferWriteOp :
  Vector_Op<"transfer_write", [
      DeclareOpInterfaceMethods<VectorTransferOpInterface>,
      DeclareOpInterfaceMethods<VectorUnrollOpInterface, ["getShapeForUnroll"]>,
      DeclareOpInterfaceMethods<MaskableOpInterface>,
      DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
      DeclareOpInterfaceMethods<ConditionallySpeculatable>,
      AttrSizedOperandSegments,
      DestinationStyleOpInterface
  ]>,
    Arguments<(ins AnyVectorOfAnyRank:$vector,
                   AnyShaped:$source,
                   Variadic<Index>:$indices,
                   AffineMapAttr:$permutation_map,
                   Optional<VectorOfNonZeroRankOf<[I1]>>:$mask,
                   BoolArrayAttr:$in_bounds)>,
    Results<(outs Optional<AnyRankedTensor>:$result)> {

  let summary = "The vector.transfer_write op writes a supervector to memory.";

  let description = [{
    The `vector.transfer_write` op performs a write from a
    [vector](../LangRef.md#vector-type), supplied as its first operand, into a
    slice within a [MemRef](../LangRef.md#memref-type) or a Ranked
    [Tensor](../LangRef.md#tensor-type) of the same base elemental type,
    supplied as its second operand.

    A vector memref/tensor operand must have its vector element type match a
    suffix (shape and element type) of the vector (e.g. memref<3x2x6x4x3xf32>,
    vector<1x1x4x3xf32>). If the operand is a tensor, the operation returns a
    new tensor of the same type.

    The slice is further defined by a full-rank index within the MemRef/Tensor,
    supplied as the operands `[2 .. 2 + rank(memref/tensor))` that defines the
    starting point of the transfer (e.g. `%A[%i0, %i1, %i2, %i3]`).

    The permutation_map [attribute](../LangRef.md#attributes) is an
    [affine-map](Affine.md#affine-maps) which specifies the transposition on the
    slice to match the vector shape. The permutation map may be implicit and
    omitted from parsing and printing if it is the canonical minor identity map
    (i.e. if it does not permute any dimension). In contrast to `transfer_read`,
    write ops cannot have broadcast dimensions.

    The size of the slice is specified by the size of the vector.

    An optional SSA value `mask` may be specified to mask out elements written
    to the MemRef/Tensor. The `mask` type is an `i1` vector with a shape that
    matches how elements are written into the MemRef/Tensor, *after* applying
    any permutation. Elements whose corresponding mask element is `0` are
    masked out.

    For every vector dimension, the boolean array attribute `in_bounds`
    specifies if the transfer is guaranteed to be within the source bounds. If
    set to "false", accesses (including the starting point) may run
    out-of-bounds along the respective vector dimension as the index increases.
    Non-vector dimensions *must* always be in-bounds. The `in_bounds` array
    length has to be equal to the vector rank. This attribute has a default
    value: `false` (i.e. "out-of-bounds"). When skipped in the textual IR, the
    default value is assumed. Similarly, the OP printer will omit this
    attribute when all dimensions are out-of-bounds (i.e. the default value is
    used).

     A `vector.transfer_write` can be lowered to a simple store if all
     dimensions are specified to be within bounds and no `mask` was specified.

    This operation is called 'write' by opposition to 'store' because the
    super-vector granularity is generally not representable with a single
    hardware register. A `vector.transfer_write` is thus a
    mid-level abstraction that supports super-vectorization with non-effecting
    padding for full-tile-only code. It is the responsibility of
    `vector.transfer_write`'s implementation to ensure the memory writes are
    valid. Different lowerings may be pertinent depending on the hardware
    support.

    Example:

    ```mlir
    // write vector<16x32x64xf32> into the slice
    //   `%A[%i0, %i1:%i1+32, %i2:%i2+64, %i3:%i3+16]`:
    for %i0 = 0 to %0 {
      affine.for %i1 = 0 to %1 step 32 {
        affine.for %i2 = 0 to %2 step 64 {
          affine.for %i3 = 0 to %3 step 16 {
            %val = `ssa-value` : vector<16x32x64xf32>
            vector.transfer_write %val, %A[%i0, %i1, %i2, %i3]
              {permutation_map: (d0, d1, d2, d3) -> (d3, d1, d2)} :
              vector<16x32x64xf32>, memref<?x?x?x?xf32>
    }}}}

    // or equivalently (rewrite with vector.transpose)
    for %i0 = 0 to %0 {
      affine.for %i1 = 0 to %1 step 32 {
        affine.for %i2 = 0 to %2 step 64 {
          affine.for %i3 = 0 to %3 step 16 {
            %val = `ssa-value` : vector<16x32x64xf32>
            %valt = vector.transpose %val, [1, 2, 0] :
                  vector<16x32x64xf32> -> vector<32x64x16xf32>
            vector.transfer_write %valt, %A[%i0, %i1, %i2, %i3]
              {permutation_map: (d0, d1, d2, d3) -> (d1, d2, d3)} :
              vector<32x64x16xf32>, memref<?x?x?x?xf32>
    }}}}

    // write to a memref with vector element type.
    vector.transfer_write %4, %arg1[%c3, %c3]
      {permutation_map = (d0, d1)->(d0, d1)}
        : vector<1x1x4x3xf32>, memref<?x?xvector<4x3xf32>>

    // return a tensor where the vector is inserted into the source tensor.
    %5 = vector.transfer_write %4, %arg1[%c3, %c3]
      {permutation_map = (d0, d1)->(d0, d1)}
        : vector<1x1x4x3xf32>, tensor<?x?xvector<4x3xf32>>

    // Special encoding for 0-d transfer with 0-d tensor/memref, vector shape
    // {1} and permutation_map () -> (0).
    %1 = vector.transfer_write %0, %arg0[] {permutation_map = affine_map<()->(0)>} :
      vector<1xf32>, tensor<f32>
    ```
  }];

  let builders = [
    /// 1. Builder with type inference.
    OpBuilder<(ins "Value":$vector,
                   "Value":$dest,
                   "ValueRange":$indices,
                   "AffineMapAttr":$permutationMapAttr,
                   "Value":$mask,
                   "ArrayAttr":$inBoundsAttr)>,
    /// 2. Builder with type inference that sets an empty mask (variant with attrs).
    OpBuilder<(ins "Value":$vector,
                   "Value":$dest,
                   "ValueRange":$indices,
                   "AffineMapAttr":$permutationMapAttr,
                   "ArrayAttr":$inBoundsAttr)>,
    /// 3. Builder with type inference that sets an empty mask (variant without attrs).
    OpBuilder<(ins "Value":$vector,
                   "Value":$dest,
                   "ValueRange":$indices,
                   "AffineMap":$permutationMap,
                   CArg<"std::optional<ArrayRef<bool>>", "::std::nullopt">:$inBounds)>,
    /// 4. Builder with type inference that sets an empty mask and sets permutation
    /// map to 'getMinorIdentityMap'.
    OpBuilder<(ins "Value":$vector,
                   "Value":$dest,
                   "ValueRange":$indices,
                   CArg<"std::optional<ArrayRef<bool>>", "::std::nullopt">:$inBounds)>,
  ];

  let extraClassDeclaration = [{
    /// This method is added to maintain uniformity with load/store
    ///  ops of other dialects.
    Value getValue() { return getVector(); }

    MutableOperandRange getDpsInitsMutable() { return getSourceMutable(); }
  }];

  let hasFolder = 1;
  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

def Vector_LoadOp : Vector_Op<"load"> {
  let summary = "reads an n-D slice of memory into an n-D vector";
  let description = [{
    The 'vector.load' operation reads an n-D slice of memory into an n-D
    vector. It takes a 'base' memref, an index for each memref dimension and a
    result vector type as arguments. It returns a value of the result vector
    type. The 'base' memref and indices determine the start memory address from
    which to read. Each index provides an offset for each memref dimension
    based on the element type of the memref. The shape of the result vector
    type determines the shape of the slice read from the start memory address.
    The elements along each dimension of the slice are strided by the memref
    strides. When loading more than 1 element, only unit strides are allowed
    along the most minor memref dimension. These constraints guarantee that
    elements read along the first dimension of the slice are contiguous in
    memory.

    The memref element type can be a scalar or a vector type. If the memref
    element type is a scalar, it should match the element type of the result
    vector. If the memref element type is vector, it should match the result
    vector type.

    Example: 0-D vector load on a scalar memref.
    ```mlir
    %result = vector.load %base[%i, %j] : memref<100x100xf32>, vector<f32>
    ```

    Example: 1-D vector load on a scalar memref.
    ```mlir
    %result = vector.load %base[%i, %j] : memref<100x100xf32>, vector<8xf32>
    ```

    Example: 1-D vector load on a vector memref.
    ```mlir
    %result = vector.load %memref[%i, %j] : memref<200x100xvector<8xf32>>, vector<8xf32>
    ```

    Example:  2-D vector load on a scalar memref.
    ```mlir
    %result = vector.load %memref[%i, %j] : memref<200x100xf32>, vector<4x8xf32>
    ```

    Example:  2-D vector load on a vector memref.
    ```mlir
    %result = vector.load %memref[%i, %j] : memref<200x100xvector<4x8xf32>>, vector<4x8xf32>
    ```

    Representation-wise, the 'vector.load' operation permits out-of-bounds
    reads. Support and implementation of out-of-bounds vector loads is
    target-specific. No assumptions should be made on the value of elements
    loaded out of bounds. Not all targets may support out-of-bounds vector
    loads.

    Example:  Potential out-of-bound vector load.
    ```mlir
    %result = vector.load %memref[%index] : memref<?xf32>, vector<8xf32>
    ```

    Example:  Explicit out-of-bound vector load.
    ```mlir
    %result = vector.load %memref[%c0] : memref<7xf32>, vector<8xf32>
    ```
  }];

  let arguments = (ins Arg<AnyMemRef, "the reference to load from",
      [MemRead]>:$base,
      Variadic<Index>:$indices,
      DefaultValuedOptionalAttr<BoolAttr, "false">:$nontemporal);
  let results = (outs AnyVectorOfAnyRank:$result);

  let extraClassDeclaration = [{
    MemRefType getMemRefType() {
      return ::llvm::cast<MemRefType>(getBase().getType());
    }

    VectorType getVectorType() {
      return ::llvm::cast<VectorType>(getResult().getType());
    }
  }];

  let hasFolder = 1;
  let hasVerifier = 1;

  let assemblyFormat =
      "$base `[` $indices `]` attr-dict `:` type($base) `,` type($result)";
}

def Vector_StoreOp : Vector_Op<"store"> {
  let summary = "writes an n-D vector to an n-D slice of memory";
  let description = [{
    The 'vector.store' operation writes an n-D vector to an n-D slice of memory.
    It takes the vector value to be stored, a 'base' memref and an index for
    each memref dimension. The 'base' memref and indices determine the start
    memory address from which to write. Each index provides an offset for each
    memref dimension based on the element type of the memref. The shape of the
    vector value to store determines the shape of the slice written from the
    start memory address. The elements along each dimension of the slice are
    strided by the memref strides. When storing more than 1 element, only unit
    strides are allowed along the most minor memref dimension. These constraints
    guarantee that elements written along the first dimension of the slice are
    contiguous in memory.

    The memref element type can be a scalar or a vector type. If the memref
    element type is a scalar, it should match the element type of the value
    to store. If the memref element type is vector, it should match the type
    of the value to store.

    Example: 0-D vector store on a scalar memref.
    ```mlir
    vector.store %valueToStore, %memref[%i, %j] : memref<200x100xf32>, vector<f32>
    ```

    Example: 1-D vector store on a scalar memref.
    ```mlir
    vector.store %valueToStore, %memref[%i, %j] : memref<200x100xf32>, vector<8xf32>
    ```

    Example: 1-D vector store on a vector memref.
    ```mlir
    vector.store %valueToStore, %memref[%i, %j] : memref<200x100xvector<8xf32>>, vector<8xf32>
    ```

    Example:  2-D vector store on a scalar memref.
    ```mlir
    vector.store %valueToStore, %memref[%i, %j] : memref<200x100xf32>, vector<4x8xf32>
    ```

    Example:  2-D vector store on a vector memref.
    ```mlir
    vector.store %valueToStore, %memref[%i, %j] : memref<200x100xvector<4x8xf32>>, vector<4x8xf32>
    ```

    Representation-wise, the 'vector.store' operation permits out-of-bounds
    writes. Support and implementation of out-of-bounds vector stores are
    target-specific. No assumptions should be made on the memory written out of
    bounds. Not all targets may support out-of-bounds vector stores.

    Example:  Potential out-of-bounds vector store.
    ```mlir
    vector.store %valueToStore, %memref[%index] : memref<?xf32>, vector<8xf32>
    ```

    Example:  Explicit out-of-bounds vector store.
    ```mlir
    vector.store %valueToStore, %memref[%c0] : memref<7xf32>, vector<8xf32>
    ```
  }];

  let arguments = (ins
      AnyVectorOfAnyRank:$valueToStore,
      Arg<AnyMemRef, "the reference to store to",
      [MemWrite]>:$base,
      Variadic<Index>:$indices,
      DefaultValuedOptionalAttr<BoolAttr, "false">:$nontemporal
  );

  let extraClassDeclaration = [{
    MemRefType getMemRefType() {
      return ::llvm::cast<MemRefType>(getBase().getType());
    }

    VectorType getVectorType() {
      return ::llvm::cast<VectorType>(getValueToStore().getType());
    }
  }];

  let hasFolder = 1;
  let hasVerifier = 1;

  let assemblyFormat = "$valueToStore `,` $base `[` $indices `]` attr-dict "
                       "`:` type($base) `,` type($valueToStore)";
}

def Vector_MaskedLoadOp :
  Vector_Op<"maskedload">,
    Arguments<(ins Arg<AnyMemRef, "", [MemRead]>:$base,
               Variadic<Index>:$indices,
               VectorOfNonZeroRankOf<[I1]>:$mask,
               AnyVectorOfNonZeroRank:$pass_thru)>,
    Results<(outs AnyVectorOfNonZeroRank:$result)> {

  let summary = "loads elements from memory into a vector as defined by a mask vector";

  let description = [{
    The masked load reads elements from memory into a vector as defined
    by a base with indices and a mask vector. When the mask is set, the
    element is read from memory. Otherwise, the corresponding element is taken
    from a pass-through vector. Informally the semantics are:
    ```
    result[0] := if mask[0] then base[i + 0] else pass_thru[0]
    result[1] := if mask[1] then base[i + 1] else pass_thru[1]
    etc.
    ```

    If a mask bit is set and the corresponding index is out-of-bounds for the
    given base, the behavior is undefined. If a mask bit is not set, the value
    comes from the pass-through vector regardless of the index, and the index is
    allowed to be out-of-bounds.

    The masked load can be used directly where applicable, or can be used
    during progressively lowering to bring other memory operations closer to
    hardware ISA support for a masked load. The semantics of the operation
    closely correspond to those of the `llvm.masked.load`
    [intrinsic](https://llvm.org/docs/LangRef.html#llvm-masked-load-intrinsics).

    Examples:

    ```mlir
    %0 = vector.maskedload %base[%i], %mask, %pass_thru
       : memref<?xf32>, vector<8xi1>, vector<8xf32> into vector<8xf32>

    %1 = vector.maskedload %base[%i, %j], %mask, %pass_thru
       : memref<?x?xf32>, vector<16xi1>, vector<16xf32> into vector<16xf32>
    ```
  }];
  let extraClassDeclaration = [{
    MemRefType getMemRefType() {
      return ::llvm::cast<MemRefType>(getBase().getType());
    }
    VectorType getMaskVectorType() {
      return ::llvm::cast<VectorType>(getMask().getType());
    }
    VectorType getPassThruVectorType() {
      return ::llvm::cast<VectorType>(getPassThru().getType());
    }
    VectorType getVectorType() {
      return ::llvm::cast<VectorType>(getResult().getType());
    }
  }];
  let assemblyFormat = "$base `[` $indices `]` `,` $mask `,` $pass_thru attr-dict `:` "
    "type($base) `,` type($mask) `,` type($pass_thru) `into` type($result)";
  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

def Vector_MaskedStoreOp :
  Vector_Op<"maskedstore">,
    Arguments<(ins Arg<AnyMemRef, "", [MemWrite]>:$base,
               Variadic<Index>:$indices,
               VectorOfNonZeroRankOf<[I1]>:$mask,
               AnyVectorOfNonZeroRank:$valueToStore)> {

  let summary = "stores elements from a vector into memory as defined by a mask vector";

  let description = [{
    The masked store operation writes elements from a vector into memory
    as defined by a base with indices and a mask vector. When the mask is
    set, the corresponding element from the vector is written to memory. Otherwise,
    no action is taken for the element. Informally the semantics are:
    ```
    if (mask[0]) base[i+0] = value[0]
    if (mask[1]) base[i+1] = value[1]
    etc.
    ```

    If a mask bit is set and the corresponding index is out-of-bounds for the
    given base, the behavior is undefined. If a mask bit is not set, no value
    is stored regardless of the index, and the index is allowed to be
    out-of-bounds.

    The masked store can be used directly where applicable, or can be used
    during progressively lowering to bring other memory operations closer to
    hardware ISA support for a masked store. The semantics of the operation
    closely correspond to those of the `llvm.masked.store`
    [intrinsic](https://llvm.org/docs/LangRef.html#llvm-masked-store-intrinsics).

    Examples:

    ```mlir
    vector.maskedstore %base[%i], %mask, %value
      : memref<?xf32>, vector<8xi1>, vector<8xf32>

    vector.maskedstore %base[%i, %j], %mask, %value
      : memref<?x?xf32>, vector<16xi1>, vector<16xf32>
    ```
  }];
  let extraClassDeclaration = [{
    MemRefType getMemRefType() {
      return ::llvm::cast<MemRefType>(getBase().getType());
    }
    VectorType getMaskVectorType() {
      return ::llvm::cast<VectorType>(getMask().getType());
    }
    VectorType getVectorType() {
      return ::llvm::cast<VectorType>(getValueToStore().getType());
    }
  }];
  let assemblyFormat =
      "$base `[` $indices `]` `,` $mask `,` $valueToStore "
      "attr-dict `:` type($base) `,` type($mask) `,` type($valueToStore)";
  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

def Vector_GatherOp :
  Vector_Op<"gather", [
    DeclareOpInterfaceMethods<MaskableOpInterface>,
    DeclareOpInterfaceMethods<VectorUnrollOpInterface, ["getShapeForUnroll"]>
  ]>,
    Arguments<(ins Arg<AnyShaped, "", [MemRead]>:$base,
               Variadic<Index>:$indices,
               VectorOfNonZeroRankOf<[AnyInteger, Index]>:$index_vec,
               VectorOfNonZeroRankOf<[I1]>:$mask,
               AnyVectorOfNonZeroRank:$pass_thru)>,
    Results<(outs AnyVectorOfNonZeroRank:$result)> {

  let summary = [{
    gathers elements from memory or ranked tensor into a vector as defined by an
    index vector and a mask vector
  }];

  let description = [{
    The gather operation returns an n-D vector whose elements are either loaded
    from memory or ranked tensor, or taken from a pass-through vector, depending
    on the values of an n-D mask vector.
    If a mask bit is set, the corresponding result element is defined by the base
    with indices and the n-D index vector (each index is a 1-D offset on the base).
    Otherwise, the corresponding element is taken from the n-D pass-through vector.
    Informally the semantics are:
    ```
    result[0] := if mask[0] then base[index[0]] else pass_thru[0]
    result[1] := if mask[1] then base[index[1]] else pass_thru[1]
    etc.
    ```

    If a mask bit is set and the corresponding index is out-of-bounds for the
    given base, the behavior is undefined. If a mask bit is not set, the value
    comes from the pass-through vector regardless of the index, and the index is
    allowed to be out-of-bounds.

    The gather operation can be used directly where applicable, or can be used
    during progressively lowering to bring other memory operations closer to
    hardware ISA support for a gather.

    Examples:

    ```mlir
    %0 = vector.gather %base[%c0][%v], %mask, %pass_thru
       : memref<?xf32>, vector<2x16xi32>, vector<2x16xi1>, vector<2x16xf32> into vector<2x16xf32>

    %1 = vector.gather %base[%i, %j][%v], %mask, %pass_thru
       : memref<16x16xf32>, vector<16xi32>, vector<16xi1>, vector<16xf32> into vector<16xf32>
    ```
  }];

  let extraClassDeclaration = [{
    ShapedType getBaseType() { return getBase().getType(); }
    VectorType getIndexVectorType() { return getIndexVec().getType(); }
    VectorType getMaskVectorType() { return getMask().getType(); }
    VectorType getPassThruVectorType() { return getPassThru().getType(); }
    VectorType getVectorType() { return getResult().getType(); }
  }];

  let assemblyFormat =
    "$base `[` $indices `]` `[` $index_vec `]` `,` "
    "$mask `,` $pass_thru attr-dict `:` type($base) `,` "
    "type($index_vec)  `,` type($mask) `,` type($pass_thru) "
    "`into` type($result)";
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

def Vector_ScatterOp :
  Vector_Op<"scatter">,
    Arguments<(ins Arg<AnyMemRef, "", [MemWrite]>:$base,
               Variadic<Index>:$indices,
               VectorOfRankAndType<[1], [AnyInteger, Index]>:$index_vec,
               VectorOfRankAndType<[1], [I1]>:$mask,
               VectorOfRank<[1]>:$valueToStore)> {

  let summary = [{
    scatters elements from a vector into memory as defined by an index vector
    and a mask vector
  }];

  let description = [{
    The scatter operation stores elements from a 1-D vector into memory as
    defined by a base with indices and an additional 1-D index vector, but
    only if the corresponding bit in a 1-D mask vector is set. Otherwise, no
    action is taken for that element. Informally the semantics are:
    ```
    if (mask[0]) base[index[0]] = value[0]
    if (mask[1]) base[index[1]] = value[1]
    etc.
    ```

    If a mask bit is set and the corresponding index is out-of-bounds for the
    given base, the behavior is undefined. If a mask bit is not set, no value
    is stored regardless of the index, and the index is allowed to be
    out-of-bounds.

    If the index vector contains two or more duplicate indices, the behavior is
    undefined. Underlying implementation may enforce strict sequential
    semantics.
    TODO: always enforce strict sequential semantics?

    The scatter operation can be used directly where applicable, or can be used
    during progressively lowering to bring other memory operations closer to
    hardware ISA support for a scatter. The semantics of the operation closely
    correspond to those of the `llvm.masked.scatter`
    [intrinsic](https://llvm.org/docs/LangRef.html#llvm-masked-scatter-intrinsics).

    Examples:

    ```mlir
    vector.scatter %base[%c0][%v], %mask, %value
        : memref<?xf32>, vector<16xi32>, vector<16xi1>, vector<16xf32>

    vector.scatter %base[%i, %j][%v], %mask, %value
        : memref<16x16xf32>, vector<16xi32>, vector<16xi1>, vector<16xf32>
    ```
  }];

  let extraClassDeclaration = [{
    MemRefType getMemRefType() { return getBase().getType(); }
    VectorType getIndexVectorType() { return getIndexVec().getType(); }
    VectorType getMaskVectorType() { return getMask().getType(); }
    VectorType getVectorType() { return getValueToStore().getType(); }
  }];

  let assemblyFormat =
      "$base `[` $indices `]` `[` $index_vec `]` `,` "
      "$mask `,` $valueToStore attr-dict `:` type($base) `,` "
      "type($index_vec)  `,` type($mask) `,` type($valueToStore)";
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

def Vector_ExpandLoadOp :
  Vector_Op<"expandload">,
    Arguments<(ins Arg<AnyMemRef, "", [MemRead]>:$base,
               Variadic<Index>:$indices,
               FixedVectorOfNonZeroRankOf<[I1]>:$mask,
               AnyVectorOfNonZeroRank:$pass_thru)>,
    Results<(outs AnyVectorOfNonZeroRank:$result)> {

  let summary = "reads elements from memory and spreads them into a vector as defined by a mask";

  let description = [{
    The expand load reads elements from memory into a vector as defined by a
    base with indices and a mask vector. Expansion only applies to the innermost
    dimension. When the mask is set, the next element is read from memory.
    Otherwise, the corresponding element is taken from a pass-through vector.
    Informally the semantics are:

    ```
    index = i
    result[0] := if mask[0] then base[index++] else pass_thru[0]
    result[1] := if mask[1] then base[index++] else pass_thru[1]
    etc.
    ```

    Note that the index increment is done conditionally.

    If a mask bit is set and the corresponding index is out-of-bounds for the
    given base, the behavior is undefined. If a mask bit is not set, the value
    comes from the pass-through vector regardless of the index, and the index is
    allowed to be out-of-bounds.

    The expand load can be used directly where applicable, or can be used
    during progressively lowering to bring other memory operations closer to
    hardware ISA support for an expand. The semantics of the operation closely
    correspond to those of the `llvm.masked.expandload`
    [intrinsic](https://llvm.org/docs/LangRef.html#llvm-masked-expandload-intrinsics).

    Note, at the moment this Op is only available for fixed-width vectors.

    Examples:

    ```mlir
    %0 = vector.expandload %base[%i], %mask, %pass_thru
       : memref<?xf32>, vector<8xi1>, vector<8xf32> into vector<8xf32>

    %1 = vector.expandload %base[%i, %j], %mask, %pass_thru
       : memref<?x?xf32>, vector<16xi1>, vector<16xf32> into vector<16xf32>
    ```
  }];
  let extraClassDeclaration = [{
    MemRefType getMemRefType() {
      return ::llvm::cast<MemRefType>(getBase().getType());
    }
    VectorType getMaskVectorType() {
      return ::llvm::cast<VectorType>(getMask().getType());
    }
    VectorType getPassThruVectorType() {
      return ::llvm::cast<VectorType>(getPassThru().getType());
    }
    VectorType getVectorType() {
      return ::llvm::cast<VectorType>(getResult().getType());
    }
  }];
  let assemblyFormat = "$base `[` $indices `]` `,` $mask `,` $pass_thru attr-dict `:` "
    "type($base) `,` type($mask) `,` type($pass_thru) `into` type($result)";
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

def Vector_CompressStoreOp :
  Vector_Op<"compressstore">,
    Arguments<(ins Arg<AnyMemRef, "", [MemWrite]>:$base,
               Variadic<Index>:$indices,
               FixedVectorOfNonZeroRankOf<[I1]>:$mask,
               AnyVectorOfNonZeroRank:$valueToStore)> {

  let summary = "writes elements selectively from a vector as defined by a mask";

  let description = [{
    The compress store operation writes elements from a vector into memory as
    defined by a base with indices and a mask vector. Compression only applies
    to the innermost dimension. When the mask is set, the corresponding element
    from the vector is written next to memory.  Otherwise, no action is taken
    for the element. Informally the semantics are:

    ```
    index = i
    if (mask[0]) base[index++] = value[0]
    if (mask[1]) base[index++] = value[1]
    etc.
    ```

    Note that the index increment is done conditionally.

    If a mask bit is set and the corresponding index is out-of-bounds for the
    given base, the behavior is undefined. If a mask bit is not set, no value
    is stored regardless of the index, and the index is allowed to be
    out-of-bounds.

    The compress store can be used directly where applicable, or can be used
    during progressively lowering to bring other memory operations closer to
    hardware ISA support for a compress. The semantics of the operation closely
    correspond to those of the `llvm.masked.compressstore`
    [intrinsic](https://llvm.org/docs/LangRef.html#llvm-masked-compressstore-intrinsics).

    Note, at the moment this Op is only available for fixed-width vectors.

    Examples:

    ```mlir
    vector.compressstore %base[%i], %mask, %value
      : memref<?xf32>, vector<8xi1>, vector<8xf32>

    vector.compressstore %base[%i, %j], %mask, %value
      : memref<?x?xf32>, vector<16xi1>, vector<16xf32>
    ```
  }];
  let extraClassDeclaration = [{
    MemRefType getMemRefType() {
      return ::llvm::cast<MemRefType>(getBase().getType());
    }
    VectorType getMaskVectorType() {
      return ::llvm::cast<VectorType>(getMask().getType());
    }
    VectorType getVectorType() {
      return ::llvm::cast<VectorType>(getValueToStore().getType());
    }
  }];
  let assemblyFormat =
      "$base `[` $indices `]` `,` $mask `,` $valueToStore attr-dict `:` "
      "type($base) `,` type($mask) `,` type($valueToStore)";
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

def Vector_ShapeCastOp :
  Vector_Op<"shape_cast", [Pure,
    DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>
  ]>,
    Arguments<(ins AnyVectorOfAnyRank:$source)>,
    Results<(outs AnyVectorOfAnyRank:$result)> {
  let summary = "shape_cast casts between vector shapes";
  let description = [{
    The shape_cast operation casts between an n-D source vector shape and
    a k-D result vector shape (the element type remains the same).

    If reducing rank (n > k), result dimension sizes must be a product
    of contiguous source dimension sizes.
    If expanding rank (n < k), source dimensions must factor into a
    contiguous sequence of destination dimension sizes.
    Each source dim is expanded (or contiguous sequence of source dims combined)
    in source dimension list order (i.e. 0 <= i < n), to produce a contiguous
    sequence of result dims (or a single result dim), in result dimension list
    order (i.e. 0 <= j < k). The product of all source dimension sizes and all
    result dimension sizes must match.

    It is currently assumed that this operation does not require moving data,
    and that it will be folded away before lowering vector operations.

    There is an exception to the folding expectation when targeting
    llvm.intr.matrix operations. We need a type conversion back and forth from a
    2-D MLIR vector to a 1-D flattened LLVM vector.shape_cast lowering to LLVM
    is supported in that particular case, for now.

    Example:

    ```mlir
    // Example casting to a lower vector rank.
    %1 = vector.shape_cast %0 : vector<5x1x4x3xf32> to vector<20x3xf32>

    // Example casting to a higher vector rank.
    %3 = vector.shape_cast %2 : vector<10x12x8xf32> to vector<5x2x3x4x8xf32>

    ```
  }];
  let extraClassDeclaration = [{
    VectorType getSourceVectorType() {
      return ::llvm::cast<VectorType>(getSource().getType());
    }
    VectorType getResultVectorType() {
      return ::llvm::cast<VectorType>(getResult().getType());
    }
  }];
  let assemblyFormat = "$source attr-dict `:` type($source) `to` type($result)";
  let hasFolder = 1;
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

def Vector_BitCastOp :
  Vector_Op<"bitcast", [Pure, AllRanksMatch<["source", "result"]>]>,
    Arguments<(ins AnyVectorOfAnyRank:$source)>,
    Results<(outs AnyVectorOfAnyRank:$result)>{
  let summary = "bitcast casts between vectors";
  let description = [{
    The bitcast operation casts between vectors of the same rank, the minor 1-D
    vector size is casted to a vector with a different element type but same
    bitwidth. In case of 0-D vectors, the bitwidth of element types must be
    equal.

    Example:

    ```mlir
    // Example casting to a smaller element type.
    %1 = vector.bitcast %0 : vector<5x1x4x3xf32> to vector<5x1x4x6xi16>

    // Example casting to a bigger element type.
    %3 = vector.bitcast %2 : vector<10x12x8xi8> to vector<10x12x2xi32>

    // Example casting to an element type of the same size.
    %5 = vector.bitcast %4 : vector<5x1x4x3xf32> to vector<5x1x4x3xi32>

    // Example casting of 0-D vectors.
    %7 = vector.bitcast %6 : vector<f32> to vector<i32>
    ```
  }];
  let extraClassDeclaration = [{
    VectorType getSourceVectorType() {
      return ::llvm::cast<VectorType>(getSource().getType());
    }
    VectorType getResultVectorType() {
      return ::llvm::cast<VectorType>(getResult().getType());
    }
  }];
  let assemblyFormat = "$source attr-dict `:` type($source) `to` type($result)";
  let hasFolder = 1;
  let hasVerifier = 1;
}

def Vector_TypeCastOp :
  Vector_Op<"type_cast", [Pure, ViewLikeOpInterface]>,
    Arguments<(ins StaticShapeMemRefOf<[AnyType]>:$memref)>,
    Results<(outs AnyMemRef:$result)> {
  let summary = "type_cast op converts a scalar memref to a vector memref";
  let description = [{
    Performs a conversion from a memref with scalar element to a memref with a
    *single* vector element, copying the shape of the memref to the vector. This
    is the minimal viable operation that is required to makeke
    super-vectorization operational. It can be seen as a special case of the
    `view` operation but scoped in the super-vectorization context.

    Example:

    ```mlir
    %A  = memref.alloc() : memref<5x4x3xf32>
    %VA = vector.type_cast %A : memref<5x4x3xf32> to memref<vector<5x4x3xf32>>
    ```
  }];

  /// Build the canonical memRefType with a single vector.
  /// E.g. memref<4 x 5 x vector<6 x f32>> -> memref<vector<4 x 5 x 6 x f32>>.
  let builders = [OpBuilder<(ins "Value":$source)>];

  let extraClassDeclaration = [{
    MemRefType getMemRefType() {
      return ::llvm::cast<MemRefType>(getMemref().getType());
    }
    MemRefType getResultMemRefType() {
      return ::llvm::cast<MemRefType>(getResult().getType());
    }
    // Implement ViewLikeOpInterface.
    Value getViewSource() { return getMemref(); }
  }];

  let assemblyFormat = [{
    $memref attr-dict `:` type($memref) `to` type($result)
  }];
  let hasVerifier = 1;
}

def Vector_ConstantMaskOp :
  Vector_Op<"constant_mask", [Pure]>,
    Arguments<(ins DenseI64ArrayAttr:$mask_dim_sizes)>,
    Results<(outs VectorOfAnyRankOf<[I1]>)> {
  let summary = "creates a constant vector mask";
  let description = [{
    Creates and returns a vector mask where elements of the result vector
    are set to '0' or '1', based on whether the element indices are contained
    within a hyper-rectangular region specified by the 'mask_dim_sizes'
    array attribute argument. Each element of the 'mask_dim_sizes' array,
    specifies an exclusive upper bound [0, mask-dim-size-element-value)
    for a unique dimension in the vector result. The conjunction of the ranges
    define a hyper-rectangular region within which elements values are set to 1
    (otherwise element values are set to 0). Each value of 'mask_dim_sizes' must
    be non-negative and not greater than the size of the corresponding vector
    dimension (as opposed to vector.create_mask which allows this). Sizes that
    correspond to scalable dimensions are implicitly multiplied by vscale,
    though currently only zero (none set) or the size of the dim/vscale
    (all set) are supported.

    Example:

    ```mlir
    // create a constant vector mask of size 4x3xi1 with elements in range
    // 0 <= row <= 2 and 0 <= col <= 1 are set to 1 (others to 0).
    %1 = vector.constant_mask [3, 2] : vector<4x3xi1>

    print %1
                  columns
                0    1    2
              |------------
            0 | 1    1    0
      rows  1 | 1    1    0
            2 | 1    1    0
            3 | 0    0    0
    ```
  }];

  let builders = [
    // Build with mixed static/dynamic operands.
    OpBuilder<(ins "VectorType":$type, "ConstantMaskKind":$kind)>
  ];

  let extraClassDeclaration = [{
    /// Return the result type of this op.
    VectorType getVectorType() {
      return cast<VectorType>(getOperation()->getResultTypes()[0]);
    }

    /// Return whether the mask is a uniform vector of `1`s.
    bool isAllOnesMask();
  }];

  let assemblyFormat = "$mask_dim_sizes attr-dict `:` type(results)";
  let hasVerifier = 1;
}

def Vector_CreateMaskOp :
  Vector_Op<"create_mask", [Pure]>,
    Arguments<(ins Variadic<Index>:$operands)>,
    Results<(outs VectorOfAnyRankOf<[I1]>)> {
  let summary = "creates a vector mask";
  let description = [{
    Creates and returns a vector mask where elements of the result vector
    are set to '0' or '1', based on whether the element indices are contained
    within a hyper-rectangular region specified by the operands. Specifically,
    each operand specifies a range [0, operand-value) for a unique dimension in
    the vector result. The conjunction of the operand ranges define a
    hyper-rectangular region within which elements values are set to 1
    (otherwise element values are set to 0). If operand-value is negative, it is
    treated as if it were zero, and if it is greater than the corresponding
    dimension size, it is treated as if it were equal to the dimension size.

    Example:

    ```mlir
    // create a vector mask of size 4x3xi1 where elements in range
    // 0 <= row <= 2 and 0 <= col <= 1 are set to 1 (others to 0).
    %1 = vector.create_mask %c3, %c2 : vector<4x3xi1>

    print %1
                  columns
                0    1    2
              |------------
            0 | 1    1    0
      rows  1 | 1    1    0
            2 | 1    1    0
            3 | 0    0    0
    ```
  }];

  let builders = [
    // Build with mixed static/dynamic operands.
    OpBuilder<(ins "VectorType":$type, "ArrayRef<OpFoldResult>":$mixedOperands)>
  ];

  let extraClassDeclaration = [{
    /// Return the result type of this op.
    VectorType getVectorType() {
      return cast<VectorType>(getOperation()->getResultTypes()[0]);
    }
  }];

  let hasCanonicalizer = 1;
  let hasVerifier = 1;
  let assemblyFormat = "$operands attr-dict `:` type(results)";
}

def Vector_MaskOp : Vector_Op<"mask", [
  SingleBlockImplicitTerminator<"vector::YieldOp">,
  DeclareOpInterfaceMethods<MaskingOpInterface>,
  RecursiveMemoryEffects, NoRegionArguments
]> {
  let summary = "Predicates a maskable vector operation";
  let description = [{
    The `vector.mask` is a `MaskingOpInterface` operation that predicates the
    execution of another operation. It takes an `i1` vector mask and an
    optional passthru vector as arguments.

    A implicitly `vector.yield`-terminated region encloses the operation to be
    masked. Values used within the region are captured from above. Only one
    *maskable* operation can be masked with a `vector.mask` operation at a time.
    An operation is *maskable* if it implements the `MaskableOpInterface`. The
    terminator yields all results of the maskable operation to the result of
    this operation.

    The vector mask argument holds a bit for each vector lane and determines
    which vector lanes should execute the maskable operation and which ones
    should not. The `vector.mask` operation returns the value produced by the
    masked execution of the nested operation, if any. The masked-off lanes in
    the result vector are taken from the corresponding lanes of the pass-thru
    argument, if provided, or left unmodified, otherwise. At this point, 0-D
    vectors are not supported by `vector.mask`. They may be supported in the
    future.

    The `vector.mask` operation does not prescribe how a maskable operation
    should be masked or how a masked operation should be lowered. Masking
    constraints and some semantic details are provided by each maskable
    operation through the `MaskableOpInterface`. Lowering of masked operations
    is implementation defined. For instance, scalarizing the masked operation
    or executing the operation for the masked-off lanes are valid lowerings as
    long as the execution of masked-off lanes does not change the observable
    behavior of the program.

    Examples:

    ```
      %0 = vector.mask %mask { vector.reduction <add>, %a : vector<8xi32> into i32 } : vector<8xi1> -> i32
    ```

    ```
      %0 = vector.mask %mask, %passthru { arith.divsi %a, %b : vector<8xi32> } : vector<8xi1> -> vector<8xi32>
    ```

    ```
      vector.mask %mask { vector.transfer_write %val, %t0[%idx] : vector<16xf32>, memref<?xf32> } : vector<16xi1>
    ```

    ```
      vector.mask %mask { vector.transfer_write %val, %t0[%idx] : vector<16xf32>, tensor<?xf32> } : vector<16xi1> -> tensor<?xf32>
    ```
  }];

  // TODO: Support multiple passthru values.
  let arguments = (ins VectorOfNonZeroRankOf<[I1]>:$mask,
                   Optional<AnyType>:$passthru);
  let results = (outs Variadic<AnyType>:$results);
  let regions = (region SizedRegion<1>:$maskRegion);

  let skipDefaultBuilders = 1;
  let builders = [
    OpBuilder<(ins "Value":$mask, "Operation *":$maskableOp,
                   CArg<"function_ref<void(OpBuilder &, Operation *)>">:$maskRegion)>,
    OpBuilder<(ins "TypeRange":$resultTypes, "Value":$mask, "Operation *":$maskableOp,
                   CArg<"function_ref<void(OpBuilder &, Operation *)>">:$maskRegion)>,
    OpBuilder<(ins "TypeRange":$resultTypes, "Value":$mask, "Value":$passthru,
                   "Operation *":$maskableOp,
                   CArg<"function_ref<void(OpBuilder &, Operation *)>">:$maskRegion)>
  ];

  let extraClassDeclaration = [{
    Block *getMaskBlock() { return &getMaskRegion().front(); }

    /// Returns true if mask op is not masking any operation.
    bool isEmpty() {
      Block *block = getMaskBlock();
      if (block->getOperations().size() > 1)
        return false;
      return true;
    }

    static void ensureTerminator(Region &region, Builder &builder,
                                 Location loc);
  }];

  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

def Vector_TransposeOp :
  Vector_Op<"transpose", [Pure,
    DeclareOpInterfaceMethods<VectorUnrollOpInterface, ["getShapeForUnroll"]>,
    PredOpTrait<"operand and result have same element type",
                 TCresVTEtIsSameAsOpBase<0, 0>>]> {
  let summary = "vector transpose operation";
  let description = [{
    Takes a n-D vector and returns the transposed n-D vector defined by
    the permutation of ranks in the n-sized integer array attribute (in case
    of 0-D vectors the array attribute must be empty).

    In the operation

    ```mlir
    %1 = vector.transpose %0, [i_1, .., i_n]
      : vector<d_1 x .. x d_n x f32>
      to vector<d_trans[0] x .. x d_trans[n-1] x f32>
    ```

    the `permutation` array [i_1, .., i_n] must be a permutation of [0, .., n-1].

    Example:

    ```mlir
    %1 = vector.transpose %0, [1, 0] : vector<2x3xf32> to vector<3x2xf32>

     [ [a, b, c],       [ [a, d],
       [d, e, f] ]  ->    [b, e],
                          [c, f] ]
    ```
  }];

  let arguments = (ins AnyVectorOfAnyRank:$vector,
                       DenseI64ArrayAttr:$permutation);
  let results = (outs AnyVectorOfAnyRank:$result);

  let builders = [
    OpBuilder<(ins "Value":$vector, "ArrayRef<int64_t>":$permutation)>
  ];
  let extraClassDeclaration = [{
    VectorType getSourceVectorType() {
      return ::llvm::cast<VectorType>(getVector().getType());
    }
    VectorType getResultVectorType() {
      return ::llvm::cast<VectorType>(getResult().getType());
    }
  }];
  let assemblyFormat = [{
    $vector `,` $permutation attr-dict `:` type($vector) `to` type($result)
  }];
  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

def Vector_PrintOp :
  Vector_Op<"print", [
    MemoryEffects<[MemWrite]>,
    PredOpTrait<
      "`source` or `punctuation` are not set when printing strings",
      CPred<"!getStringLiteral() || (!getSource() && getPunctuation() == PrintPunctuation::NewLine)">
    >,
  ]>,
  Arguments<(ins Optional<Type<Or<[
    AnyVectorOfAnyRank.predicate,
    AnyInteger.predicate, Index.predicate, AnyFloat.predicate
  ]>>>:$source, DefaultValuedAttr<Vector_PrintPunctuation,
                      "::mlir::vector::PrintPunctuation::NewLine">:$punctuation,
                OptionalAttr<Builtin_StringAttr>:$stringLiteral)
  > {
  let summary = "print operation (for testing and debugging)";
  let description = [{
    Prints the source vector (or scalar) to stdout in a human-readable format
    (for testing and debugging). No return value.

    Example:

    ```mlir
    %v = arith.constant dense<0.0> : vector<4xf32>
    vector.print %v : vector<4xf32>
    ```

    When lowered to LLVM, the vector print is decomposed into elementary
    printing method calls that at runtime will yield:

    ```
    ( 0.0, 0.0, 0.0, 0.0 )
    ```

    This is printed to stdout via a small runtime support library, which only
    needs to provide a few printing methods (single value for all data
    types, opening/closing bracket, comma, newline).

    By default `vector.print` adds a newline after the vector, but this can be
    controlled by the `punctuation` attribute. For example, to print a comma
    after instead do:

    ```mlir
    vector.print %v : vector<4xf32> punctuation <comma>
    ```

    Note that it is possible to use the punctuation attribute alone. The
    following will print a single newline:

    ```mlir
    vector.print punctuation <newline>
    ```

    Additionally, to aid with debugging and testing `vector.print` can also
    print constant strings:

    ```mlir
    vector.print str "Hello, World!"
    ```
  }];
  let extraClassDeclaration = [{
    Type getPrintType() {
      return getSource().getType();
    }
  }];
  let builders = [
    OpBuilder<(ins "PrintPunctuation":$punctuation), [{
      build($_builder, $_state, {}, punctuation, {});
    }]>,
    OpBuilder<(ins "::mlir::Value":$source), [{
      build($_builder, $_state, source, PrintPunctuation::NewLine);
    }]>,
    OpBuilder<(ins "::mlir::Value":$source, "PrintPunctuation":$punctuation), [{
      build($_builder, $_state, source, punctuation, {});
    }]>,
    OpBuilder<(ins "::llvm::StringRef":$string), [{
      build($_builder, $_state, {}, PrintPunctuation::NewLine, $_builder.getStringAttr(string));
    }]>,
  ];

  let assemblyFormat = [{
      ($source^ `:` type($source))?
        oilist(
            `str` $stringLiteral
          | `punctuation` $punctuation)
        attr-dict
    }];
}

//===----------------------------------------------------------------------===//
// Ops used for supporting progressive lowering and conversion type changes.
// The Ops are typically not used directly by higher level dialects, but are
// used by intra-dialect rewriting rules to bring vector operations closer
// to the hardware ISA.
//===----------------------------------------------------------------------===//

/// Vector dialect matrix multiplication op that operates on flattened 1-D
/// MLIR vectors. This is the counterpart of llvm.matrix.multiply in MLIR.
/// This may seem redundant with vector.contract but it serves the purposes of
/// more progressive lowering and localized type conversion on the path:
///   `vector<...x...xf32> -> vector<...xf32> -> !llvm<... x float>`.
def Vector_MatmulOp : Vector_Op<"matrix_multiply", [Pure,
        PredOpTrait<"lhs operand and result have same element type",
                    TCresVTEtIsSameAsOpBase<0, 0>>,
        PredOpTrait<"rhs operand and result have same element type",
                    TCresVTEtIsSameAsOpBase<0, 1>>]>,
      Arguments<(
        // TODO: tighten vector element types that make sense.
        ins FixedVectorOfRankAndType<[1],
              [AnySignlessInteger, AnySignedInteger, Index, AnyFloat]>:$lhs,
            FixedVectorOfRankAndType<[1],
              [AnySignlessInteger, AnySignedInteger, Index, AnyFloat]>:$rhs,
            I32Attr:$lhs_rows, I32Attr:$lhs_columns, I32Attr:$rhs_columns)>,
      Results<(
        outs FixedVectorOfRankAndType<[1],
               [AnySignlessInteger, AnySignedInteger, Index, AnyFloat]>:$res)>
{
  let summary = "Vector matrix multiplication op that operates on flattened 1-D"
    " MLIR vectors";
  let description = [{
    This is the counterpart of llvm.matrix.multiply in MLIR. It serves the
    purposes of more progressive lowering and localized type conversion.
    Higher levels typically lower matrix multiplications into 'vector.contract'
    operations. Subsequent rewriting rule progressively lower these operations
    into 'vector.matrix_multiply' operations to bring the operations closer
    to the hardware ISA.

    The ‘vector.matrix_multiply’ op treats `lhs` as matrix with <lhs_rows> rows
    and <lhs_columns> columns, `rhs` as matrix with <lhs_columns> rows and
    <rhs_columns> and multiplies them. The result matrix is returned embedded in
    the result vector.

    Note, the corresponding LLVM intrinsic, `@llvm.matrix.multiply.*`, does not
    support scalable vectors. Hence, this Op is only available for fixed-width
    vectors. Also see:

    http://llvm.org/docs/LangRef.html#llvm-matrix-multiply-intrinsic

    Example:

    ```mlir
    %C = vector.matrix_multiply %A, %B
      { lhs_rows = 4: i32, lhs_columns = 16: i32 , rhs_columns = 3: i32 } :
      (vector<64xf64>, vector<48xf64>) -> vector<12xf64>
    ```
  }];
  let builders = [
   OpBuilder<(ins "Value":$lhs, "Value":$rhs, "unsigned":$lhsRows,
     "unsigned":$lhsColumns, "unsigned":$rhsColumns),
   [{
     $_state.addOperands({lhs, rhs});
     $_state.addAttribute("lhs_rows",$_builder.getI32IntegerAttr(lhsRows));
     $_state.addAttribute("lhs_columns",$_builder.getI32IntegerAttr(lhsColumns));
     $_state.addAttribute("rhs_columns",$_builder.getI32IntegerAttr(rhsColumns));
     $_state.addTypes(VectorType::get(lhsRows * rhsColumns,
       ::llvm::cast<VectorType>(lhs.getType()).getElementType()));
   }]>,
  ];
  let assemblyFormat = "$lhs `,` $rhs attr-dict "
    "`:` `(` type($lhs) `,` type($rhs) `)` `->` type($res)";
}

/// Vector dialect matrix tranposition op that operates on flattened 1-D
/// MLIR vectors. This is the counterpart of llvm.matrix.transpose in MLIR.
/// This may seem redundant with vector.transpose but it serves the purposes of
/// more progressive lowering and localized type conversion on the path:
///   `vector<...x...xf32> -> vector<...xf32> -> !llvm<... x float>`.
def Vector_FlatTransposeOp : Vector_Op<"flat_transpose", [Pure,
  PredOpTrait<"source operand and result have same element type",
                 TCresVTEtIsSameAsOpBase<0, 0>>]>,
    Arguments<(
      // TODO: tighten vector element types that make sense.
      ins FixedVectorOfRankAndType<[1],
            [AnySignlessInteger, AnySignedInteger, Index, AnyFloat]>:$matrix,
          I32Attr:$rows, I32Attr:$columns)>,
    Results<(
      outs FixedVectorOfRankAndType<[1],
             [AnySignlessInteger, AnySignedInteger, Index, AnyFloat]>:$res)> {
  let summary = "Vector matrix transposition on flattened 1-D MLIR vectors";
  let description = [{
    This is the counterpart of llvm.matrix.transpose in MLIR. It serves
    the purposes of more progressive lowering and localized type conversion.
    Higher levels typically lower matrix tranpositions into 'vector.transpose'
    operations. Subsequent rewriting rule progressively lower these operations
    into 'vector.flat_transpose' operations to bring the operations closer
    to the hardware ISA.

    The `vector.flat_transpose` op treats the 1-D input `matrix` as
    a 2-D matrix with <rows> rows and <columns> columns, and returns the
    transposed matrix in flattened form in 'res'.

    Note, the corresponding LLVM intrinsic, `@llvm.matrix.transpose.*`, does not
    support scalable vectors. Hence, this Op is only available for fixed-width
    vectors. Also see:

    http://llvm.org/docs/LangRef.html#llvm-matrix-transpose-intrinsic

    Example:

    ```mlir
    %1 = vector.flat_transpose %0 {columns = 4 : i32, rows = 4 : i32}
       : vector<16xf32> -> vector<16xf32>
    ```
  }];
  let assemblyFormat = "$matrix attr-dict `:` type($matrix) `->` type($res)";
}

//===----------------------------------------------------------------------===//
// SplatOp
//===----------------------------------------------------------------------===//

def Vector_SplatOp : Vector_Op<"splat", [
    Pure,
    DeclareOpInterfaceMethods<InferIntRangeInterface, ["inferResultRanges"]>,
    TypesMatchWith<"operand type matches element type of result",
                   "aggregate", "input",
                   "::llvm::cast<VectorType>($_self).getElementType()">
  ]> {
  let summary = "vector splat or broadcast operation";
  let description = [{
    Broadcast the operand to all elements of the result vector. The operand is
    required to be of integer/index/float type.

    Example:

    ```mlir
    %s = arith.constant 10.1 : f32
    %t = vector.splat %s : vector<8x16xf32>
    ```
  }];

  let arguments = (ins AnyTypeOf<[AnySignlessInteger, Index, AnyFloat],
                                 "integer/index/float type">:$input);
  let results = (outs AnyVectorOfAnyRank:$aggregate);

  let builders = [
    OpBuilder<(ins "Value":$element, "Type":$aggregateType),
    [{ build($_builder, $_state, aggregateType, element); }]>];
  let assemblyFormat = "$input attr-dict `:` type($aggregate)";

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// VectorScaleOp
//===----------------------------------------------------------------------===//

// TODO: In the future, we might want to have scalable vectors with different
//       scales for different dimensions. E.g.: vector<[16]x[16]xf32>, in
//       which case we might need to add an index to 'vscale' to select one
//       of them. In order to support GPUs, we might also want to differentiate
//       between a 'global' scale, a scale that's fixed throughout the
//       execution, and a 'local' scale that is fixed but might vary with each
//       call to the function. For that, it might be useful to have a
//       'vector.scale.global' and a 'vector.scale.local' operation.
def VectorScaleOp : Vector_Op<"vscale",
  [Pure, DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>]
> {
  let summary = "Load vector scale size";
  let description = [{
    The `vscale` op returns the scale of the scalable vectors, a positive
    integer value that is constant at runtime but unknown at compile-time.
    The scale of the vector indicates the multiplicity of the vectors and
    vector operations. For example, a `vector<[4]xi32>` is equivalent to
    `vscale` consecutive `vector<4xi32>`; and an operation on a
    `vector<[4]xi32>` is equivalent to performing that operation `vscale`
    times, once on each `<4xi32>` segment of the scalable vector. The `vscale`
    op can be used to calculate the step in vector-length agnostic (VLA) loops.
    Right now we only support one contiguous set of scalable dimensions, all of
    them grouped and scaled with the value returned by 'vscale'.
  }];
  let results = (outs Index:$res);
  let assemblyFormat = "attr-dict";

  let extraClassDefinition = [{
    void $cppClass::getAsmResultNames(
        ::llvm::function_ref<void(mlir::Value, mlir::StringRef)> setNameFn) {
      setNameFn(getResult(), "vscale");
    }
  }];
}

//===----------------------------------------------------------------------===//
// VectorScanOp
//===----------------------------------------------------------------------===//

def Vector_ScanOp :
  Vector_Op<"scan", [Pure,
    AllTypesMatch<["source", "dest"]>,
    AllTypesMatch<["initial_value", "accumulated_value"]> ]>,
    Arguments<(ins Vector_CombiningKindAttr:$kind,
                   AnyVectorOfNonZeroRank:$source,
                   AnyVectorOfAnyRank:$initial_value,
                   I64Attr:$reduction_dim,
                   BoolAttr:$inclusive)>,
    Results<(outs AnyVectorOfNonZeroRank:$dest,
                  AnyVectorOfAnyRank:$accumulated_value)> {
  let summary = "Scan operation";
  let description = [{
    Performs an inclusive/exclusive scan on an n-D vector along a single
    dimension returning an n-D result vector using the given
    operation (`add`/`mul`/`minsi`/`minui`/`maxsi`/`maxui`/`and`/`or`/`xor` for
    integers, and `add`/`mul`/`minnumf`/`maxnumf`/`minimumf`/`maximumf` for
    floats), and a specified value for the initial value. The operator returns
    the result of scan as well as the result of the last reduction in the scan.

    Example:

    ```mlir
    %1:2 = vector.scan <add>, %0, %acc {inclusive = false, reduction_dim = 1 : i64} :
      vector<4x8x16x32xf32>, vector<4x16x32xf32>
    ```
  }];
  let builders = [
    OpBuilder<(ins "Value":$source, "Value":$initial_value,
                   "CombiningKind":$kind,
                   CArg<"int64_t", "0">:$reduction_dim,
                   CArg<"bool", "true">:$inclusive)>
  ];
  let extraClassDeclaration = [{
    VectorType getSourceType() {
      return ::llvm::cast<VectorType>(getSource().getType());
    }
    VectorType getDestType() {
      return ::llvm::cast<VectorType>(getDest().getType());
    }
    VectorType getAccumulatorType() {
      return ::llvm::cast<VectorType>(getAccumulatedValue().getType());
    }
    VectorType getInitialValueType() {
      return ::llvm::cast<VectorType>(getInitialValue().getType());
    }
  }];
  let assemblyFormat =
    "$kind `,` $source `,` $initial_value attr-dict `:` "
    "type($source) `,` type($initial_value) ";
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// VectorStepOp
//===----------------------------------------------------------------------===//

def Vector_StepOp : Vector_Op<"step", [Pure]> {
  let summary = "A linear sequence of values from 0 to N";
  let description = [{
    A `step` operation produces an index vector, i.e. a 1-D vector of values of
    index type that represents a linear sequence from 0 to N-1, where N is the
    number of elements in the `result` vector.

    Supports fixed-width and scalable vectors.

    Examples:

    ```mlir
    %0 = vector.step : vector<4xindex> ; [0, 1, 2, 3]
    %1 = vector.step : vector<[4]xindex> ; [0, 1, .., <vscale * 4 - 1>]
    ```
  }];
  let results = (outs VectorOfRankAndType<[1], [Index]>:$result);
  let assemblyFormat = "attr-dict `:` type($result)";
}

def Vector_YieldOp : Vector_Op<"yield", [
    Pure, ReturnLike, Terminator]> {
  let summary = "Terminates and yields values from vector regions.";
  let description = [{
    "vector.yield" yields an SSA value from the Vector dialect op region and
    terminates the regions. The semantics of how the values are yielded is
    defined by the parent operation.
    If "vector.yield" has any operands, the operands must correspond to the
    parent operation's results.
    If the parent operation defines no value the vector.yield may be omitted
    when printing the region.
  }];

  let arguments = (ins Variadic<AnyType>:$operands);

  let builders = [
    OpBuilder<(ins), [{ /* nothing to do */ }]>,
  ];

  let assemblyFormat = "attr-dict ($operands^ `:` type($operands))?";
}


#endif // MLIR_DIALECT_VECTOR_IR_VECTOR_OPS


//===- MemRefOps.td - MemRef op definitions ----------------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef MEMREF_OPS
#define MEMREF_OPS

include "mlir/Dialect/Arith/IR/ArithBase.td"
include "mlir/Dialect/MemRef/IR/MemRefBase.td"
include "mlir/Interfaces/CastInterfaces.td"
include "mlir/Interfaces/ControlFlowInterfaces.td"
include "mlir/Interfaces/CopyOpInterface.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/MemorySlotInterfaces.td"
include "mlir/Interfaces/ShapedOpInterfaces.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/Interfaces/ViewLikeInterface.td"
include "mlir/IR/OpAsmInterface.td"
include "mlir/IR/SymbolInterfaces.td"

/// A TypeAttr for memref types.
def MemRefTypeAttr
    : TypeAttrBase<"::mlir::MemRefType", "memref type attribute"> {
  let constBuilderCall = "::mlir::TypeAttr::get($0)";
}

class MemRef_Op<string mnemonic, list<Trait> traits = []>
    : Op<MemRef_Dialect, mnemonic, traits>;

// Base class for ops with static/dynamic offset, sizes and strides
// attributes/arguments.
class MemRef_OpWithOffsetSizesAndStrides<string mnemonic,
                                         list<Trait> traits = []>
    : MemRef_Op<mnemonic, traits> {
  code extraBaseClassDeclaration = [{
    /// Returns the dynamic sizes for this subview operation if specified.
    ::mlir::Operation::operand_range getDynamicSizes() { return getSizes(); }

    /// Return the list of Range (i.e. offset, size, stride). Each
    /// Range entry contains either the dynamic value or a ConstantIndexOp
    /// constructed with `b` at location `loc`.
    ::mlir::SmallVector<::mlir::Range, 8> getOrCreateRanges(
        ::mlir::OpBuilder &b, ::mlir::Location loc) {
      return ::mlir::getOrCreateRanges(*this, b, loc);
    }
  }];
}

//===----------------------------------------------------------------------===//
// AllocLikeOp
//===----------------------------------------------------------------------===//

// Base class for memref allocating ops: alloca and alloc.
//
//   %0 = alloclike(%m)[%s] : memref<8x?xf32, affine_map<(d0, d1)[s0] -> (d0 + s0, d1)>>
//
class AllocLikeOp<string mnemonic,
                  Resource resource,
                  list<Trait> traits = []> :
    MemRef_Op<mnemonic,
    !listconcat([
      AttrSizedOperandSegments
    ], traits)> {

  let arguments = (ins Variadic<Index>:$dynamicSizes,
                       // The symbolic operands (the ones in square brackets)
                       // bind to the symbols of the memref's layout map.
                       Variadic<Index>:$symbolOperands,
                       ConfinedAttr<OptionalAttr<I64Attr>,
                                [IntMinValue<0>]>:$alignment);
  let results = (outs Res<AnyMemRef, "",
                          [MemAlloc<resource, 0, FullEffect>]>:$memref);

  let builders = [
    OpBuilder<(ins "MemRefType":$memrefType,
                  CArg<"IntegerAttr", "IntegerAttr()">:$alignment), [{
      return build($_builder, $_state, memrefType, {}, alignment);
    }]>,
    OpBuilder<(ins "MemRefType":$memrefType, "ValueRange":$dynamicSizes,
                  CArg<"IntegerAttr", "IntegerAttr()">:$alignment), [{
      return build($_builder, $_state, memrefType, dynamicSizes, {}, alignment);
    }]>,
    OpBuilder<(ins "MemRefType":$memrefType, "ValueRange":$dynamicSizes,
                  "ValueRange":$symbolOperands,
                  CArg<"IntegerAttr", "{}">:$alignment), [{
      $_state.types.push_back(memrefType);
      $_state.addOperands(dynamicSizes);
      $_state.addOperands(symbolOperands);
      $_state.addAttribute(getOperandSegmentSizeAttr(),
          $_builder.getDenseI32ArrayAttr({
              static_cast<int32_t>(dynamicSizes.size()),
              static_cast<int32_t>(symbolOperands.size())}));
      if (alignment)
        $_state.addAttribute(getAlignmentAttrStrName(), alignment);
    }]>,
    OpBuilder<(ins "ArrayRef<OpFoldResult>":$sizes, "Type":$elementType,
                   CArg<"Attribute", "{}">:$memorySpace), [{
      SmallVector<int64_t> staticShape;
      SmallVector<Value> dynamicSizes;
      dispatchIndexOpFoldResults(sizes, dynamicSizes, staticShape);
      MemRefLayoutAttrInterface layout;
      MemRefType memrefType = MemRefType::get(staticShape, elementType, layout,
                                              memorySpace);
      return build($_builder, $_state, memrefType, dynamicSizes);
    }]>
  ];

  let extraClassDeclaration = [{
    static StringRef getAlignmentAttrStrName() { return "alignment"; }

    MemRefType getType() { return ::llvm::cast<MemRefType>(getResult().getType()); }

    SmallVector<OpFoldResult> getMixedSizes() {
      SmallVector<OpFoldResult> result;
      unsigned ctr = 0;
      OpBuilder b(getContext());
      for (int64_t i = 0, e = getType().getRank(); i < e; ++i) {
        if (getType().isDynamicDim(i)) {
          result.push_back(getDynamicSizes()[ctr++]);
        } else {
          result.push_back(b.getIndexAttr(getType().getShape()[i]));
        }
      }
      return result;
    }
  }];

  let assemblyFormat = [{
    `(`$dynamicSizes`)` (`` `[` $symbolOperands^ `]`)? attr-dict `:` type($memref)
  }];

  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// AssumeAlignmentOp
//===----------------------------------------------------------------------===//

def AssumeAlignmentOp : MemRef_Op<"assume_alignment"> {
  let summary =
      "assertion that gives alignment information to the input memref";
  let description = [{
    The `assume_alignment` operation takes a memref and an integer of alignment
    value, and internally annotates the buffer with the given alignment. If
    the buffer isn't aligned to the given alignment, the behavior is undefined.

    This operation doesn't affect the semantics of a correct program. It's for
    optimization only, and the optimization is best-effort.
  }];
  let arguments = (ins AnyMemRef:$memref,
                       ConfinedAttr<I32Attr, [IntPositive]>:$alignment);
  let results = (outs);

  let assemblyFormat = "$memref `,` $alignment attr-dict `:` type($memref)";
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// AllocOp
//===----------------------------------------------------------------------===//

def MemRef_AllocOp : AllocLikeOp<"alloc", DefaultResource, [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>]> {
  let summary = "memory allocation operation";
  let description = [{
    The `alloc` operation allocates a region of memory, as specified by its
    memref type.

    Example:

    ```mlir
    %0 = memref.alloc() : memref<8x64xf32, 1>
    ```

    The optional list of dimension operands are bound to the dynamic dimensions
    specified in its memref type. In the example below, the ssa value '%d' is
    bound to the second dimension of the memref (which is dynamic).

    ```mlir
    %0 = memref.alloc(%d) : memref<8x?xf32, 1>
    ```

    The optional list of symbol operands are bound to the symbols of the
    memrefs affine map. In the example below, the ssa value '%s' is bound to
    the symbol 's0' in the affine map specified in the allocs memref type.

    ```mlir
    %0 = memref.alloc()[%s] : memref<8x64xf32,
                              affine_map<(d0, d1)[s0] -> ((d0 + s0), d1)>, 1>
    ```

    This operation returns a single ssa value of memref type, which can be used
    by subsequent load and store operations.

    The optional `alignment` attribute may be specified to ensure that the
    region of memory that will be indexed is aligned at the specified byte
    boundary.

    ```mlir
    %0 = memref.alloc()[%s] {alignment = 8} :
      memref<8x64xf32, affine_map<(d0, d1)[s0] -> ((d0 + s0), d1)>, 1>
    ```
  }];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// ReallocOp
//===----------------------------------------------------------------------===//


def MemRef_ReallocOp : MemRef_Op<"realloc"> {
  let summary = "memory reallocation operation";
  let description = [{
    The `realloc` operation changes the size of a memory region. The memory
    region is specified by a 1D source memref and the size of the new memory
    region is specified by a 1D result memref type and an optional dynamic Value
    of `Index` type. The source and the result memref must be in the same memory
    space and have the same element type.

    The operation may move the memory region to a new location. In this case,
    the content of the memory block is preserved up to the lesser of the new
    and old sizes. If the new size if larger, the value of the extended memory
    is undefined. This is consistent with the ISO C realloc.

    The operation returns an SSA value for the memref.

    Example:

    ```mlir
    %0 = memref.realloc %src : memref<64xf32> to memref<124xf32>
    ```

    The source memref may have a dynamic shape, in which case, the compiler will
    generate code to extract its size from the runtime data structure for the
    memref.

    ```mlir
    %1 = memref.realloc %src : memref<?xf32> to memref<124xf32>
    ```

    If the result memref has a dynamic shape, a result dimension operand is
    needed to spefify its dynamic dimension. In the example below, the ssa value
    '%d' specifies the unknown dimension of the result memref.

    ```mlir
    %2 = memref.realloc %src(%d) : memref<?xf32> to memref<?xf32>
    ```

    An optional `alignment` attribute may be specified to ensure that the
    region of memory that will be indexed is aligned at the specified byte
    boundary.  This is consistent with the fact that memref.alloc supports such
    an optional alignment attribute. Note that in ISO C standard, neither alloc
    nor realloc supports alignment, though there is aligned_alloc but not
    aligned_realloc.

    ```mlir
    %3 = memref.realloc %src {alignment = 8} : memref<64xf32> to memref<124xf32>
    ```

    Referencing the memref through the old SSA value after realloc is undefined
    behavior.

    ```mlir
    %new = memref.realloc %old : memref<64xf32> to memref<124xf32>
    %4 = memref.load %new[%index]   // ok
    %5 = memref.load %old[%index]   // undefined behavior
    ```
  }];

  // Note that we conceptually mark the operands as freeing the incoming
  // memref and allocating the outcoming memref, even though this may not
  // physically happen on each execution.

  let arguments = (ins Arg<MemRefRankOf<[AnyType], [1]>, "",
                                        [MemFreeAt<0, FullEffect>]>:$source,
                   Optional<Index>:$dynamicResultSize,
                   ConfinedAttr<OptionalAttr<I64Attr>,
                                [IntMinValue<0>]>:$alignment);

  let results = (outs Res<MemRefRankOf<[AnyType], [1]>, "",
                                       [MemAlloc<DefaultResource, 1,
                                                 FullEffect>]>);

  let builders = [
    OpBuilder<(ins "MemRefType":$resultType,
                  "Value":$source,
                  CArg<"Value", "Value()">:$dynamicResultSize), [{
      return build($_builder, $_state, resultType, source, dynamicResultSize,
                   IntegerAttr());
    }]>];

    let extraClassDeclaration = [{
    /// The result of a realloc is always a memref.
    MemRefType getType() { return ::llvm::cast<MemRefType>(getResult().getType()); }
  }];

  let assemblyFormat = [{
    $source (`(` $dynamicResultSize^ `)`)? attr-dict
    `:` type($source) `to` type(results)
  }];

  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// AllocaOp
//===----------------------------------------------------------------------===//

def MemRef_AllocaOp : AllocLikeOp<"alloca", AutomaticAllocationScopeResource,[
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    DeclareOpInterfaceMethods<PromotableAllocationOpInterface>,
    DeclareOpInterfaceMethods<DestructurableAllocationOpInterface>]> {
  let summary = "stack memory allocation operation";
  let description = [{
    The `alloca` operation allocates memory on the stack, to be automatically
    released when control transfers back from the region of its closest
    surrounding operation with an
    [`AutomaticAllocationScope`](../Traits.md/#automaticallocationscope) trait.
    The amount of memory allocated is specified by its memref and additional
    operands. For example:

    ```mlir
    %0 = memref.alloca() : memref<8x64xf32>
    ```

    The optional list of dimension operands are bound to the dynamic dimensions
    specified in its memref type. In the example below, the SSA value '%d' is
    bound to the second dimension of the memref (which is dynamic).

    ```mlir
    %0 = memref.alloca(%d) : memref<8x?xf32>
    ```

    The optional list of symbol operands are bound to the symbols of the
    memref's affine map. In the example below, the SSA value '%s' is bound to
    the symbol 's0' in the affine map specified in the allocs memref type.

    ```mlir
    %0 = memref.alloca()[%s] : memref<8x64xf32,
                               affine_map<(d0, d1)[s0] -> ((d0 + s0), d1)>>
    ```

    This operation returns a single SSA value of memref type, which can be used
    by subsequent load and store operations. An optional alignment attribute, if
    specified, guarantees alignment at least to that boundary. If not specified,
    an alignment on any convenient boundary compatible with the type will be
    chosen.
  }];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// AllocaScopeOp
//===----------------------------------------------------------------------===//

def MemRef_AllocaScopeOp : MemRef_Op<"alloca_scope",
      [AutomaticAllocationScope,
       DeclareOpInterfaceMethods<RegionBranchOpInterface>,
       SingleBlockImplicitTerminator<"AllocaScopeReturnOp">,
       RecursiveMemoryEffects,
       NoRegionArguments]> {
  let summary = "explicitly delimited scope for stack allocation";
  let description = [{
    The `memref.alloca_scope` operation represents an explicitly-delimited
    scope for the alloca allocations. Any `memref.alloca` operations that are
    used within this scope are going to be cleaned up automatically once
    the control-flow exits the nested region. For example:

    ```mlir
    memref.alloca_scope {
      %myalloca = memref.alloca(): memref<4x3xf32>
      ...
    }
    ```

    Here, `%myalloca` memref is valid within the explicitly delimited scope
    and is automatically deallocated at the end of the given region. Conceptually,
    `memref.alloca_scope` is a passthrough operation with
    `AutomaticAllocationScope` that spans the body of the region within the operation.

    `memref.alloca_scope` may also return results that are defined in the nested
    region. To return a value, one should use `memref.alloca_scope.return`
    operation:

    ```mlir
    %result = memref.alloca_scope {
      ...
      memref.alloca_scope.return %value
    }
    ```

    If `memref.alloca_scope` returns no value, the `memref.alloca_scope.return ` can
    be left out, and will be inserted implicitly.
  }];

  let results = (outs Variadic<AnyType>:$results);
  let regions = (region SizedRegion<1>:$bodyRegion);
  let hasCustomAssemblyFormat = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// AllocaScopeReturnOp
//===----------------------------------------------------------------------===//

def MemRef_AllocaScopeReturnOp : MemRef_Op<"alloca_scope.return",
      [HasParent<"AllocaScopeOp">,
       Pure,
       ReturnLike,
       Terminator]> {
  let summary = "terminator for alloca_scope operation";
  let description = [{
    `memref.alloca_scope.return` operation returns zero or more SSA values
    from the region within `memref.alloca_scope`. If no values are returned,
    the return operation may be omitted. Otherwise, it has to be present
    to indicate which values are going to be returned. For example:

    ```mlir
    memref.alloca_scope.return %value
    ```
  }];

  let arguments = (ins Variadic<AnyType>:$results);
  let builders = [OpBuilder<(ins), [{ /*nothing to do */ }]>];

  let assemblyFormat = "attr-dict ($results^ `:` type($results))?";
}

//===----------------------------------------------------------------------===//
// CastOp
//===----------------------------------------------------------------------===//

def MemRef_CastOp : MemRef_Op<"cast", [
      DeclareOpInterfaceMethods<CastOpInterface>,
      DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
      MemRefsNormalizable,
      Pure,
      SameOperandsAndResultShape,
      ViewLikeOpInterface
    ]> {
  let summary = "memref cast operation";
  let description = [{
    The `memref.cast` operation converts a memref from one type to an equivalent
    type with a compatible shape. The source and destination types are
    compatible if:

    a. Both are ranked memref types with the same element type, address space,
    and rank and:
      1. Both have the same layout or both have compatible strided layouts.
      2. The individual sizes (resp. offset and strides in the case of strided
         memrefs) may convert constant dimensions to dynamic dimensions and
         vice-versa.

    If the cast converts any dimensions from an unknown to a known size, then it
    acts as an assertion that fails at runtime if the dynamic dimensions
    disagree with resultant destination size.

    Example:

    ```mlir
    // Assert that the input dynamic shape matches the destination static shape.
    %2 = memref.cast %1 : memref<?x?xf32> to memref<4x4xf32>
    // Erase static shape information, replacing it with dynamic information.
    %3 = memref.cast %1 : memref<4xf32> to memref<?xf32>

    // The same holds true for offsets and strides.

    // Assert that the input dynamic shape matches the destination static stride.
    %4 = memref.cast %1 : memref<12x4xf32, strided<[?, ?], offset: ?>> to
                          memref<12x4xf32, strided<[4, 1], offset: 5>>
    // Erase static offset and stride information, replacing it with
    // dynamic information.
    %5 = memref.cast %1 : memref<12x4xf32, strided<[4, 1], offset: 5>> to
                          memref<12x4xf32, strided<[?, ?], offset: ?>>
    ```

    b. Either or both memref types are unranked with the same element type, and
    address space.

    Example:

    ```mlir
    Cast to concrete shape.
        %4 = memref.cast %1 : memref<*xf32> to memref<4x?xf32>

    Erase rank information.
        %5 = memref.cast %1 : memref<4x?xf32> to memref<*xf32>
    ```
  }];

  let arguments = (ins AnyRankedOrUnrankedMemRef:$source);
  let results = (outs AnyRankedOrUnrankedMemRef:$dest);
  let assemblyFormat = "$source attr-dict `:` type($source) `to` type($dest)";

  let extraClassDeclaration = [{
    /// Fold the given CastOp into consumer op.
    static bool canFoldIntoConsumerOp(CastOp castOp);

    Value getViewSource() { return getSource(); }
  }];

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// CopyOp
//===----------------------------------------------------------------------===//

def CopyOp : MemRef_Op<"copy", [CopyOpInterface, SameOperandsElementType,
    SameOperandsShape]> {

  let description = [{
    Copies the data from the source to the destination memref.

    Usage:

    ```mlir
    memref.copy %arg0, %arg1 : memref<?xf32> to memref<?xf32>
    ```

    Source and destination are expected to have the same element type and shape.
    Otherwise, the result is undefined. They may have different layouts.
  }];

  let arguments = (ins Arg<AnyRankedOrUnrankedMemRef, "the memref to copy from",
                           [MemReadAt<0, FullEffect>]>:$source,
                       Arg<AnyRankedOrUnrankedMemRef, "the memref to copy to",
                           [MemWriteAt<0, FullEffect>]>:$target);

  let assemblyFormat = [{
    $source `,` $target attr-dict `:` type($source) `to` type($target)
  }];

  let hasCanonicalizer = 1;
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// DeallocOp
//===----------------------------------------------------------------------===//

def MemRef_DeallocOp : MemRef_Op<"dealloc", [MemRefsNormalizable]> {
  let summary = "memory deallocation operation";
  let description = [{
    The `dealloc` operation frees the region of memory referenced by a memref
    which was originally created by the `alloc` operation.
    The `dealloc` operation should not be called on memrefs which alias an
    alloc'd memref (e.g. memrefs returned by `view` operations).

    Example:

    ```mlir
    %0 = memref.alloc() : memref<8x64xf32, affine_map<(d0, d1) -> (d0, d1), 1>>
    memref.dealloc %0 : memref<8x64xf32,  affine_map<(d0, d1) -> (d0, d1), 1>>
    ```
  }];

  let arguments = (ins Arg<AnyRankedOrUnrankedMemRef, "",
                           [MemFreeAt<0, FullEffect>]>:$memref);

  let hasFolder = 1;
  let assemblyFormat = "$memref attr-dict `:` type($memref)";
}

//===----------------------------------------------------------------------===//
// DimOp
//===----------------------------------------------------------------------===//

def MemRef_DimOp : MemRef_Op<"dim", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    MemRefsNormalizable,
    ConditionallySpeculatable, NoMemoryEffect,
    ShapedDimOpInterface]> {
  let summary = "dimension index operation";
  let description = [{
    The `dim` operation takes a memref and a dimension operand of type `index`.
    It returns the size of the requested dimension of the given memref.
    If the dimension index is out of bounds the behavior is undefined.

    The specified memref type is that of the first operand.

    Example:

    ```mlir
    // Always returns 4, can be constant folded:
    %c0 = arith.constant 0 : index
    %x = memref.dim %A, %c0 : memref<4 x ? x f32>

    // Returns the dynamic dimension of %A.
    %c1 = arith.constant 1 : index
    %y = memref.dim %A, %c1 : memref<4 x ? x f32>

    // Equivalent generic form:
    %x = "memref.dim"(%A, %c0) : (memref<4 x ? x f32>, index) -> index
    %y = "memref.dim"(%A, %c1) : (memref<4 x ? x f32>, index) -> index
    ```
  }];

  let arguments = (ins AnyNon0RankedOrUnrankedMemRef:$source,
                       Index:$index);
  let results = (outs Index:$result);

  let assemblyFormat = [{
    attr-dict $source `,` $index `:` type($source)
  }];

  let builders = [
    OpBuilder<(ins "Value":$source, "int64_t":$index)>,
  ];

  let extraClassDeclaration = [{
    /// Helper function to get the index as a simple integer if it is constant.
    std::optional<int64_t> getConstantIndex();

    /// Interface method of ShapedDimOpInterface: Return the source memref.
    Value getShapedValue() { return getSource(); }

    /// Interface method of ShapedDimOpInterface: Return the dimension.
    OpFoldResult getDimension() { return getIndex(); }

    /// Interface method for ConditionallySpeculatable.
    Speculation::Speculatability getSpeculatability();
  }];

  let hasCanonicalizer = 1;
  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// DmaStartOp
//===----------------------------------------------------------------------===//

def MemRef_DmaStartOp : MemRef_Op<"dma_start"> {
  let summary = "non-blocking DMA operation that starts a transfer";
  let description = [{
    Syntax:

    ```
    operation ::= `memref.dma_start` ssa-use`[`ssa-use-list`]` `,`
                   ssa-use`[`ssa-use-list`]` `,` ssa-use `,`
                   ssa-use`[`ssa-use-list`]` (`,` ssa-use `,` ssa-use)?
                  `:` memref-type `,` memref-type `,` memref-type
    ```

    DmaStartOp starts a non-blocking DMA operation that transfers data from a
    source memref to a destination memref. The source and destination memref
    need not be of the same dimensionality, but need to have the same elemental
    type. The operands include the source and destination memref's each followed
    by its indices, size of the data transfer in terms of the number of elements
    (of the elemental type of the memref), a tag memref with its indices, and
    optionally at the end, a stride and a number_of_elements_per_stride
    arguments. The tag location is used by a DmaWaitOp to check for completion.
    The indices of the source memref, destination memref, and the tag memref
    have the same restrictions as any load/store. The optional stride arguments
    should be of 'index' type, and specify a stride for the slower memory space
    (memory space with a lower memory space id), transferring chunks of
    number_of_elements_per_stride every stride until %num_elements are
    transferred. Either both or no stride arguments should be specified. If the
    source and destination locations overlap the behavior of this operation is
    not defined.

    For example, a DmaStartOp operation that transfers 256 elements of a memref
    '%src' in memory space 0 at indices [%i, %j] to memref '%dst' in memory
    space 1 at indices [%k, %l], would be specified as follows:

    ```mlir
    %num_elements = arith.constant 256
    %idx = arith.constant 0 : index
    %tag = memref.alloc() : memref<1 x i32, affine_map<(d0) -> (d0)>, 4>
    dma_start %src[%i, %j], %dst[%k, %l], %num_elements, %tag[%idx] :
      memref<40 x 128 x f32>, affine_map<(d0) -> (d0)>, 0>,
      memref<2 x 1024 x f32>, affine_map<(d0) -> (d0)>, 1>,
      memref<1 x i32>, affine_map<(d0) -> (d0)>, 2>
    ```

    If %stride and %num_elt_per_stride are specified, the DMA is expected to
    transfer %num_elt_per_stride elements every %stride elements apart from
    memory space 0 until %num_elements are transferred.

    ```mlir
    dma_start %src[%i, %j], %dst[%k, %l], %num_elements, %tag[%idx], %stride,
              %num_elt_per_stride :
    ```

    * TODO: add additional operands to allow source and destination striding, and
    multiple stride levels.
    * TODO: Consider replacing src/dst memref indices with view memrefs.
  }];
  let arguments = (ins Variadic<AnyType>:$operands);

  let builders = [
    OpBuilder<(ins "Value":$srcMemRef, "ValueRange":$srcIndices,
                   "Value":$destMemRef, "ValueRange":$destIndices,
                   "Value":$numElements, "Value":$tagMemRef,
                   "ValueRange":$tagIndices, CArg<"Value", "{}">:$stride,
                   CArg<"Value", "{}">:$elementsPerStride)>
  ];

  let extraClassDeclaration = [{
    // Returns the source MemRefType for this DMA operation.
    Value getSrcMemRef() { return getOperand(0); }
    OpOperand &getSrcMemRefMutable() { return getOperation()->getOpOperand(0); }
    // Returns the rank (number of indices) of the source MemRefType.
    unsigned getSrcMemRefRank() {
      return ::llvm::cast<MemRefType>(getSrcMemRef().getType()).getRank();
    }
    // Returns the source memref indices for this DMA operation.
    operand_range getSrcIndices() {
      return {(*this)->operand_begin() + 1,
              (*this)->operand_begin() + 1 + getSrcMemRefRank()};
    }

    // Returns the destination MemRefType for this DMA operations.
    Value getDstMemRef() { return getOperand(1 + getSrcMemRefRank()); }
    OpOperand &getDstMemRefMutable() { return getOperation()->getOpOperand(1 + getSrcMemRefRank()); }
    // Returns the rank (number of indices) of the destination MemRefType.
    unsigned getDstMemRefRank() {
      return ::llvm::cast<MemRefType>(getDstMemRef().getType()).getRank();
    }
    unsigned getSrcMemorySpace() {
      return ::llvm::cast<MemRefType>(getSrcMemRef().getType()).getMemorySpaceAsInt();
    }
    unsigned getDstMemorySpace() {
      return ::llvm::cast<MemRefType>(getDstMemRef().getType()).getMemorySpaceAsInt();
    }

    // Returns the destination memref indices for this DMA operation.
    operand_range getDstIndices() {
      return {(*this)->operand_begin() + 1 + getSrcMemRefRank() + 1,
              (*this)->operand_begin() + 1 + getSrcMemRefRank() + 1 +
                  getDstMemRefRank()};
    }

    // Returns the number of elements being transferred by this DMA operation.
    Value getNumElements() {
      return getOperand(1 + getSrcMemRefRank() + 1 + getDstMemRefRank());
    }

    // Returns the Tag MemRef for this DMA operation.
    Value getTagMemRef() {
      return getOperand(1 + getSrcMemRefRank() + 1 + getDstMemRefRank() + 1);
    }
    OpOperand &getTagMemRefMutable() {
      return getOperation()->getOpOperand(1 + getSrcMemRefRank() + 1 + getDstMemRefRank() + 1);
    }

    // Returns the rank (number of indices) of the tag MemRefType.
    unsigned getTagMemRefRank() {
      return ::llvm::cast<MemRefType>(getTagMemRef().getType()).getRank();
    }

    // Returns the tag memref index for this DMA operation.
    operand_range getTagIndices() {
      unsigned tagIndexStartPos =
          1 + getSrcMemRefRank() + 1 + getDstMemRefRank() + 1 + 1;
      return {(*this)->operand_begin() + tagIndexStartPos,
              (*this)->operand_begin() + tagIndexStartPos + getTagMemRefRank()};
    }

    /// Returns true if this is a DMA from a faster memory space to a slower
    /// one.
    bool isDestMemorySpaceFaster() {
      return (getSrcMemorySpace() < getDstMemorySpace());
    }

    /// Returns true if this is a DMA from a slower memory space to a faster
    /// one.
    bool isSrcMemorySpaceFaster() {
      // Assumes that a lower number is for a slower memory space.
      return (getDstMemorySpace() < getSrcMemorySpace());
    }

    /// Given a DMA start operation, returns the operand position of either the
    /// source or destination memref depending on the one that is at the higher
    /// level of the memory hierarchy. Asserts failure if neither is true.
    unsigned getFasterMemPos() {
      assert(isSrcMemorySpaceFaster() || isDestMemorySpaceFaster());
      return isSrcMemorySpaceFaster() ? 0 : getSrcMemRefRank() + 1;
    }

    bool isStrided() {
      return getNumOperands() != 1 + getSrcMemRefRank() + 1 +
                                 getDstMemRefRank() + 1 + 1 +
                                 getTagMemRefRank();
    }

    Value getStride() {
      if (!isStrided())
        return nullptr;
      return getOperand(getNumOperands() - 1 - 1);
    }

    Value getNumElementsPerStride() {
      if (!isStrided())
        return nullptr;
      return getOperand(getNumOperands() - 1);
    }

    void getEffects(
        SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>> &
        effects) {
      effects.emplace_back(MemoryEffects::Read::get(), &getSrcMemRefMutable(),
                           SideEffects::DefaultResource::get());
      effects.emplace_back(MemoryEffects::Write::get(), &getDstMemRefMutable(),
                           SideEffects::DefaultResource::get());
      effects.emplace_back(MemoryEffects::Read::get(), &getTagMemRefMutable(),
                           SideEffects::DefaultResource::get());
    }
  }];
  let hasCustomAssemblyFormat = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// DmaWaitOp
//===----------------------------------------------------------------------===//

def MemRef_DmaWaitOp : MemRef_Op<"dma_wait"> {
  let summary = "blocking DMA operation that waits for transfer completion";
  let description = [{
   DmaWaitOp blocks until the completion of a DMA operation associated with the
   tag element '%tag[%index]'. %tag is a memref, and %index has to be an index
   with the same restrictions as any load/store index. %num_elements is the
   number of elements associated with the DMA operation.

   Example:

   ```mlir
    dma_start %src[%i, %j], %dst[%k, %l], %num_elements, %tag[%index] :
      memref<2048 x f32>, affine_map<(d0) -> (d0)>, 0>,
      memref<256 x f32>, affine_map<(d0) -> (d0)>, 1>
      memref<1 x i32>, affine_map<(d0) -> (d0)>, 2>
    ...
    ...
    dma_wait %tag[%index], %num_elements : memref<1 x i32, affine_map<(d0) -> (d0)>, 2>
    ```
  }];
  let arguments = (ins AnyMemRef:$tagMemRef,
                       Variadic<Index>:$tagIndices,
                       Index:$numElements);
  let assemblyFormat = [{
    $tagMemRef `[` $tagIndices `]` `,` $numElements attr-dict `:` type($tagMemRef)
  }];
  let extraClassDeclaration = [{
    /// Returns the rank (number of indices) of the tag memref.
    unsigned getTagMemRefRank() {
      return ::llvm::cast<MemRefType>(getTagMemRef().getType()).getRank();
    }
    void getEffects(
        SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>> &
        effects) {
      effects.emplace_back(MemoryEffects::Read::get(), &getTagMemRefMutable(),
                           SideEffects::DefaultResource::get());
    }
  }];
  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// ExtractAlignedPointerAsIndexOp
//===----------------------------------------------------------------------===//

def MemRef_ExtractAlignedPointerAsIndexOp :
  MemRef_Op<"extract_aligned_pointer_as_index", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    Pure,
    SameVariadicResultSize]> {
  let summary = "Extracts a memref's underlying aligned pointer as an index";
  let description = [{
    Extracts the underlying aligned pointer as an index.

    This operation is useful for lowering to lower-level dialects while still
    avoiding the need to define a pointer type in higher-level dialects such as
    the memref dialect.

    This operation is intended solely as step during lowering, it has no side
    effects. A reverse operation that creates a memref from an index interpreted
    as a pointer is explicitly discouraged.

    Example:

    ```
      %0 = memref.extract_aligned_pointer_as_index %arg : memref<4x4xf32> -> index
      %1 = arith.index_cast %0 : index to i64
      %2 = llvm.inttoptr %1 : i64 to !llvm.ptr
      call @foo(%2) : (!llvm.ptr) ->()
    ```
  }];

  let arguments = (ins
    AnyRankedOrUnrankedMemRef:$source
  );
  let results = (outs Index:$aligned_pointer);

  let assemblyFormat = [{
    $source `:` type($source) `->` type(results) attr-dict
  }];
}

//===----------------------------------------------------------------------===//
// ExtractStridedMetadataOp
//===----------------------------------------------------------------------===//

def MemRef_ExtractStridedMetadataOp : MemRef_Op<"extract_strided_metadata", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    Pure,
    SameVariadicResultSize,
    ViewLikeOpInterface,
    InferTypeOpAdaptor]> {
  let summary = "Extracts a buffer base with offset and strides";
  let description = [{
    Extracts a base buffer, offset and strides. This op allows additional layers
    of transformations and foldings to be added as lowering progresses from
    higher-level dialect to lower-level dialects such as the LLVM dialect.

    The op requires a strided memref source operand. If the source operand is not
    a strided memref, then verification fails.

    This operation is also useful for completeness to the existing memref.dim op.
    While accessing strides, offsets and the base pointer independently is not
    available, this is useful for composing with its natural complement op:
    `memref.reinterpret_cast`.

    Intended Use Cases:

    The main use case is to expose the logic for manipulate memref metadata at a
    higher level than the LLVM dialect.
    This makes lowering more progressive and brings the following benefits:
      - not all users of MLIR want to lower to LLVM and the information to e.g.
        lower to library calls---like libxsmm---or to SPIR-V was not available.
      - foldings and canonicalizations can happen at a higher level in MLIR:
        before this op existed, lowering to LLVM would create large amounts of
        LLVMIR. Even when LLVM does a good job at folding the low-level IR from
        a performance perspective, it is unnecessarily opaque and inefficient to
        send unkempt IR to LLVM.

    Example:

    ```mlir
      %base, %offset, %sizes:2, %strides:2 =
        memref.extract_strided_metadata %memref :
          memref<10x?xf32>, index, index, index, index, index

      // After folding, the type of %m2 can be memref<10x?xf32> and further
      // folded to %memref.
      %m2 = memref.reinterpret_cast %base to
          offset: [%offset],
          sizes: [%sizes#0, %sizes#1],
          strides: [%strides#0, %strides#1]
        : memref<f32> to memref<?x?xf32, offset: ?, strides: [?, ?]>
    ```
  }];

  let arguments = (ins
    AnyStridedMemRef:$source
  );
  let results = (outs
    AnyStridedMemRefOfRank<0>:$base_buffer,
    Index:$offset,
    Variadic<Index>:$sizes,
    Variadic<Index>:$strides
  );

  let assemblyFormat = [{
    $source `:` type($source) `->` type(results) attr-dict
  }];

  let extraClassDeclaration = [{
    /// Return a vector of all the static or dynamic sizes of the op, while
    /// statically inferring the sizes of the dynamic sizes, when possible.
    /// This is best effort.
    /// E.g., if `getSizes` returns `[%dyn_size0, %dyn_size1]`, but the
    /// source memref type is `memref<2x8xi16>`, this method will
    /// return `[2, 8]`.
    /// Similarly if the resulting memref type is `memref<2x?xi16>`, but
    /// `%dyn_size1` can statically be pinned to a constant value, this
    /// constant value is returned instead of `%dyn_size`.
    SmallVector<OpFoldResult> getConstifiedMixedSizes();
    /// Similar to `getConstifiedMixedSizes` but for strides.
    SmallVector<OpFoldResult> getConstifiedMixedStrides();
    /// Similar to `getConstifiedMixedSizes` but for the offset.
    OpFoldResult getConstifiedMixedOffset();

    ::mlir::Value getViewSource() { return getSource(); }
  }];

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// GenericAtomicRMWOp
//===----------------------------------------------------------------------===//

def GenericAtomicRMWOp : MemRef_Op<"generic_atomic_rmw", [
      SingleBlockImplicitTerminator<"AtomicYieldOp">,
      TypesMatchWith<"result type matches element type of memref",
                     "memref", "result",
                     "::llvm::cast<MemRefType>($_self).getElementType()">
    ]> {
  let summary = "atomic read-modify-write operation with a region";
  let description = [{
    The `memref.generic_atomic_rmw` operation provides a way to perform a
    read-modify-write sequence that is free from data races. The memref operand
    represents the buffer that the read and write will be performed against, as
    accessed by the specified indices. The arity of the indices is the rank of
    the memref. The result represents the latest value that was stored. The
    region contains the code for the modification itself. The entry block has
    a single argument that represents the value stored in `memref[indices]`
    before the write is performed. No side-effecting ops are allowed in the
    body of `GenericAtomicRMWOp`.

    Example:

    ```mlir
    %x = memref.generic_atomic_rmw %I[%i] : memref<10xf32> {
      ^bb0(%current_value : f32):
        %c1 = arith.constant 1.0 : f32
        %inc = arith.addf %c1, %current_value : f32
        memref.atomic_yield %inc : f32
    }
    ```
  }];

  let arguments = (ins
      Arg<MemRefOf<[AnySignlessInteger, AnyFloat]>, "the reference to read from and write to", [MemRead, MemWrite]>:$memref,
      Variadic<Index>:$indices);

  let results = (outs
      AnyTypeOf<[AnySignlessInteger, AnyFloat]>:$result);

  let regions = (region AnyRegion:$atomic_body);

  let skipDefaultBuilders = 1;
  let builders = [OpBuilder<(ins "Value":$memref, "ValueRange":$ivs)>];

  let extraClassDeclaration = [{
    // TODO: remove post migrating callers.
    Region &body() { return getRegion(); }

    // The value stored in memref[ivs].
    Value getCurrentValue() {
      return getRegion().getArgument(0);
    }
    MemRefType getMemRefType() {
      return ::llvm::cast<MemRefType>(getMemref().getType());
    }
  }];
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}

def AtomicYieldOp : MemRef_Op<"atomic_yield", [
      HasParent<"GenericAtomicRMWOp">,
      Pure,
      Terminator
    ]> {
  let summary = "yield operation for GenericAtomicRMWOp";
  let description = [{
    "memref.atomic_yield" yields an SSA value from a
    GenericAtomicRMWOp region.
  }];

  let arguments = (ins AnyType:$result);
  let assemblyFormat = "$result attr-dict `:` type($result)";
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// GetGlobalOp
//===----------------------------------------------------------------------===//

def MemRef_GetGlobalOp : MemRef_Op<"get_global",
    [Pure, DeclareOpInterfaceMethods<SymbolUserOpInterface>]> {
  let summary = "get the memref pointing to a global variable";
  let description = [{
     The `memref.get_global` operation retrieves the memref pointing to a
     named global variable. If the global variable is marked constant, writing
     to the result memref (such as through a `memref.store` operation) is
     undefined.

     Example:

     ```mlir
     %x = memref.get_global @foo : memref<2xf32>
     ```
  }];

  let arguments = (ins FlatSymbolRefAttr:$name);
  let results = (outs AnyStaticShapeMemRef:$result);
  let assemblyFormat = "$name `:` type($result) attr-dict";
}

//===----------------------------------------------------------------------===//
// GlobalOp
//===----------------------------------------------------------------------===//

def MemRef_GlobalOp : MemRef_Op<"global", [Symbol]> {
  let summary = "declare or define a global memref variable";
  let description = [{
    The `memref.global` operation declares or defines a named global memref
    variable. The backing memory for the variable is allocated statically and is
    described by the type of the variable (which should be a statically shaped
    memref type). The operation is a declaration if no `initial_value` is
    specified, else it is a definition. The `initial_value` can either be a unit
    attribute to represent a definition of an uninitialized global variable, or
    an elements attribute to represent the definition of a global variable with
    an initial value. The global variable can also be marked constant using the
    `constant` unit attribute. Writing to such constant global variables is
    undefined.

    The global variable can be accessed by using the `memref.get_global` to
    retrieve the memref for the global variable. Note that the memref
    for such global variable itself is immutable (i.e., memref.get_global for a
    given global variable will always return the same memref descriptor).

    Example:

    ```mlir
    // Private variable with an initial value.
    memref.global "private" @x : memref<2xf32> = dense<0.0,2.0>

    // Private variable with an initial value and an alignment (power of 2).
    memref.global "private" @x : memref<2xf32> = dense<0.0,2.0> {alignment = 64}

    // Declaration of an external variable.
    memref.global "private" @y : memref<4xi32>

    // Uninitialized externally visible variable.
    memref.global @z : memref<3xf16> = uninitialized

    // Externally visible constant variable.
    memref.global constant @c : memref<2xi32> = dense<1, 4>
    ```
  }];

  let arguments = (ins SymbolNameAttr:$sym_name,
                       OptionalAttr<StrAttr>:$sym_visibility,
                       MemRefTypeAttr:$type,
                       OptionalAttr<AnyAttr>:$initial_value,
                       UnitAttr:$constant,
                       OptionalAttr<I64Attr>:$alignment);

  let assemblyFormat = [{
       ($sym_visibility^)?
       (`constant` $constant^)?
       $sym_name `:`
       custom<GlobalMemrefOpTypeAndInitialValue>($type, $initial_value)
       attr-dict
  }];

  let extraClassDeclaration = [{
     bool isExternal() { return !getInitialValue(); }
     bool isUninitialized() {
       return !isExternal() && ::llvm::isa<UnitAttr>(*getInitialValue());
     }
     /// Returns the constant initial value if the memref.global is a constant,
     /// or null otherwise.
     ElementsAttr getConstantInitValue();
  }];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// LoadOp
//===----------------------------------------------------------------------===//

def LoadOp : MemRef_Op<"load",
     [TypesMatchWith<"result type matches element type of 'memref'",
                     "memref", "result",
                     "::llvm::cast<MemRefType>($_self).getElementType()">,
      MemRefsNormalizable,
      DeclareOpInterfaceMethods<PromotableMemOpInterface>,
      DeclareOpInterfaceMethods<DestructurableAccessorOpInterface>]> {
  let summary = "load operation";
  let description = [{
    The `load` op reads an element from a memref specified by an index list. The
    output of load is a new value with the same type as the elements of the
    memref. The arity of indices is the rank of the memref (i.e., if the memref
    loaded from is of rank 3, then 3 indices are required for the load following
    the memref identifier).

    In an `affine.if` or `affine.for` body, the indices of a load are restricted
    to SSA values bound to surrounding loop induction variables,
    [symbols](Affine.md/#dimensions-and-symbols), results of a
    constant operations, or the result of an
    `affine.apply` operation that can in turn take as arguments all of the
    aforementioned SSA values or the recursively result of such an
    `affine.apply` operation.

    Example:

    ```mlir
    %1 = affine.apply affine_map<(d0, d1) -> (3*d0)> (%i, %j)
    %2 = affine.apply affine_map<(d0, d1) -> (d1+1)> (%i, %j)
    %12 = memref.load %A[%1, %2] : memref<8x?xi32, #layout, memspace0>

    // Example of an indirect load (treated as non-affine)
    %3 = affine.apply affine_map<(d0) -> (2*d0 + 1)>(%12)
    %13 = memref.load %A[%3, %2] : memref<4x?xi32, #layout, memspace0>
    ```

    **Context:** The `load` and `store` operations are specifically crafted to
    fully resolve a reference to an element of a memref, and (in affine
    `affine.if` and `affine.for` operations) the compiler can follow use-def
    chains (e.g. through [`affine.apply`](Affine.md/#affineapply-affineapplyop)
    operations) to precisely analyze references at compile-time using polyhedral
    techniques. This is possible because of the
    [restrictions on dimensions and symbols](Affine.md/#restrictions-on-dimensions-and-symbols)
    in these contexts.
  }];

  let arguments = (ins Arg<AnyMemRef, "the reference to load from",
                           [MemRead]>:$memref,
                       Variadic<Index>:$indices,
                       DefaultValuedOptionalAttr<BoolAttr, "false">:$nontemporal);
  let results = (outs AnyType:$result);

  let extraClassDeclaration = [{
    Value getMemRef() { return getOperand(0); }
    void setMemRef(Value value) { setOperand(0, value); }
    MemRefType getMemRefType() {
      return ::llvm::cast<MemRefType>(getMemRef().getType());
    }
  }];

  let hasFolder = 1;
  let hasVerifier = 1;

  let assemblyFormat = "$memref `[` $indices `]` attr-dict `:` type($memref)";
}

//===----------------------------------------------------------------------===//
// MemorySpaceCastOp
//===----------------------------------------------------------------------===//
def MemRef_MemorySpaceCastOp : MemRef_Op<"memory_space_cast", [
      DeclareOpInterfaceMethods<CastOpInterface>,
      DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
      MemRefsNormalizable,
      Pure,
      SameOperandsAndResultElementType,
      SameOperandsAndResultShape,
      ViewLikeOpInterface
    ]> {
  let summary = "memref memory space cast operation";
  let description = [{
    This operation casts memref values between memory spaces.
    The input and result will be memrefs of the same types and shape that alias
    the same underlying memory, though, for some casts on some targets,
    the underlying values of the pointer stored in the memref may be affected
    by the cast.

    The input and result must have the same shape, element type, rank, and layout.

    If the source and target address spaces are the same, this operation is a noop.

    Example:

    ```mlir
    // Cast a GPU private memory attribution into a generic pointer
    %2 = memref.memory_space_cast %1 : memref<?xf32, 5> to memref<?xf32>
    // Cast a generic pointer to workgroup-local memory
    %4 = memref.memory_space_cast %3 : memref<5x4xi32> to memref<5x34xi32, 3>
    // Cast between two non-default memory spaces
    %6 = memref.memory_space_cast %5
      : memref<*xmemref<?xf32>, 5> to memref<*xmemref<?xf32>, 3>
    ```
  }];

  let arguments = (ins AnyRankedOrUnrankedMemRef:$source);
  let results = (outs AnyRankedOrUnrankedMemRef:$dest);
  let assemblyFormat = "$source attr-dict `:` type($source) `to` type($dest)";

  let extraClassDeclaration = [{
    Value getViewSource() { return getSource(); }
  }];

  let hasFolder = 1;
}

//===----------------------------------------------------------------------===//
// PrefetchOp
//===----------------------------------------------------------------------===//

def MemRef_PrefetchOp : MemRef_Op<"prefetch"> {
  let summary = "prefetch operation";
  let description = [{
    The "prefetch" op prefetches data from a memref location described with
    subscript indices similar to memref.load, and with three attributes: a
    read/write specifier, a locality hint, and a cache type specifier as shown
    below:

    ```mlir
    memref.prefetch %0[%i, %j], read, locality<3>, data : memref<400x400xi32>
    ```

    The read/write specifier is either 'read' or 'write', the locality hint
    ranges from locality<0> (no locality) to locality<3> (extremely local keep
    in cache). The cache type specifier is either 'data' or 'instr'
    and specifies whether the prefetch is performed on data cache or on
    instruction cache.
  }];

  let arguments = (ins AnyMemRef:$memref, Variadic<Index>:$indices,
                       BoolAttr:$isWrite,
                       ConfinedAttr<I32Attr, [IntMinValue<0>,
                                          IntMaxValue<3>]>:$localityHint,
                       BoolAttr:$isDataCache);

  let extraClassDeclaration = [{
    MemRefType getMemRefType() {
      return ::llvm::cast<MemRefType>(getMemref().getType());
    }
    static StringRef getLocalityHintAttrStrName() { return "localityHint"; }
    static StringRef getIsWriteAttrStrName() { return "isWrite"; }
    static StringRef getIsDataCacheAttrStrName() { return "isDataCache"; }
  }];

  let hasCustomAssemblyFormat = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// ReinterpretCastOp
//===----------------------------------------------------------------------===//

def MemRef_ReinterpretCastOp
  : MemRef_OpWithOffsetSizesAndStrides<"reinterpret_cast", [
      DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
      AttrSizedOperandSegments,
      MemRefsNormalizable,
      Pure,
      OffsetSizeAndStrideOpInterface,
      ViewLikeOpInterface
    ]> {
  let summary = "memref reinterpret cast operation";
  let description = [{
    Modify offset, sizes and strides of an unranked/ranked memref.

    Example:
    ```mlir
    memref.reinterpret_cast %ranked to
      offset: [0],
      sizes: [%size0, 10],
      strides: [1, %stride1]
    : memref<?x?xf32> to memref<?x10xf32, strided<[1, ?], offset: 0>>

    memref.reinterpret_cast %unranked to
      offset: [%offset],
      sizes: [%size0, %size1],
      strides: [%stride0, %stride1]
    : memref<*xf32> to memref<?x?xf32, strided<[?, ?], offset: ?>>
    ```

    This operation creates a new memref descriptor using the base of the
    source and applying the input arguments to the other metadata.
    In other words:
    ```mlir
    %dst = memref.reinterpret_cast %src to
      offset: [%offset],
      sizes: [%sizes],
      strides: [%strides]
    ```
    means that `%dst`'s descriptor will be:
    ```mlir
    %dst.base = %src.base
    %dst.aligned = %src.aligned
    %dst.offset = %offset
    %dst.sizes = %sizes
    %dst.strides = %strides
    ```
  }];

  let arguments = (ins Arg<AnyRankedOrUnrankedMemRef, "", []>:$source,
                       Variadic<Index>:$offsets,
                       Variadic<Index>:$sizes,
                       Variadic<Index>:$strides,
                       DenseI64ArrayAttr:$static_offsets,
                       DenseI64ArrayAttr:$static_sizes,
                       DenseI64ArrayAttr:$static_strides);
  let results = (outs AnyMemRef:$result);

  let assemblyFormat = [{
    $source `to` `offset` `` `:`
    custom<DynamicIndexList>($offsets, $static_offsets)
    `` `,` `sizes` `` `:`
    custom<DynamicIndexList>($sizes, $static_sizes)
    `` `,` `strides` `` `:`
    custom<DynamicIndexList>($strides, $static_strides)
    attr-dict `:` type($source) `to` type($result)
  }];

  let hasVerifier = 1;

  let builders = [
    // Build a ReinterpretCastOp with mixed static and dynamic entries.
    OpBuilder<(ins "MemRefType":$resultType, "Value":$source,
      "OpFoldResult":$offset, "ArrayRef<OpFoldResult>":$sizes,
      "ArrayRef<OpFoldResult>":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build a ReinterpretCastOp and infer the result type.
    OpBuilder<(ins "Value":$source, "OpFoldResult":$offset,
      "ArrayRef<OpFoldResult>":$sizes, "ArrayRef<OpFoldResult>":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build a ReinterpretCastOp with static entries.
    OpBuilder<(ins "MemRefType":$resultType, "Value":$source,
      "int64_t":$offset, "ArrayRef<int64_t>":$sizes,
      "ArrayRef<int64_t>":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build a ReinterpretCastOp with dynamic entries.
    OpBuilder<(ins "MemRefType":$resultType, "Value":$source,
      "Value":$offset, "ValueRange":$sizes,
      "ValueRange":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>
  ];

  let extraClassDeclaration = extraBaseClassDeclaration # [{
    // The result of the op is always a ranked memref.
    MemRefType getType() { return getResult().getType(); }
    Value getViewSource() { return getSource(); }

    /// Return the rank of the result type.
    unsigned getResultRank() { return getType().getRank(); }

    /// Return the expected rank of each of the`static_offsets`, `static_sizes`
    /// and `static_strides` attributes.
    std::array<unsigned, 3> getArrayAttrMaxRanks() {
      unsigned resultRank = getType().getRank();
      return {1, resultRank, resultRank};
    }

    /// Return the number of leading operands before the `offsets`, `sizes` and
    /// and `strides` operands.
    static unsigned getOffsetSizeAndStrideStartOperandIndex() { return 1; }

    /// Return a vector of all the static or dynamic sizes of the op, while
    /// statically inferring the sizes of the dynamic sizes, when possible.
    /// This is best effort.
    /// E.g., if `getMixedSizes` returns `[2, %dyn_size]`, but the resulting
    /// memref type is `memref<2x8xi16>`, this method will return `[2, 8]`.
    /// Similarly if the resulting memref type is `memref<2x?xi16>`, but
    /// `%dyn_size` can statically be pinned to a constant value, this
    /// constant value is returned instead of `%dyn_size`.
    SmallVector<OpFoldResult> getConstifiedMixedSizes();
    /// Similar to `getConstifiedMixedSizes` but for strides.
    SmallVector<OpFoldResult> getConstifiedMixedStrides();
    /// Similar to `getConstifiedMixedSizes` but for the offset.
    OpFoldResult getConstifiedMixedOffset();
  }];

  let hasFolder = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// RankOp
//===----------------------------------------------------------------------===//

def MemRef_RankOp : MemRef_Op<"rank", [Pure]> {
  let summary = "rank operation";
  let description = [{
    The `memref.rank` operation takes a memref operand and returns its rank.

    Example:

    ```mlir
    %0 = memref.rank %arg0 : memref<*xf32>
    %1 = memref.rank %arg1 : memref<?x?xf32>
    ```
  }];

  let arguments = (ins AnyRankedOrUnrankedMemRef:$memref);
  let results = (outs Index);

  let hasFolder = 1;
  let assemblyFormat = "$memref attr-dict `:` type($memref)";
}

//===----------------------------------------------------------------------===//
// ReshapeOp
//===----------------------------------------------------------------------===//

def MemRef_ReshapeOp: MemRef_Op<"reshape", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    Pure,
    ViewLikeOpInterface]>  {
  let summary = "memref reshape operation";
  let description = [{
    The `reshape` operation converts a memref from one type to an
    equivalent type with a provided shape. The data is never copied or
    modified. The source and destination types are compatible if both have the
    same element type, same number of elements, address space and identity
    layout map. The following combinations are possible:

    a. Source type is ranked or unranked. Shape argument has static size.
    Result type is ranked.

    ```mlir
    // Reshape statically-shaped memref.
    %dst = memref.reshape %src(%shape)
             : (memref<4x1xf32>, memref<1xi32>) to memref<4xf32>
    %dst0 = memref.reshape %src(%shape0)
             : (memref<4x1xf32>, memref<2xi32>) to memref<2x2xf32>
    // Flatten unranked memref.
    %dst = memref.reshape %src(%shape)
             : (memref<*xf32>, memref<1xi32>) to memref<?xf32>
    ```

    b. Source type is ranked or unranked. Shape argument has dynamic size.
    Result type is unranked.

    ```mlir
    // Reshape dynamically-shaped 1D memref.
    %dst = memref.reshape %src(%shape)
             : (memref<?xf32>, memref<?xi32>) to memref<*xf32>
    // Reshape unranked memref.
    %dst = memref.reshape %src(%shape)
             : (memref<*xf32>, memref<?xi32>) to memref<*xf32>
    ```
  }];

  let arguments = (ins AnyRankedOrUnrankedMemRef:$source,
                       Arg<MemRefRankOf<[AnySignlessInteger, Index], [1]>,
                       "dynamically-sized shape", [MemRead]>:$shape);
  let results = (outs AnyRankedOrUnrankedMemRef:$result);

  let builders = [OpBuilder<
     (ins "MemRefType":$resultType, "Value":$operand, "Value":$shape), [{
       $_state.addOperands(operand);
       $_state.addOperands(shape);
       $_state.addTypes(resultType);
     }]>];

  let extraClassDeclaration = [{
    MemRefType getType() { return ::llvm::cast<MemRefType>(getResult().getType()); }
    Value getViewSource() { return getSource(); }
  }];

  let assemblyFormat = [{
    $source `(` $shape `)` attr-dict `:` functional-type(operands, results)
  }];
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// ExpandShapeOp / CollapseShapeOp
//===----------------------------------------------------------------------===//

class MemRef_ReassociativeReshapeOp<string mnemonic, list<Trait> traits = []> :
    MemRef_Op<mnemonic, !listconcat(traits,
      [Pure, ViewLikeOpInterface])>,
    Results<(outs AnyStridedMemRef:$result)>{

  code commonExtraClassDeclaration = [{
    SmallVector<AffineMap, 4> getReassociationMaps();

    SmallVector<ReassociationExprs, 4> getReassociationExprs();

    SmallVector<ReassociationIndices, 4> getReassociationIndices() {
      SmallVector<ReassociationIndices, 4> reassociationIndices;
      for (auto attr : getReassociation())
        reassociationIndices.push_back(llvm::to_vector<2>(
            llvm::map_range(::llvm::cast<ArrayAttr>(attr), [&](Attribute indexAttr) {
              return ::llvm::cast<IntegerAttr>(indexAttr).getInt();
            })));
      return reassociationIndices;
    };

    MemRefType getSrcType() { return ::llvm::cast<MemRefType>(getSrc().getType()); }

    MemRefType getResultType() { return ::llvm::cast<MemRefType>(getResult().getType()); }

    Value getViewSource() { return getSrc(); }
  }];

  let hasFolder = 1;
  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

def MemRef_ExpandShapeOp : MemRef_ReassociativeReshapeOp<"expand_shape", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    DeclareOpInterfaceMethods<ReifyRankedShapedTypeOpInterface>]> {
  let summary = "operation to produce a memref with a higher rank.";
  let description = [{
    The `memref.expand_shape` op produces a new view with a higher rank whose
    sizes are a reassociation of the original `view`. The operation is limited
    to such reassociations, where a dimension is expanded into one or multiple
    contiguous dimensions. Such reassociations never require additional allocs
    or copies.

    A reassociation is defined as a grouping of dimensions and is represented
    with an array of DenseI64ArrayAttr attributes.

    Example:

    ```mlir
    %r = memref.expand_shape %0 [[0, 1], [2]] output_shape [%sz0, %sz1, 32]
        : memref<?x32xf32> into memref<?x?x32xf32>
    ```

    If an op can be statically proven to be invalid (e.g, an expansion from
    `memref<10xf32>` to `memref<2x6xf32>`), it is rejected by the verifier. If
    it cannot statically be proven invalid (e.g., the full example above; it is
    unclear whether the first source dimension is divisible by 5), the op is
    accepted by the verifier. However, if the op is in fact invalid at runtime,
    the behavior is undefined.

    The source memref can be zero-ranked. In that case, the reassociation
    indices must be empty and the result shape may only consist of unit
    dimensions.

    For simplicity, this op may not be used to cast dynamicity of dimension
    sizes and/or strides. I.e., if and only if a source dimension is dynamic,
    there must be a dynamic result dimension in the corresponding reassociation
    group. Same for strides.

    The representation for the output shape supports a partially-static
    specification via attributes specified through the `static_output_shape`
    argument.  A special sentinel value `ShapedType::kDynamic` encodes that the
    corresponding entry has a dynamic value.  There must be exactly as many SSA
    inputs in `output_shape` as there are `ShapedType::kDynamic` entries in
    `static_output_shape`.

    Note: This op currently assumes that the inner strides are of the
    source/result layout map are the faster-varying ones.
  }];

  let arguments = (ins AnyStridedMemRef:$src, IndexListArrayAttr:$reassociation,
                       Variadic<Index>:$output_shape,
                       DenseI64ArrayAttr:$static_output_shape);

  let assemblyFormat = [{
    $src $reassociation `output_shape`
    custom<DynamicIndexList>($output_shape, $static_output_shape) attr-dict `:`
    type($src) `into` type($result)
  }];

  let builders = [
    // Builders using ReassociationIndices.
    OpBuilder<(ins "Type":$resultType, "Value":$src,
      "ArrayRef<ReassociationIndices>":$reassociation,
      "ArrayRef<OpFoldResult>":$outputShape)>,

    // It will infer output shape using inferOutputShape() method.
    OpBuilder<(ins "Type":$resultType, "Value":$src,
      "ArrayRef<ReassociationIndices>":$reassociation)>,

    // Builder using ReassociationExprs.
    OpBuilder<(ins "Type":$resultType, "Value":$src,
      "ArrayRef<ReassociationExprs>":$reassociation),
    [{
      auto reassociationIndices =
          convertReassociationMapsToIndices(reassociation);
      build($_builder, $_state, resultType, src, reassociationIndices);
    }]>,

    OpBuilder<(ins "Type":$resultType, "Value":$src,
      "ArrayRef<ReassociationExprs>":$reassociation,
      "ArrayRef<OpFoldResult>":$outputShape),
    [{
      auto reassociationMaps =
          convertReassociationMapsToIndices(reassociation);
      build($_builder, $_state, resultType, src, reassociationMaps,
            outputShape);
    }]>,

    // Builder that infers the result layout map. The result shape must be
    // specified. Otherwise, the op may be ambiguous. The output shape for 
    // the op will be inferred using the inferOutputShape() method.
    OpBuilder<(ins "ArrayRef<int64_t>":$resultShape, "Value":$src,
               "ArrayRef<ReassociationIndices>":$reassociation)>,

    // Builder that infers the result layout map. The result shape must be
    // specified. Otherwise, the op may be ambiguous.
    OpBuilder<(ins "ArrayRef<int64_t>":$resultShape, "Value":$src,
               "ArrayRef<ReassociationIndices>":$reassociation,
               "ArrayRef<OpFoldResult>":$outputShape)>
  ];

  let extraClassDeclaration = commonExtraClassDeclaration # [{
    static FailureOr<MemRefType> computeExpandedType(
        MemRefType srcType, ArrayRef<int64_t> resultShape,
        ArrayRef<ReassociationIndices> reassociation);

    // Infer the output shape for a memref.expand_shape when it is possible
    // to do so.
    static FailureOr<SmallVector<OpFoldResult>> inferOutputShape(
        OpBuilder &b, Location loc, MemRefType expandedType,
        ArrayRef<ReassociationIndices> reassociation,
        ArrayRef<OpFoldResult> inputShape);
  }];

  let hasVerifier = 1;
}

def MemRef_CollapseShapeOp : MemRef_ReassociativeReshapeOp<"collapse_shape", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>]> {
  let summary = "operation to produce a memref with a smaller rank.";
  let description = [{
    The `memref.collapse_shape` op produces a new view with a smaller rank
    whose sizes are a reassociation of the original `view`. The operation is
    limited to such reassociations, where subsequent, contiguous dimensions are
    collapsed into a single dimension. Such reassociations never require
    additional allocs or copies.

    Collapsing non-contiguous dimensions is undefined behavior. When a group of
    dimensions can be statically proven to be non-contiguous, collapses of such
    groups are rejected in the verifier on a best-effort basis. In the general
    case, collapses of dynamically-sized dims with dynamic strides cannot be
    proven to be contiguous or non-contiguous due to limitations in the memref
    type.

    A reassociation is defined as a continuous grouping of dimensions and is
    represented with an array of DenseI64ArrayAttr attribute.

    Note: Only the dimensions within a reassociation group must be contiguous.
    The remaining dimensions may be non-contiguous.

    The result memref type can be zero-ranked if the source memref type is
    statically shaped with all dimensions being unit extent. In such a case, the
    reassociation indices must be empty.

    Examples:

    ```mlir
    // Dimension collapse (i, j) -> i' and k -> k'
    %1 = memref.collapse_shape %0 [[0, 1], [2]] :
        memref<?x?x?xf32, stride_spec> into memref<?x?xf32, stride_spec_2>
    ```

    For simplicity, this op may not be used to cast dynamicity of dimension
    sizes and/or strides. I.e., a result dimension must be dynamic if and only
    if at least one dimension in the corresponding reassociation group is
    dynamic. Similarly, the stride of a result dimension must be dynamic if and
    only if the corresponding start dimension in the source type is dynamic.

    Note: This op currently assumes that the inner strides are of the
    source/result layout map are the faster-varying ones.
  }];

  let arguments = (ins AnyStridedMemRef:$src, IndexListArrayAttr:$reassociation);

  let assemblyFormat = [{
    $src $reassociation attr-dict `:` type($src) `into` type($result)
  }];

  let builders = [
    // Builders for a contracting reshape whose result type is computed from
    // `src` and `reassociation`.
    OpBuilder<(ins "Value":$src,
      "ArrayRef<ReassociationIndices>":$reassociation,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    OpBuilder<(ins "Value":$src,
      "ArrayRef<ReassociationExprs>":$reassociation,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs),
    [{
      auto reassociationMaps =
          convertReassociationMapsToIndices(reassociation);
      build($_builder, $_state, src, reassociationMaps, attrs);
    }]>,

    // Builders for a reshape whose result type is passed explicitly.
    OpBuilder<(ins "Type":$resultType, "Value":$src,
      "ArrayRef<ReassociationIndices>":$reassociation,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs),
    [{
      $_state.addAttribute("reassociation",
                          getReassociationIndicesAttribute($_builder, reassociation));
      build($_builder, $_state, resultType, src, attrs);
    }]>,
    OpBuilder<(ins "Type":$resultType, "Value":$src,
      "ArrayRef<ReassociationExprs>":$reassociation,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs),
    [{
      auto reassociationMaps =
          convertReassociationMapsToIndices(reassociation);
      build($_builder, $_state, resultType, src, reassociationMaps, attrs);
    }]>
  ];

  let extraClassDeclaration = commonExtraClassDeclaration # [{
    /// Return `true` if this source MemRef type is guaranteed to be collapsible
    /// according to the given reassociation indices. In the presence of dynamic
    /// strides this is usually not the case.
    static bool isGuaranteedCollapsible(
        MemRefType srcType, ArrayRef<ReassociationIndices> reassociation);

    static MemRefType computeCollapsedType(
        MemRefType srcType, ArrayRef<ReassociationIndices> reassociation);
  }];

  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// StoreOp
//===----------------------------------------------------------------------===//

def MemRef_StoreOp : MemRef_Op<"store",
     [TypesMatchWith<"type of 'value' matches element type of 'memref'",
                     "memref", "value",
                     "::llvm::cast<MemRefType>($_self).getElementType()">,
      MemRefsNormalizable,
      DeclareOpInterfaceMethods<PromotableMemOpInterface>,
      DeclareOpInterfaceMethods<DestructurableAccessorOpInterface>]> {
  let summary = "store operation";
  let description = [{
    Store a value to a memref location given by indices. The value stored should
    have the same type as the elemental type of the memref. The number of
    arguments provided within brackets need to match the rank of the memref.

    In an affine context, the indices of a store are restricted to SSA values
    bound to surrounding loop induction variables,
    [symbols](Affine.md/#restrictions-on-dimensions-and-symbols), results of a
    `constant` operation, or the result of an
    [`affine.apply`](Affine.md/#affineapply-affineapplyop) operation that can in
    turn take as arguments all of the aforementioned SSA values or the
    recursively result of such an `affine.apply` operation.

    Example:

    ```mlir
    memref.store %100, %A[%1, 1023] : memref<4x?xf32, #layout, memspace0>
    ```

    **Context:** The `load` and `store` operations are specifically crafted to
    fully resolve a reference to an element of a memref, and (in polyhedral
    `affine.if` and `affine.for` operations) the compiler can follow use-def
    chains (e.g. through [`affine.apply`](Affine.md/#affineapply-affineapplyop)
    operations) to precisely analyze references at compile-time using polyhedral
    techniques. This is possible because of the
    [restrictions on dimensions and symbols](Affine.md/#restrictions-on-dimensions-and-symbols)
    in these contexts.
  }];

  let arguments = (ins AnyType:$value,
                       Arg<AnyMemRef, "the reference to store to",
                           [MemWrite]>:$memref,
                       Variadic<Index>:$indices,
                       DefaultValuedOptionalAttr<BoolAttr, "false">:$nontemporal);

  let builders = [
    OpBuilder<(ins "Value":$valueToStore, "Value":$memref), [{
      $_state.addOperands(valueToStore);
      $_state.addOperands(memref);
    }]>];

  let extraClassDeclaration = [{
      Value getValueToStore() { return getOperand(0); }

      Value getMemRef() { return getOperand(1); }
      void setMemRef(Value value) { setOperand(1, value); }
      MemRefType getMemRefType() {
        return ::llvm::cast<MemRefType>(getMemRef().getType());
      }
  }];

  let hasFolder = 1;
  let hasVerifier = 1;

  let assemblyFormat = [{
    $value `,` $memref `[` $indices `]` attr-dict `:` type($memref)
  }];
}

//===----------------------------------------------------------------------===//
// SubViewOp
//===----------------------------------------------------------------------===//

def SubViewOp : MemRef_OpWithOffsetSizesAndStrides<"subview", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    DeclareOpInterfaceMethods<ViewLikeOpInterface>,
    AttrSizedOperandSegments,
    OffsetSizeAndStrideOpInterface,
    Pure
  ]> {
  let summary = "memref subview operation";
  let description = [{
    The "subview" operation converts a memref type to another memref type
    which represents a reduced-size view of the original memref as specified by
    the operation's offsets, sizes and strides arguments.

    The SubView operation supports the following arguments:

    * source: the "base" memref on which to create a "view" memref.
    * offsets: memref-rank number of offsets into the "base" memref at which to
               create the "view" memref.
    * sizes: memref-rank number of sizes which specify the sizes of the result
             "view" memref type.
    * strides: memref-rank number of strides that compose multiplicatively with
               the base memref strides in each dimension.

    The representation based on offsets, sizes and strides support a
    partially-static specification via attributes specified through the
    `static_offsets`, `static_sizes` and `static_strides` arguments. A special
    sentinel value ShapedType::kDynamic encodes that the corresponding entry has
    a dynamic value.

    A subview operation may additionally reduce the rank of the resulting view
    by removing dimensions that are statically known to be of size 1.

    Example 1:

    ```mlir
    %0 = memref.alloc() : memref<64x4xf32, affine_map<(d0, d1) -> (d0 * 4 + d1)>>

    // Create a sub-view of "base" memref '%0' with offset arguments '%c0',
    // dynamic sizes for each dimension, and stride arguments '%c1'.
    %1 = memref.subview %0[%c0, %c0][%size0, %size1][%c1, %c1]
      : memref<64x4xf32, affine_map<(d0, d1) -> (d0 * 4 + d1)>> to
        memref<?x?xf32, affine_map<(d0, d1)[s0, s1] -> (d0 * s1 + d1 + s0)>>
    ```

    Example 2:

    ```mlir
    %0 = memref.alloc() : memref<8x16x4xf32, affine_map<(d0, d1, d2) -> (d0 * 64 + d1 * 4 + d2)>>

    // Create a sub-view of "base" memref '%0' with dynamic offsets, sizes,
    // and strides.
    // Note that dynamic offsets are represented by the linearized dynamic
    // offset symbol 's0' in the subview memref layout map, and that the
    // dynamic strides operands, after being applied to the base memref
    // strides in each dimension, are represented in the view memref layout
    // map as symbols 's1', 's2' and 's3'.
    %1 = memref.subview %0[%i, %j, %k][%size0, %size1, %size2][%x, %y, %z]
      : memref<8x16x4xf32, affine_map<(d0, d1, d2) -> (d0 * 64 + d1 * 4 + d2)>> to
        memref<?x?x?xf32,
          affine_map<(d0, d1, d2)[s0, s1, s2, s3] -> (d0 * s1 + d1 * s2 + d2 * s3 + s0)>>
    ```

    Example 3:

    ```mlir
    %0 = memref.alloc() : memref<8x16x4xf32, affine_map<(d0, d1, d2) -> (d0 * 64 + d1 * 4 + d2)>>

    // Subview with constant offsets, sizes and strides.
    %1 = memref.subview %0[0, 2, 0][4, 4, 4][1, 1, 1]
      : memref<8x16x4xf32, affine_map<(d0, d1, d2) -> (d0 * 64 + d1 * 4 + d2)>> to
        memref<4x4x4xf32, affine_map<(d0, d1, d2) -> (d0 * 64 + d1 * 4 + d2 + 8)>>
    ```

    Example 4:

    ```mlir
    %0 = memref.alloc(%arg0, %arg1) : memref<?x?xf32>

    // Subview with constant size, but dynamic offsets and
    // strides. The resulting memref has a static shape, but if the
    // base memref has an affine map to describe the layout, the result
    // memref also uses an affine map to describe the layout. The
    // strides of the result memref is computed as follows:
    //
    // Let #map1 represents the layout of the base memref, and #map2
    // represents the layout of the result memref. A #mapsubview can be
    // constructed to map an index from the result memref to the base
    // memref (note that the description below uses more convenient
    // naming for symbols, while in affine maps, symbols are
    // represented as unsigned numbers that identify that symbol in the
    // given affine map.
    //
    // #mapsubview = (d0, d1)[o0, o1, t0, t1] -> (d0 * t0 + o0, d1 * t1 + o1)
    //
    // where, o0, o1, ... are offsets, and t0, t1, ... are strides. Then,
    //
    // #map2 = #map1.compose(#mapsubview)
    //
    // If the layout map is represented as
    //
    // #map1 = (d0, d1)[s0, s1, s2] -> (d0 * s1 + d1 * s2 + s0)
    //
    // then,
    //
    // #map2 = (d0, d1)[s0, s1, s2, o0, o1, t0, t1] ->
    //              (d0 * s1 * t0 + d1 * s2 * t1 + o0 * s1 + o1 * s2 + s0)
    //
    // Representing this canonically
    //
    // #map2 = (d0, d1)[r0, r1, r2] -> (d0 * r1 + d1 * r2 + r0)
    //
    // where, r0 = o0 * s1 + o1 * s2 + s0, r1 = s1 * t0, r2 = s2 * t1.
    %1 = memref.subview %0[%i, %j][4, 4][%x, %y] :
      : memref<?x?xf32, affine_map<(d0, d1)[s0, s1, s2] -> (d0 * s1 + d1 * s2 + s0)>> to
        memref<4x4xf32, affine_map<(d0, d1)[r0, r1, r2] -> (d0 * r1 + d1 * r2 + r0)>>

    // Note that the subview op does not guarantee that the result
    // memref is "inbounds" w.r.t to base memref. It is upto the client
    // to ensure that the subview is accessed in a manner that is
    // in-bounds.
    ```

    Example 5:

    ```mlir
    // Rank-reducing subview.
    %1 = memref.subview %0[0, 0, 0][1, 16, 4][1, 1, 1] :
      memref<8x16x4xf32> to memref<16x4xf32>

    // Original layout:
    // (d0, d1, d2) -> (64 * d0 + 16 * d1 + d2)
    // Subviewed layout:
    // (d0, d1, d2) -> (64 * (d0 + 3) + 4 * (d1 + 4) + d2 + 2) = (64 * d0 + 4 * d1 + d2 + 210)
    // After rank reducing:
    // (d0, d1) -> (4 * d0 + d1 + 210)
    %3 = memref.subview %2[3, 4, 2][1, 6, 3][1, 1, 1] :
      memref<8x16x4xf32> to memref<6x3xf32, strided<[4, 1], offset: 210>>
    ```
  }];

  let arguments = (ins AnyMemRef:$source,
                       Variadic<Index>:$offsets,
                       Variadic<Index>:$sizes,
                       Variadic<Index>:$strides,
                       DenseI64ArrayAttr:$static_offsets,
                       DenseI64ArrayAttr:$static_sizes,
                       DenseI64ArrayAttr:$static_strides);
  let results = (outs AnyMemRef:$result);

  let assemblyFormat = [{
    $source ``
    custom<DynamicIndexList>($offsets, $static_offsets)
    custom<DynamicIndexList>($sizes, $static_sizes)
    custom<DynamicIndexList>($strides, $static_strides)
    attr-dict `:` type($source) `to` type($result)
  }];

  let builders = [
    // Build a SubViewOp with mixed static and dynamic entries and inferred
    // result type.
    OpBuilder<(ins "Value":$source, "ArrayRef<OpFoldResult>":$offsets,
      "ArrayRef<OpFoldResult>":$sizes, "ArrayRef<OpFoldResult>":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build a SubViewOp with mixed static and dynamic entries and custom
    // result type. If the type passed is nullptr, it is inferred.
    OpBuilder<(ins "MemRefType":$resultType, "Value":$source,
      "ArrayRef<OpFoldResult>":$offsets, "ArrayRef<OpFoldResult>":$sizes,
      "ArrayRef<OpFoldResult>":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build a SubViewOp with static entries and custom result type. If the
    // type passed is nullptr, it is inferred.
    OpBuilder<(ins "Value":$source, "ArrayRef<int64_t>":$offsets,
      "ArrayRef<int64_t>":$sizes, "ArrayRef<int64_t>":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build a SubViewOp with static entries and inferred result type.
    OpBuilder<(ins "MemRefType":$resultType, "Value":$source,
      "ArrayRef<int64_t>":$offsets, "ArrayRef<int64_t>":$sizes,
      "ArrayRef<int64_t>":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build a SubViewOp with dynamic entries and custom result type. If the
    // type passed is nullptr, it is inferred.
    OpBuilder<(ins "Value":$source, "ValueRange":$offsets,
      "ValueRange":$sizes, "ValueRange":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>,
    // Build a SubViewOp with dynamic entries and inferred result type.
    OpBuilder<(ins "MemRefType":$resultType, "Value":$source,
      "ValueRange":$offsets, "ValueRange":$sizes, "ValueRange":$strides,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>
  ];

  let extraClassDeclaration = extraBaseClassDeclaration # [{
    /// Returns the type of the base memref operand.
    MemRefType getSourceType() {
      return ::llvm::cast<MemRefType>(getSource().getType());
    }

    /// The result of a subview is always a memref.
    MemRefType getType() { return ::llvm::cast<MemRefType>(getResult().getType()); }

    /// A subview result type can be fully inferred from the source type and the
    /// static representation of offsets, sizes and strides. Special sentinels
    /// encode the dynamic case.
    static Type inferResultType(MemRefType sourceMemRefType,
                                ArrayRef<int64_t> staticOffsets,
                                ArrayRef<int64_t> staticSizes,
                                ArrayRef<int64_t> staticStrides);
    static Type inferResultType(MemRefType sourceMemRefType,
                                ArrayRef<OpFoldResult> staticOffsets,
                                ArrayRef<OpFoldResult> staticSizes,
                                ArrayRef<OpFoldResult> staticStrides);

    /// A rank-reducing result type can be inferred from the desired result
    /// shape. Only the layout map is inferred.
    ///
    /// Note: The result shape cannot be inferred with just the result rank and
    /// and the desired sizes. In case there are more "ones" among the sizes
    /// than the difference in source/result rank, it is not clear which dims of
    /// size one should be dropped.
    static Type inferRankReducedResultType(ArrayRef<int64_t> resultShape,
                                           MemRefType sourceMemRefType,
                                           ArrayRef<int64_t> staticOffsets,
                                           ArrayRef<int64_t> staticSizes,
                                           ArrayRef<int64_t> staticStrides);
    static Type inferRankReducedResultType(ArrayRef<int64_t> resultShape,
                                           MemRefType sourceMemRefType,
                                           ArrayRef<OpFoldResult> staticOffsets,
                                           ArrayRef<OpFoldResult> staticSizes,
                                           ArrayRef<OpFoldResult> staticStrides);

    /// Return the expected rank of each of the`static_offsets`, `static_sizes`
    /// and `static_strides` attributes.
    std::array<unsigned, 3> getArrayAttrMaxRanks() {
      unsigned rank = getSourceType().getRank();
      return {rank, rank, rank};
    }

    /// Return the number of leading operands before the `offsets`, `sizes` and
    /// and `strides` operands.
    static unsigned getOffsetSizeAndStrideStartOperandIndex() { return 1; }

    /// Return the dimensions of the source type that are dropped when
    /// the result is rank-reduced.
    llvm::SmallBitVector getDroppedDims();

    /// Given a `value`, asserted to be of MemRefType, build a SubViewOp that
    /// results in a rank reduction to the desired memref shape and return the
    /// new value created.
    /// If the shape of `value` is already the `desiredShape`, just return
    /// `value`.
    /// If the shape of `value` cannot be rank-reduced to `desiredShape`, fail.
    static FailureOr<Value> rankReduceIfNeeded(
      OpBuilder &b, Location loc, Value value, ArrayRef<int64_t> desiredShape);
  }];

  let hasCanonicalizer = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// TransposeOp
//===----------------------------------------------------------------------===//

def MemRef_TransposeOp : MemRef_Op<"transpose", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    Pure]>,
    Arguments<(ins AnyStridedMemRef:$in, AffineMapAttr:$permutation)>,
    Results<(outs AnyStridedMemRef)> {
  let summary = "`transpose` produces a new strided memref (metadata-only)";
  let description = [{
    The `transpose` op produces a strided memref whose sizes and strides
    are a permutation of the original `in` memref. This is purely a metadata
    transformation.

    Example:

    ```mlir
    %1 = memref.transpose %0 (i, j) -> (j, i) : memref<?x?xf32> to memref<?x?xf32, affine_map<(d0, d1)[s0] -> (d1 * s0 + d0)>>
    ```
  }];

  let builders = [
    OpBuilder<(ins "Value":$in, "AffineMapAttr":$permutation,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs)>];

  let extraClassDeclaration = [{
    static StringRef getPermutationAttrStrName() { return "permutation"; }
  }];

  let hasCustomAssemblyFormat = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// ViewOp
//===----------------------------------------------------------------------===//

def MemRef_ViewOp : MemRef_Op<"view", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    DeclareOpInterfaceMethods<ViewLikeOpInterface>,
    Pure]> {
  let summary = "memref view operation";
  let description = [{
    The "view" operation extracts an N-D contiguous memref with empty layout map
    with arbitrary element type from a 1-D contiguous memref with empty layout
    map of i8 element  type. The ViewOp supports the following arguments:

    * A single dynamic byte-shift operand must be specified which represents a
      a shift of the base 1-D memref pointer from which to create the resulting
      contiguous memref view with identity layout.
    * A dynamic size operand that must be specified for each dynamic dimension
      in the resulting view memref type.

    The "view" operation gives a structured indexing form to a flat 1-D buffer.
    Unlike "subview" it can perform a type change. The type change behavior
    requires the op to have special semantics because, e.g. a byte shift of 3
    cannot be represented as an offset on f64.
    For now, a "view" op:

    1. Only takes a contiguous source memref with 0 offset and empty layout.
    2. Must specify a byte_shift operand (in the future, a special integer
       attribute may be added to support the folded case).
    3. Returns a contiguous memref with 0 offset and empty layout.

    Example:

    ```mlir
    // Allocate a flat 1D/i8 memref.
    %0 = memref.alloc() : memref<2048xi8>

    // ViewOp with dynamic offset and static sizes.
    %1 = memref.view %0[%offset_1024][] : memref<2048xi8> to memref<64x4xf32>

    // ViewOp with dynamic offset and two dynamic size.
    %2 = memref.view %0[%offset_1024][%size0, %size1] :
      memref<2048xi8> to memref<?x4x?xf32>
    ```
  }];

  let arguments = (ins MemRefRankOf<[I8], [1]>:$source,
                       Index:$byte_shift,
                       Variadic<Index>:$sizes);
  let results = (outs AnyMemRef);

  let extraClassDeclaration = [{
    /// The result of a view is always a memref.
    MemRefType getType() { return ::llvm::cast<MemRefType>(getResult().getType()); }

    /// Returns the dynamic sizes for this view operation. This is redundant
    /// with `sizes` but needed in template implementations. More specifically:
    /// ```
    /// template <typename AnyMemRefDefOp>
    /// bool isMemRefSizeValidSymbol(AnyMemRefDefOp memrefDefOp, unsigned index,
    ///                              Region *region)
    /// ```
    operand_range getDynamicSizes() {
      return {getSizes().begin(), getSizes().end()};
    }
  }];

  let assemblyFormat = [{
    $source `[` $byte_shift `]` `` `[` $sizes `]` attr-dict
    `:` type($source) `to` type(results)
  }];

  let hasCanonicalizer = 1;
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// AtomicRMWOp
//===----------------------------------------------------------------------===//

def AtomicRMWOp : MemRef_Op<"atomic_rmw", [
      AllTypesMatch<["value", "result"]>,
      TypesMatchWith<"value type matches element type of memref",
                     "memref", "value",
                     "::llvm::cast<MemRefType>($_self).getElementType()">
    ]> {
  let summary = "atomic read-modify-write operation";
  let description = [{
    The `memref.atomic_rmw` operation provides a way to perform a read-modify-write
    sequence that is free from data races. The kind enumeration specifies the
    modification to perform. The value operand represents the new value to be
    applied during the modification. The memref operand represents the buffer
    that the read and write will be performed against, as accessed by the
    specified indices. The arity of the indices is the rank of the memref. The
    result represents the latest value that was stored.

    Example:

    ```mlir
    %x = memref.atomic_rmw "addf" %value, %I[%i] : (f32, memref<10xf32>) -> f32
    ```
  }];

  let arguments = (ins
      AtomicRMWKindAttr:$kind,
      AnyTypeOf<[AnySignlessInteger, AnyFloat]>:$value,
      Arg<MemRefOf<[AnySignlessInteger, AnyFloat]>, "the reference to read from and write to", [MemRead, MemWrite]>:$memref,
      Variadic<Index>:$indices);
  let results = (outs AnyTypeOf<[AnySignlessInteger, AnyFloat]>:$result);

  let assemblyFormat = [{
    $kind $value `,` $memref `[` $indices `]` attr-dict `:` `(` type($value) `,`
    type($memref) `)` `->` type($result)
  }];

  let extraClassDeclaration = [{
    MemRefType getMemRefType() {
      return ::llvm::cast<MemRefType>(getMemref().getType());
    }
  }];
  let hasFolder = 1;
  let hasVerifier = 1;
}

#endif // MEMREF_OPS


//===- LinalgStructuredOps.td - Linalg dialect library ops -*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This is the operation definition file for structured operations on buffers
// that correspond to underlying library calls (e.g. BLAS).
//
//===----------------------------------------------------------------------===//

#ifndef LINALG_STRUCTURED_OPS
#define LINALG_STRUCTURED_OPS

include "mlir/Dialect/Linalg/IR/LinalgBase.td"
include "mlir/Dialect/Linalg/IR/LinalgInterfaces.td"
include "mlir/Interfaces/DestinationStyleOpInterface.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/IR/OpAsmInterface.td"

// Base Tablegen class for Linalg ops.
// Linalg ops that correspond to library calls operate on ShapedType as their
// first operands. These may be optionally followed by non-view operands
// depending on the specific Linalg op.
class LinalgStructuredBase_Op<string mnemonic, list<Trait> props>
  : Op<Linalg_Dialect, mnemonic, !listconcat([
       SingleBlockImplicitTerminator<"YieldOp">,
       DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
       DeclareOpInterfaceMethods<ConditionallySpeculatable>,
       RecursiveMemoryEffects,
       DestinationStyleOpInterface,
       LinalgStructuredInterface,
       ReifyRankedShapedTypeOpInterface], props)> {
  code structuredOpsBaseDecls = [{
    // Return whether the op accesses the iteration indices.
    bool hasIndexSemantics() {
      return !this->getBody()->getOps<IndexOp>().empty();
    }

    LogicalResult reifyResultShapes(OpBuilder &b,
        ReifiedRankedShapedTypeDims &reifiedReturnShapes) {
      return llvm::cast<LinalgOp>(getOperation()).reifyResultShapes(b,
          reifiedReturnShapes);
    }
  }];
}

//===----------------------------------------------------------------------===//
// Generic Linalg ops.
//===----------------------------------------------------------------------===//

def GenericOp : LinalgStructuredBase_Op<"generic", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
    AttrSizedOperandSegments]> {
  let description = [{
    Generic Linalg op form where the key properties of the computation are
    specified as attributes. In pretty form, a `linalg.generic` op is written
    as:

      ```mlir
      linalg.generic #trait_attribute
          ins(%A, %B : memref<?x?xf32, stride_specification>,
                       memref<?x?xf32, stride_specification>)
          outs(%C : memref<?x?xf32, stride_specification>)
          attrs = {other-optional-attributes}
          {region}
      ```

    Where #trait_attributes is an alias of a dictionary attribute containing:
      - doc [optional]: a documentation string
      - indexing_maps: a list of AffineMapAttr, one AffineMapAttr per each input
        and output view. Such AffineMapAttr specifies the mapping between the
        loops and the indexing within each view.
      - library_call [optional]: a StringAttr containing the name of an
        external library function that the linalg.generic operation maps to.
        The external library is assumed to be dynamically linked and no strong
        compile-time guarantees are provided. In the absence of such a library
        call, linalg.generic will always lower to loops.
      - iterator_types: an ArrayAttr specifying the type of the enclosing loops.
        Each element of the list represents and iterator of one of the following
        types:
          parallel, reduction, window

    Example:
    Defining a #matmul_trait attribute in MLIR can be done as follows:
      ```mlir
      #matmul_accesses = [
        (m, n, k) -> (m, k),
        (m, n, k) -> (k, n),
        (m, n, k) -> (m, n)
      ]
      #matmul_trait = {
        doc = "C(m, n) += A(m, k) * B(k, n)",
        indexing_maps = #matmul_accesses,
        library_call = "linalg_matmul",
        iterator_types = ["parallel", "parallel", "reduction"]
      }
      ```

    And can be reused in multiple places as:
      ```mlir
      linalg.generic #matmul_trait
        ins(%A, %B : memref<?x?xf32, stride_specification>,
                     memref<?x?xf32, stride_specification>)
        outs(%C : memref<?x?xf32, stride_specification>)
        {other-optional-attributes} {
        ^bb0(%a: f32, %b: f32, %c: f32) :
          %d = arith.mulf %a, %b: f32
          %e = arith.addf %c, %d: f32
          linalg.yield %e : f32
      }
      ```

    This may lower to either:
      ```mlir
      call @linalg_matmul(%A, %B, %C) :
        (memref<?x?xf32, stride_specification>,
         memref<?x?xf32, stride_specification>,
         memref<?x?xf32, stride_specification>)
        -> ()
      ```

    or IR resembling:
    ```mlir
    scf.for %m = %c0 to %M step %c1 {
      scf.for %n = %c0 to %N step %c1 {
        scf.for %k = %c0 to %K step %c1 {
          %a = load %A[%m, %k] : memref<?x?xf32, stride_specification>
          %b = load %B[%k, %n] : memref<?x?xf32, stride_specification>
          %c = load %C[%m, %n] : memref<?x?xf32, stride_specification>
          %d = arith.mulf %a, %b: f32
          %e = arith.addf %c, %d: f32
          store %e, %C[%m, %n] : memref<?x?x?xf32, stride_specification>
        }
      }
    }
    ```

    To allow progressive lowering from the value world (a.k.a tensor values) to
    the buffer world (a.k.a memref values), a `linalg.generic` op allows mixing
    tensors and buffers operands and tensor results.

    ```mlir
    %C = linalg.generic #trait_attribute
      ins(%A, %B : tensor<?x?xf32>, memref<?x?xf32, stride_specification>)
      outs(%C : tensor<?x?xf32>)
      {other-optional-attributes}
      {region}
      -> (tensor<?x?xf32>)
    ```
  }];

  let arguments = (ins Variadic<AnyType>:$inputs,
                       Variadic<AnyShaped>:$outputs,
                       AffineMapArrayAttr:$indexing_maps,
                       IteratorTypeArrayAttr:$iterator_types,
                       OptionalAttr<StrAttr>:$doc,
                       OptionalAttr<StrAttr>:$library_call);
  let results = (outs Variadic<AnyRankedTensor>:$result_tensors);
  let regions = (region AnyRegion:$region);

  let builders = [
    OpBuilder<(ins "TypeRange":$resultTensorTypes, "ValueRange":$inputs,
      "ValueRange":$outputs, "ArrayAttr":$indexingMaps,
      "ArrayAttr":$iteratorTypes, "StringAttr":$doc,
      "StringAttr":$libraryCall,
      "function_ref<void(OpBuilder &, Location, ValueRange)>",
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attributes)>,
    OpBuilder<(ins "TypeRange":$resultTensorTypes, "ValueRange":$inputs,
      "ValueRange":$outputs, "ArrayRef<AffineMap>":$indexingMaps,
      "ArrayRef<utils::IteratorType>":$iteratorTypes, "StringRef":$doc,
      "StringRef":$libraryCall,
      CArg<"function_ref<void(OpBuilder &, Location, ValueRange)>", "nullptr">,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attributes)>,
    OpBuilder<(ins "ValueRange":$inputs, "ValueRange":$outputBuffers,
      "ArrayRef<AffineMap>":$indexingMaps, "ArrayRef<utils::IteratorType>":$iteratorTypes,
      "StringRef":$doc, "StringRef":$libraryCall,
      CArg<"function_ref<void(OpBuilder &, Location, ValueRange)>", "nullptr">,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attributes)>,
    OpBuilder<(ins "TypeRange":$resultTensorTypes, "ValueRange":$inputs,
      "ValueRange":$outputs, "ArrayRef<AffineMap>":$indexingMaps,
      "ArrayRef<utils::IteratorType>":$iteratorTypes,
      CArg<"function_ref<void(OpBuilder &, Location, ValueRange)>", "nullptr">,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attributes)>,
    OpBuilder<(ins "ValueRange":$inputs, "ValueRange":$outputBuffers,
      "ArrayRef<AffineMap>":$indexingMaps, "ArrayRef<utils::IteratorType>":$iteratorTypes,
      CArg<"function_ref<void(OpBuilder &, Location, ValueRange)>", "nullptr">,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attributes)>
  ];

  let extraClassDeclaration = structuredOpsBaseDecls # [{
    SmallVector<StringRef, 8> linalgTraitAttrNames() {
      return SmallVector<StringRef, 8>{
        getDocAttrName(),
        getIndexingMapsAttrName(), getLibraryCallAttrName(),
        getIteratorTypesAttrName(),
      };
    }
    std::string getLibraryCallName() {
      return getLibraryCall() ?
        getLibraryCall()->str() : "op_has_no_registered_library_name";
    }

    static std::function<void(ImplicitLocOpBuilder &,
                              Block &, ArrayRef<NamedAttribute>)>
    getRegionBuilder() {
      return nullptr;
    }

    MutableOperandRange getDpsInitsMutable() { return getOutputsMutable(); }

    // Return true only if GenericOp has a single input and single
    // output, and the body is a single yieldOp that yields the input.
    // This check is useful when trying to determine if the op is
    // essentially a transpose, broadcast, copy or something like that.
    bool isSingleYieldOp() {
      if (!isSingleInputOutput())
        return false;
     Block *body = getBody();
     if (body->getOperations().size() != 1)
       return false;

     auto yieldOp = dyn_cast<linalg::YieldOp>(body->back());
       if (!yieldOp || yieldOp.getNumOperands() != 1 ||
           yieldOp->getOperand(0) != body->getArgument(0))
         return false;
     return true;
   }
  }];

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasFolder = 1;
  let hasVerifier = 1;
}


//===----------------------------------------------------------------------===//
// Map op.
//===----------------------------------------------------------------------===//

def TensorOrMemref :
  AnyTypeOf<[AnyMemRef, AnyRankedTensor], "", "::mlir::ShapedType">;

def MapOp : LinalgStructuredBase_Op<"map", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
    SingleBlockImplicitTerminator<"YieldOp">]> {
  let summary = "Elementwise operations";
  let description = [{
    Models elementwise operations on tensors in terms of arithmetic operations
    on the corresponding elements.

    Example:
    ```
      %add = linalg.map
          ins(%lhs, %rhs : tensor<64xf32>, tensor<64xf32>)
          outs(%init: tensor<64xf32>)
          (%lhs_elem: f32, %rhs_elem: f32) {
            %0 = arith.addf %lhs_elem, %rhs_elem: f32
            linalg.yield %0: f32
          }
    ```

    Shortened print form is available. Applies to simple maps with one
    non-yield operation inside the body.

    The example above will be printed as:
    ```
      %add = linalg.map { arith.addf }
          ins(%lhs, %rhs : tensor<64xf32>, tensor<64xf32>)
          outs(%init: tensor<64xf32>)
    ```
  }];

  let arguments = (ins
    // Input args
    Variadic<TensorOrMemref>:$inputs,

    // Output arg
    TensorOrMemref:$init
  );
  let results = (outs Variadic<AnyTensor>:$result);
  let regions = (region SizedRegion<1>:$mapper);

  let builders = [
    OpBuilder<(ins "ValueRange":$inputs, "Value":$init,
      "function_ref<void(OpBuilder &, Location, ValueRange)>",
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attributes)>
  ];

  let extraClassDeclaration = structuredOpsBaseDecls # [{
    // Implement functions necessary for LinalgStructuredInterface.
    SmallVector<utils::IteratorType> getIteratorTypesArray();
    ArrayAttr getIndexingMaps();
    std::string getLibraryCallName() {
      return "op_has_no_registered_library_name";
    }

    // Implement functions necessary for DestinationStyleOpInterface.
    MutableOperandRange getDpsInitsMutable() { return getInitMutable(); }

    SmallVector<OpOperand *> getOpOperandsMatchingBBargs() {
      return getDpsInputOperands();
    }

    bool payloadUsesValueFromOperand(OpOperand * opOperand) {
      if (isDpsInit(opOperand)) return false;
      return !getMatchingBlockArgument(opOperand).use_empty();
    }

    static std::function<void(mlir::ImplicitLocOpBuilder &, mlir::Block &,
                              mlir::ArrayRef<mlir::NamedAttribute>)>
    getRegionBuilder() {
      return nullptr;
    }
  }];

  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}


//===----------------------------------------------------------------------===//
// Reduce op.
//===----------------------------------------------------------------------===//

def ReduceOp : LinalgStructuredBase_Op<"reduce", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
    SameVariadicOperandSize,
    SingleBlockImplicitTerminator<"YieldOp">]> {
  let summary = "Reduce operator";
  let description = [{
    Executes `combiner` on the `dimensions` of `inputs` and returns the
    reduced result. The `dimensions` attribute needs to list the reduction
    dimensions in increasing order.

    Example:
    ```
      %reduce = linalg.reduce
          ins(%input:tensor<16x32x64xf32>)
          outs(%init:tensor<16x64xf32>)
          dimensions = [1]
          (%in: f32, %out: f32) {
            %0 = arith.addf %out, %in: f32
            linalg.yield %0: f32
          }
    ```

    Shortened print form is available. Applies to simple (not variadic) reduces
    with one non-yield operation inside the body. Applies only if the operation
    takes `%out` as the first argument.

    The example above will be printed as:
    ```
          %reduce = linalg.reduce { arith.addf }
          ins(%input:tensor<16x32x64xf32>)
          outs(%init:tensor<16x64xf32>)
          dimensions = [1]
    ```
  }];

  let arguments = (ins
    // Input arg
    Variadic<TensorOrMemref>:$inputs,
    // Output arg
    Variadic<TensorOrMemref>:$inits,

    ConfinedAttr<DenseI64ArrayAttr,
                 [DenseArrayStrictlySorted<DenseI64ArrayAttr>]>:$dimensions
  );
  let results = (outs Variadic<AnyTensor>);
  let regions = (region SizedRegion<1>:$combiner);

  let builders = [
    OpBuilder<(ins "ValueRange":$inputs, "ValueRange":$inits,
      "ArrayRef<int64_t>":$dimensions,
      "function_ref<void(OpBuilder &, Location, ValueRange)>",
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attributes)>
  ];

  let extraClassDeclaration = structuredOpsBaseDecls # [{
    // Declare functions necessary for LinalgStructuredInterface.
    SmallVector<utils::IteratorType> getIteratorTypesArray();
    ArrayAttr getIndexingMaps();
    std::string getLibraryCallName() {
      return "op_has_no_registered_library_name";
    }

    // Implement functions necessary for DestinationStyleOpInterface.
    static std::function<void(mlir::ImplicitLocOpBuilder &, mlir::Block &,
                              mlir::ArrayRef<mlir::NamedAttribute>)>
    getRegionBuilder() {
      return nullptr;
    }
    MutableOperandRange getDpsInitsMutable() { return getInitsMutable(); }
  }];

  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}


//===----------------------------------------------------------------------===//
// Transpose op.
//===----------------------------------------------------------------------===//

def TransposeOp : LinalgStructuredBase_Op<"transpose", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    SingleBlockImplicitTerminator<"YieldOp">]> {
  let summary = "Transpose operator";
  let description = [{
    Permutes the dimensions of `input` according to the given `permutation`.
      `dim(result, i) = dim(input, permutation[i])`

    This op actually moves data, unlike `memref.transpose` which is a metadata
    operation only that produces a transposed "view".

    Example:
    ```
      %transpose = linalg.transpose
          ins(%input:tensor<16x64xf32>)
          outs(%init:tensor<64x16xf32>)
          permutation = [1, 0]
    ```
  }];

  let arguments = (ins
    // Input arg
    TensorOrMemref:$input,
    // Output arg
    TensorOrMemref:$init,

    DenseI64ArrayAttr:$permutation
  );
  let results = (outs Variadic<AnyTensor>:$result);
  let regions = (region SizedRegion<1>:$region);

  let skipDefaultBuilders = 1;
  let builders = [
    OpBuilder<(ins "Value":$input, "Value":$init,
        "DenseI64ArrayAttr":$permutation, CArg<"ArrayRef<NamedAttribute>",
        "{}">:$attributes)>,
    OpBuilder<(ins "Value":$input, "Value":$init,
        "ArrayRef<int64_t>":$permutation, CArg<"ArrayRef<NamedAttribute>",
        "{}">:$attributes)>,
  ];

  let extraClassDeclaration = structuredOpsBaseDecls # [{
    // Declare functions necessary for LinalgStructuredInterface.
    SmallVector<utils::IteratorType> getIteratorTypesArray();
    ArrayAttr getIndexingMaps();
    std::string getLibraryCallName() {
      return "op_has_no_registered_library_name";
    }

    // Implement functions necessary for DestinationStyleOpInterface.
    MutableOperandRange getDpsInitsMutable() { return getInitMutable(); }

    static void regionBuilder(mlir::ImplicitLocOpBuilder &b, mlir::Block &block,
        mlir::ArrayRef<mlir::NamedAttribute>) {
      OpBuilder::InsertionGuard guard(b);
      b.create<linalg::YieldOp>(b.getLoc(), block.getArgument(0));
    }

    static std::function<void(mlir::ImplicitLocOpBuilder &, mlir::Block &,
        mlir::ArrayRef<mlir::NamedAttribute>)>
      getRegionBuilder() {
      return regionBuilder;
    }

    static void createRegion(::mlir::OpBuilder &opBuilder,
                             ::mlir::OperationState & odsState);
  }];

  let hasFolder = 1;
  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
}


//===----------------------------------------------------------------------===//
// Broadcast op.
//===----------------------------------------------------------------------===//

def BroadcastOp : LinalgStructuredBase_Op<"broadcast", [
    DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
    SingleBlockImplicitTerminator<"YieldOp">]> {
  let summary = "Static broadcast operator";
  let description = [{
    Broadcast the input into the given shape by adding `dimensions`.

    Example:
    ```
      %bcast = linalg.broadcast
          ins(%input:tensor<16xf32>)
          outs(%init:tensor<16x64xf32>)
          dimensions = [1]
    ```
  }];

  let arguments = (ins
    // Input arg
    TensorOrMemref:$input,
    // Output arg
    TensorOrMemref:$init,

    DenseI64ArrayAttr:$dimensions
  );
  let results = (outs Variadic<AnyTensor>:$result);
  let regions = (region SizedRegion<1>:$region);

  let skipDefaultBuilders = 1;
  let builders = [
    OpBuilder<(ins "Value":$input, "Value":$init,
        "DenseI64ArrayAttr":$dimensions, CArg<"ArrayRef<NamedAttribute>",
        "{}">:$attributes)>,
    OpBuilder<(ins "Value":$input, "Value":$init,
        "ArrayRef<int64_t>":$dimensions, CArg<"ArrayRef<NamedAttribute>",
        "{}">:$attributes)>,
  ];

  let extraClassDeclaration = structuredOpsBaseDecls # [{
    // Declare functions necessary for LinalgStructuredInterface.
    SmallVector<utils::IteratorType> getIteratorTypesArray();
    ArrayAttr getIndexingMaps();
    std::string getLibraryCallName() {
      return "op_has_no_registered_library_name";
    }

    // Implement functions necessary for DestinationStyleOpInterface.
    MutableOperandRange getDpsInitsMutable() { return getInitMutable(); }

    static void regionBuilder(mlir::ImplicitLocOpBuilder &b, mlir::Block &block,
        mlir::ArrayRef<mlir::NamedAttribute>) {
      OpBuilder::InsertionGuard guard(b);
      b.create<linalg::YieldOp>(b.getLoc(), block.getArgument(0));
    }

    static std::function<void(mlir::ImplicitLocOpBuilder &, mlir::Block &,
        mlir::ArrayRef<mlir::NamedAttribute>)>
      getRegionBuilder() {
      return regionBuilder;
    }
  }];

  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;
  let hasCanonicalizer = 1;
}

//===----------------------------------------------------------------------===//
// Op definition for MatmulOp
//===----------------------------------------------------------------------===//

def MatmulOp : LinalgStructuredBase_Op<"matmul", [
               AttrSizedOperandSegments,
               LinalgContractionOpInterface]> {

  let summary = [{
    Performs a matrix multiplication of two 2D inputs without broadcast or transpose.
    }];
  let description = [{
    Numeric casting is performed on the operands to the inner multiply,
    promoting them to the same data type as the accumulator/output.

    Broadcast and Transpose semantics can be appiled by specifying the explicit attribute
    'indexing_maps' as shown below.This is a list attribute, so the list must include all
    the maps if specified.

    Example Transpose:
    ```
    linalg.matmul indexing_maps = [
                   affine_map<(d0, d1, d2) -> (d2, d0)>, // transpose
                   affine_map<(d0, d1, d2) -> (d2, d1)>,
                   affine_map<(d0, d1, d2) -> (d0, d1)>
                   ]
                   ins(%arg0, %arg1 : memref<5x3xf32>,memref<5x7xf32>)
                   outs(%arg2: memref<3x7xf32>)
     ```

    Example Broadcast:
     ```
    linalg.matmul indexing_maps = [
                   affine_map<(d0, d1, d2) -> (d2)>,     // broadcast
                   affine_map<(d0, d1, d2) -> (d2, d1)>,
                   affine_map<(d0, d1, d2) -> (d0, d1)>
                  ]
                  ins(%arg0, %arg1 : memref<3xf32>, memref<5x7xf32>)
                  outs(%arg2: memref<3x7xf32>)
    ```

    Example Broadcast and transpose:
    ```
    linalg.matmul indexing_maps = [
                      affine_map<(d0, d1, d2) -> (d2, d0)>, // transpose
                      affine_map<(d0, d1, d2) -> (d2)>,     // broadcast
                      affine_map<(d0, d1, d2) -> (d0, d1)>
                    ]
                    ins(%arg0, %arg1 : memref<5x3xf32>, memref<7xf32>) outs(%arg2: memref<3x7xf32>)
    ```
    }];

    let arguments = (ins
      Variadic<AnyType>:$inputs,
      Variadic<AnyShaped>:$outputs,
      DefaultValuedOptionalAttr<AffineMapArrayAttr, "{}">:$indexing_maps,
      DefaultValuedOptionalAttr<TypeFnAttr, "TypeFn::cast_signed">:$cast
    );
    let results = (outs Variadic<AnyRankedTensor>:$result_tensors);
    let regions = (region AnyRegion:$region);

    let skipDefaultBuilders = 1;
    let builders = [
      OpBuilder<
      (ins "ValueRange":$inputs, "ValueRange":$outputs,
            CArg<"ArrayRef<NamedAttribute>", "{}">:$attributes),
      [{
        buildMatmulOp($_builder, $_state, std::nullopt, inputs, outputs,
          attributes, MatmulOp::getRegionBuilder(),
          MatmulOp::getDefaultIndexingMaps($_builder.getContext()));
      }]>,
      OpBuilder<
      (ins "TypeRange":$resultTensorTypes, "ValueRange":$inputs,
            "ValueRange":$outputs,
            CArg<"ArrayRef<NamedAttribute>", "{}">:$attributes),
      [{
        buildMatmulOp($_builder, $_state, resultTensorTypes,
          inputs, outputs, attributes, MatmulOp::getRegionBuilder(),
          MatmulOp::getDefaultIndexingMaps($_builder.getContext()));
      }]>,
      OpBuilder<
      (ins "TypeRange":$resultTensorTypes, "ValueRange":$inputs,
       "ValueRange":$outputs,
       "Attribute":$cast, CArg<"ArrayRef<NamedAttribute>", "{}">:$attributes),
      [{
        $_state.addAttribute("cast", cast);
        buildMatmulOp($_builder, $_state, resultTensorTypes, inputs, outputs,
          attributes, MatmulOp::getRegionBuilder(),
          MatmulOp::getDefaultIndexingMaps($_builder.getContext()));
      }]>

    ];
    let hasCustomAssemblyFormat = 1;
    let hasFolder = 1;
    let hasVerifier = 1;

    let extraClassDeclaration = structuredOpsBaseDecls # [{
      SmallVector<utils::IteratorType> getIteratorTypesArray();

      /// Implements the block region builder.
      static void regionBuilder(ImplicitLocOpBuilder &b,
                                Block &block, ArrayRef<NamedAttribute> attrs);

      /// Returns a list of AffineMap with the typical matmul indexing charactristic.
      static SmallVector<AffineMap> getDefaultIndexingMaps(MLIRContext *context);

      /// Returns true if the given broadcast map \p bcastMap is valid for this op.
      bool isValidLhsRhsBroadcastMap(AffineMap bcastMap);

      static std::function<void(ImplicitLocOpBuilder &,
                                Block &, ArrayRef<NamedAttribute>)>
      getRegionBuilder() {
        return regionBuilder;
      }

      ::mlir::MutableOperandRange getDpsInitsMutable() {
        return getOutputsMutable();
      }

      // Generic methods.
      static unsigned getNumRegionArgs();
      std::string getLibraryCallName();
      bool hasDynamicIndexingMaps();
      /// Check if the op has broadcast and/or transpose semantic. Returns true if the
      /// user defined indexing maps are not equal to default map.
      bool hasUserDefinedMaps();
    }];
}

//===----------------------------------------------------------------------===//
// Named Linalg ops, implemented as a declarative configurations of generic ops.
//===----------------------------------------------------------------------===//

include "mlir/Dialect/Linalg/IR/LinalgNamedStructuredOps.yamlgen.td"

#endif // LINALG_STRUCTURED_OPS


//===- TransformOps.td - Transform dialect operations ------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef MLIR_DIALECT_TRANSFORM_IR_TRANSFORMOPS
#define MLIR_DIALECT_TRANSFORM_IR_TRANSFORMOPS

include "mlir/Interfaces/CallInterfaces.td"
include "mlir/Interfaces/CastInterfaces.td"
include "mlir/Interfaces/ControlFlowInterfaces.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/Interfaces/FunctionInterfaces.td"
include "mlir/IR/OpAsmInterface.td"
include "mlir/IR/RegionKindInterface.td"
include "mlir/IR/SymbolInterfaces.td"
include "mlir/Dialect/Transform/Interfaces/MatchInterfaces.td"
include "mlir/Dialect/Transform/IR/TransformAttrs.td"
include "mlir/Dialect/Transform/IR/TransformDialect.td"
include "mlir/Dialect/Transform/Interfaces/TransformInterfaces.td"

def AlternativesOp : TransformDialectOp<"alternatives",
    [DeclareOpInterfaceMethods<RegionBranchOpInterface,
        ["getEntrySuccessorOperands", "getSuccessorRegions",
         "getRegionInvocationBounds"]>,
     DeclareOpInterfaceMethods<TransformOpInterface>,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     IsolatedFromAbove, PossibleTopLevelTransformOpTrait,
     SingleBlockImplicitTerminator<"::mlir::transform::YieldOp">]> {
  let summary = "Attempts sequences of transforms until one succeeds";
  let description = [{
    This op may have an arbitrary number of regions, each of which represents a
    sequence of transform operations to be applied to the same payload IR. The
    regions are visited in order of appearance, and transforms in them are
    applied in their respective order of appearance. If one of these transforms
    fails to apply, the remaining ops in the same region are skipped an the next
    region is attempted. If all transformations in a region succeed, the
    remaining regions are skipped and the entire "alternatives" transformation
    succeeds. If all regions contained a failing transformation, the entire
    "alternatives" transformation fails.

    It is up to the nested operations to define which errors are "recoverable"
    (or "silenceable") and allow another alternatives to be attempted, and which
    errors should be propagated without attempting the other alternatives.

    The single operand of this operation is the scope in which the alternative
    transformation sequences are attempted, that is, an operation in the payload
    IR that contains all the other operations that may be modified by the
    transformations. The scope operation must be isolated from above. There is
    no check that the transforms are indeed scoped as their "apply" methods can
    be arbitrarily complex. Therefore it is the responsibility of the user to
    ensure that the transforms are scoped correctly, or to produce an
    irrecoverable error and thus abort the execution without attempting the
    remaining alternatives. Note that the payload IR outside of the given scope
    is not necessarily in the valid state, or even accessible to the
    transformation.

    The changes to the IR within the scope performed by transforms in the failed
    alternative region are reverted before attempting the next region.
    Practically, this is achieved by cloning the scope. Therefore it is advised
    to limit the scope as much as possible and place the most likely
    alternatives early in the region list. The operation is also isolated from
    above and requires rediscovering the operations within the given scope to
    avoid additional handle invalidation. The latter restriction may be lifted
    in the future.

    Each of the regions may yield transform IR handles. The handles of the first
    successful alternative region are returned as the results of the
    "alternatives" op. Therefore, each alternative region must yield the same
    number of results, which should also match the number and the types of the
    "alternatives" op results.

    Remark: this op allows one to implement a simple "try" construct as follows:

    ```mlir
    %result = transform.alternatives %scope {
    ^bb0(%arg0: !transform.any_op):
      // Try a fallible transformation.
      %0 = transform.fallible %arg0 // ...
      // If succeeded, yield the the result of the transformation.
      transform.yield %0 : !transform.any_op
    }, {
    ^bb0(%arg0: !transform.any_op):
      // Otherwise, the second alternative is tried and it always succeeds by
      // returning the original handle.
      transform.yield %arg0 : !transform.any_op
    }
    ```
  }];

  let arguments = (ins Optional<TransformHandleTypeInterface>:$scope);
  let results = (outs Variadic<TransformHandleTypeInterface>:$results);
  let regions = (region VariadicRegion<SizedRegion<1>>:$alternatives);

  let assemblyFormat =
    "($scope^ `:` type($scope))? (`->` type($results)^)? "
    "attr-dict-with-keyword regions";
  let hasVerifier = 1;
}

def AnnotateOp : TransformDialectOp<"annotate",
    [DeclareOpInterfaceMethods<TransformOpInterface>,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
  let summary = "Annotates the target operation with an attribute by name";
  let description = [{
    Adds an attribute with the given `name` to the `target` operation. An
    optional `param` handle can be provided to give the attribute a specific
    value, else a UnitAttr is added. A single attribute will be broadcasted to
    all target operations, otherwise the attributes will be mapped 1:1 based on
    the order within the handles.

    Produces a silenceable failure if the length of the parameter payload does
    not match the length of the target payload. Does not consume the provided
    handles.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       StrAttr:$name,
                       Optional<TransformParamTypeInterface>:$param);
  let results = (outs);

  let assemblyFormat =
    "$target $name attr-dict (`=` $param^)?"
    "`:` type($target) (`,` type($param)^)?";
}

def ApplyCommonSubexpressionEliminationOp : TransformDialectOp<"apply_cse",
    [TransformOpInterface, TransformEachOpTrait,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     ReportTrackingListenerFailuresOpTrait]> {
  let summary = "Eliminate common subexpressions in the body of the target op";
  let description = [{
    This transform applies common subexpression elimination (CSE) to the body
    of the targeted op.

    This transform reads the target handle and modifies the payload. Existing
    handles to operations inside of the targeted op are retained and updated if
    necessary. Note that this can lead to situations where a handle, that was
    previously mapped to multiple distinct (but equivalent) operations, is now
    mapped to the same operation multiple times.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs);
  let assemblyFormat = "`to` $target attr-dict `:` type($target)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
      ::mlir::transform::TransformRewriter &rewriter,
      ::mlir::Operation *target,
      ::mlir::transform::ApplyToEachResultList &results,
      ::mlir::transform::TransformState &state);
  }];
}

def ApplyConversionPatternsOp : TransformDialectOp<"apply_conversion_patterns",
    [DeclareOpInterfaceMethods<TransformOpInterface>,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     ReportTrackingListenerFailuresOpTrait]
        # GraphRegionNoTerminator.traits> {
  let summary = "Applies conversion patterns to the body of the targeted op";
  let description = [{
    This transform applies the specified conversion patterns to the targeted op
    and all nested ops. By default, this transform applies a "full" dialect
    conversion. If the `partial_conversion` unit attribute is present, this
    transform applies a partial dialect conversion.

    The patterns that should be applied are specified in the first graph region
    of this op. They must implement the
    `ConversionPatternDescriptorOpInterface`. The order in which patterns are
    applied is unspecified; i.e., the ordering of ops in the region of this op
    is irrelevant.

    The second, optional graph region contains exactly one op that specifies
    default type converter that should be used with this dialect conversion. If
    provided, this op must implement the `TypeConverterBuilderOpInterface`.
    Type converters are a property of conversion patterns: each conversion
    pattern stores the type converter that should be used in its C++ class. Each
    conversion pattern descriptor can optionally specify a type converter in its
    `getTypeConverter` interface method. If no type converter is specified in
    this method, the default type converter of the dialect conversion is used.
    Default type converters are useful if the same type converter should be used
    for multiple sets of conversion patterns. (Patterns that should not use this
    default type converter specify their own type converter.)

    The `legal_ops`, `illegal_ops`, `legal_dialects`, `illegal_dialects`
    attributes specify the conversion target.

    This transform modifies the payload. By default, it consumes the `target`
    handle. It does not produce any handles.

    If the `preserve_handles` attribute is set, this transform does not consume
    the `target` handle and instead updates handles based on notifications from
    a tracking listener that is attached to the dialect conversion, similar to
    `transform.apply_patterns`. Only replacements via `RewriterBase::replaceOp`
    or `replaceOpWithNewOp` are considered "payload op replacements". In
    contrast to `transform.apply_patterns`, we allow replacement ops even if the
    op name has changed. This is because conversion patterns are expected to
    lower ops to different ops (from a different dialect). More details can be
    found at the documentation site of `TrackingListener`.

    This transform produces a silenceable failure if the dialect conversion was
    unsuccessful or the tracking listener failed to find a replacement op.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       OptionalAttr<StrArrayAttr>:$legal_ops,
                       OptionalAttr<StrArrayAttr>:$illegal_ops,
                       OptionalAttr<StrArrayAttr>:$legal_dialects,
                       OptionalAttr<StrArrayAttr>:$illegal_dialects,
                       UnitAttr:$partial_conversion,
                       UnitAttr:$preserve_handles);
  let results = (outs);
  let regions = (region
      MaxSizedRegion<1>:$patterns,
      VariadicRegion<MaxSizedRegion<1>>:$default_type_converter_region);

  let assemblyFormat = [{
    `to` $target $patterns
    (`with` `type_converter` $default_type_converter_region^)?
    attr-dict `:` type($target)
  }];
  let hasVerifier = 1;

  let skipDefaultBuilders = 1;
  let builders = [
    OpBuilder<(ins
        "Value":$target,
        CArg<"function_ref<void(OpBuilder &, Location)>", "nullptr">:
            $patternsBodyBuilder,
        CArg<"function_ref<void(OpBuilder &, Location)>", "nullptr">:
            $typeConverterBodyBuilder)>,
  ];

  let extraClassDeclaration = [{
    ::mlir::transform::TypeConverterBuilderOpInterface getDefaultTypeConverter() {
      if (getDefaultTypeConverterRegion().size() == 0)
        return {};
      return ::llvm::cast<::mlir::transform::TypeConverterBuilderOpInterface>(
          &getDefaultTypeConverterRegion()[0].front().front());
    }
  }];
}

def ApplyToLLVMConversionPatternsOp : Op<Transform_Dialect,
    "apply_conversion_patterns.dialect_to_llvm",
    [DeclareOpInterfaceMethods<ConversionPatternDescriptorOpInterface,
                               ["verifyTypeConverter"]>]> {
  let description = [{
    Collects patterns that convert ops from the specified dialect to LLVM
    dialect ops. These patterns require an "LLVMTypeConverter".

    Note: Only dialects that implement the `ConvertToLLVMPatternInterface` are
    supported. Any conversion target modifications by interface implementations
    are currently ignored. The conversion target is fully specified by the
    enclosing "apply_conversion_patterns" op.
  }];

  let arguments = (ins StrAttr:$dialect_name);
  let assemblyFormat = "$dialect_name attr-dict";
  let hasVerifier = 1;
}

def ApplyDeadCodeEliminationOp : TransformDialectOp<"apply_dce",
    [TransformOpInterface, TransformEachOpTrait,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     ReportTrackingListenerFailuresOpTrait]> {
  let summary = "Eliminate dead operations in the body of the target op";
  let description = [{
    This transform applies dead code elimination (DCE) to the body of the
    targeted op.

    Note: "transform.apply_patterns" with an empty region can also be used to
    remove dead ops. However, that op applies additional simplifications such as
    op folding and region simplification.

    This transform reads the target handle and modifies the payload. Note that
    this transform may silently remove payload ops from handles.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs);
  let assemblyFormat = "`to` $target attr-dict `:` type($target)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
      ::mlir::transform::TransformRewriter &rewriter,
      ::mlir::Operation *target,
      ::mlir::transform::ApplyToEachResultList &results,
      ::mlir::transform::TransformState &state);
  }];
}

def ApplyPatternsOp : TransformDialectOp<"apply_patterns",
    [TransformOpInterface, TransformEachOpTrait,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     ReportTrackingListenerFailuresOpTrait]
        # GraphRegionNoTerminator.traits> {
  let summary = "Greedily applies patterns to the body of the targeted op";
  let description = [{
    This transform greedily applies the specified patterns to the body of the
    targeted op until a fixpoint was reached. Patterns are not applied to the
    targeted op itself.

    The patterns that should be applied are specified in the graph region of
    this op. They must implement the `PatternDescriptorOpInterface`. The order
    in which patterns are applied is unspecified; i.e., the ordering of ops in
    the region of this op is irrelevant.

    If `apple_cse` is set, the greedy pattern rewrite is interleaved with
    common subexpression elimination (CSE): both are repeated until a fixpoint
    is reached.

    This transform only reads the target handle and modifies the payload. If a
    pattern erases or replaces a tracked op, the mapping is updated accordingly.

    Only replacements via `RewriterBase::replaceOp` or `replaceOpWithNewOp` are
    considered "payload op replacements". Furthermore, only if the replacement
    values are defined by the same op and that op has the same type as the
    original op, the mapping is updated. Otherwise, this transform produces a
    silenceable failure. More details can be found at the documentation site of
    `TrackingListener`.

    This transform also produces a silenceable failure if the pattern
    application did not converge within the default number of
    iterations/rewrites of the greedy pattern rewrite driver.
  }];

  let arguments = (ins
    TransformHandleTypeInterface:$target,
    UnitAttr:$apply_cse,
    DefaultValuedAttr<I64Attr, "static_cast<uint64_t>(-1)">:$max_iterations,
    DefaultValuedAttr<I64Attr, "static_cast<uint64_t>(-1)">:$max_num_rewrites);
  let results = (outs);
  let regions = (region MaxSizedRegion<1>:$patterns);

  let assemblyFormat = "`to` $target $patterns attr-dict `:` type($target)";
  let hasVerifier = 1;

  let skipDefaultBuilders = 1;
  let builders = [
    OpBuilder<(ins
        "Value":$target,
        CArg<"function_ref<void(OpBuilder &, Location)>", "nullptr">:
            $bodyBuilder)>,
  ];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
      ::mlir::transform::TransformRewriter &rewriter,
      ::mlir::Operation *target,
      ::mlir::transform::ApplyToEachResultList &results,
      ::mlir::transform::TransformState &state);
  }];
}

def ApplyCanonicalizationPatternsOp
    : TransformDialectOp<"apply_patterns.canonicalization",
        [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let summary = "Populates canonicalization patterns";
  let description = [{
    This op populates all canonicalization patterns of all loaded dialects in
    an `apply_patterns` transform.
  }];
  let assemblyFormat = "attr-dict";
}

def ApplyLoopInvariantCodeMotionOp : TransformDialectOp<"apply_licm",
    [TransformOpInterface, TransformEachOpTrait,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     ReportTrackingListenerFailuresOpTrait]> {
  let summary = "Move loop-invariant code out of a loop-like op";
  let description = [{
    This transform moves side-effect free, loop invariant code out of the
    targeted loop-like op. The targeted op must implement the
    `LoopLikeOpInterface`.

    Note: To move invariant ops from a loop nest, this transform must be applied
    to each loop of the loop nest, starting with the inner-most loop.

    This transform reads the target handle and modifies the payload.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs);
  let assemblyFormat = "`to` $target attr-dict `:` type($target)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
      ::mlir::transform::TransformRewriter &rewriter,
      ::mlir::LoopLikeOpInterface target,
      ::mlir::transform::ApplyToEachResultList &results,
      ::mlir::transform::TransformState &state);
  }];
}

def ApplyRegisteredPassOp : TransformDialectOp<"apply_registered_pass",
    [TransformOpInterface, TransformEachOpTrait,
     FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface]> {
  let summary = "Applies the specified registered pass or pass pipeline";
  let description = [{
    This transform applies the specified pass or pass pipeline to the targeted
    ops. The name of the pass/pipeline is specified as a string attribute, as
    set during pass/pipeline registration. Optionally, pass options may be
    specified as a string attribute. The pass options syntax is identical to the
    one used with "mlir-opt".

    This op first looks for a pass pipeline with the specified name. If no such
    pipeline exists, it looks for a pass with the specified name. If no such
    pass exists either, this op fails definitely.

    This transform consumes the target handle and produces a new handle that is
    mapped to the same op. Passes are not allowed to remove/modify the operation
    that they operate on, so the target op is guaranteed to still exist. The
    target handle is invalidated because a pass may arbitrarily modify the body
    of targeted ops.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       StrAttr:$pass_name,
                       DefaultValuedAttr<StrAttr, "\"\"">:$options);
  let results = (outs TransformHandleTypeInterface:$result);
  let assemblyFormat = [{
    $pass_name `to` $target attr-dict `:` functional-type(operands, results)
  }];

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
      ::mlir::transform::TransformRewriter &rewriter,
      ::mlir::Operation *target,
      ::mlir::transform::ApplyToEachResultList &results,
      ::mlir::transform::TransformState &state);
  }];
}

def CastOp : TransformDialectOp<"cast",
    [TransformOpInterface, TransformEachOpTrait,
     DeclareOpInterfaceMethods<CastOpInterface>,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
  let arguments = (ins TransformHandleTypeInterface:$input);
  let results = (outs TransformHandleTypeInterface:$output);
  let assemblyFormat = "$input attr-dict `:` type($input) `to` type($output)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
      ::mlir::transform::TransformRewriter &rewriter,
      ::mlir::Operation *target,
      ::mlir::transform::ApplyToEachResultList &results,
      ::mlir::transform::TransformState &state);
  }];
}

def NumAssociationsOp : TransformDialectOp<"num_associations",
    [MemoryEffectsOpInterface, ParamProducerTransformOpTrait,
     DeclareOpInterfaceMethods<TransformOpInterface>,
     MatchOpInterface]> {
  let summary =
    "Returns the number of payload objects associated with the argument";
  let description = [{
    Given an argument, handle or parameter, returns a new parameter associated
    with a single 64-bit number that corresponds to the number of payload
    objects (operations or values for a handle, attributes for a parameter)
    associated with the argument.

    Always succeeds.
  }];
  let arguments = (ins Transform_AnyHandleOrParamType:$handle);
  let results = (outs TransformParamTypeInterface:$num);
  let assemblyFormat = [{
    $handle attr-dict `:` functional-type(operands, results)
  }];
  let hasVerifier = 1;
}

def CollectMatchingOp : TransformDialectOp<"collect_matching", [
    DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
    DeclareOpInterfaceMethods<SymbolUserOpInterface>,
    DeclareOpInterfaceMethods<TransformOpInterface>]> {
  let summary = "Collects all payload ops that match the given named matcher";
  let description = [{
    Collects operations or other payload IR objects nested under `root`
    (inclusive) that match the given matcher expressed as a named sequence. The
    matcher sequence must accept exactly one argument that it is not allowed to
    modify. It must yield as many values as this op has results. Each of the
    yielded values must be associated with exactly one payload object. If any
    operation in the matcher sequence produces a silenceable failure, the
    matcher advances to the next payload operation in the walk order without
    finishing the sequence.

    The i-th result of this operation is constructed by concatenating the i-th
    yielded payload IR objects of all successful matcher sequence applications.
    All results are guaranteed to be mapped to the same number of payload IR
    objects.

    The operation succeeds unless the matcher sequence produced a definite
    failure for any invocation.
  }];

  let arguments = (ins TransformHandleTypeInterface:$root,
                       SymbolRefAttr:$matcher);
  let results = (outs Variadic<Transform_AnyHandleOrParamType>:$results);

  let assemblyFormat = [{
    $matcher `in` $root attr-dict `:` functional-type($root, $results)
  }];
}

def ForeachMatchOp : TransformDialectOp<"foreach_match", [
    DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
    DeclareOpInterfaceMethods<SymbolUserOpInterface>,
    DeclareOpInterfaceMethods<TransformOpInterface,
                              ["allowsRepeatedHandleOperands"]>,
    DeclareOpInterfaceMethods<OpAsmOpInterface,
                              ["getAsmResultNames"]>]> {
  let summary = "Applies named sequences when a named matcher succeeds";
  let description = [{
    Given a pair of co-indexed lists of transform dialect symbols (such as
    `transform.named_sequence`), walks the payload IR associated with the root
    handle and interprets the symbols as matcher/action pairs by applying the
    body of the corresponding symbol definition. The symbol from the first list
    is the matcher part: if it results in a silenceable error, the error is
    silenced and the next matcher is attempted. Definite failures from any
    matcher stop the application immediately and are propagated unconditionally.
    If none of the matchers succeeds, the next payload operation in walk order
    (post-order at the moment of writing, double check `Operation::walk`) is
    matched. If a matcher succeeds, the co-indexed action symbol is applied and
    the following matchers are not applied to the same payload operation. If the
    action succeeds, the next payload operation in walk order is matched. If it
    fails, both silenceable and definite errors are propagated as the result of
    this op; propagation of silenceable errors is postponed until the end of the
    walk.

    The matcher symbol must take at least one operand of a type that implements
    the same transform dialect interface as the `root` operand (a check is
    performed at application time to see if the associated payload satisfies the
    constraints of the actual type), and may take additional operands with a
    similar type requirement. It must not consume operands as multiple matchers
    may be applied. The matcher may produce any number of results. The action
    symbol paired with the matcher must take the same number of arguments as the
    matcher has results, and these arguments must implement the same transform
    dialect interfaces, but not necessarily have the exact same type (again, a
    check is performed at application time to see if the associated payload
    satisfies the constraints of actual types on both sides).

    The action symbol may have results that are accumulated from all actions and
    returned from the `foreach_match` operation on success. Unless the
    `flatten_results` attribute is present, each action result must be
    associated with exactly one payload entity. The actions are expected to only
    modify payload operations nested in the `root` payload operations associated
    with the operand of this transform operation. Furthermore, the actions may
    not modify operations outside of the currently matched payload operation,
    e.g., they may not modify sibling or parent operations. If such behavior is
    desired, the parent must be matched first and the nested operations obtained
    by traversing the IR from the parent. This is due to the matching being
    performed as a post-order IR walk.

    This operation consumes the operand and produces a new handle associated
    with the same payload. This is necessary to trigger invalidation of handles
    to any of the payload operations nested in the payload operations associated
    with the operand, as those are likely to be modified by actions.

    By default, the root payload operation associated with the operand is not
    matched. This is to support the conservative case where applied actions may
    invalidate the root payload operation. If the optional `restrict_root`
    attribute is set, the root operand is guaranteed to not be invalidated by any
    of the applied actions. In such cases, the root payload operation is also
    matched. This is useful because matching the root payload operation is a
    common idiom, when e.g. matching a func.func directly and operations nested
    under it.

    The operation succeeds if none of the matchers produced a definite failure
    during application and if all of the applied actions produced success. Note
    that it also succeeds if all the matchers failed on all payload operations,
    i.e. failure to apply is not an error. The operation produces a silenceable
    failure if any applied action produced a silenceable failure. In this case,
    the resulting handle is associated with an empty payload. The operation
    produces a definite failure if any of the applied matchers or actions
    produced a definite failure.
  }];

  let arguments =
      (ins TransformHandleTypeInterface:$root,
           Variadic<Transform_AnyHandleOrParamType>:$forwarded_inputs,
           UnitAttr:$restrict_root,
           UnitAttr:$flatten_results,
           SymbolRefArrayAttr:$matchers,
           SymbolRefArrayAttr:$actions);
  let results =
      (outs TransformHandleTypeInterface:$updated,
            Variadic<Transform_AnyHandleOrParamType>:$forwarded_outputs);

  let assemblyFormat = [{
    oilist( `restrict_root` $restrict_root
          | `flatten_results` $flatten_results
          )
    `in`
    $root (`,` $forwarded_inputs^)?
    custom<ForeachMatchSymbols>($matchers, $actions)
    attr-dict
    `:` functional-type(operands, results)
  }];

  let hasVerifier = 1;
}

def ForeachOp : TransformDialectOp<"foreach",
    [DeclareOpInterfaceMethods<TransformOpInterface>,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     DeclareOpInterfaceMethods<RegionBranchOpInterface, [
         "getSuccessorRegions", "getEntrySuccessorOperands"]>,
     SingleBlockImplicitTerminator<"::mlir::transform::YieldOp">
    ]> {
  let summary = "Executes the body for each element of the payload";
  let description = [{
    Execute the op's body - its single region block - exactly once per
    element of the payload associated to a target handle. The body's
    transformations are applied in order of appearance until reaching the
    (implicit) YieldOp terminator.

    Each iteration gets executed by co-indexing the payloads of the arguments
    and mapping the body's arguments to these tuples, as though iterating over
    the zipped together `targets`. As such, in each iteration, the size of the
    payload of each of the body's block arguments is exactly one. The attribute
    `zip_shortest` can be used if the targets vary in their number of payloads;
    this will limit the iterations to only the number of payloads found in the
    shortest target.

    This op always reads the target handles. Furthermore, it consumes a handle
    if there is a transform op in the body that consumes the corresponding
    block argument. Handles can point to ops, values, or parameters.

    #### Return Modes

    This op produces as many result handles as the body's terminating YieldOp
    has operands. For each result, the payloads of the corresponding YieldOp
    operand are merged and mapped to the same resulting handle.

    If the target handles do not associate payloads of the same size, a
    silencable failure will be generated.

    During application, if any transformation in the sequence fails, the entire
    sequence fails immediately with the same failure, leaving the payload IR in
    a potentially invalid state, i.e., this operation offers no transformation
    rollback capabilities.
  }];

  let arguments = (ins Variadic<Transform_AnyHandleOrParamType>:$targets,
                       UnitAttr:$with_zip_shortest);
  let results = (outs Variadic<Transform_AnyHandleOrParamType>:$results);
  let regions = (region SizedRegion<1>:$body);
  let assemblyFormat =
    "$targets oilist(`with_zip_shortest` $with_zip_shortest) `:` "
    "type($targets) (`->` type($results)^)? $body attr-dict";
  let hasVerifier = 1;

  let extraClassDeclaration = [{
    /// Allow the dialect prefix to be omitted.
    static StringRef getDefaultDialect() { return "transform"; }

    transform::YieldOp getYieldOp();
  }];
}

def GetConsumersOfResult : TransformDialectOp<"get_consumers_of_result",
    [DeclareOpInterfaceMethods<TransformOpInterface>,
     NavigationTransformOpTrait, MemoryEffectsOpInterface]> {
  let summary = "Get handle to the consumers of this operation's result number";
  let description = [{
    The handle defined by this Transform op corresponds to all operations that
    consume the SSA value defined by the `target` and `result_number`
    arguments.
    This operation applies to a single payload operation, otherwise it produces
    a definite failure.
    The return handle points to the consuming operations operations, which can
    be empty.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       I64Attr:$result_number);
  let results = (outs TransformHandleTypeInterface:$consumers);
  let assemblyFormat = "$target `[` $result_number `]` attr-dict `:` "
                       "functional-type(operands, results)";
}

def GetDefiningOp : TransformDialectOp<"get_defining_op",
    [DeclareOpInterfaceMethods<TransformOpInterface>,
     MatchOpInterface,
     NavigationTransformOpTrait, MemoryEffectsOpInterface]> {
  let summary = "Get handle to the defining op of a value";
  let description = [{
    The handle defined by this Transform op corresponds to the defining op of
    the targeted value.

    This transform produces a silenceable failure if the targeted value is a
    block argument.
  }];

  let arguments = (ins TransformValueHandleTypeInterface:$target);
  let results = (outs TransformHandleTypeInterface:$result);
  let assemblyFormat = "$target attr-dict `:` "
                       "functional-type(operands, results)";
}

def GetParentOp : TransformDialectOp<"get_parent_op",
    [DeclareOpInterfaceMethods<TransformOpInterface>,
     MatchOpInterface,
     NavigationTransformOpTrait, MemoryEffectsOpInterface]> {
  let summary = "Gets handles to the closest parent ops";
  let description = [{
    The handle defined by this Transform op corresponds to the parents of the
    targeted payload ops (in the same order).

    Requirements that parent ops must fulfill can be optionally specified. In
    that case for each target op, the closest parent op that fulfills all
    requirements, is returned.
    - `isolated_from_above`: the parent op must be isolated from above
    - `allow_empty_results`: get_parent_op is allowed to return an empty list
      and still succeeds. In such a case, if `get_parent_op` fails for any
      operation in the list, the entire transform returns an empty handle.
    - `op_name`: the parent op must have the specified name
    - `nth_parent`: get the n-th parent of that satisfies the above requirements

    If `deduplicate` is set, the result handle does not contain any duplicate
    ops. For example, given the list
    "(childof(A), childof(B), childof(B), childof(A), childof(B))", the
    resulting list will be just "(A, B)". Note that no other semantic ordering
    is applied, e.g., "B" may itself be a parent of "A". This may have an impact
    on the further transformation applied to the handle produced here.

    If any of the given Payload IR ops has no such suitable parent, then:
      - if `allow_empty_results` is set, the result handle is empty
      - otherwise, the transformation produces a silenceable failure.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       UnitAttr:$isolated_from_above,
                       UnitAttr:$allow_empty_results,
                       OptionalAttr<StrAttr>:$op_name,
                       UnitAttr:$deduplicate,
                       DefaultValuedAttr<ConfinedAttr<I64Attr, [IntPositive]>,
                                         "1">:$nth_parent);
  let results = (outs TransformHandleTypeInterface:$parent);
  let assemblyFormat =
    "$target attr-dict `:` functional-type(operands, results)";
}

def GetProducerOfOperand : TransformDialectOp<"get_producer_of_operand",
    [DeclareOpInterfaceMethods<TransformOpInterface>,
     NavigationTransformOpTrait, MatchOpInterface, MemoryEffectsOpInterface]> {
  let summary = "Get handle to the producer of this operation's operand number";
  let description = [{
    The handle defined by this Transform op corresponds to operation that
    produces the SSA value defined by the `target` and `operand_number`
    arguments. If the origin of the SSA value is not an operations (i.e. it is
    a block argument), the transform produces a silenceable failure.
    The return handle points to only the subset of successfully produced
    computational operations, which can be empty.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       I64Attr:$operand_number);
  let results = (outs TransformHandleTypeInterface:$producer);
  let assemblyFormat = "$target `[` $operand_number `]` attr-dict `:` "
                       "functional-type(operands, results)";
}

def GetOperandOp : TransformDialectOp<"get_operand",
    [DeclareOpInterfaceMethods<TransformOpInterface>,
     NavigationTransformOpTrait, MatchOpInterface, MemoryEffectsOpInterface]> {
  let summary = "Get a handle to the operand(s) of the targeted op";
  let description = [{
    The handle defined by this Transform op corresponds to the operands of the
    given `target` operation specified by the given set of positions. There are
    three possible modes:

     - Position list directly, i.e. `%target[0, 1, 2]`. This will return the
       operands at the specified positions.
     - Inverted position list, i.e. `%target[except(0, 1, 2)]`. This will return
       all operands except those at the given positions.
     - All, i.e. `%target[all]`. This will return all operands of the operation.
    
    This transform produces a silenceable failure if any of the operand indices
    exceeds the number of operands in the target. It reads the target handle and
    produces the result handle.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       DenseI64ArrayAttr:$raw_position_list,
                       UnitAttr:$is_inverted,
                       UnitAttr:$is_all);
  let results = (outs TransformValueHandleTypeInterface:$result);
  let assemblyFormat =
      "$target `[`"
      "custom<TransformMatchDims>($raw_position_list, $is_inverted, $is_all)"
      "`]` attr-dict `:` functional-type(operands, results)";
  let hasVerifier = 1;
}

def GetResultOp : TransformDialectOp<"get_result",
    [DeclareOpInterfaceMethods<TransformOpInterface>,
     NavigationTransformOpTrait, MemoryEffectsOpInterface]> {
  let summary = "Get a handle to the result(s) of the targeted op";
  let description = [{
    The handle defined by this Transform op correspond to the OpResults of the
    given `target` operation. Optionally `result_number` can be specified to
    select a specific result.
    
    This transform fails silently if the targeted operation does not have enough
    results. It reads the target handle and produces the result handle.

    The handle defined by this Transform op corresponds to the results of the
    given `target` operation specified by the given set of positions. There are
    three possible modes:

     - Position list directly, i.e. `%target[0, 1, 2]`. This will return the
       results at the specified positions.
     - Inverted position list, i.e. `%target[except(0, 1, 2)]`. This will return
       all results except those at the given positions.
     - All, i.e. `%target[all]`. This will return all results of the operation.
    
    This transform produces a silenceable failure if any of the result indices
    exceeds the number of results returned by the target. It reads the target
    handle and produces the result handle.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       DenseI64ArrayAttr:$raw_position_list,
                       UnitAttr:$is_inverted,
                       UnitAttr:$is_all);
  let results = (outs TransformValueHandleTypeInterface:$result);
  let assemblyFormat =
      "$target `[`"
      "custom<TransformMatchDims>($raw_position_list, $is_inverted, $is_all)"
      "`]` attr-dict `:` functional-type(operands, results)";
  let hasVerifier = 1;
}

def GetTypeOp : TransformDialectOp<"get_type",
    [DeclareOpInterfaceMethods<TransformOpInterface>,
     MatchOpInterface,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
  let summary = "Get a parameter containing the type of the given value";
  let description = [{
    This operation creates a new Transform parameter containing the
    type(s) of the value(s) associated with the operand handle.

    This transform never fails.
  }];

  let arguments = (ins TransformValueHandleTypeInterface:$value,
                       UnitAttr:$elemental);
  let results = (outs TransformParamTypeInterface:$type_param);
  let assemblyFormat = "(`elemental` $elemental^)? $value attr-dict `:`"
                       "functional-type(operands, results)";
}

def IncludeOp : TransformDialectOp<"include",
    [CallOpInterface,
     MatchOpInterface,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     DeclareOpInterfaceMethods<SymbolUserOpInterface>,
     DeclareOpInterfaceMethods<TransformOpInterface>]> {
  let summary = "Includes a named transform sequence";
  let description = [{
    The application of this transform operation is equivalent to applying the
    operations contained in the named transform sequence with operands being
    remapped to block arguments. The behavior of the operation when a
    transformation in the included named sequence produces a silenceable error
    is controlled by the `failure_propagation_mode` attribute. When set to
    `propagate`, the failure of any nested transformation in the sequence
    implies immediate failure of the entire sequence with a silenceable error,
    and no further transformation is attempted. When set to `suppress`,
    silenceable errors in nested operations are ignored and further
    transformations are applied. Beware that even silenceable errors may leave
    the payload IR in a state unsuitable for further transformations. It is the
    responsibility of the user to ensure the following transformations are
    robust enough when errors are suppressed. Definite errors are propagated
    immediately regardless of the mode. The objects associated with the results
    of this operation are the same as those associated with the operands of the
    `transform.yield` in the referenced named sequence.
  }];

  let arguments = (ins SymbolRefAttr:$target,
                       FailurePropagationMode:$failure_propagation_mode,
                       Variadic<Transform_AnyHandleOrParamType>:$operands);
  let results = (outs Variadic<Transform_AnyHandleOrParamType>:$results);

  let assemblyFormat =
      "$target `failures` `(` $failure_propagation_mode `)`"
      "`(` $operands `)` attr-dict `:` functional-type($operands, $results)";

  let extraClassDeclaration = [{
    ::mlir::CallInterfaceCallable getCallableForCallee() {
      return getTarget();
    }

    void setCalleeFromCallable(::mlir::CallInterfaceCallable callee) {
      setTargetAttr(callee.get<SymbolRefAttr>());
    }

    ::mlir::Operation::operand_range getArgOperands() {
      return getOperands();
    }

    ::mlir::MutableOperandRange getArgOperandsMutable() {
      return getOperandsMutable();
    }
  }];
}

def MatchOperationEmptyOp : Op<Transform_Dialect, "match.operation_empty", [
    AtMostOneOpMatcher,
    MatchOpInterface,
    MemoryEffectsOpInterface]> {
  let summary =
    "Matches if the handle is not associated to any op";
  let description = [{
    Succeeds if the handle is not associated to any op.
  }];
  let arguments = (ins TransformHandleTypeInterface:$operand_handle);
  let assemblyFormat =
      "$operand_handle attr-dict `:` type($operand_handle)";
  let extraClassDeclaration = AtMostOneOpMatcher.extraDeclaration;
}

def MatchOperationNameOp : TransformDialectOp<"match.operation_name",
    [SingleOpMatcher,
     MatchOpInterface,
     MemoryEffectsOpInterface]> {
  let summary = "Matches a single operation of one of the given kinds";
  let description = [{
    Succeeds if the operation associated with the operand handle has one of the
    given operation names. Produces a silenceable failure otherwise.

    If more than one payload operation is associated with the operand handle,
    produces a definite failure.
  }];

  let arguments = (ins TransformHandleTypeInterface:$operand_handle,
                       StrArrayAttr:$op_names);
  let assemblyFormat =
      "$operand_handle $op_names attr-dict `:` type($operand_handle)";
  let extraClassDeclaration = SingleOpMatcher.extraDeclaration;
}

def MatchParamCmpIOp : Op<Transform_Dialect, "match.param.cmpi", [
    DeclareOpInterfaceMethods<TransformOpInterface>,
    MatchOpInterface,
    DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
    SameTypeOperands]> {
  let summary =
    "Matches if two parameter lists are associated with the same value";
  let description = [{
    Succeeds if all of the co-indexed values associated with the given
    parameters relate as specified by the predicate (greater than, less than,
    equal to, or their combinations). Comparison treats all values as signed.
    Produces a silenceable failure otherwise.
  }];
  let arguments = (ins TransformParamTypeInterface:$param,
                       TransformParamTypeInterface:$reference,
                       MatchCmpIPredicateAttr:$predicate);
  let assemblyFormat =
      "$predicate $param `,` $reference attr-dict `:` type($param)";
}

def MergeHandlesOp : TransformDialectOp<"merge_handles",
    [DeclareOpInterfaceMethods<TransformOpInterface, ["allowsRepeatedHandleOperands"]>,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     MatchOpInterface, SameOperandsAndResultType]> {
  let summary = "Merges handles into one pointing to the union of payload ops";
  let description = [{
    Creates a new Transform IR handle value that points to the same Payload IR
    operations/values/parameters as the operand handles. The Payload IR elements
    are listed in the same order as they are in the operand handles, grouped by
    operand handle, e.g., all Payload IR associated with the first handle comes
    first, then all Payload IR associated with the second handle and so on. If
    `deduplicate` is set, do not add the given Payload IR operation, value, or
    parameter more than once to the final list regardless of it coming from the
    same or different handles. Consumes the operands and produces a new handle.
  }];

  let arguments = (ins Variadic<Transform_AnyHandleOrParamType>:$handles,
                       UnitAttr:$deduplicate);
  let results = (outs Transform_AnyHandleOrParamType:$result);
  let assemblyFormat = "(`deduplicate` $deduplicate^)? $handles attr-dict `:` type($result)";
  let hasFolder = 1;
}

def NamedSequenceOp : TransformDialectOp<"named_sequence",
    [FunctionOpInterface,
     IsolatedFromAbove,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     DeclareOpInterfaceMethods<TransformOpInterface>]> {
  let summary = "Named transform sequence that can be included elsewhere";
  let description = [{
    Defines a named (callable, function-like) sequence of other Transform
    dialect operations that can be included using `transform.include` as part of
    another Transform dialect construct. This sequence is not processed
    immediately but rather dispatched to when the inclusion is processed. The
    arguments and results can be used to communicate a subset of mapping into
    the named sequence. The sequence must consist of a single block and end with
    a `transform.yield` terminator. The operands of the terminator become the
    results of the `transform.include`.

    When dispatched to, the operations in the named sequence are executed one by
    one, similarly to the regular unnamed sequence. The failure propagation mode
    is specified on the `transform.include`. Different inclusions may use
    different failure propagation modes. This transform operation always
    succeeds by itself, but the inclusion may fail if any of the operations
    fail.

    Named sequences can only appear at the top-level of the Transform dialect
    nesting structure. That is, they cannot be nested in other Transform dialect
    operations. Furthermore, one of the ancestors must have the `SymbolTable`
    trait and have the `transform.with_named_sequence` attribute attached.

    Named sequences may include other named sequences via `transform.include`,
    but recursion is *not* allowed.
  }];

  let arguments = (ins
    SymbolNameAttr:$sym_name,
    TypeAttrBase<"::mlir::FunctionType",
                 "function type attribute">:$function_type,
    OptionalAttr<StrAttr>:$sym_visibility,
    OptionalAttr<DictArrayAttr>:$arg_attrs,
    OptionalAttr<DictArrayAttr>:$res_attrs);
  let regions = (region MaxSizedRegion<1>:$body);

  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;

  let builders = [
    // Build a named sequence.
    OpBuilder<(ins
      "StringRef":$symName,
      "Type":$rootType,
      "TypeRange":$resultType,
      "SequenceBodyBuilderFn":$bodyBuilder,
      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs,
      CArg<"ArrayRef<DictionaryAttr>", "{}">:$argAttrs)>
  ];

  let extraClassDeclaration = [{
    ::llvm::ArrayRef<::mlir::Type> getArgumentTypes() {
      return getFunctionType().getInputs();
    }
    ::llvm::ArrayRef<::mlir::Type> getResultTypes() {
      return getFunctionType().getResults();
    }
    ::mlir::Region *getCallableRegion() {
      return &getBody();
    }
  }];
}

def SplitHandleOp : TransformDialectOp<"split_handle",
    [FunctionalStyleTransformOpTrait,
     DeclareOpInterfaceMethods<TransformOpInterface>,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
  let summary = "Splits a handle of payload ops into handles with a single op";
  let description = [{
    Splits `handle` into one or multiple handles, as specified by the number
    of results of this operation. `handle` should be mapped to as many payload
    ops as there are results. Otherwise, this transform will fail produces a
    silenceable failure by default. Each result handle is mapped to exactly one
    payload op. The order of the payload ops is preserved, i.e., the i-th
    payload op is mapped to the i-th result handle.

    This operation is useful for ensuring a statically known number of
    operations are tracked by the source `handle` and to extract them into
    individual handles that can be further manipulated in isolation.

    If there are more payload ops than results, the remaining ops are mapped to
    the result with index `overflow_result`. If no `overflow_result` is
    specified, the transform produces a silenceable failure.

    If there are fewer payload ops than results, the transform produces a
    silenceable failure if `fail_on_payload_too_small` is set to "true".
    Otherwise, it succeeds and the remaining result handles are not mapped to
    any op. It also succeeds if `handle` is empty and
    `pass_through_empty_handle` is set to "true", regardless of
    `fail_on_payload_too_small`.
  }];

  let arguments = (ins TransformHandleTypeInterface:$handle,
                       DefaultValuedAttr<BoolAttr, "true">:$pass_through_empty_handle,
                       DefaultValuedAttr<BoolAttr, "true">:$fail_on_payload_too_small,
                       OptionalAttr<I64Attr>:$overflow_result);
  let results = (outs Variadic<TransformHandleTypeInterface>:$results);
  let hasVerifier = 1;

  let builders = [
    OpBuilder<(ins "Value":$handle, "int64_t":$numResultHandles)>
  ];

  let assemblyFormat = [{
    $handle attr-dict `:` functional-type(operands, results)
  }];
}

def ParamConstantOp : Op<Transform_Dialect, "param.constant", [
    MatchOpInterface,
    DeclareOpInterfaceMethods<TransformOpInterface>,
    MemoryEffectsOpInterface,
    ParamProducerTransformOpTrait]> {
  let summary = "Produces a new transform dialect parameter value associated "
                "with the given attribute";
  let description = [{
    Produces a new transform dialect parameter associated with the singleton
    list containing the given attribute. The operation itself always succeeds,
    but the general association check may fail if the parameter type does not
    accept the given kind of attribute as valid.
  }];
  let arguments = (ins AnyAttr:$value);
  let results = (outs TransformParamTypeInterface:$param);
  let assemblyFormat = "$value attr-dict `->` type($param)";
}

def PrintOp : TransformDialectOp<"print",
    [DeclareOpInterfaceMethods<TransformOpInterface>,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     MatchOpInterface]> {
  let summary = "Dump each payload op";
  let description = [{
    Prints each payload op that is associated with the `target` operand to
    `stdout`. It also prints the `name` string attribute. If no target is
    specified, the top-level op is dumped.

    This op is useful for printf-style debugging.

    Supported printing flag attributes:
    * `assume_verified` -- skips verification when the unit attribute is
      specified. This improves performace but may lead to crashes and
      unexpected behavior when the printed payload op is invalid.
    * `use_local_scope` -- prints in local scope when the unit attribute is
      specified. This improves performance but may not be identical to
      printing within the full module.
    * `skip_regions` -- does not print regions of operations when the unit
      attribute is specified.
  }];

  let arguments = (ins Optional<TransformHandleTypeInterface>:$target,
                       OptionalAttr<StrAttr>:$name,
                       OptionalAttr<UnitAttr>:$assume_verified,
                       OptionalAttr<UnitAttr>:$use_local_scope,
                       OptionalAttr<UnitAttr>:$skip_regions);
  let results = (outs);

  let builders = [
    OpBuilder<(ins CArg<"StringRef", "StringRef()">:$name)>,
    OpBuilder<(ins "Value":$target, CArg<"StringRef", "StringRef()">:$name)>
  ];

  let assemblyFormat = "$target attr-dict (`:` type($target)^)?";
}

def ReplicateOp : TransformDialectOp<"replicate",
    [DeclareOpInterfaceMethods<TransformOpInterface>,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     AllTypesMatch<["handles", "replicated"]>]> {
  let summary = "Lists payload ops multiple times in the new handle";
  let description = [{
    Produces a new handle associated with a list of payload IR ops that is
    computed by repeating the list of payload IR ops associated with the
    operand handle as many times as the "pattern" handle has associated
    operations. For example, if pattern is associated with [op1, op2] and the
    operand handle is associated with [op3, op4, op5], the resulting handle
    will be associated with [op3, op4, op5, op3, op4, op5].

    This transformation is useful to "align" the sizes of payload IR lists
    before a transformation that expects, e.g., identically-sized lists. For
    example, a transformation may be parameterized by same notional per-target
    size computed at runtime and supplied as another handle, the replication
    allows this size to be computed only once and used for every target instead
    of replicating the computation itself.

    Note that it is undesirable to pass a handle with duplicate operations to
    an operation that consumes the handle. Handle consumption often indicates
    that the associated payload IR ops are destroyed, so having the same op
    listed more than once will lead to double-free. Single-operand
    MergeHandlesOp may be used to deduplicate the associated list of payload IR
    ops when necessary. Furthermore, a combination of ReplicateOp and
    MergeHandlesOp can be used to construct arbitrary lists with repetitions.
  }];

  let arguments = (ins TransformHandleTypeInterface:$pattern,
                       Variadic<Transform_AnyHandleOrParamType>:$handles);
  let results = (outs Variadic<Transform_AnyHandleOrParamType>:$replicated);
  let assemblyFormat = "`num` `(` $pattern `)` $handles attr-dict `:` "
                       "type($pattern) `,` type($handles)";
}

def SelectOp : TransformDialectOp<"select",
    [DeclareOpInterfaceMethods<TransformOpInterface>,
     NavigationTransformOpTrait, MemoryEffectsOpInterface]> {
  let summary = "Select payload ops by name";
  let description = [{
    The handle defined by this Transform op corresponds to all operations among
    `target` that have the specified properties. Currently the following
    properties are supported:

    - `op_name`: The op must have the specified name.

    The result payload ops are in the same relative order as the targeted ops.
    This transform op reads the `target` handle and produces the `result`
    handle. It reads the payload, but does not modify it.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target,
                       StrAttr:$op_name);
  let results = (outs TransformHandleTypeInterface:$result);
  let assemblyFormat = [{
    $op_name `in` $target attr-dict `:` functional-type(operands, results)
  }];
}

def SequenceOp : TransformDialectOp<"sequence",
    [DeclareOpInterfaceMethods<RegionBranchOpInterface,
        ["getEntrySuccessorOperands", "getSuccessorRegions",
         "getRegionInvocationBounds"]>,
     MatchOpInterface,
     DeclareOpInterfaceMethods<TransformOpInterface>,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     OpAsmOpInterface, PossibleTopLevelTransformOpTrait,
     SingleBlockImplicitTerminator<"::mlir::transform::YieldOp">,
     AttrSizedOperandSegments]> {
  let summary = "Contains a sequence of other transform ops to apply";
  let description = [{
    The transformations indicated by the sequence are applied in order of their
    appearance. Each value produced by a transformation within the sequence
    corresponds to a group of operations or values in the payload IR, or to a
    group of parameters, depending on the type of the value. The behavior of the
    operation when a nested transformation produces a silenceable error is
    controlled by the `failure_propagation_mode` attribute. When set to
    `propagate`, the failure of any nested transformation in the sequence
    implies immediate failure of the entire sequence with a silenceable error,
    and no further transformation is attempted. When set to `suppress`,
    silenceable errors in nested operations are ignored and further
    transformations are applied. Beware that even silenceable errors may leave
    the payload IR in a state unsuitable for further transformations. It is the
    responsibility of the caller to ensure the following transformations are
    robust enough when errors are suppressed. Definite errors reported by nested
    transformations abort the sequence regardless of the propagation mode. The
    set of modes may be extended in the future, e.g., to collect silenceable
    errors and report them after attempting all transformations in the sequence.

    The entry block of this operation has a single argument that maps to either
    the operand if provided or the top-level container operation of the payload
    IR, typically the root operation of the pass interpreting the transform
    dialect. Operand omission is only allowed for sequences not contained in
    another sequence.

    The type of the block argument must match the type of the operand. If the
    sequence is a top-level transform (without an operand), it can be used for
    matching operations if the specified type within the top-level container
    payload IR (including the container op itself). E.g.:

    ```mlir
    transform.sequence failures(propagate) {
    ^bb1(%arg1: !transform.any_op):
      // %arg1 is mapped to the top-level container of the payload IR, which is
      // typically a module
    }

    transform.sequence failures(propagate) {
    ^bb1(%arg1: !transform.op<"func.func>"):
      // %arg1 is mapped to all "func.func" ops within and including the
      // top-level container of the payload IR. Nested operations that have the
      // specified op type are not included.
    }
    ```

    The body of the sequence terminates with an implicit or explicit
    `transform.yield` op. The operands of the terminator are returned as the
    results of the sequence op.
  }];

  let arguments = (ins FailurePropagationMode:$failure_propagation_mode,
                       Optional<TransformHandleTypeInterface>:$root,
                       Variadic<Transform_AnyHandleOrParamType>:$extra_bindings);
  let results = (outs Variadic<TransformHandleTypeInterface>:$results);
  let regions = (region SizedRegion<1>:$body);

  let assemblyFormat =
    "custom<SequenceOpOperands>($root, type($root), $extra_bindings, type($extra_bindings))"
    " (`->` type($results)^)? `failures` `(` "
    "$failure_propagation_mode `)` attr-dict-with-keyword regions";

  let builders = [
    // Build a sequence with a root.
    OpBuilder<(ins
        "::mlir::TypeRange":$resultTypes,
        "::mlir::transform::FailurePropagationMode":$failure_propagation_mode,
        "::mlir::Value":$root, "SequenceBodyBuilderFn":$bodyBuilder)>,

    // Build a sequence with a root and additional arguments.
    OpBuilder<(ins
        "::mlir::TypeRange":$resultTypes,
        "::mlir::transform::FailurePropagationMode":$failure_propagation_mode,
        "::mlir::Value":$root, "::mlir::ValueRange":$extraBindings,
        "SequenceBodyBuilderArgsFn":$bodyBuilder)>,

    // Build a top-level sequence (no root).
    OpBuilder<(ins
        "::mlir::TypeRange":$resultTypes,
        "::mlir::transform::FailurePropagationMode":$failure_propagation_mode,
        "::mlir::Type":$bbArgType, "SequenceBodyBuilderFn":$bodyBuilder)>,

    // Build a top-level sequence (no root) with extra arguments.
    OpBuilder<(ins
        "::mlir::TypeRange":$resultTypes,
        "::mlir::transform::FailurePropagationMode":$failure_propagation_mode,
        "::mlir::Type":$bbArgType, "::mlir::TypeRange":$extraBindingTypes,
        "SequenceBodyBuilderArgsFn":$bodyBuilder)>
  ];

  let extraClassDeclaration = [{
    /// Allow the dialect prefix to be omitted.
    static StringRef getDefaultDialect() { return "transform"; }
  }];

  let hasVerifier = 1;
}

def VerifyOp : TransformDialectOp<"verify",
    [TransformOpInterface, TransformEachOpTrait,
     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
     ReportTrackingListenerFailuresOpTrait]> {
  let summary = "Verifies the targeted ops";
  let description = [{
    This transform verifies the targeted ops. If at least one op fails to
    verify, the transform produces a definite failure.

    Note: This op was designed for debugging purposes and should be used like an
    assertion. It is intentional that this op produces a definite failure and
    not a silenceable one. Correctness of the program should not depend on this
    op.

    This transform reads the target handle.
  }];

  let arguments = (ins TransformHandleTypeInterface:$target);
  let results = (outs);
  let assemblyFormat = "$target attr-dict `:` type($target)";

  let extraClassDeclaration = [{
    ::mlir::DiagnosedSilenceableFailure applyToOne(
      ::mlir::transform::TransformRewriter &rewriter,
      ::mlir::Operation *target,
      ::mlir::transform::ApplyToEachResultList &results,
      ::mlir::transform::TransformState &state);
  }];
}

def YieldOp : TransformDialectOp<"yield",
    [Terminator, DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
  let summary = "Yields operation handles from a transform IR region";
  let description = [{
    This terminator operation yields operation handles from regions of the
    transform IR ops back to the containing op. It is not itself associated with
    any transformation on the payload IR and is used for flow purposes only.
  }];

  let arguments = (ins
    Arg<Variadic<Transform_AnyHandleOrParamType>,
        "Transform values yielded back to the parent"
        >:$operands);
  let assemblyFormat = "operands attr-dict (`:` type($operands)^)?";

  let builders = [
    OpBuilder<(ins), [{
      return build($_builder, $_state, ::mlir::ValueRange());
    }]>
  ];
}

#endif // MLIR_DIALECT_TRANSFORM_IR_TRANSFORMOPS


//===- VectorTransformOps.td - Vector transform ops --------*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef VECTOR_TRANSFORM_OPS
#define VECTOR_TRANSFORM_OPS

include "mlir/Dialect/Transform/IR/TransformDialect.td"
include "mlir/Dialect/Transform/Interfaces/TransformInterfaces.td"
include "mlir/Dialect/Vector/Transforms/VectorTransformsBase.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/IR/OpBase.td"

def ApplyVectorToLLVMConversionPatternsOp : Op<Transform_Dialect,
    "apply_conversion_patterns.vector.vector_to_llvm",
    [DeclareOpInterfaceMethods<ConversionPatternDescriptorOpInterface,
                               ["verifyTypeConverter"]>]> {
  let description = [{
    Collects patterns that convert vector dialect ops to LLVM dialect ops. These
    patterns require an "LLVMTypeConverter".

    The patterns can be customized as follows:
    - `reassociate_fp_reductions`: Allows LLVM to reassociate floating-point
      reductions for speed.
    - `force_32bit_vector_indices`: Allows the compiler to assume that vector
      indices fit in 32-bit if that yields faster code.
  }];

  let arguments = (ins
      DefaultValuedAttr<BoolAttr, "false">:$reassociate_fp_reductions,
      DefaultValuedAttr<BoolAttr, "true">:$force_32bit_vector_indices);
  let assemblyFormat = "attr-dict";
}


def ApplyCastAwayVectorLeadingOneDimPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.cast_away_vector_leading_one_dim",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Collect a set of leading one dimension removal patterns.

    These patterns insert vector.shape_cast to remove leading one dimensions
    to expose more canonical forms of read/write/insert/extract operations.
    With them, there are more chances that we can cancel out extract-insert
    pairs or forward write-read pairs.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyRankReducingSubviewPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.rank_reducing_subview_patterns",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Apply opt-in vector transfer permutation patterns that include:
      - TransferReadDropUnitDimsPattern
      - TransferWriteDropUnitDimsPattern

    These patterns have the effect of rewriting a vector.transfer with unit
    dimensions into a rank-reduced version thanks to subview operations.
    This is complemented by shape_cast folding patterns.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyDropUnitDimWithShapeCastPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.drop_unit_dims_with_shape_cast",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
     Apply vector patterns to fold unit dims with vector.shape_cast Ops:
      - DropUnitDimFromElementwiseOps
      - DropUnitDimsFromScfForOp
      - DropUnitDimsFromTransposeOp

    Excludes patterns for vector.transfer Ops. This is complemented by
    shape_cast folding patterns.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyTransferPermutationPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.transfer_permutation_patterns",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Apply opt-in vector transfer permutation patterns that include:
      - TransferReadPermutationLowering
      - TransferWritePermutationLowering
      - TransferOpReduceRank
      - TransferWriteNonPermutationLowering

    These patterns have the effect of rewriting a vector.transfer with an
    arbitrary permutation_map to a vector.transfer with a permutation_map that
    is a minor identity followed by a vector.transpose.

    In other words, this makes the vector.transfer contiguous on the most minor
    dimensions and materializes the permutation_map as a vector.transpose.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyLowerBitCastPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.lower_bitcast",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that vector bitcast operations should be lowered to
    finer-grained vector primitives.

    This is usally a late step that is run after bufferization as part of the
    process of lowering to e.g. LLVM or NVVM.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyLowerBroadcastPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.lower_broadcast",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that vector broadcast operations should be lowered to
    finer-grained vector primitives.

    This is usally a late step that is run after bufferization as part of the
    process of lowering to e.g. LLVM or NVVM.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyLowerContractionPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.lower_contraction",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that vector contraction-like operations should be lowered to
    finer-grained vector primitives.

    This is usually a late step that is run after bufferization as part of the
    process of lowering to e.g. LLVM or NVVM.
  }];

  let arguments = (ins DefaultValuedAttr<VectorContractLoweringAttr,
      "vector::VectorContractLowering::OuterProduct">:$lowering_strategy
  );
  let assemblyFormat = [{
    (`lowering_strategy` `=` $lowering_strategy^)? attr-dict
  }];
}

def ApplyLowerCreateMaskPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.lower_create_mask",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that vector create_mask-like operations should be lowered to
    finer-grained vector primitives.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyLowerMasksPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.lower_masks",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that vector.create_mask and vector.constant_mask operations
    should be lowered to finer-grained vector primitives.

    This is usually a late step that is run after bufferization as part of the
    process of lowering to e.g. LLVM or NVVM.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyLowerMaskedTransfersPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.lower_masked_transfers",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Apply opt-in patterns that lower vector.mask operations surrounding
    side-effecting ops:
      - MaskedTransferReadOpPattern
      - MaskedTransferWriteOpPattern
      - MaskedGatherOpPattern

    This is usually a late step that is run after bufferization as part of the
    process of lowering to e.g. LLVM or NVVM.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyMaterializeMasksPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.materialize_masks",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that mask operations should be lowered to fine-grained arithemtic
    operations.

    This is usually the last step that is run after bufferization as part of the
    process of lowering to e.g. LLVM or NVVM.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyLowerMultiReductionPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.lower_multi_reduction",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that vector multi_reduction-like operations should be lowered to
    finer-grained vector primitives.

    This is usually a late step that is run after bufferization as part of the
    process of lowering to e.g. LLVM or NVVM.
  }];

  let arguments = (ins DefaultValuedAttr<VectorMultiReductionLoweringAttr,
      "vector::VectorMultiReductionLowering::InnerParallel">:$lowering_strategy
  );

  let assemblyFormat = [{
    (`lowering_strategy` `=` $lowering_strategy^)? attr-dict
  }];
}

def ApplyLowerOuterProductPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.lower_outerproduct",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that the vector outerproduct operations should be lowered to
    finer-grained vector primitives.

    This is usually a late step that is run after bufferization as part of the
    process of lowering to e.g. LLVM or NVVM.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyLowerGatherPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.lower_gather",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that vector.gather operations should be lowered to
    finer-grained vector primitives.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyLowerScanPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.lower_scan",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that vector.scan operations should be lowered to
    finer-grained vector primitives.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyLowerShapeCastPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.lower_shape_cast",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that vector shape_cast operations should be lowered to
    finer-grained vector primitives.

    This is usually a late step that is run after bufferization as part of the
    process of lowering to e.g. LLVM or NVVM.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyLowerTransferPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.lower_transfer",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that vector transfer operations should be lowered to finer-grained
    vector primitives.

    This is usually a late step that is run after bufferization as part of the
    process of lowering to e.g. LLVM or NVVM.
  }];

  let arguments = (ins DefaultValuedAttr<I64Attr, "1">:$max_transfer_rank);

  let assemblyFormat = [{
    (`max_transfer_rank` `=` $max_transfer_rank^)? attr-dict
  }];
}

def ApplyLowerTransposePatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.lower_transpose",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that vector transpose-like operations should be lowered to
    finer-grained vector primitives.

    This is usually a late step that is run after bufferization as part of the
    process of lowering to e.g. LLVM or NVVM.
  }];

  let arguments = (ins
     DefaultValuedAttr<VectorTransposeLoweringAttr,
       "vector::VectorTransposeLowering::EltWise">:$lowering_strategy,
     DefaultValuedAttr<BoolAttr, "false">:$avx2_lowering_strategy
  );

  let assemblyFormat = [{
    oilist (
      `lowering_strategy` `=` $lowering_strategy
      | `avx2_lowering_strategy` `=` $avx2_lowering_strategy
    )
    attr-dict
  }];
}

def ApplyLowerInterleavePatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.lower_interleave",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that vector interleave operations should be lowered to
    finer-grained vector primitives.

    This is usally a late step that is run after bufferization as part of the
    process of lowering to e.g. LLVM or NVVM.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyInterleaveToShufflePatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.interleave_to_shuffle",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that 1D vector interleave operations should be rewritten as
    vector shuffle operations.

    This is motivated by some current codegen backends not handling vector
    interleave operations.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyRewriteNarrowTypePatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.rewrite_narrow_types",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that vector narrow rewrite operations should be applied.

    This is usually a late step that is run after bufferization as part of the
    process of lowering to e.g. LLVM or NVVM.

    Warning: these patterns currently only work for little endian targets.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplySplitTransferFullPartialPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.split_transfer_full_partial",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that vector transfer operations should be split to full and
    partial parts.

    This is usually a late step that is run after bufferization as part of the
    process of lowering to e.g. LLVM or NVVM.
  }];

  let arguments = (ins
     DefaultValuedAttr<VectorTransferSplitAttr,
       "vector::VectorTransferSplit::LinalgCopy">:$split_transfer_strategy
  );

  let assemblyFormat = [{
    (`split_transfer_strategy` `=` $split_transfer_strategy^)? attr-dict
  }];
}

def ApplyTransferToScfPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.transfer_to_scf",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Indicates that vector transfer operations should be rewritten with scf.for
    loops over finer-grained vector primitives.

    This is usually a late step that is run after bufferization as part of the
    process of lowering to e.g. LLVM or NVVM.
  }];

  let arguments = (ins
     DefaultValuedAttr<I64Attr, "1">:$max_transfer_rank,
     DefaultValuedAttr<BoolAttr, "false">:$full_unroll
  );

  let assemblyFormat = [{
    oilist (
        `max_transfer_rank` `=` $max_transfer_rank
      | `full_unroll` `=` $full_unroll
    )
    attr-dict
  }];
}

def ApplyFoldArithExtensionPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.fold_arith_extension",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Collect a set of patterns that fold arithmetic extension on floating point
    into vector contract for the backends with native support.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyFoldElementwiseToVectorPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.elementwise_to_vector",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Collect a set of patterns that fold elementwise op on vectors to the vector
    dialect.
  }];

  let assemblyFormat = "attr-dict";
}

def ApplyVectorReductionToContractPatternsOp : Op<Transform_Dialect,
    "apply_patterns.vector.reduction_to_contract",
    [DeclareOpInterfaceMethods<PatternDescriptorOpInterface>]> {
  let description = [{
    Apply opt-in patterns that convert reductions to contract:
      - MultiReduceToContract
      - CombineContractBroadcast
      - CombineContractABTranspose
      - CombineContractResultTranspose
      - ReorderElementwiseOpsOnTranspose
      - ReorderElementwiseOpsOnBroadcast
      - ReorderCastOpsOnBroadcast

    These patterns have the effect of rewriting a vector.multi_reduce into a
    vector.contract.
  }];

  let assemblyFormat = "attr-dict";
}

#endif // VECTOR_TRANSFORM_OPS


#ifndef LLVM_INTRINSIC_OPS
#define LLVM_INTRINSIC_OPS

include "mlir/IR/OpBase.td"
include "mlir/Dialect/LLVMIR/LLVMAttrDefs.td"
include "mlir/Dialect/LLVMIR/LLVMEnums.td"
include "mlir/Dialect/LLVMIR/LLVMOpBase.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/MemorySlotInterfaces.td"

// Operations that correspond to LLVM intrinsics. With MLIR operation set being
// extendable, there is no reason to introduce a hard boundary between "core"
// operations and intrinsics. However, we systematically prefix them with
// "intr." to avoid potential name clashes.

class LLVM_UnaryIntrOpBase<string func, Type element,
                           list<Trait> traits = [], bit requiresFastmath = 0> :
    LLVM_OneResultIntrOp<func, [], [0],
           !listconcat([Pure, SameOperandsAndResultType], traits),
           requiresFastmath> {
  dag commonArgs = (ins LLVM_ScalarOrVectorOf<element>:$in);
  let assemblyFormat = "`(` operands `)` attr-dict `:` "
      "functional-type(operands, results)";
}

class LLVM_UnaryIntrOpI<string func, list<Trait> traits = []> :
    LLVM_UnaryIntrOpBase<func, AnySignlessInteger, traits> {
  let arguments = commonArgs;
}

class LLVM_UnaryIntrOpF<string func, list<Trait> traits = []> :
    LLVM_UnaryIntrOpBase<func, LLVM_AnyFloat, traits, /*requiresFastmath=*/1> {
  dag fmfArg = (
    ins DefaultValuedAttr<LLVM_FastmathFlagsAttr, "{}">:$fastmathFlags);
  let arguments = !con(commonArgs, fmfArg);
}

class LLVM_BinarySameArgsIntrOpBase<string func, Type element,
              list<Trait> traits = [], bit requiresFastmath = 0> :
    LLVM_OneResultIntrOp<func, [], [0],
           !listconcat([Pure, SameOperandsAndResultType], traits),
           requiresFastmath> {
  dag commonArgs = (ins LLVM_ScalarOrVectorOf<element>:$a,
                        LLVM_ScalarOrVectorOf<element>:$b);
  let assemblyFormat = "`(` operands `)` attr-dict `:` "
      "functional-type(operands, results)";
}

class LLVM_BinarySameArgsIntrOpI<string func, list<Trait> traits = []> :
    LLVM_BinarySameArgsIntrOpBase<func, AnySignlessInteger, traits> {
  let arguments = commonArgs;
}

class LLVM_BinarySameArgsIntrOpF<string func, list<Trait> traits = []> :
    LLVM_BinarySameArgsIntrOpBase<func, LLVM_AnyFloat, traits,
                                  /*requiresFastmath=*/1> {
  dag fmfArg = (
    ins DefaultValuedAttr<LLVM_FastmathFlagsAttr, "{}">:$fastmathFlags);
  let arguments = !con(commonArgs, fmfArg);
}

class LLVM_TernarySameArgsIntrOpBase<string func, Type element,
              list<Trait> traits = [], bit requiresFastmath = 0> :
    LLVM_OneResultIntrOp<func, [], [0],
           !listconcat([Pure, SameOperandsAndResultType], traits),
           requiresFastmath> {
  dag commonArgs = (ins LLVM_ScalarOrVectorOf<element>:$a,
                       LLVM_ScalarOrVectorOf<element>:$b,
                       LLVM_ScalarOrVectorOf<element>:$c);
  let assemblyFormat = "`(` operands `)` attr-dict `:` "
      "functional-type(operands, results)";
}

class LLVM_TernarySameArgsIntrOpI<string func, list<Trait> traits = []> :
    LLVM_TernarySameArgsIntrOpBase<func, AnySignlessInteger, traits> {
  let arguments = commonArgs;
}

class LLVM_TernarySameArgsIntrOpF<string func, list<Trait> traits = []> :
    LLVM_TernarySameArgsIntrOpBase<func, LLVM_AnyFloat, traits,
                                  /*requiresFastmath=*/1> {
  dag fmfArg = (
    ins DefaultValuedAttr<LLVM_FastmathFlagsAttr, "{}">:$fastmathFlags);
  let arguments = !con(commonArgs, fmfArg);
}

class LLVM_CountZerosIntrOp<string func, list<Trait> traits = []> :
    LLVM_OneResultIntrOp<func, [], [0],
           !listconcat([Pure], traits),
            /*requiresFastmath=*/0,
            /*immArgPositions=*/[1], /*immArgAttrNames=*/["is_zero_poison"]> {
  let arguments = (ins LLVM_ScalarOrVectorOf<AnySignlessInteger>:$in,
                   I1Attr:$is_zero_poison);
}

def LLVM_AbsOp : LLVM_OneResultIntrOp<"abs", [], [0], [Pure],
    /*requiresFastmath=*/0,
    /*immArgPositions=*/[1], /*immArgAttrNames=*/["is_int_min_poison"]> {
  let arguments = (ins LLVM_ScalarOrVectorOf<AnySignlessInteger>:$in,
                   I1Attr:$is_int_min_poison);
}

def LLVM_IsFPClass : LLVM_OneResultIntrOp<"is.fpclass", [], [0], [Pure],
  /*requiresFastmath=*/0,
  /*immArgPositions=*/[1], /*immArgAttrNames=*/["bit"]> {
  let arguments = (ins LLVM_ScalarOrVectorOf<LLVM_AnyFloat>:$in, I32Attr:$bit);
}

def LLVM_CopySignOp : LLVM_BinarySameArgsIntrOpF<"copysign">;
def LLVM_CosOp : LLVM_UnaryIntrOpF<"cos">;
def LLVM_ExpOp : LLVM_UnaryIntrOpF<"exp">;
def LLVM_Exp2Op : LLVM_UnaryIntrOpF<"exp2">;
def LLVM_FAbsOp : LLVM_UnaryIntrOpF<"fabs">;
def LLVM_FCeilOp : LLVM_UnaryIntrOpF<"ceil">;
def LLVM_FFloorOp : LLVM_UnaryIntrOpF<"floor">;
def LLVM_FMAOp : LLVM_TernarySameArgsIntrOpF<"fma">;
def LLVM_FMulAddOp : LLVM_TernarySameArgsIntrOpF<"fmuladd">;
def LLVM_Log10Op : LLVM_UnaryIntrOpF<"log10">;
def LLVM_Log2Op : LLVM_UnaryIntrOpF<"log2">;
def LLVM_LogOp : LLVM_UnaryIntrOpF<"log">;
def LLVM_Prefetch : LLVM_ZeroResultIntrOp<"prefetch", [0],
  /*traits=*/[], /*requiresAccessGroup=*/0, /*requiresAliasAnalysis=*/0,
  /*requiresOpBundles=*/0, /*immArgPositions=*/[1, 2, 3],
  /*immArgAttrNames=*/["rw", "hint", "cache"]
> {
  let arguments = (ins LLVM_AnyPointer:$addr, I32Attr:$rw, I32Attr:$hint, I32Attr:$cache);
}
def LLVM_SinOp : LLVM_UnaryIntrOpF<"sin">;
def LLVM_RoundEvenOp : LLVM_UnaryIntrOpF<"roundeven">;
def LLVM_RoundOp : LLVM_UnaryIntrOpF<"round">;
def LLVM_FTruncOp : LLVM_UnaryIntrOpF<"trunc">;
def LLVM_SqrtOp : LLVM_UnaryIntrOpF<"sqrt">;
def LLVM_PowOp : LLVM_BinarySameArgsIntrOpF<"pow">;
def LLVM_PowIOp : LLVM_OneResultIntrOp<"powi", [], [0,1],
                                       [Pure], /*requiresFastmath=*/1> {
  let arguments =
      (ins LLVM_ScalarOrVectorOf<LLVM_AnyFloat>:$val,
           AnySignlessInteger:$power,
           DefaultValuedAttr<LLVM_FastmathFlagsAttr, "{}">:$fastmathFlags);
  let assemblyFormat = "`(` operands `)` attr-dict `:` "
      "functional-type(operands, results)";
}
def LLVM_RintOp : LLVM_UnaryIntrOpF<"rint">;
def LLVM_NearbyintOp : LLVM_UnaryIntrOpF<"nearbyint">;
class LLVM_IntRoundIntrOpBase<string func> :
        LLVM_OneResultIntrOp<func, [0], [0], [Pure]> {
  let arguments = (ins LLVM_AnyFloat:$val);
  let assemblyFormat = "`(` operands `)` attr-dict `:` "
      "functional-type(operands, results)";
}
def LLVM_LroundOp : LLVM_IntRoundIntrOpBase<"lround">;
def LLVM_LlroundOp : LLVM_IntRoundIntrOpBase<"llround">;
def LLVM_LrintOp : LLVM_IntRoundIntrOpBase<"lrint">;
def LLVM_LlrintOp : LLVM_IntRoundIntrOpBase<"llrint">;
def LLVM_BitReverseOp : LLVM_UnaryIntrOpI<"bitreverse">;
def LLVM_ByteSwapOp : LLVM_UnaryIntrOpI<"bswap">;
def LLVM_CountLeadingZerosOp : LLVM_CountZerosIntrOp<"ctlz">;
def LLVM_CountTrailingZerosOp : LLVM_CountZerosIntrOp<"cttz">;
def LLVM_CtPopOp : LLVM_UnaryIntrOpI<"ctpop">;
def LLVM_FshlOp : LLVM_TernarySameArgsIntrOpI<"fshl">;
def LLVM_FshrOp : LLVM_TernarySameArgsIntrOpI<"fshr">;
def LLVM_MaxNumOp : LLVM_BinarySameArgsIntrOpF<"maxnum">;
def LLVM_MinNumOp : LLVM_BinarySameArgsIntrOpF<"minnum">;
def LLVM_MaximumOp : LLVM_BinarySameArgsIntrOpF<"maximum">;
def LLVM_MinimumOp : LLVM_BinarySameArgsIntrOpF<"minimum">;
def LLVM_SMaxOp : LLVM_BinarySameArgsIntrOpI<"smax">;
def LLVM_SMinOp : LLVM_BinarySameArgsIntrOpI<"smin">;
def LLVM_UMaxOp : LLVM_BinarySameArgsIntrOpI<"umax">;
def LLVM_UMinOp : LLVM_BinarySameArgsIntrOpI<"umin">;
def LLVM_SinhOp : LLVM_UnaryIntrOpF<"sinh">;
def LLVM_CoshOp : LLVM_UnaryIntrOpF<"cosh">;
def LLVM_TanhOp : LLVM_UnaryIntrOpF<"tanh">;

class LLVM_MemcpyIntrOpBase<string name> :
    LLVM_ZeroResultIntrOp<name, [0, 1, 2],
    [DeclareOpInterfaceMethods<PromotableMemOpInterface>,
     DeclareOpInterfaceMethods<DestructurableAccessorOpInterface>,
     DeclareOpInterfaceMethods<SafeMemorySlotAccessOpInterface>],
    /*requiresAccessGroup=*/1, /*requiresAliasAnalysis=*/1,
    /*requiresOpBundles=*/0, /*immArgPositions=*/[3],
    /*immArgAttrNames=*/["isVolatile"]> {
  dag args = (ins Arg<LLVM_AnyPointer,"",[MemWrite]>:$dst,
                  Arg<LLVM_AnyPointer,"",[MemRead]>:$src,
                  AnySignlessInteger:$len, I1Attr:$isVolatile);
  // Append the alias attributes defined by LLVM_IntrOpBase.
  let arguments = !con(args, aliasAttrs);
  let builders = [
    OpBuilder<(ins "Value":$dst, "Value":$src, "Value":$len,
                   "bool":$isVolatile), [{
      build($_builder, $_state, dst, src, len,
            $_builder.getBoolAttr(isVolatile));
    }]>,
    OpBuilder<(ins "Value":$dst, "Value":$src, "Value":$len,
                   "IntegerAttr":$isVolatile), [{
      build($_builder, $_state, dst, src, len, isVolatile,
            /*access_groups=*/nullptr, /*alias_scopes=*/nullptr,
            /*noalias_scopes=*/nullptr, /*tbaa=*/nullptr);
    }]>
  ];
}

def LLVM_MemcpyOp : LLVM_MemcpyIntrOpBase<"memcpy">;
def LLVM_MemmoveOp : LLVM_MemcpyIntrOpBase<"memmove">;

def LLVM_MemcpyInlineOp :
    LLVM_ZeroResultIntrOp<"memcpy.inline", [0, 1, 2],
    [DeclareOpInterfaceMethods<PromotableMemOpInterface>,
     DeclareOpInterfaceMethods<DestructurableAccessorOpInterface>,
     DeclareOpInterfaceMethods<SafeMemorySlotAccessOpInterface>],
    /*requiresAccessGroup=*/1, /*requiresAliasAnalysis=*/1,
    /*requiresOpBundles=*/0, /*immArgPositions=*/[2, 3],
    /*immArgAttrNames=*/["len", "isVolatile"]> {
  dag args = (ins Arg<LLVM_AnyPointer,"",[MemWrite]>:$dst,
                  Arg<LLVM_AnyPointer,"",[MemRead]>:$src,
                  APIntAttr:$len, I1Attr:$isVolatile);
  // Append the alias attributes defined by LLVM_IntrOpBase.
  let arguments = !con(args, aliasAttrs);
  let builders = [
    OpBuilder<(ins "Value":$dst, "Value":$src, "IntegerAttr":$len,
                   "bool":$isVolatile), [{
      build($_builder, $_state, dst, src, len,
            $_builder.getBoolAttr(isVolatile));
    }]>,
    OpBuilder<(ins "Value":$dst, "Value":$src, "IntegerAttr":$len,
                   "IntegerAttr":$isVolatile), [{
      build($_builder, $_state, dst, src, len, isVolatile,
            /*access_groups=*/nullptr, /*alias_scopes=*/nullptr,
            /*noalias_scopes=*/nullptr, /*tbaa=*/nullptr);
    }]>
  ];
}

def LLVM_MemsetOp : LLVM_ZeroResultIntrOp<"memset", [0, 2],
    [DeclareOpInterfaceMethods<PromotableMemOpInterface>,
     DeclareOpInterfaceMethods<DestructurableAccessorOpInterface>,
     DeclareOpInterfaceMethods<SafeMemorySlotAccessOpInterface>],
    /*requiresAccessGroup=*/1, /*requiresAliasAnalysis=*/1,
    /*requiresOpBundles=*/0, /*immArgPositions=*/[3],
    /*immArgAttrNames=*/["isVolatile"]> {
  dag args = (ins Arg<LLVM_AnyPointer,"",[MemWrite]>:$dst,
                  I8:$val, AnySignlessInteger:$len, I1Attr:$isVolatile);
  // Append the alias attributes defined by LLVM_IntrOpBase.
  let arguments = !con(args, aliasAttrs);
  let builders = [
    OpBuilder<(ins "Value":$dst, "Value":$val, "Value":$len,
                    "bool":$isVolatile), [{
      build($_builder, $_state, dst, val, len,
            $_builder.getBoolAttr(isVolatile));
    }]>,
    OpBuilder<(ins "Value":$dst, "Value":$val, "Value":$len,
                    "IntegerAttr":$isVolatile), [{
      build($_builder, $_state, dst, val, len, isVolatile,
            /*access_groups=*/nullptr, /*alias_scopes=*/nullptr,
            /*noalias_scopes=*/nullptr, /*tbaa=*/nullptr);
    }]>
  ];
}

def LLVM_MemsetInlineOp : LLVM_ZeroResultIntrOp<"memset.inline", [0, 2],
    [DeclareOpInterfaceMethods<PromotableMemOpInterface>,
     DeclareOpInterfaceMethods<DestructurableAccessorOpInterface>,
     DeclareOpInterfaceMethods<SafeMemorySlotAccessOpInterface>],
    /*requiresAccessGroup=*/1, /*requiresAliasAnalysis=*/1,
    /*requiresOpBundles=*/0, /*immArgPositions=*/[2, 3],
    /*immArgAttrNames=*/["len", "isVolatile"]> {
  dag args = (ins Arg<LLVM_AnyPointer,"",[MemWrite]>:$dst,
                  I8:$val, APIntAttr:$len, I1Attr:$isVolatile);
  // Append the alias attributes defined by LLVM_IntrOpBase.
  let arguments = !con(args, aliasAttrs);
  let builders = [
    OpBuilder<(ins "Value":$dst, "Value":$val, "IntegerAttr":$len,
                    "bool":$isVolatile), [{
      build($_builder, $_state, dst, val, len,
            $_builder.getBoolAttr(isVolatile));
    }]>,
    OpBuilder<(ins "Value":$dst, "Value":$val, "IntegerAttr":$len,
                    "IntegerAttr":$isVolatile), [{
      build($_builder, $_state, dst, val, len, isVolatile,
            /*access_groups=*/nullptr, /*alias_scopes=*/nullptr,
            /*noalias_scopes=*/nullptr, /*tbaa=*/nullptr);
    }]>
  ];
}

def LLVM_NoAliasScopeDeclOp
    : LLVM_ZeroResultIntrOp<"experimental.noalias.scope.decl"> {
  let arguments = (ins LLVM_AliasScopeAttr:$scope);
  string llvmBuilder = [{
    // Wrap the scope argument into a list since the LLVM IR intrinsic takes
    // a list containing exactly one scope rather than a scope itself.
    llvm::MDNode* node = moduleTranslation.getOrCreateAliasScopes({$scope});
    builder.CreateNoAliasScopeDeclaration(node);
  }];
  string mlirBuilder = [{
    FailureOr<SmallVector<LLVM::AliasScopeAttr>> scopeAttrs =
      moduleImport.matchAliasScopeAttrs(llvmOperands[0]);
    // Drop the intrinsic if the alias scope translation fails since the scope
    // is not used by an aliasing operation, such as a load or store, that is
    // used to convert the alias scope metadata.
    if (failed(scopeAttrs))
      return success();
    if (scopeAttrs->size() != 1)
      return failure();
    $_op = $_builder.create<LLVM::NoAliasScopeDeclOp>(
      $_location, (*scopeAttrs)[0]);
  }];
  let assemblyFormat = "$scope attr-dict";
}

//
// Memory marker intrinsics.
//

/// Base operation for lifetime markers. The LLVM intrinsics require the size
/// operand to be an immediate. In MLIR it is encoded as an attribute.
class LLVM_LifetimeBaseOp<string opName> : LLVM_ZeroResultIntrOp<opName, [1],
    [DeclareOpInterfaceMethods<PromotableOpInterface>],
    /*requiresAccessGroup=*/0, /*requiresAliasAnalysis=*/0,
    /*requiresOpBundles=*/0, /*immArgPositions=*/[0],
    /*immArgAttrNames=*/["size"]> {
  let arguments = (ins I64Attr:$size, LLVM_AnyPointer:$ptr);
  let assemblyFormat = "$size `,` $ptr attr-dict `:` qualified(type($ptr))";
}

def LLVM_LifetimeStartOp : LLVM_LifetimeBaseOp<"lifetime.start">;
def LLVM_LifetimeEndOp : LLVM_LifetimeBaseOp<"lifetime.end">;

def LLVM_InvariantStartOp : LLVM_OneResultIntrOp<"invariant.start", [], [1],
    [DeclareOpInterfaceMethods<PromotableOpInterface>],
    /*requiresFastmath=*/0, /*immArgPositions=*/[0],
    /*immArgAttrNames=*/["size"]> {
  let arguments = (ins I64Attr:$size, LLVM_AnyPointer:$ptr);
  let results = (outs LLVM_DefaultPointer:$res);
  let assemblyFormat = "$size `,` $ptr attr-dict `:` qualified(type($ptr))";
}

def LLVM_InvariantEndOp : LLVM_ZeroResultIntrOp<"invariant.end", [2],
    [DeclareOpInterfaceMethods<PromotableOpInterface>],
    /*requiresAccessGroup=*/0, /*requiresAliasAnalysis=*/0,
    /*requiresOpBundles=*/0, /*immArgPositions=*/[1],
    /*immArgAttrNames=*/["size"]> {
  let arguments = (ins LLVM_DefaultPointer:$start,
                       I64Attr:$size,
                       LLVM_AnyPointer:$ptr);
  let assemblyFormat = "$start `,` $size `,` $ptr attr-dict `:` "
      "qualified(type($ptr))";
}

def LLVM_LaunderInvariantGroupOp
    : LLVM_OneResultIntrOp<"launder.invariant.group", [], [0],
        [DeclareOpInterfaceMethods<PromotableOpInterface>,
         SameOperandsAndResultType]> {
  let arguments = (ins LLVM_AnyPointer:$ptr);
  let results = (outs LLVM_AnyPointer:$res);
  let assemblyFormat = "$ptr attr-dict `:` qualified(type($ptr))";
}

def LLVM_StripInvariantGroupOp
    : LLVM_OneResultIntrOp<"strip.invariant.group", [], [0],
        [DeclareOpInterfaceMethods<PromotableOpInterface>,
         SameOperandsAndResultType]> {
  let arguments = (ins LLVM_AnyPointer:$ptr);
  let results = (outs LLVM_AnyPointer:$res);
  let assemblyFormat = "$ptr attr-dict `:` qualified(type($ptr))";
}

// Constrained Floating-Point Intrinsics.

class LLVM_ConstrainedIntr<string mnem, int numArgs,
                           bit overloadedResult, list<int> overloadedOperands,
                           bit hasRoundingMode>
    : LLVM_OneResultIntrOp<"experimental.constrained." # mnem,
                           /*overloadedResults=*/
                           !cond(!gt(overloadedResult, 0) : [0],
                                 true : []),
                           overloadedOperands,
                           /*traits=*/[Pure, DeclareOpInterfaceMethods<FPExceptionBehaviorOpInterface>]
                           # !cond(
                               !gt(hasRoundingMode, 0) : [DeclareOpInterfaceMethods<RoundingModeOpInterface>],
                               true : []),
                           /*requiresFastmath=*/0,
                           /*immArgPositions=*/[],
                           /*immArgAttrNames=*/[]> {
  dag regularArgs = !dag(ins, !listsplat(LLVM_Type, numArgs), !foreach(i, !range(numArgs), "arg_" #i));
  dag attrArgs = !con(!cond(!gt(hasRoundingMode, 0) : (ins ValidRoundingModeAttr:$roundingmode),
                            true : (ins)),
                      (ins FPExceptionBehaviorAttr:$fpExceptionBehavior));
  let arguments = !con(regularArgs, attrArgs);
  let llvmBuilder = [{
    SmallVector<llvm::Value *> args =
      moduleTranslation.lookupValues(opInst.getOperands());
    SmallVector<llvm::Type *> overloadedTypes; }] #
    !cond(!gt(overloadedResult, 0) : [{
    // Take into account overloaded result type.
    overloadedTypes.push_back($_resultType); }],
    // No overloaded result type.
          true : "") # [{
    llvm::transform(ArrayRef<unsigned>}] # overloadedOperandsCpp # [{,
                    std::back_inserter(overloadedTypes),
                    [&args](unsigned index) { return args[index]->getType(); });
    llvm::Module *module = builder.GetInsertBlock()->getModule();
    llvm::Function *callee =
      llvm::Intrinsic::getOrInsertDeclaration(module,
        llvm::Intrinsic::experimental_constrained_}] #
    mnem # [{, overloadedTypes); }] #
    !cond(!gt(hasRoundingMode, 0) : [{
    // Get rounding mode using interface.
    llvm::RoundingMode rounding =
        moduleTranslation.translateRoundingMode($roundingmode); }],
          true : [{
    // No rounding mode.
    std::optional<llvm::RoundingMode> rounding; }]) # [{
    llvm::fp::ExceptionBehavior except =
      moduleTranslation.translateFPExceptionBehavior($fpExceptionBehavior);
    $res = builder.CreateConstrainedFPCall(callee, args, "", rounding, except);
  }];
  let mlirBuilder = [{
    SmallVector<Value> mlirOperands;
    SmallVector<NamedAttribute> mlirAttrs;
    if (failed(moduleImport.convertIntrinsicArguments(
        llvmOperands.take_front( }] # numArgs # [{), {}, false,
        {}, {}, mlirOperands, mlirAttrs))) {
      return failure();
    }

    FPExceptionBehaviorAttr fpExceptionBehaviorAttr =
        $_fpExceptionBehavior_attr($fpExceptionBehavior);
    mlirAttrs.push_back(
        $_builder.getNamedAttr(
            $_qualCppClassName::getFPExceptionBehaviorAttrName(),
            fpExceptionBehaviorAttr)); }] #
    !cond(!gt(hasRoundingMode, 0) : [{
    RoundingModeAttr roundingModeAttr = $_roundingMode_attr($roundingmode);
    mlirAttrs.push_back(
        $_builder.getNamedAttr($_qualCppClassName::getRoundingModeAttrName(),
                               roundingModeAttr));
    }], true : "") # [{
    $res = $_builder.create<$_qualCppClassName>($_location,
      $_resultType, mlirOperands, mlirAttrs);
  }];
}

def LLVM_ConstrainedFPTruncIntr
    : LLVM_ConstrainedIntr<"fptrunc", /*numArgs=*/1,
        /*overloadedResult=*/1, /*overloadedOperands=*/[0],
        /*hasRoundingMode=*/1> {
  let assemblyFormat = [{
    $arg_0 $roundingmode $fpExceptionBehavior attr-dict `:` type($arg_0) `to` type(results)
  }];
}

// Intrinsics with multiple returns.

class LLVM_ArithWithOverflowOp<string mnem>
    : LLVM_IntrOp<mnem, [0], [], [Pure, SameOperandsElementType], 2>,
      Arguments<(ins LLVM_ScalarOrVectorOf<AnySignlessInteger>,
                 LLVM_ScalarOrVectorOf<AnySignlessInteger>)>;

def LLVM_SAddWithOverflowOp : LLVM_ArithWithOverflowOp<"sadd.with.overflow">;
def LLVM_UAddWithOverflowOp : LLVM_ArithWithOverflowOp<"uadd.with.overflow">;
def LLVM_SSubWithOverflowOp : LLVM_ArithWithOverflowOp<"ssub.with.overflow">;
def LLVM_USubWithOverflowOp : LLVM_ArithWithOverflowOp<"usub.with.overflow">;
def LLVM_SMulWithOverflowOp : LLVM_ArithWithOverflowOp<"smul.with.overflow">;
def LLVM_UMulWithOverflowOp : LLVM_ArithWithOverflowOp<"umul.with.overflow">;

//
// Saturation Arithmetic Intrinsics.
//

def LLVM_SAddSat : LLVM_BinarySameArgsIntrOpI<"sadd.sat">;
def LLVM_UAddSat : LLVM_BinarySameArgsIntrOpI<"uadd.sat">;
def LLVM_SSubSat : LLVM_BinarySameArgsIntrOpI<"ssub.sat">;
def LLVM_USubSat : LLVM_BinarySameArgsIntrOpI<"usub.sat">;
def LLVM_SSHLSat : LLVM_BinarySameArgsIntrOpI<"sshl.sat">;
def LLVM_USHLSat : LLVM_BinarySameArgsIntrOpI<"ushl.sat">;

//
// Optimization hint intrinsics.
//

def LLVM_AssumeOp
    : LLVM_ZeroResultIntrOp<"assume", /*overloadedOperands=*/[], /*traits=*/[],
                            /*requiresAccessGroup=*/0,
                            /*requiresAliasAnalysis=*/0,
                            /*requiresOpBundles=*/1> {
  dag args = (ins I1:$cond);
  let arguments = !con(args, opBundleArgs);

  let assemblyFormat = [{
    $cond
    ( custom<OpBundles>($op_bundle_operands, type($op_bundle_operands),
                        $op_bundle_tags)^ )?
    `:` type($cond) attr-dict
  }];

  let builders = [
    OpBuilder<(ins "Value":$cond)>,
    OpBuilder<(ins "Value":$cond,
                   "ArrayRef<llvm::OperandBundleDefT<Value>>":$opBundles)>,
    OpBuilder<(ins "Value":$cond, "llvm::StringRef":$tag, "ValueRange":$args)>,
    OpBuilder<(ins "Value":$cond, "AssumeAlignTag":$tag, "Value":$ptr,
                   "Value":$align)>,
    OpBuilder<(ins "Value":$cond, "AssumeSeparateStorageTag":$tag,
                   "Value":$ptr1, "Value":$ptr2)>
  ];

  let hasVerifier = 1;
}

def LLVM_SSACopyOp : LLVM_OneResultIntrOp<"ssa.copy", [], [0],
                                            [Pure, SameOperandsAndResultType]> {
  let arguments = (ins AnyType:$operand);

  let assemblyFormat = "$operand attr-dict `:` type($operand)";
}

def LLVM_IsConstantOp : LLVM_IntrOp<"is.constant", [], [0], [Pure], 1> {
  let arguments = (ins LLVM_Type:$val);
  let results = (outs I1:$res);
}

def LLVM_ExpectOp
  : LLVM_OneResultIntrOp<"expect", [], [0],
                         [Pure, SameOperandsAndResultType]> {
  let arguments = (ins AnySignlessInteger:$val,
                       AnySignlessInteger:$expected);
  let assemblyFormat = "$val `,` $expected attr-dict `:` type($val)";
}

def LLVM_ExpectWithProbabilityOp
  : LLVM_OneResultIntrOp<"expect.with.probability", [], [0],
                         [Pure, AllTypesMatch<["val", "expected", "res"]>],
                         /*requiresFastmath=*/0,
                         /*immArgPositions=*/[2], /*immArgAttrNames=*/["prob"]> {
  let arguments = (ins AnySignlessInteger:$val,
                       AnySignlessInteger:$expected,
                       F64Attr:$prob);
  let assemblyFormat = "$val `,` $expected `,` $prob attr-dict `:` type($val)";
}

def LLVM_ThreadlocalAddressOp : LLVM_OneResultIntrOp<"threadlocal.address", [],
                                [0], [Pure]> {
  let arguments = (ins LLVM_AnyPointer:$global);
}

//
// Coroutine intrinsics.
//

def LLVM_CoroIdOp : LLVM_IntrOp<"coro.id", [], [], [], 1> {
  let arguments = (ins I32:$align,
                       LLVM_AnyPointer:$promise,
                       LLVM_AnyPointer:$coroaddr,
                       LLVM_AnyPointer:$fnaddrs);
  let assemblyFormat = "$align `,` $promise `,` $coroaddr `,` $fnaddrs"
    " attr-dict `:` functional-type(operands, results)";
}

def LLVM_CoroBeginOp : LLVM_IntrOp<"coro.begin", [], [], [], 1> {
  let arguments = (ins LLVM_TokenType:$token,
                       LLVM_AnyPointer:$mem);
  let assemblyFormat = "$token `,` $mem attr-dict `:` functional-type(operands, results)";
}

def LLVM_CoroSizeOp : LLVM_IntrOp<"coro.size", [0], [], [], 1> {
  let assemblyFormat = "attr-dict `:` type($res)";
}

def LLVM_CoroAlignOp : LLVM_IntrOp<"coro.align", [0], [], [], 1> {
  let assemblyFormat = "attr-dict `:` type($res)";
}

def LLVM_CoroSaveOp : LLVM_IntrOp<"coro.save", [], [], [], 1> {
  let arguments = (ins LLVM_AnyPointer:$handle);
  let assemblyFormat = "$handle attr-dict `:` functional-type(operands, results)";
}

def LLVM_CoroSuspendOp : LLVM_IntrOp<"coro.suspend", [], [], [], 1> {
  let arguments = (ins LLVM_TokenType:$save,
                       I1:$final);
  let assemblyFormat = "$save `,` $final attr-dict `:` type($res)";
}

def LLVM_CoroEndOp : LLVM_IntrOp<"coro.end", [], [], [], 1> {
  let arguments = (ins LLVM_AnyPointer:$handle,
                       I1:$unwind,
                       LLVM_TokenType:$retvals);
  let assemblyFormat = "$handle `,` $unwind `,` $retvals attr-dict `:` functional-type(operands, results)";
}

def LLVM_CoroFreeOp : LLVM_IntrOp<"coro.free", [], [], [], 1> {
  let arguments = (ins LLVM_TokenType:$id,
                       LLVM_AnyPointer:$handle);
  let assemblyFormat = "$id `,` $handle attr-dict `:` functional-type(operands, results)";
}

def LLVM_CoroResumeOp : LLVM_IntrOp<"coro.resume", [], [], [], 0> {
  let arguments = (ins LLVM_AnyPointer:$handle);
  let assemblyFormat = "$handle attr-dict `:` qualified(type($handle))";
}

def LLVM_CoroPromiseOp : LLVM_IntrOp<"coro.promise", [], [], [], 1> {
  let arguments = (ins LLVM_AnyPointer:$handle,
                       I32:$align,
                       I1:$from);
  let results = (outs LLVM_AnyPointer:$res);
  let assemblyFormat = "$handle `,` $align `,` $from attr-dict `:` functional-type(operands, results)";
}

//
// Debug function intrinsics.
//

class LLVM_DbgIntrOp<string name, string argName, list<Trait> traits = []>
    : LLVM_IntrOp<name, [], [], traits, 0> {
  let llvmBuilder = [{
    // Debug intrinsics without debug locations are invalid.
    if(!builder.getCurrentDebugLocation())
      return success();
    llvm::Module *module = builder.GetInsertBlock()->getModule();
    llvm::LLVMContext &ctx = module->getContext();
    llvm::Function *fn =
      llvm::Intrinsic::getOrInsertDeclaration(module, llvm::Intrinsic::}]
       # !subst(".", "_", name) # [{);
    builder.CreateCall(fn, {
        llvm::MetadataAsValue::get(ctx,
            llvm::ValueAsMetadata::get(moduleTranslation.lookupValue(opInst.getOperand(0)))),
        llvm::MetadataAsValue::get(ctx, moduleTranslation.translateDebugInfo($varInfo)),
        llvm::MetadataAsValue::get(ctx, moduleTranslation.translateExpression($locationExpr)),
      });
  }];
  let mlirBuilder = [{
    // Add debug intrindic to the list of intrinsics that need to be converted once the
    // full function was converted.
    moduleImport.addDebugIntrinsic(inst);
    return success();
  }];
  let assemblyFormat = [{
    qualified($varInfo) (qualified($locationExpr)^)? `=` $}] # argName #
      [{ `:` qualified(type($}] # argName # [{)) attr-dict
  }];
}

def LLVM_DbgDeclareOp : LLVM_DbgIntrOp<"dbg.declare", "addr", [
    DeclareOpInterfaceMethods<PromotableOpInterface, [
      "requiresReplacedValues", "visitReplacedValues"
    ]>]> {
  let summary = "Describes how the address relates to a source language variable.";
  let arguments = (ins
    LLVM_AnyPointer:$addr,
    LLVM_DILocalVariableAttr:$varInfo,
    DefaultValuedAttr<LLVM_DIExpressionAttr, "{}">:$locationExpr
  );
}

def LLVM_DbgValueOp : LLVM_DbgIntrOp<"dbg.value", "value",
    [DeclareOpInterfaceMethods<PromotableOpInterface>]> {
  let summary = "Describes how the value relates to a source language variable.";
  let arguments = (ins
    LLVM_Type:$value,
    LLVM_DILocalVariableAttr:$varInfo,
    DefaultValuedAttr<LLVM_DIExpressionAttr, "{}">:$locationExpr
  );
}

def LLVM_DbgLabelOp : LLVM_IntrOp<"dbg.label", [], [], [], 0> {
  let summary = "Relates the program to a debug information label.";
  let arguments = (ins LLVM_DILabelAttr:$label);
  let llvmBuilder = [{
    // Debug intrinsics without debug locations are invalid.
    if(!builder.getCurrentDebugLocation())
      return success();
    llvm::Module *module = builder.GetInsertBlock()->getModule();
    llvm::LLVMContext &ctx = module->getContext();
    llvm::Function *fn =
      llvm::Intrinsic::getOrInsertDeclaration(module, llvm::Intrinsic::dbg_label);
    builder.CreateCall(fn, {
        llvm::MetadataAsValue::get(ctx, moduleTranslation.translateDebugInfo($label))
      });
  }];
  let mlirBuilder = [{
    DILabelAttr labelAttr = $_label_attr($label);
    // Drop the intrinsic if the label translation fails due to cylic metadata.
    if (!labelAttr)
      return success();
    $_op = $_builder.create<$_qualCppClassName>($_location, labelAttr);
  }];
  let assemblyFormat = "$label attr-dict";
}

//
// Variadic function intrinsics.
//

def LLVM_VaStartOp : LLVM_ZeroResultIntrOp<"vastart", [0]>,
                     Arguments<(ins LLVM_AnyPointer:$arg_list)> {
  let assemblyFormat = "$arg_list attr-dict `:` qualified(type($arg_list))";
  let summary = "Initializes `arg_list` for subsequent variadic argument extractions.";
}

def LLVM_VaCopyOp : LLVM_ZeroResultIntrOp<"vacopy", [0]>,
                    Arguments<(ins LLVM_AnyPointer:$dest_list, LLVM_AnyPointer:$src_list)> {
  let assemblyFormat = "$src_list `to` $dest_list attr-dict `:` type(operands)";
  let summary = "Copies the current argument position from `src_list` to `dest_list`.";
}

def LLVM_VaEndOp : LLVM_ZeroResultIntrOp<"vaend", [0]>,
                   Arguments<(ins LLVM_AnyPointer:$arg_list)> {
  let assemblyFormat = "$arg_list attr-dict `:` qualified(type($arg_list))";
  let summary = "Destroys `arg_list`, which has been initialized by `intr.vastart` or `intr.vacopy`.";
}

//
// Exception handling intrinsics.
//

def LLVM_EhTypeidForOp : LLVM_OneResultIntrOp<"eh.typeid.for", [], [0]> {
    let arguments = (ins LLVM_AnyPointer:$type_info);
    let assemblyFormat = "$type_info attr-dict `:` functional-type(operands, results)";
}

//
// Stack save/restore intrinsics.
//

def LLVM_StackSaveOp : LLVM_OneResultIntrOp<"stacksave", [0]> {
  let assemblyFormat = "attr-dict `:` qualified(type($res))";
}

def LLVM_StackRestoreOp : LLVM_ZeroResultIntrOp<"stackrestore", [0]> {
  let arguments = (ins LLVM_AnyPointer:$ptr);
  let assemblyFormat = "$ptr attr-dict `:` qualified(type($ptr))";
}

//
// Vector Reductions.
//

// LLVM vector reduction over a single vector.
class LLVM_VecReductionBase<string mnem, Type element, bit requiresFastmath=0>
    : LLVM_OneResultIntrOp<"vector.reduce." # mnem, [], [0],
                           [Pure, SameOperandsAndResultElementType],
                           requiresFastmath> {
      dag commonArgs = (ins LLVM_VectorOf<element>:$in);
}

class LLVM_VecReductionF<string mnem>
    : LLVM_VecReductionBase<mnem, AnyFloat, /*requiresFastmath=*/1> {
  dag fmfArg = (
    ins DefaultValuedAttr<LLVM_FastmathFlagsAttr, "{}">:$fastmathFlags);
  let arguments = !con(commonArgs, fmfArg);

  let assemblyFormat = "`(` operands `)` attr-dict `:` "
      "functional-type(operands, results)";
}

class LLVM_VecReductionI<string mnem>
    : LLVM_VecReductionBase<mnem, AnySignlessInteger> {
      let arguments = commonArgs;
}

// LLVM vector reduction over a single vector, with an initial value,
// and with permission to reassociate the reduction operations.
class LLVM_VecReductionAccBase<string mnem, Type element>
    : LLVM_OneResultIntrOp</*mnem=*/"vector.reduce." # mnem,
                           /*overloadedResults=*/[],
                           /*overloadedOperands=*/[1],
                           /*traits=*/[Pure, SameOperandsAndResultElementType],
                           /*equiresFastmath=*/1>,
      Arguments<(ins element:$start_value,
                     LLVM_VectorOf<element>:$input,
                     DefaultValuedAttr<LLVM_FastmathFlagsAttr, "{}">:$fastmathFlags)>;

class LLVM_VecReductionAccF<string mnem>
    : LLVM_VecReductionAccBase<mnem, AnyFloat>;

def LLVM_vector_reduce_add : LLVM_VecReductionI<"add">;
def LLVM_vector_reduce_and : LLVM_VecReductionI<"and">;
def LLVM_vector_reduce_mul : LLVM_VecReductionI<"mul">;
def LLVM_vector_reduce_or : LLVM_VecReductionI<"or">;
def LLVM_vector_reduce_smax : LLVM_VecReductionI<"smax">;
def LLVM_vector_reduce_smin : LLVM_VecReductionI<"smin">;
def LLVM_vector_reduce_umax : LLVM_VecReductionI<"umax">;
def LLVM_vector_reduce_umin : LLVM_VecReductionI<"umin">;
def LLVM_vector_reduce_xor : LLVM_VecReductionI<"xor">;

def LLVM_vector_reduce_fmax : LLVM_VecReductionF<"fmax">;
def LLVM_vector_reduce_fmin : LLVM_VecReductionF<"fmin">;
def LLVM_vector_reduce_fmaximum : LLVM_VecReductionF<"fmaximum">;
def LLVM_vector_reduce_fminimum : LLVM_VecReductionF<"fminimum">;

def LLVM_vector_reduce_fadd : LLVM_VecReductionAccF<"fadd">;
def LLVM_vector_reduce_fmul : LLVM_VecReductionAccF<"fmul">;

//
// LLVM Matrix operations.
//

/// Create a column major, strided 2-D matrix load, as specified in the LLVM
/// MatrixBuilder.
/// data       - Start address of the matrix read
/// rows       - Number of rows in matrix (must be a constant)
/// isVolatile - True if the load operation is marked as volatile.
/// columns    - Number of columns in matrix (must be a constant)
/// stride     - Space between columns
def LLVM_MatrixColumnMajorLoadOp : LLVM_OneResultIntrOp<"matrix.column.major.load"> {
  let arguments = (ins LLVM_AnyPointer:$data, AnySignlessInteger:$stride, I1Attr:$isVolatile,
                   I32Attr:$rows, I32Attr:$columns);
  let results = (outs LLVM_AnyVector:$res);
  let builders = [LLVM_OneResultOpBuilder];
  let assemblyFormat = "$data `,` `<` `stride` `=` $stride `>` attr-dict"
    "`:` type($res) `from` qualified(type($data)) `stride` type($stride)";

  string llvmBuilder = [{
    llvm::MatrixBuilder mb(builder);
    const llvm::DataLayout &dl =
      builder.GetInsertBlock()->getModule()->getDataLayout();
    llvm::Type *ElemTy = moduleTranslation.convertType(
        getVectorElementType(op.getType()));
    llvm::Align align = dl.getABITypeAlign(ElemTy);
    $res = mb.CreateColumnMajorLoad(
      ElemTy, $data, align, $stride, $isVolatile, $rows,
      $columns);
  }];
  string mlirBuilder = [{
    $res = $_builder.create<LLVM::MatrixColumnMajorLoadOp>(
      $_location, $_resultType, $data, $stride,
      $_int_attr($isVolatile), $_int_attr($rows), $_int_attr($columns));
  }];
}

/// Create a column major, strided 2-D matrix store, as specified in the LLVM
/// MatrixBuilder.
/// matrix     - Matrix to store
/// ptr        - Pointer to write back to
/// isVolatile - True if the load operation is marked as volatile.
/// rows       - Number of rows in matrix (must be a constant)
/// columns    - Number of columns in matrix (must be a constant)
/// stride     - Space between columns
def LLVM_MatrixColumnMajorStoreOp : LLVM_ZeroResultIntrOp<"matrix.column.major.store"> {
  let arguments = (ins LLVM_AnyVector:$matrix, LLVM_AnyPointer:$data,
                   AnySignlessInteger:$stride, I1Attr:$isVolatile, I32Attr:$rows,
                   I32Attr:$columns);
  let builders = [LLVM_VoidResultTypeOpBuilder, LLVM_ZeroResultOpBuilder];
  let assemblyFormat = "$matrix `,` $data `,` `<` `stride` `=` $stride `>` "
    "attr-dict`:` type($matrix) `to` qualified(type($data)) `stride` type($stride)";

  string llvmBuilder = [{
    llvm::MatrixBuilder mb(builder);
    const llvm::DataLayout &dl =
      builder.GetInsertBlock()->getModule()->getDataLayout();
    Type elementType = getVectorElementType(op.getMatrix().getType());
    llvm::Align align = dl.getABITypeAlign(
      moduleTranslation.convertType(elementType));
    mb.CreateColumnMajorStore(
      $matrix, $data, align, $stride, $isVolatile,
      $rows, $columns);
  }];
  string mlirBuilder = [{
    $_op = $_builder.create<LLVM::MatrixColumnMajorStoreOp>(
      $_location, $matrix, $data, $stride,
      $_int_attr($isVolatile), $_int_attr($rows), $_int_attr($columns));
  }];
}

/// Create a llvm.matrix.multiply call, multiplying 2-D matrices LHS and RHS, as
/// specified in the LLVM MatrixBuilder.
def LLVM_MatrixMultiplyOp : LLVM_OneResultIntrOp<"matrix.multiply"> {
  let arguments = (ins LLVM_AnyVector:$lhs, LLVM_AnyVector:$rhs, I32Attr:$lhs_rows,
                   I32Attr:$lhs_columns, I32Attr:$rhs_columns);
  let results = (outs LLVM_AnyVector:$res);
  let builders = [LLVM_OneResultOpBuilder];
  let assemblyFormat = "$lhs `,` $rhs attr-dict "
    "`:` `(` type($lhs) `,` type($rhs) `)` `->` type($res)";

  string llvmBuilder = [{
    llvm::MatrixBuilder mb(builder);
    $res = mb.CreateMatrixMultiply(
      $lhs, $rhs, $lhs_rows, $lhs_columns,
      $rhs_columns);
  }];
  string mlirBuilder = [{
    $res = $_builder.create<LLVM::MatrixMultiplyOp>(
      $_location, $_resultType, $lhs, $rhs,
      $_int_attr($lhs_rows), $_int_attr($lhs_columns), $_int_attr($rhs_columns));
  }];
}

/// Create a llvm.matrix.transpose call, transposing a `rows` x `columns` 2-D
/// `matrix`, as specified in the LLVM MatrixBuilder.
def LLVM_MatrixTransposeOp : LLVM_OneResultIntrOp<"matrix.transpose"> {
  let arguments = (ins LLVM_AnyVector:$matrix, I32Attr:$rows, I32Attr:$columns);
  let results = (outs LLVM_AnyVector:$res);
  let builders = [LLVM_OneResultOpBuilder];
  let assemblyFormat = "$matrix attr-dict `:` type($matrix) `into` type($res)";

  string llvmBuilder = [{
    llvm::MatrixBuilder mb(builder);
    $res = mb.CreateMatrixTranspose(
      $matrix, $rows, $columns);
  }];
  string mlirBuilder = [{
    $res = $_builder.create<LLVM::MatrixTransposeOp>(
      $_location, $_resultType, $matrix,
      $_int_attr($rows), $_int_attr($columns));
  }];
}

//
// LLVM masked operations.
//

/// Create a llvm.get.active.lane.mask to set a mask up to a given position.
def LLVM_GetActiveLaneMaskOp
    : LLVM_OneResultIntrOp<"get.active.lane.mask", [0], [0], [Pure]> {
  let arguments = (ins AnySignlessInteger:$base, AnySignlessInteger:$n);
  let assemblyFormat = "$base `,` $n attr-dict `:` "
    "type($base) `,` type($n) `to` type($res)";
}

/// Create a call to Masked Load intrinsic.
def LLVM_MaskedLoadOp : LLVM_OneResultIntrOp<"masked.load"> {
  let arguments = (ins LLVM_AnyPointer:$data, LLVM_VectorOf<I1>:$mask,
                   Variadic<LLVM_AnyVector>:$pass_thru, I32Attr:$alignment,
                   UnitAttr:$nontemporal);
  let results = (outs LLVM_AnyVector:$res);
  let assemblyFormat =
    "operands attr-dict `:` functional-type(operands, results)";

  string llvmBuilder = [{
    auto *inst = $pass_thru.empty() ? builder.CreateMaskedLoad(
        $_resultType, $data, llvm::Align($alignment), $mask) :
      builder.CreateMaskedLoad(
        $_resultType, $data, llvm::Align($alignment), $mask, $pass_thru[0]);
    $res = inst;
  }] #setNonTemporalMetadataCode;
  string mlirBuilder = [{
    auto *intrinInst = dyn_cast<llvm::IntrinsicInst>(inst);
    bool nontemporal = intrinInst->hasMetadata(llvm::LLVMContext::MD_nontemporal);
    $res = $_builder.create<LLVM::MaskedLoadOp>($_location,
      $_resultType, $data, $mask, $pass_thru, $_int_attr($alignment),
        nontemporal ? $_builder.getUnitAttr() : nullptr);
  }];
  list<int> llvmArgIndices = [0, 2, 3, 1, -1];
}

/// Create a call to Masked Store intrinsic.
def LLVM_MaskedStoreOp : LLVM_ZeroResultIntrOp<"masked.store"> {
  let arguments = (ins LLVM_AnyVector:$value, LLVM_AnyPointer:$data,
                   LLVM_VectorOf<I1>:$mask, I32Attr:$alignment);
  let builders = [LLVM_VoidResultTypeOpBuilder, LLVM_ZeroResultOpBuilder];
  let assemblyFormat = "$value `,` $data `,` $mask attr-dict `:` "
    "type($value) `,` type($mask) `into` qualified(type($data))";

  string llvmBuilder = [{
    builder.CreateMaskedStore(
      $value, $data, llvm::Align($alignment), $mask);
  }];
  string mlirBuilder = [{
    $_op = $_builder.create<LLVM::MaskedStoreOp>($_location,
      $value, $data, $mask, $_int_attr($alignment));
  }];
  list<int> llvmArgIndices = [0, 1, 3, 2];
}

/// Create a call to Masked Gather intrinsic.
def LLVM_masked_gather : LLVM_OneResultIntrOp<"masked.gather"> {
  let arguments = (ins LLVM_VectorOf<LLVM_AnyPointer>:$ptrs,
                   LLVM_VectorOf<I1>:$mask, Variadic<LLVM_AnyVector>:$pass_thru,
                   I32Attr:$alignment);
  let results = (outs LLVM_AnyVector:$res);
  let builders = [LLVM_OneResultOpBuilder];
  let assemblyFormat =
    "operands attr-dict `:` functional-type(operands, results)";

  string llvmBuilder = [{
    $res = $pass_thru.empty() ? builder.CreateMaskedGather(
        $_resultType, $ptrs, llvm::Align($alignment), $mask) :
      builder.CreateMaskedGather(
        $_resultType, $ptrs, llvm::Align($alignment), $mask, $pass_thru[0]);
  }];
  string mlirBuilder = [{
    $res = $_builder.create<LLVM::masked_gather>($_location,
      $_resultType, $ptrs, $mask, $pass_thru, $_int_attr($alignment));
  }];
  list<int> llvmArgIndices = [0, 2, 3, 1];

  let hasVerifier = 1;
}

/// Create a call to Masked Scatter intrinsic.
def LLVM_masked_scatter : LLVM_ZeroResultIntrOp<"masked.scatter"> {
  let arguments = (ins LLVM_AnyVector:$value, LLVM_VectorOf<LLVM_AnyPointer>:$ptrs,
                   LLVM_VectorOf<I1>:$mask, I32Attr:$alignment);
  let builders = [LLVM_VoidResultTypeOpBuilder, LLVM_ZeroResultOpBuilder];
  let assemblyFormat = "$value `,` $ptrs `,` $mask attr-dict `:` "
    "type($value) `,` type($mask) `into` type($ptrs)";

  string llvmBuilder = [{
    builder.CreateMaskedScatter(
      $value, $ptrs, llvm::Align($alignment), $mask);
  }];
  string mlirBuilder = [{
    $_op = $_builder.create<LLVM::masked_scatter>($_location,
      $value, $ptrs, $mask, $_int_attr($alignment));
  }];
  list<int> llvmArgIndices = [0, 1, 3, 2];

  let hasVerifier = 1;
}

/// Create a call to Masked Expand Load intrinsic.
def LLVM_masked_expandload : LLVM_IntrOp<"masked.expandload", [0], [], [], 1> {
  let arguments = (ins LLVM_AnyPointer, LLVM_VectorOf<I1>, LLVM_AnyVector);
}

/// Create a call to Masked Compress Store intrinsic.
def LLVM_masked_compressstore
    : LLVM_IntrOp<"masked.compressstore", [], [0], [], 0> {
  let arguments = (ins LLVM_AnyVector, LLVM_AnyPointer, LLVM_VectorOf<I1>);
}

//
// Annotate intrinsics.
//

def LLVM_VarAnnotation
    : LLVM_ZeroResultIntrOp<"var.annotation", [0, 1],
        [AllTypesMatch<["annotation", "fileName", "attr"]>]> {
  let arguments = (ins LLVM_AnyPointer:$val,
                       LLVM_AnyPointer:$annotation,
                       LLVM_AnyPointer:$fileName,
                       I32:$line,
                       LLVM_AnyPointer:$attr);
}

def LLVM_PtrAnnotation
    : LLVM_OneResultIntrOp<"ptr.annotation", [0], [2],
        [AllTypesMatch<["res", "ptr"]>,
         AllTypesMatch<["annotation", "fileName", "attr"]>]> {
  let arguments = (ins LLVM_AnyPointer:$ptr,
                       LLVM_AnyPointer:$annotation,
                       LLVM_AnyPointer:$fileName,
                       I32:$line,
                       LLVM_AnyPointer:$attr);
  let results = (outs LLVM_AnyPointer:$res);
}

def LLVM_Annotation
    : LLVM_OneResultIntrOp<"annotation", [0], [2],
        [AllTypesMatch<["res", "integer"]>,
         AllTypesMatch<["annotation", "fileName"]>]> {
  let arguments = (ins AnySignlessInteger:$integer,
                       LLVM_AnyPointer:$annotation,
                       LLVM_AnyPointer:$fileName,
                       I32:$line);
  let results = (outs AnySignlessInteger:$res);
}

//
// Trap intrinsics.
//

def LLVM_Trap : LLVM_ZeroResultIntrOp<"trap">;

def LLVM_DebugTrap : LLVM_ZeroResultIntrOp<"debugtrap">;

def LLVM_UBSanTrap : LLVM_ZeroResultIntrOp<"ubsantrap",
  /*overloadedOperands=*/[], /*traits=*/[],
  /*requiresAccessGroup=*/0, /*requiresAliasAnalysis=*/0,
  /*requiresOpBundles=*/0, /*immArgPositions=*/[0],
  /*immArgAttrNames=*/["failureKind"]> {
  let arguments = (ins I8Attr:$failureKind);
}

/// Create a call to vscale intrinsic.
def LLVM_vscale : LLVM_IntrOp<"vscale", [0], [], [], 1>;

/// Create a call to stepvector intrinsic.
def LLVM_StepVectorOp
    : LLVM_IntrOp<"stepvector", [0], [], [Pure], 1> {
  let arguments = (ins);
  let results = (outs LLVM_VectorOf<AnySignlessInteger>:$res);
  let assemblyFormat = "attr-dict `:` type($res)";
}

/// Create a call to vector.insert intrinsic
def LLVM_vector_insert
    : LLVM_OneResultIntrOp<"vector.insert",
                  /*overloadedResults=*/[0], /*overloadedOperands=*/[1],
                  /*traits=*/[Pure, AllTypesMatch<["dstvec", "res"]>,
                  PredOpTrait<"vectors are not bigger than 2^17 bits.", And<[
                    CPred<"getSrcVectorBitWidth() <= 131072">,
                    CPred<"getDstVectorBitWidth() <= 131072">
                  ]>>,
                  PredOpTrait<"it is not inserting scalable into fixed-length vectors.",
                    CPred<"!isScalableVectorType($srcvec.getType()) || "
                          "isScalableVectorType($dstvec.getType())">>],
                  /*requiresFastmath=*/0,
                  /*immArgPositions=*/[2], /*immArgAttrNames=*/["pos"]> {
  let arguments = (ins LLVM_AnyVector:$dstvec, LLVM_AnyVector:$srcvec,
                       I64Attr:$pos);
  let results = (outs LLVM_AnyVector:$res);
  let assemblyFormat = "$srcvec `,` $dstvec `[` $pos `]` attr-dict `:` "
    "type($srcvec) `into` type($res)";
  let extraClassDeclaration = [{
    uint64_t getVectorBitWidth(Type vector) {
      return getVectorNumElements(vector).getKnownMinValue() *
             getVectorElementType(vector).getIntOrFloatBitWidth();
    }
    uint64_t getSrcVectorBitWidth() {
      return getVectorBitWidth(getSrcvec().getType());
    }
    uint64_t getDstVectorBitWidth() {
      return getVectorBitWidth(getDstvec().getType());
    }
  }];
}

/// Create a call to vector.extract intrinsic
def LLVM_vector_extract
    : LLVM_OneResultIntrOp<"vector.extract",
                 /*overloadedResults=*/[0], /*overloadedOperands=*/[0],
                 /*traits=*/[Pure,
                  PredOpTrait<"vectors are not bigger than 2^17 bits.", And<[
                    CPred<"getSrcVectorBitWidth() <= 131072">,
                    CPred<"getResVectorBitWidth() <= 131072">
                  ]>>,
                  PredOpTrait<"it is not extracting scalable from fixed-length vectors.",
                    CPred<"!isScalableVectorType($res.getType()) || "
                          "isScalableVectorType($srcvec.getType())">>],
                  /*requiresFastmath=*/0,
                  /*immArgPositions=*/[1], /*immArgAttrNames=*/["pos"]> {
  let arguments = (ins LLVM_AnyVector:$srcvec, I64Attr:$pos);
  let results = (outs LLVM_AnyVector:$res);
  let assemblyFormat = "$srcvec `[` $pos `]` attr-dict `:` "
    "type($res) `from` type($srcvec)";
  let extraClassDeclaration = [{
    uint64_t getVectorBitWidth(Type vector) {
      return getVectorNumElements(vector).getKnownMinValue() *
             getVectorElementType(vector).getIntOrFloatBitWidth();
    }
    uint64_t getSrcVectorBitWidth() {
      return getVectorBitWidth(getSrcvec().getType());
    }
    uint64_t getResVectorBitWidth() {
      return getVectorBitWidth(getRes().getType());
    }
  }];
}

def LLVM_vector_interleave2
    : LLVM_OneResultIntrOp<"vector.interleave2",
        /*overloadedResults=*/[0], /*overloadedOperands=*/[],
        /*traits=*/[
          Pure, AllTypesMatch<["vec1", "vec2"]>,
          PredOpTrait<
            "result has twice as many elements as 'vec1'",
            And<[CPred<"getVectorNumElements($res.getType()) == "
                       "getVectorNumElements($vec1.getType()) * 2">,
                 CPred<"getVectorElementType($vec1.getType()) == "
                       "getVectorElementType($res.getType())">]>>,
        ]>,
        Arguments<(ins LLVM_AnyVector:$vec1, LLVM_AnyVector:$vec2)>;

def LLVM_vector_deinterleave2
    : LLVM_OneResultIntrOp<"vector.deinterleave2",
        /*overloadedResults=*/[], /*overloadedOperands=*/[0],
        /*traits=*/[Pure]>,
        Arguments<(ins LLVM_AnyVector:$vec)>;

//
// LLVM Vector Predication operations.
//

class LLVM_VPBinaryBase<string mnem, Type element>
    : LLVM_OneResultIntrOp<"vp." # mnem, [0], [], [Pure]>,
      Arguments<(ins LLVM_VectorOf<element>:$lhs, LLVM_VectorOf<element>:$rhs,
                     LLVM_VectorOf<I1>:$mask, I32:$evl)>;

class LLVM_VPBinaryI<string mnem> : LLVM_VPBinaryBase<mnem, AnySignlessInteger>;

class LLVM_VPBinaryF<string mnem> : LLVM_VPBinaryBase<mnem, AnyFloat>;

class LLVM_VPUnaryBase<string mnem, Type element>
    : LLVM_OneResultIntrOp<"vp." # mnem, [0], [], [Pure]>,
      Arguments<(ins LLVM_VectorOf<element>:$op,
                     LLVM_VectorOf<I1>:$mask, I32:$evl)>;

class LLVM_VPUnaryF<string mnem> : LLVM_VPUnaryBase<mnem, AnyFloat>;

class LLVM_VPTernaryBase<string mnem, Type element>
    : LLVM_OneResultIntrOp<"vp." # mnem, [0], [], [Pure]>,
      Arguments<(ins LLVM_VectorOf<element>:$op1, LLVM_VectorOf<element>:$op2,
                     LLVM_VectorOf<element>:$op3, LLVM_VectorOf<I1>:$mask,
                     I32:$evl)>;

class LLVM_VPTernaryF<string mnem> : LLVM_VPTernaryBase<mnem, AnyFloat>;

class LLVM_VPReductionBase<string mnem, Type element>
    : LLVM_OneResultIntrOp<"vp.reduce." # mnem, [], [1], [Pure]>,
      Arguments<(ins element:$satrt_value, LLVM_VectorOf<element>:$val,
                     LLVM_VectorOf<I1>:$mask, I32:$evl)>;

class LLVM_VPReductionI<string mnem> : LLVM_VPReductionBase<mnem, AnySignlessInteger>;

class LLVM_VPReductionF<string mnem> : LLVM_VPReductionBase<mnem, AnyFloat>;

class LLVM_VPSelectBase<string mnem>
    : LLVM_OneResultIntrOp<"vp." # mnem, [], [1], [Pure]>,
      Arguments<(ins LLVM_VectorOf<I1>:$cond, LLVM_AnyVector:$true_val,
                     LLVM_AnyVector:$false_val, I32:$evl)>;

class LLVM_VPCastBase<string mnem, Type element>
    : LLVM_OneResultIntrOp<"vp." # mnem, [0], [0], [Pure]>,
      Arguments<(ins LLVM_VectorOf<element>:$src,
                     LLVM_VectorOf<I1>:$mask, I32:$evl)>;

class LLVM_VPCastI<string mnem>   : LLVM_VPCastBase<mnem, AnySignlessInteger>;

class LLVM_VPCastF<string mnem>   : LLVM_VPCastBase<mnem, AnyFloat>;

class LLVM_VPCastPtr<string mnem> : LLVM_VPCastBase<mnem, LLVM_AnyPointer>;

// Integer Binary
def LLVM_VPAddOp  : LLVM_VPBinaryI<"add">;
def LLVM_VPSubOp  : LLVM_VPBinaryI<"sub">;
def LLVM_VPMulOp  : LLVM_VPBinaryI<"mul">;
def LLVM_VPSDivOp : LLVM_VPBinaryI<"sdiv">;
def LLVM_VPUDivOp : LLVM_VPBinaryI<"udiv">;
def LLVM_VPSRemOp : LLVM_VPBinaryI<"srem">;
def LLVM_VPURemOp : LLVM_VPBinaryI<"urem">;
def LLVM_VPAShrOp : LLVM_VPBinaryI<"ashr">;
def LLVM_VPLShrOp : LLVM_VPBinaryI<"lshr">;
def LLVM_VPShlOp  : LLVM_VPBinaryI<"shl">;
def LLVM_VPOrOp   : LLVM_VPBinaryI<"or">;
def LLVM_VPAndOp  : LLVM_VPBinaryI<"and">;
def LLVM_VPXorOp  : LLVM_VPBinaryI<"xor">;
def LLVM_VPSMaxOp : LLVM_VPBinaryI<"smax">;
def LLVM_VPSMinOp : LLVM_VPBinaryI<"smin">;
def LLVM_VPUMaxOp : LLVM_VPBinaryI<"umax">;
def LLVM_VPUMinOp : LLVM_VPBinaryI<"umin">;

// Float Binary
def LLVM_VPFAddOp : LLVM_VPBinaryF<"fadd">;
def LLVM_VPFSubOp : LLVM_VPBinaryF<"fsub">;
def LLVM_VPFMulOp : LLVM_VPBinaryF<"fmul">;
def LLVM_VPFDivOp : LLVM_VPBinaryF<"fdiv">;
def LLVM_VPFRemOp : LLVM_VPBinaryF<"frem">;

// Float Unary
def LLVM_VPFNegOp : LLVM_VPUnaryF<"fneg">;

// Float Ternary
def LLVM_VPFMulAddOp  : LLVM_VPTernaryF<"fmuladd">;
def LLVM_VPFmaOp      : LLVM_VPTernaryF<"fma">;

// Integer Reduction
def LLVM_VPReduceAddOp  : LLVM_VPReductionI<"add">;
def LLVM_VPReduceMulOp  : LLVM_VPReductionI<"mul">;
def LLVM_VPReduceAndOp  : LLVM_VPReductionI<"and">;
def LLVM_VPReduceOrOp   : LLVM_VPReductionI<"or">;
def LLVM_VPReduceXorOp  : LLVM_VPReductionI<"xor">;
def LLVM_VPReduceSMaxOp : LLVM_VPReductionI<"smax">;
def LLVM_VPReduceSMinOp : LLVM_VPReductionI<"smin">;
def LLVM_VPReduceUMaxOp : LLVM_VPReductionI<"umax">;
def LLVM_VPReduceUMinOp : LLVM_VPReductionI<"umin">;

// Float Reduction
def LLVM_VPReduceFAddOp : LLVM_VPReductionF<"fadd">;
def LLVM_VPReduceFMulOp : LLVM_VPReductionF<"fmul">;
def LLVM_VPReduceFMaxOp : LLVM_VPReductionF<"fmax">;
def LLVM_VPReduceFMinOp : LLVM_VPReductionF<"fmin">;

def LLVM_VPSelectMinOp : LLVM_VPSelectBase<"select">;
def LLVM_VPMergeMinOp  : LLVM_VPSelectBase<"merge">;

// Load/store
def LLVM_VPLoadOp
    : LLVM_OneResultIntrOp<"vp.load", [0], [0], []>,
      Arguments<(ins LLVM_AnyPointer:$ptr,
                     LLVM_VectorOf<I1>:$mask, I32:$evl)>;

def LLVM_VPStoreOp
    : LLVM_ZeroResultIntrOp<"vp.store", [0, 1], []>,
      Arguments<(ins LLVM_AnyVector:$val,
                     LLVM_AnyPointer:$ptr,
                     LLVM_VectorOf<I1>:$mask, I32:$evl)>;

// Strided load/store
def LLVM_VPStridedLoadOp
    : LLVM_OneResultIntrOp<"experimental.vp.strided.load", [0], [0, 1], []>,
      Arguments<(ins LLVM_AnyPointer:$ptr, AnySignlessInteger:$stride,
                     LLVM_VectorOf<I1>:$mask, I32:$evl)>;

def LLVM_VPStridedStoreOp
    : LLVM_ZeroResultIntrOp<"experimental.vp.strided.store",[0, 1, 2], []>,
      Arguments<(ins LLVM_AnyVector:$val, LLVM_AnyPointer:$ptr,
                     AnySignlessInteger:$stride, LLVM_VectorOf<I1>:$mask, I32:$evl)>;

def LLVM_VPTruncOp : LLVM_VPCastI<"trunc">;
def LLVM_VPZExtOp  : LLVM_VPCastI<"zext">;
def LLVM_VPSExtOp  : LLVM_VPCastI<"sext">;

def LLVM_VPFPTruncOp : LLVM_VPCastF<"fptrunc">;
def LLVM_VPFPExtOp   : LLVM_VPCastF<"fpext">;

def LLVM_VPFPToUIOp : LLVM_VPCastF<"fptoui">;
def LLVM_VPFPToSIOp : LLVM_VPCastF<"fptosi">;

def LLVM_VPUIToFPOp : LLVM_VPCastI<"uitofp">;
def LLVM_VPSIToFPOp : LLVM_VPCastI<"sitofp">;

def LLVM_VPPtrToIntOp : LLVM_VPCastPtr<"ptrtoint">;
def LLVM_VPIntToPtrOp : LLVM_VPCastI<"inttoptr">;

#endif // LLVM_INTRINSIC_OP
